# æ•°æ®åº“æµ‹è¯•ç­–ç•¥å®Œå…¨æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›å…¨é¢çš„æ•°æ®åº“æµ‹è¯•ç­–ç•¥ï¼Œæ¶µç›–ä»å•å…ƒæµ‹è¯•åˆ°æ•°æ®è¿ç§»éªŒè¯çš„å®Œæ•´æµ‹è¯•ä½“ç³»ã€‚æ•°æ®åº“æµ‹è¯•æ˜¯ç¡®ä¿æ•°æ®å®Œæ•´æ€§ã€åº”ç”¨å¯é æ€§å’Œç³»ç»Ÿæ€§èƒ½çš„å…³é”®ç¯èŠ‚ï¼Œéœ€è¦é‡‡ç”¨å¤šå±‚æ¬¡ã€å¤šç»´åº¦çš„æµ‹è¯•æ–¹æ³•ã€‚

---

## ç›®å½•

1. [å•å…ƒæµ‹è¯•æ•°æ®åº“æ“ä½œ](#1-å•å…ƒæµ‹è¯•æ•°æ®åº“æ“ä½œ)
2. [é›†æˆæµ‹è¯•ä¸æ•°æ®åº“](#2-é›†æˆæµ‹è¯•ä¸æ•°æ®åº“)
3. [å±æ€§æµ‹è¯•åœ¨æ•°æ®åº“ä¸­çš„åº”ç”¨](#3-å±æ€§æµ‹è¯•åœ¨æ•°æ®åº“ä¸­çš„åº”ç”¨)
4. [æ•°æ®åº“æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯](#4-æ•°æ®åº“æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯)
5. [Schemaè¿ç§»æµ‹è¯•](#5-schemaè¿ç§»æµ‹è¯•)
6. [æ•°æ®è¿ç§»éªŒè¯](#6-æ•°æ®è¿ç§»éªŒè¯)
7. [æµ‹è¯•æœ€ä½³å®è·µæ€»ç»“](#7-æµ‹è¯•æœ€ä½³å®è·µæ€»ç»“)

---

## 1. å•å…ƒæµ‹è¯•æ•°æ®åº“æ“ä½œ

### 1.1 å•å…ƒæµ‹è¯•çš„æ ¸å¿ƒåŸåˆ™

å•å…ƒæµ‹è¯•æ•°æ®åº“æ“ä½œçš„æ ¸å¿ƒç›®æ ‡æ˜¯å°†æ•°æ®åº“äº¤äº’ä¸ä¸šåŠ¡é€»è¾‘è§£è€¦ï¼Œç¡®ä¿æ¯ä¸ªå‡½æ•°æˆ–æ–¹æ³•åœ¨éš”ç¦»ç¯å¢ƒä¸‹èƒ½å¤Ÿæ­£ç¡®æ‰§è¡Œã€‚å•å…ƒæµ‹è¯•åº”å½“å¿«é€Ÿã€å¯é ã€å¯é‡å¤æ‰§è¡Œï¼Œå¹¶ä¸”ä¸ä¾èµ–å¤–éƒ¨æ•°æ®åº“å®ä¾‹ã€‚

#### å•å…ƒæµ‹è¯•çš„å…³é”®ç‰¹æ€§

| ç‰¹æ€§ | æè¿° | ä¼˜å…ˆçº§ |
|------|------|--------|
| éš”ç¦»æ€§ | æ¯ä¸ªæµ‹è¯•ç‹¬ç«‹è¿è¡Œï¼Œä¸ç›¸äº’å½±å“ | é«˜ |
| å¿«é€Ÿæ€§ | æµ‹è¯•æ‰§è¡Œæ—¶é—´åº”åœ¨æ¯«ç§’çº§ | é«˜ |
| å¯é‡å¤æ€§ | ç›¸åŒè¾“å…¥äº§ç”Ÿç›¸åŒè¾“å‡º | é«˜ |
| å¯ç»´æŠ¤æ€§ | æµ‹è¯•ä»£ç æ¸…æ™°æ˜“æ‡‚ | ä¸­ |
| è¦†ç›–ç‡ | ç¡®ä¿å…³é”®è·¯å¾„è¢«æµ‹è¯•è¦†ç›– | é«˜ |

### 1.2 æµ‹è¯•æ›¿èº«çš„ä½¿ç”¨

åœ¨å•å…ƒæµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æµ‹è¯•æ›¿èº«ï¼ˆTest Doublesï¼‰æ¥æ¨¡æ‹Ÿæ•°æ®åº“è¡Œä¸ºã€‚å¸¸è§çš„æµ‹è¯•æ›¿èº«åŒ…æ‹¬ï¼š

- **Mockï¼ˆæ¨¡æ‹Ÿå¯¹è±¡ï¼‰**ï¼šé¢„å…ˆç¼–ç¨‹çš„å‡å¯¹è±¡ï¼ŒéªŒè¯ç‰¹å®šçš„è°ƒç”¨
- **Stubï¼ˆæ¡©å¯¹è±¡ï¼‰**ï¼šæä¾›é¢„å®šä¹‰çš„å“åº”
- **Fakeï¼ˆå‡å¯¹è±¡ï¼‰**ï¼šå®ç°ç®€åŒ–ç‰ˆæœ¬çš„åŠŸèƒ½
- **Spyï¼ˆé—´è°å¯¹è±¡ï¼‰**ï¼šè®°å½•è°ƒç”¨ä¿¡æ¯ä¾›éªŒè¯

#### Python Mock ç¤ºä¾‹

```python
import unittest
from unittest.mock import Mock, MagicMock, patch
from datetime import datetime

class TestUserRepository(unittest.TestCase):
    """ç”¨æˆ·ä»“å‚¨å±‚çš„å•å…ƒæµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰ç½®è®¾ç½®"""
        self.mock_connection = MagicMock()
        self.mock_cursor = MagicMock()
        self.mock_connection.cursor.return_value = self.mock_cursor
    
    def test_create_user_success(self):
        """æµ‹è¯•åˆ›å»ºç”¨æˆ·æˆåŠŸåœºæ™¯"""
        # Arrange - å‡†å¤‡æµ‹è¯•æ•°æ®
        user_data = {
            'username': 'testuser',
            'email': 'test@example.com',
            'created_at': datetime.now()
        }
        
        # é…ç½®æ¨¡æ‹Ÿå¯¹è±¡
        self.mock_cursor.lastrowid = 1
        self.mock_cursor.rowcount = 1
        
        # Act - æ‰§è¡Œè¢«æµ‹å‡½æ•°
        result = self._create_user(self.mock_connection, user_data)
        
        # Assert - éªŒè¯ç»“æœ
        self.assertEqual(result['id'], 1)
        self.assertTrue(result['success'])
        self.mock_cursor.execute.assert_called_once()
        self.mock_connection.commit.assert_called_once()
    
    def test_create_user_duplicate_email(self):
        """æµ‹è¯•åˆ›å»ºç”¨æˆ·æ—¶é‚®ç®±é‡å¤"""
        # é…ç½®æ¨¡æ‹Ÿå¯¹è±¡æŠ›å‡ºé‡å¤é”®å¼‚å¸¸
        from pymysql import IntegrityError
        self.mock_cursor.execute.side_effect = IntegrityError(
            1062, "Duplicate entry 'test@example.com' for key 'email'"
        )
        
        user_data = {
            'username': 'testuser',
            'email': 'test@example.com'
        }
        
        result = self._create_user(self.mock_connection, user_data)
        
        self.assertFalse(result['success'])
        self.assertEqual(result['error'], 'DUPLICATE_EMAIL')
    
    def test_get_user_by_id_not_found(self):
        """æµ‹è¯•æŸ¥è¯¢ä¸å­˜åœ¨çš„ç”¨æˆ·"""
        self.mock_cursor.fetchone.return_value = None
        
        result = self._get_user_by_id(self.mock_connection, 999)
        
        self.assertIsNone(result)
    
    def _create_user(self, connection, user_data):
        """å®é™…çš„ç”¨æˆ·åˆ›å»ºé€»è¾‘ï¼ˆè¢«æµ‹è¯•çš„ç›®æ ‡ï¼‰"""
        cursor = connection.cursor()
        try:
            cursor.execute(
                """INSERT INTO users (username, email, created_at) 
                   VALUES (%(username)s, %(email)s, %(created_at)s)""",
                user_data
            )
            connection.commit()
            return {'success': True, 'id': cursor.lastrowid}
        except Exception as e:
            connection.rollback()
            return {'success': False, 'error': str(e)}
    
    def _get_user_by_id(self, connection, user_id):
        """å®é™…çš„ç”¨æˆ·æŸ¥è¯¢é€»è¾‘ï¼ˆè¢«æµ‹è¯•çš„ç›®æ ‡ï¼‰"""
        cursor = connection.cursor()
        cursor.execute("SELECT * FROM users WHERE id = %s", (user_id,))
        return cursor.fetchone()


class TestOrderService(unittest.TestCase):
    """è®¢å•æœåŠ¡çš„å•å…ƒæµ‹è¯•"""
    
    @patch('models.order.OrderRepository')
    def test_calculate_order_total(self, mock_repo_class):
        """æµ‹è¯•è®¢å•æ€»é¢è®¡ç®—"""
        # æ¨¡æ‹Ÿä»“å‚¨è¿”å›è®¢å•é¡¹
        mock_repo = Mock()
        mock_repo_class.return_value = mock_repo
        mock_repo.get_order_items.return_value = [
            {'product_id': 1, 'quantity': 2, 'price': 100.00},
            {'product_id': 2, 'quantity': 1, 'price': 250.00},
        ]
        
        from services.order import OrderService
        service = OrderService(mock_repo)
        
        total = service.calculate_order_total(1)
        
        self.assertEqual(total, 450.00)
        mock_repo.get_order_items.assert_called_once_with(1)
```

#### Java JUnit æµ‹è¯•ç¤ºä¾‹

```java
import org.junit.jupiter.api.BeforeEach;
import org.junit.jupiter.api.Test;
import org.junit.jupiter.api.extension.ExtendWith;
import org.mockito.Mock;
import org.mockito.junit.jupiter.MockitoExtension;

import javax.sql.DataSource;
import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.util.Optional;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.ArgumentMatchers.*;
import static org.mockito.Mockito.*;

@ExtendWith(MockitoExtension.class)
public class UserServiceTest {
    
    @Mock
    private DataSource dataSource;
    
    @Mock
    private Connection connection;
    
    @Mock
    private PreparedStatement preparedStatement;
    
    @Mock
    private ResultSet resultSet;
    
    private UserService userService;
    
    @BeforeEach
    void setUp() throws SQLException {
        when(dataSource.getConnection()).thenReturn(connection);
        when(connection.prepareStatement(anyString())).thenReturn(preparedStatement);
        when(preparedStatement.executeQuery()).thenReturn(resultSet);
        
        userService = new UserService(dataSource);
    }
    
    @Test
    void testFindUserById_Success() throws SQLException {
        // Arrange
        when(resultSet.next()).thenReturn(true);
        when(resultSet.getInt("id")).thenReturn(1);
        when(resultSet.getString("username")).thenReturn("testuser");
        when(resultSet.getString("email")).thenReturn("test@example.com");
        
        // Act
        Optional<User> result = userService.findUserById(1);
        
        // Assert
        assertTrue(result.isPresent());
        assertEquals(1, result.get().getId());
        assertEquals("testuser", result.get().getUsername());
        
        verify(preparedStatement).setInt(1, 1);
        verify(preparedStatement).executeQuery();
    }
    
    @Test
    void testFindUserById_NotFound() throws SQLException {
        // Arrange
        when(resultSet.next()).thenReturn(false);
        
        // Act
        Optional<User> result = userService.findUserById(999);
        
        // Assert
        assertFalse(result.isPresent());
    }
    
    @Test
    void testCreateUser_Success() throws SQLException {
        // Arrange
        when(preparedStatement.executeUpdate()).thenReturn(1);
        when(preparedStatement.getGeneratedKeys()).thenReturn(resultSet);
        when(resultSet.next()).thenReturn(true);
        when(resultSet.getLong(1)).thenReturn(1L);
        
        User user = new User("newuser", "new@example.com");
        
        // Act
        Long userId = userService.createUser(user);
        
        // Assert
        assertEquals(1L, userId);
        verify(connection).commit();
    }
    
    @Test
    void testCreateUser_DatabaseError() throws SQLException {
        // Arrange
        when(preparedStatement.executeUpdate()).thenThrow(
            new SQLException("Database connection failed")
        );
        
        User user = new User("newuser", "new@example.com");
        
        // Act & Assert
        assertThrows(DataAccessException.class, () -> {
            userService.createUser(user);
        });
        
        verify(connection).rollback();
    }
}
```

### 1.3 äº‹åŠ¡ç®¡ç†æµ‹è¯•

äº‹åŠ¡ç®¡ç†æ˜¯æ•°æ®åº“æ“ä½œä¸­çš„å…³é”®éƒ¨åˆ†ï¼Œéœ€è¦ç¡®ä¿åŸå­æ€§ã€ä¸€è‡´æ€§ã€éš”ç¦»æ€§å’ŒæŒä¹…æ€§ï¼ˆACIDï¼‰ã€‚

```python
import unittest
from unittest.mock import MagicMock, call
from contextlib import contextmanager

class TestTransactionManagement(unittest.TestCase):
    """äº‹åŠ¡ç®¡ç†æµ‹è¯•"""
    
    def setUp(self):
        self.mock_connection = MagicMock()
    
    def test_successful_transaction(self):
        """æµ‹è¯•æˆåŠŸçš„äº‹åŠ¡æäº¤"""
        # åˆ›å»ºæ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨çš„æ¨¡æ‹Ÿè¿æ¥
        mock_transaction = MagicMock()
        self.mock_connection.begin.return_value = mock_transaction
        
        result = self._execute_transaction(
            self.mock_connection,
            [
                ("INSERT INTO orders (id, total) VALUES (1, 100)", None),
                ("INSERT INTO order_items (order_id, product_id) VALUES (1, 1)", None),
                ("UPDATE inventory SET quantity = quantity - 1 WHERE product_id = 1", None),
            ]
        )
        
        self.assertTrue(result['success'])
        self.assertEqual(result['operations'], 3)
        self.mock_connection.commit.assert_called_once()
        self.mock_connection.rollback.assert_not_called()
    
    def test_transaction_rollback_on_error(self):
        """æµ‹è¯•äº‹åŠ¡å¤±è´¥å›æ»š"""
        mock_transaction = MagicMock()
        self.mock_connection.begin.return_value = mock_transaction
        
        # ç¬¬äºŒæ¬¡æ“ä½œå¤±è´¥
        def execute_side_effect(sql, *args):
            if "order_items" in sql:
                raise Exception("Foreign key constraint violation")
            return MagicMock(rowcount=1)
        
        self.mock_connection.cursor.return_value.execute.side_effect = execute_side_effect
        
        result = self._execute_transaction(
            self.mock_connection,
            [
                ("INSERT INTO orders (id, total) VALUES (1, 100)", None),
                ("INSERT INTO order_items (order_id, product_id) VALUES (1, 999)", None),
            ]
        )
        
        self.assertFalse(result['success'])
        self.assertEqual(result['error'], "Foreign key constraint violation")
        self.mock_connection.rollback.assert_called_once()
        self.mock_connection.commit.assert_not_called()
    
    def _execute_transaction(self, connection, operations):
        """æ‰§è¡Œäº‹åŠ¡çš„è¾…åŠ©æ–¹æ³•"""
        cursor = connection.cursor()
        try:
            connection.begin()
            for sql, params in operations:
                cursor.execute(sql, params or ())
            connection.commit()
            return {'success': True, 'operations': len(operations)}
        except Exception as e:
            connection.rollback()
            return {'success': False, 'error': str(e)}
        finally:
            cursor.close()


class TestConcurrencyControl(unittest.TestCase):
    """å¹¶å‘æ§åˆ¶æµ‹è¯•"""
    
    def test_optimistic_locking(self):
        """æµ‹è¯•ä¹è§‚é”æœºåˆ¶"""
        # æ¨¡æ‹Ÿåœºæ™¯ï¼šä¸¤ä¸ªç”¨æˆ·åŒæ—¶è¯»å–åŒä¸€è®°å½•
        mock_connection = MagicMock()
        mock_cursor = MagicMock()
        mock_connection.cursor.return_value = mock_cursor
        
        # ç¬¬ä¸€æ¬¡è¯»å–ï¼Œversion = 1
        mock_cursor.fetchone.return_value = {
            'id': 1,
            'name': 'Product',
            'version': 1,
            'price': 100
        }
        
        # æ¨¡æ‹Ÿæ›´æ–°æ—¶çš„ç‰ˆæœ¬æ£€æŸ¥
        update_calls = []
        def capture_update(sql, params):
            update_calls.append((sql, params))
            # ç¬¬ä¸€æ¬¡æ›´æ–°æˆåŠŸï¼Œç¬¬äºŒæ¬¡å¤±è´¥ï¼ˆç‰ˆæœ¬ä¸åŒ¹é…ï¼‰
            if len(update_calls) > 1:
                raise Exception("Version conflict")
            return MagicMock(rowcount=1)
        
        mock_cursor.execute.side_effect = capture_update
        
        # æ¨¡æ‹Ÿä¸¤ä¸ªå¹¶å‘æ›´æ–°
        from services.product import ProductService
        service = ProductService(mock_connection)
        
        result1 = service.update_product(1, {'price': 120}, expected_version=1)
        self.assertTrue(result1['success'])
        
        result2 = service.update_product(1, {'price': 150}, expected_version=1)
        self.assertFalse(result2['success'])
        self.assertEqual(result2['error'], 'VERSION_CONFLICT')
```

### 1.4 å¼‚å¸¸å¤„ç†æµ‹è¯•

```python
class TestDatabaseExceptionHandling(unittest.TestCase):
    """æ•°æ®åº“å¼‚å¸¸å¤„ç†æµ‹è¯•"""
    
    def test_connection_timeout(self):
        """æµ‹è¯•è¿æ¥è¶…æ—¶å¤„ç†"""
        import socket
        
        mock_connection = MagicMock()
        mock_connection.cursor.side_effect = socket.timeout("Connection timed out")
        
        from services.user import UserService
        service = UserService(mock_connection)
        
        result = service.get_user_by_id(1)
        
        self.assertIsNone(result)
        # éªŒè¯æ˜¯å¦æ­£ç¡®è®°å½•äº†é”™è¯¯æ—¥å¿—
        # mock_logger.error.assert_called()
    
    def test_deadlock_retry_logic(self):
        """æµ‹è¯•æ­»é”é‡è¯•é€»è¾‘"""
        from pymysql import OperationalError
        
        mock_connection = MagicMock()
        mock_cursor = MagicMock()
        
        # æ¨¡æ‹Ÿå‰ä¸¤æ¬¡é‡åˆ°æ­»é”ï¼Œç¬¬ä¸‰æ¬¡æˆåŠŸ
        call_count = [0]
        def execute_with_deadlock(sql, *args):
            call_count[0] += 1
            if call_count[0] <= 2:
                raise OperationalError(1213, "Deadlock found")
            return MagicMock(rowcount=1, fetchone={'id': 1})
        
        mock_cursor.execute.side_effect = execute_with_deadlock
        mock_connection.cursor.return_value = mock_cursor
        
        from services.transaction import TransactionService
        service = TransactionService(mock_connection)
        
        result = service.execute_with_retry(
            "UPDATE users SET last_login = NOW() WHERE id = 1",
            max_retries=3
        )
        
        self.assertTrue(result['success'])
        self.assertEqual(call_count[0], 3)
    
    def test_sql_injection_prevention(self):
        """æµ‹è¯•SQLæ³¨å…¥é˜²æŠ¤"""
        mock_connection = MagicMock()
        mock_cursor = MagicMock()
        mock_connection.cursor.return_value = mock_cursor
        
        from services.user import UserService
        service = UserService(mock_connection)
        
        # å°è¯•SQLæ³¨å…¥
        malicious_input = "'; DROP TABLE users; --"
        
        result = service.search_users(malicious_input)
        
        # éªŒè¯å‚æ•°åŒ–æŸ¥è¯¢è¢«æ­£ç¡®ä½¿ç”¨
        call_args = mock_cursor.execute.call_args
        if call_args:
            # ç¡®ä¿SQLè¯­å¥ä¸­æ²¡æœ‰ç›´æ¥åŒ…å«ç”¨æˆ·è¾“å…¥
            sql = call_args[0][0] if call_args[0] else ""
            self.assertNotIn("DROP TABLE", sql)
            self.assertNotIn("DROP", sql.upper())
```

---

## 2. é›†æˆæµ‹è¯•ä¸æ•°æ®åº“

### 2.1 é›†æˆæµ‹è¯•æ¶æ„è®¾è®¡

é›†æˆæµ‹è¯•éªŒè¯å¤šä¸ªç»„ä»¶ä¹‹é—´çš„äº¤äº’ï¼Œåœ¨æ•°æ®åº“ä¸Šä¸‹æ–‡ä¸­ï¼Œè¿™åŒ…æ‹¬åº”ç”¨ç¨‹åºä¸æ•°æ®åº“ä¹‹é—´çš„äº¤äº’ã€å­˜å‚¨è¿‡ç¨‹ã€è§¦å‘å™¨ä»¥åŠè·¨è¡¨çš„æ•°æ®ä¸€è‡´æ€§ã€‚

#### æµ‹è¯•é‡‘å­—å¡”

```
           /\
          /E2E\        <- E2Eæµ‹è¯•ï¼šå°‘é‡ï¼Œå…³é”®ç”¨æˆ·åœºæ™¯
         /------\
        /Integration\  <- é›†æˆæµ‹è¯•ï¼šä¸­ç­‰ï¼Œè¦†ç›–ç»„ä»¶äº¤äº’
       /------------\
      /   Unit Tests \  <- å•å…ƒæµ‹è¯•ï¼šå¤§é‡ï¼Œå¿«é€Ÿåé¦ˆ
     /----------------\
```

### 2.2 æµ‹è¯•æ•°æ®åº“å®¹å™¨åŒ–

ä½¿ç”¨Dockerå®¹å™¨ä¸ºé›†æˆæµ‹è¯•æä¾›éš”ç¦»ã€å¯é‡å¤çš„æ•°æ®åº“ç¯å¢ƒã€‚

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  postgres-test:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: testdb
      POSTGRES_USER: testuser
      POSTGRES_PASSWORD: testpass
    ports:
      - "5432:5432"
    volumes:
      - postgres-test-data:/var/lib/postgresql/data
      - ./test-init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U testuser -d testdb"]
      interval: 5s
      timeout: 5s
      retries: 5

  mysql-test:
    image: mysql:8.0
    environment:
      MYSQL_DATABASE: testdb
      MYSQL_USER: testuser
      MYSQL_PASSWORD: testpass
      MYSQL_ROOT_PASSWORD: rootpass
    ports:
      - "3306:3306"
    volumes:
      - mysql-test-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-prootpass"]
      interval: 5s
      timeout: 5s
      retries: 5

volumes:
  postgres-test-data:
  mysql-test-data:
```

```python
import pytest
import docker
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker
import time
import os

class DatabaseTestContainer:
    """æµ‹è¯•æ•°æ®åº“å®¹å™¨ç®¡ç†ç±»"""
    
    def __init__(self, db_type='postgres'):
        self.db_type = db_type
        self.client = docker.from_env()
        self.container = None
        self.engine = None
    
    def start(self):
        """å¯åŠ¨æµ‹è¯•æ•°æ®åº“å®¹å™¨"""
        image_map = {
            'postgres': 'postgres:15-alpine',
            'mysql': 'mysql:8.0',
            'mongodb': 'mongo:6.0'
        }
        
        env_map = {
            'postgres': {
                'POSTGRES_DB': 'testdb',
                'POSTGRES_USER': 'testuser',
                'POSTGRES_PASSWORD': 'testpass'
            },
            'mysql': {
                'MYSQL_DATABASE': 'testdb',
                'MYSQL_USER': 'testuser',
                'MYSQL_PASSWORD': 'testpass',
                'MYSQL_ROOT_PASSWORD': 'rootpass'
            }
        }
        
        port_map = {
            'postgres': 5432,
            'mysql': 3306,
            'mongodb': 27017
        }
        
        self.container = self.client.containers.run(
            image_map[self.db_type],
            detach=True,
            environment=env_map[self.db_type],
            ports={f'{port_map[self.db_type]}/tcp': port_map[self.db_type]},
            remove=True,
            name=f'test-{self.db_type}-{os.getpid()}'
        )
        
        # ç­‰å¾…å®¹å™¨å°±ç»ª
        self._wait_for_ready(port_map[self.db_type])
        
        # åˆ›å»ºSQLAlchemyå¼•æ“
        self.engine = self._create_engine(port_map[self.db_type])
        
        return self
    
    def _wait_for_ready(self, port, timeout=30):
        """ç­‰å¾…æ•°æ®åº“å°±ç»ª"""
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                import socket
                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
                result = sock.connect_ex(('localhost', port))
                sock.close()
                if result == 0:
                    time.sleep(2)  # é¢å¤–ç­‰å¾…ç¡®ä¿å®Œå…¨å°±ç»ª
                    return
            except Exception:
                pass
            time.sleep(1)
        raise TimeoutError(f"Database failed to start within {timeout} seconds")
    
    def _create_engine(self, port):
        """åˆ›å»ºSQLAlchemyå¼•æ“"""
        if self.db_type == 'postgres':
            url = 'postgresql://testuser:testpass@localhost:5432/testdb'
        elif self.db_type == 'mysql':
            url = 'mysql+pymysql://testuser:testpass@localhost:3306/testdb'
        
        engine = create_engine(url, pool_pre_ping=True)
        
        # è¿è¡Œåˆå§‹åŒ–SQL
        self._init_database(engine)
        
        return engine
    
    def _init_database(self, engine):
        """åˆå§‹åŒ–æ•°æ®åº“schema"""
        with engine.connect() as conn:
            # åˆ›å»ºæµ‹è¯•è¡¨
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    username VARCHAR(50) UNIQUE NOT NULL,
                    email VARCHAR(100) UNIQUE NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            conn.execute(text("""
                CREATE TABLE IF NOT EXISTS orders (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id),
                    total DECIMAL(10,2) NOT NULL,
                    status VARCHAR(20) DEFAULT 'pending',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """))
            
            conn.commit()
    
    def get_engine(self):
        """è·å–æ•°æ®åº“å¼•æ“"""
        return self.engine
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.container:
            self.container.stop(timeout=5)


@pytest.fixture(scope='session')
def postgres_db():
    """PostgreSQLæµ‹è¯•æ•°æ®åº“fixture"""
    with DatabaseTestContainer('postgres') as db:
        yield db.get_engine()


@pytest.fixture(scope='function')
def db_session(postgres_db):
    """æ¯ä¸ªæµ‹è¯•å‡½æ•°çš„ç‹¬ç«‹æ•°æ®åº“ä¼šè¯"""
    Session = sessionmaker(bind=postgres_db)
    session = Session()
    
    # æ¯ä¸ªæµ‹è¯•å‰æ¸…ç†æ•°æ®
    session.execute(text("TRUNCATE TABLE orders, users RESTART IDENTITY CASCADE"))
    session.commit()
    
    yield session
    
    session.close()
```

### 2.3 é›†æˆæµ‹è¯•ç¤ºä¾‹

```python
import pytest
from sqlalchemy import text
from datetime import datetime

class TestUserOrderIntegration:
    """ç”¨æˆ·è®¢å•é›†æˆæµ‹è¯•"""
    
    def test_create_user_and_order(self, db_session):
        """æµ‹è¯•åˆ›å»ºç”¨æˆ·å¹¶ä¸‹å•çš„å®Œæ•´æµç¨‹"""
        # 1. åˆ›å»ºç”¨æˆ·
        result = db_session.execute(
            text("""
                INSERT INTO users (username, email)
                VALUES (:username, :email)
                RETURNING id, username, email
            """),
            {'username': 'testuser', 'email': 'test@example.com'}
        )
        user = result.fetchone()
        user_id = user[0]
        
        # 2. åˆ›å»ºè®¢å•
        result = db_session.execute(
            text("""
                INSERT INTO orders (user_id, total, status)
                VALUES (:user_id, :total, :status)
                RETURNING id, user_id, total, status
            """),
            {'user_id': user_id, 'total': 299.99, 'status': 'pending'}
        )
        order = result.fetchone()
        
        db_session.commit()
        
        # 3. éªŒè¯æ•°æ®å®Œæ•´æ€§
        user_result = db_session.execute(
            text("SELECT id, username, email FROM users WHERE id = :id"),
            {'id': user_id}
        )
        saved_user = user_result.fetchone()
        
        order_result = db_session.execute(
            text("SELECT id, user_id, total, status FROM orders WHERE user_id = :user_id"),
            {'user_id': user_id}
        )
        saved_order = order_result.fetchone()
        
        assert saved_user[1] == 'testuser'
        assert saved_user[2] == 'test@example.com'
        assert saved_order[2] == 299.99
        assert saved_order[3] == 'pending'
    
    def test_cascade_delete_user(self, db_session):
        """æµ‹è¯•åˆ é™¤ç”¨æˆ·æ—¶è®¢å•çº§è”åˆ é™¤"""
        # åˆ›å»ºç”¨æˆ·å’Œè®¢å•
        user_id = db_session.execute(
            text("""
                INSERT INTO users (username, email)
                VALUES ('testuser', 'test@example.com')
                RETURNING id
            """)
        ).fetchone()[0]
        
        db_session.execute(
            text("""
                INSERT INTO orders (user_id, total)
                VALUES (:user_id, 100.00), (:user_id, 200.00)
            """),
            {'user_id': user_id}
        )
        db_session.commit()
        
        # åˆ é™¤ç”¨æˆ·
        db_session.execute(
            text("DELETE FROM users WHERE id = :id"),
            {'id': user_id}
        )
        db_session.commit()
        
        # éªŒè¯è®¢å•ä¹Ÿè¢«åˆ é™¤
        order_count = db_session.execute(
            text("SELECT COUNT(*) FROM orders WHERE user_id = :user_id"),
            {'user_id': user_id}
        ).fetchone()[0]
        
        assert order_count == 0
    
    def test_foreign_key_constraint(self, db_session):
        """æµ‹è¯•å¤–é”®çº¦æŸ"""
        # å°è¯•åˆ›å»ºä¸å­˜åœ¨çš„ç”¨æˆ·çš„è®¢å•
        with pytest.raises(Exception) as exc_info:
            db_session.execute(
                text("""
                    INSERT INTO orders (user_id, total)
                    VALUES (99999, 100.00)
                """)
            )
            db_session.commit()
        
        assert 'foreign key constraint' in str(exc_info.value).lower()


class TestDatabaseConstraints:
    """æ•°æ®åº“çº¦æŸé›†æˆæµ‹è¯•"""
    
    def test_unique_constraint_violation(self, db_session):
        """æµ‹è¯•å”¯ä¸€çº¦æŸ"""
        # åˆ›å»ºç¬¬ä¸€ä¸ªç”¨æˆ·
        db_session.execute(
            text("""
                INSERT INTO users (username, email)
                VALUES ('testuser', 'test@example.com')
            """)
        )
        db_session.commit()
        
        # å°è¯•åˆ›å»ºé‡å¤ç”¨æˆ·
        with pytest.raises(Exception) as exc_info:
            db_session.execute(
                text("""
                    INSERT INTO users (username, email)
                    VALUES ('testuser', 'test2@example.com')
                """)
            )
            db_session.commit()
        
        assert 'unique constraint' in str(exc_info.value).lower()
    
    def test_check_constraint(self, db_session):
        """æµ‹è¯•æ£€æŸ¥çº¦æŸ"""
        # å‡è®¾ordersè¡¨æœ‰CHECK (total > 0)çº¦æŸ
        with pytest.raises(Exception) as exc_info:
            db_session.execute(
                text("""
                    INSERT INTO orders (user_id, total)
                    VALUES (1, -100.00)
                """)
            )
            db_session.commit()
        
        assert 'check constraint' in str(exc_info.value).lower()
```

### 2.4 é›†æˆæµ‹è¯•æœ€ä½³å®è·µ

```python
# æµ‹è¯•æ•°æ®å·¥å‚æ¨¡å¼
class UserFactory:
    """ç”¨æˆ·æµ‹è¯•æ•°æ®å·¥å‚"""
    
    def __init__(self, db_session):
        self.db_session = db_session
    
    def create(self, **kwargs):
        """åˆ›å»ºç”¨æˆ·å¹¶è¿”å›"""
        default_data = {
            'username': f'user_{uuid.uuid4().hex[:8]}',
            'email': f'{uuid.uuid4().hex[:8]}@example.com'
        }
        default_data.update(kwargs)
        
        result = self.db_session.execute(
            text("""
                INSERT INTO users (username, email)
                VALUES (:username, :email)
                RETURNING id, username, email, created_at
            """),
            default_data
        )
        self.db_session.commit()
        
        row = result.fetchone()
        return {
            'id': row[0],
            'username': row[1],
            'email': row[2],
            'created_at': row[3]
        }
    
    def create_batch(self, count, **kwargs):
        """æ‰¹é‡åˆ›å»ºç”¨æˆ·"""
        users = []
        for i in range(count):
            user = self.create(**kwargs)
            users.append(user)
        return users


class OrderFactory:
    """è®¢å•æµ‹è¯•æ•°æ®å·¥å‚"""
    
    def __init__(self, db_session, user_factory):
        self.db_session = db_session
        self.user_factory = user_factory
    
    def create(self, **kwargs):
        """åˆ›å»ºè®¢å•"""
        # ç¡®ä¿å…³è”ç”¨æˆ·å­˜åœ¨
        if 'user_id' not in kwargs:
            user = self.user_factory.create()
            kwargs['user_id'] = user['id']
        
        default_data = {
            'total': 100.00,
            'status': 'pending'
        }
        default_data.update(kwargs)
        
        result = self.db_session.execute(
            text("""
                INSERT INTO orders (user_id, total, status)
                VALUES (:user_id, :total, :status)
                RETURNING id, user_id, total, status, created_at
            """),
            default_data
        )
        self.db_session.commit()
        
        row = result.fetchone()
        return {
            'id': row[0],
            'user_id': row[1],
            'total': float(row[2]),
            'status': row[3],
            'created_at': row[4]
        }


# ä½¿ç”¨ç¤ºä¾‹
class TestOrderWorkflow:
    """è®¢å•å·¥ä½œæµé›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def factories(self, db_session):
        """åˆ›å»ºå·¥å‚å®ä¾‹"""
        user_factory = UserFactory(db_session)
        return {
            'user': user_factory,
            'order': OrderFactory(db_session, user_factory)
        }
    
    def test_complete_order_flow(self, factories):
        """æµ‹è¯•å®Œæ•´çš„è®¢å•æµç¨‹"""
        # 1. åˆ›å»ºç”¨æˆ·
        user = factories['user'].create(
            username='customer1',
            email='customer1@example.com'
        )
        
        # 2. åˆ›å»ºå¤šä¸ªè®¢å•
        order1 = factories['order'].create(user_id=user['id'], total=150.00)
        order2 = factories['order'].create(user_id=user['id'], total=250.00)
        
        # 3. éªŒè¯è®¢å•ç»Ÿè®¡
        total = self._calculate_user_total(db_session, user['id'])
        assert total == 400.00
        
        # 4. æ›´æ–°è®¢å•çŠ¶æ€
        self._update_order_status(db_session, order1['id'], 'completed')
        
        # 5. éªŒè¯çŠ¶æ€æ›´æ–°
        order = self._get_order(db_session, order1['id'])
        assert order['status'] == 'completed'
    
    def _calculate_user_total(self, db_session, user_id):
        result = db_session.execute(
            text("SELECT COALESCE(SUM(total), 0) FROM orders WHERE user_id = :user_id"),
            {'user_id': user_id}
        )
        return float(result.fetchone()[0])
    
    def _update_order_status(self, db_session, order_id, status):
        db_session.execute(
            text("UPDATE orders SET status = :status WHERE id = :id"),
            {'id': order_id, 'status': status}
        )
        db_session.commit()
    
    def _get_order(self, db_session, order_id):
        result = db_session.execute(
            text("SELECT id, user_id, total, status FROM orders WHERE id = :id"),
            {'id': order_id}
        )
        row = result.fetchone()
        return {
            'id': row[0],
            'user_id': row[1],
            'total': float(row[2]),
            'status': row[3]
        }
```

---

## 3. å±æ€§æµ‹è¯•åœ¨æ•°æ®åº“ä¸­çš„åº”ç”¨

### 3.1 å±æ€§æµ‹è¯•æ¦‚å¿µ

å±æ€§æµ‹è¯•ï¼ˆProperty-Based Testingï¼‰æ˜¯ä¸€ç§ä¸åŒäºä¼ ç»Ÿç¤ºä¾‹æµ‹è¯•çš„æµ‹è¯•æ–¹æ³•ã€‚ä¼ ç»Ÿæµ‹è¯•ä½¿ç”¨å…·ä½“ç¤ºä¾‹éªŒè¯ä»£ç è¡Œä¸ºï¼Œè€Œå±æ€§æµ‹è¯•é€šè¿‡ç”Ÿæˆå¤§é‡éšæœºè¾“å…¥æ¥éªŒè¯ä»£ç æ˜¯å¦æ»¡è¶³æŸäº›å±æ€§æˆ–ä¸å˜é‡ã€‚

#### å±æ€§æµ‹è¯•çš„ä¼˜åŠ¿

| æ–¹é¢ | ä¼ ç»Ÿæµ‹è¯• | å±æ€§æµ‹è¯• |
|------|----------|----------|
| æµ‹è¯•æ•°é‡ | æ‰‹åŠ¨ç¼–å†™æœ‰é™ç”¨ä¾‹ | è‡ªåŠ¨ç”Ÿæˆå¤§é‡ç”¨ä¾‹ |
| è¾¹ç•Œæƒ…å†µ | ä¾èµ–æµ‹è¯•äººå‘˜ç»éªŒ | è‡ªåŠ¨æ¢ç´¢è¾¹ç•Œ |
| ç»´æŠ¤æˆæœ¬ | é«˜ï¼ˆç”¨ä¾‹å¤šï¼‰ | ä½ï¼ˆå±æ€§å¤ç”¨ï¼‰ |
| å‘ç°é—®é¢˜ | è¦†ç›–å·²çŸ¥åœºæ™¯ | å‘ç°æœªçŸ¥åœºæ™¯ |

### 3.2 æ•°æ®åº“å±æ€§æµ‹è¯•æ¡†æ¶

```python
import hypothesis
from hypothesis import given, settings, assume, example
from hypothesis import strategies as st
from datetime import datetime, timedelta
import random

# é…ç½®Hypothesis
hypothesis.settings.register_profile(
    'database',
    max_examples=1000,
    deadline=500,
    database=None  # ç¦ç”¨æ•°æ®åº“ä»¥åŠ é€Ÿæµ‹è¯•
)


class TestDatabaseOperationsProperties:
    """æ•°æ®åº“æ“ä½œçš„å±æ€§æµ‹è¯•"""
    
    @given(
        username=st.text(min_size=1, max_size=50, alphabet=st.characters(whitelist_categories=['L', 'N'])),
        email_domain=st.domains()
    )
    @settings(max_examples=100)
    def test_user_creation_properties(self, username, email_domain):
        """æµ‹è¯•ç”¨æˆ·åˆ›å»ºçš„å±æ€§"""
        # å±æ€§1ï¼šç”¨æˆ·åä¸åº”ä¸ºç©º
        assert len(username) > 0
        
        # å±æ€§2ï¼šé‚®ç®±åº”è¯¥åŒ…å«@
        email = f"test@{email_domain}"
        assert "@" in email
        
        # å±æ€§3ï¼šé‚®ç®±åŸŸååº”è¯¥æœ‰æ•ˆ
        assert "." in email_domain
    
    @given(
        users=st.lists(
            st.builds(
                lambda: {
                    'username': f"user_{random.randint(1, 10000)}",
                    'email': f"user_{random.randint(1, 10000)}@example.com"
                },
            ),
            min_size=1,
            max_size=100,
            unique_by=lambda x: x['username']
        )
    )
    def test_batch_user_creation_properties(self, users):
        """æµ‹è¯•æ‰¹é‡ç”¨æˆ·åˆ›å»º"""
        # å±æ€§1ï¼šæ‰€æœ‰ç”¨æˆ·ååº”è¯¥å”¯ä¸€
        usernames = [u['username'] for u in users]
        assert len(usernames) == len(set(usernames))
        
        # å±æ€§2ï¼šæ‰€æœ‰é‚®ç®±åº”è¯¥å”¯ä¸€
        emails = [u['email'] for u in users]
        assert len(emails) == len(set(emails))
        
        # å±æ€§3ï¼šæ¯ä¸ªé‚®ç®±åº”è¯¥æœ‰æ•ˆ
        for email in emails:
            assert "@" in email
            assert "." in email.split("@")[1]


class TestTransactionProperties:
    """äº‹åŠ¡å±æ€§æµ‹è¯•"""
    
    @given(
        initial_balance=st.floats(min_value=0, max_value=100000, allow_nan=False, allow_infinity=False),
        transaction_amount=st.floats(min_value=0.01, max_value=10000, allow_nan=False, allow_infinity=False),
        transaction_count=st.integers(min_value=1, max_value=100)
    )
    @settings(max_examples=500)
    def test_balance_after_transactions(self, initial_balance, transaction_amount, transaction_count):
        """æµ‹è¯•å¤šæ¬¡äº¤æ˜“åçš„ä½™é¢å±æ€§"""
        # å‡è®¾æ¯ç¬”äº¤æ˜“æ‰£é™¤1%çš„æ‰‹ç»­è´¹
        fee_rate = 0.01
        total_amount = transaction_amount * transaction_count
        total_fee = total_amount * fee_rate
        final_balance = initial_balance - total_amount - total_fee
        
        # å±æ€§1ï¼šä½™é¢ä¸åº”ä¸ºè´Ÿ
        assert final_balance >= -10000 or initial_balance >= total_amount + total_fee
        
        # å±æ€§2ï¼šå¦‚æœåˆå§‹ä½™é¢å……è¶³ï¼Œæœ€ç»ˆä½™é¢åº”è¯¥æ­£ç¡®
        if initial_balance >= total_amount + total_fee:
            assert abs(final_balance - (initial_balance - total_amount - total_fee)) < 0.01
    
    @given(
        amounts=st.lists(
            st.floats(min_value=0.01, max_value=1000, allow_nan=False),
            min_size=1,
            max_size=50
        )
    )
    def test_order_total_calculation(self, amounts):
        """æµ‹è¯•è®¢å•æ€»é¢è®¡ç®—å±æ€§"""
        # å±æ€§1ï¼šæ€»é¢åº”ä¸ºæ­£å€¼
        total = sum(amounts)
        assert total > 0
        
        # å±æ€§2ï¼šæ€»é¢åº”ç­‰äºå„é¡¹ä¹‹å’Œ
        calculated_total = 0
        for amount in amounts:
            calculated_total += amount
        assert abs(total - calculated_total) < 0.0001
        
        # å±æ€§3ï¼šå„é¡¹é‡‘é¢ä¸åº”è¶…è¿‡æ€»é¢
        for amount in amounts:
            assert amount <= total


class TestDataIntegrityProperties:
    """æ•°æ®å®Œæ•´æ€§å±æ€§æµ‹è¯•"""
    
    @given(
        data=st.dictionaries(
            st.integers(min_value=1, max_value=10000),
            st.floats(min_value=0, max_value=1000000),
            min_size=1,
            max_size=1000
        )
    )
    def test_data_aggregation_properties(self, data):
        """æµ‹è¯•æ•°æ®èšåˆå±æ€§"""
        values = list(data.values())
        
        # å±æ€§1ï¼šæœ€å°å€¼ä¸åº”è¶…è¿‡æœ€å¤§å€¼
        min_val = min(values)
        max_val = max(values)
        assert min_val <= max_val
        
        # å±æ€§2ï¼šå¹³å‡å€¼åº”åœ¨æœ€å°å€¼å’Œæœ€å¤§å€¼ä¹‹é—´
        avg_val = sum(values) / len(values)
        assert min_val <= avg_val <= max_val
        
        # å±æ€§3ï¼šæ€»å’Œåº”è¯¥å¤§äºç­‰äºä»»ä½•å•ä¸ªå€¼ï¼ˆå¯¹äºæ­£æ•°ï¼‰
        if min_val >= 0:
            assert sum(values) >= max_val
    
    @given(
        ids=st.lists(st.integers(min_value=1, max_value=1000), min_size=1, max_size=100, unique=True),
        values=st.lists(st.integers(min_value=1, max_value=10000), min_size=1, max_size=100)
    )
    def test_unique_id_mapping(self, ids, values):
        """æµ‹è¯•å”¯ä¸€IDæ˜ å°„å±æ€§"""
        # ç¡®ä¿é•¿åº¦åŒ¹é…
        min_len = min(len(ids), len(values))
        ids = ids[:min_len]
        values = values[:min_len]
        
        # å±æ€§1ï¼šæ˜ å°„åº”è¯¥æ˜¯ä¸€å¯¹ä¸€çš„
        mapping = dict(zip(ids, values))
        assert len(mapping) == len(ids)
        
        # å±æ€§2ï¼šæ‰€æœ‰IDåº”è¯¥å­˜åœ¨
        for id_val in ids:
            assert id_val in mapping
        
        # å±æ€§3ï¼šæ‰€æœ‰å€¼åº”è¯¥å¯ä»¥è¢«è®¿é—®
        for id_val in ids:
            assert mapping[id_val] is not None
```

### 3.3 æ•°æ®åº“ç‰¹å®šå±æ€§

```python
class TestDatabaseSchemaProperties:
    """æ•°æ®åº“Schemaå±æ€§æµ‹è¯•"""
    
    @given(
        table_name=st.text(min_size=1, max_size=64, alphabet=st.characters(whitelist_categories=['L'], whitelist_characters='_')),
        column_name=st.text(min_size=1, max_size=64, alphabet=st.characters(whitelist_categories=['L'], whitelist_characters='_'))
    )
    def test_identifier_naming(self, table_name, column_name):
        """æµ‹è¯•æ ‡è¯†ç¬¦å‘½åè§„åˆ™"""
        # å±æ€§1ï¼šä¸åº”ä»¥æ•°å­—å¼€å¤´
        assert not table_name[0].isdigit()
        assert not column_name[0].isdigit()
        
        # å±æ€§2ï¼šä¸åº”åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆé™¤ä¸‹åˆ’çº¿å¤–ï¼‰
        allowed = set('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')
        for char in table_name:
            assert char in allowed
        for char in column_name:
            assert char in allowed
    
    @given(
        column_type=st.sampled_from(['VARCHAR', 'INTEGER', 'DECIMAL', 'TIMESTAMP', 'BOOLEAN']),
        value=st.one_of(
            st.text(),
            st.integers(),
            st.floats(allow_nan=False, allow_infinity=False),
            st.datetimes(),
            st.booleans()
        )
    )
    def test_type_value_compatibility(self, column_type, value):
        """æµ‹è¯•ç±»å‹å€¼å…¼å®¹æ€§"""
        # å±æ€§ï¼šæ¯ç§ç±»å‹åº”è¯¥èƒ½æ¥å—ç›¸åº”çš„å€¼
        compatibility = {
            'VARCHAR': str,
            'INTEGER': int,
            'DECIMAL': (int, float),
            'TIMESTAMP': datetime,
            'BOOLEAN': bool
        }
        
        expected_type = compatibility[column_type]
        assert isinstance(value, expected_type)


class TestQueryProperties:
    """æŸ¥è¯¢å±æ€§æµ‹è¯•"""
    
    @given(
        limit=st.integers(min_value=1, max_value=1000),
        offset=st.integers(min_value=0, max_value=10000)
    )
    def test_pagination_properties(self, limit, offset):
        """æµ‹è¯•åˆ†é¡µå±æ€§"""
        # å±æ€§1ï¼šlimitåº”è¯¥ä¸ºæ­£æ•°
        assert limit > 0
        
        # å±æ€§2ï¼šoffsetåº”è¯¥ä¸ºéè´Ÿæ•°
        assert offset >= 0
        
        # å±æ€§3ï¼šè¿”å›çš„è®°å½•æ•°ä¸åº”è¶…è¿‡limit
        total_records = 10000
        expected_count = min(limit, total_records - offset)
        assert expected_count >= 0
    
    @given(
        sort_column=st.sampled_from(['id', 'created_at', 'name', 'price']),
        sort_direction=st.sampled_from(['ASC', 'DESC'])
    )
    def test_sorting_properties(self, sort_column, sort_direction):
        """æµ‹è¯•æ’åºå±æ€§"""
        # å±æ€§ï¼šæ’åºæ–¹å‘åº”è¯¥æ˜¯ ASC æˆ– DESC
        assert sort_direction in ['ASC', 'DESC']


class TestBusinessLogicProperties:
    """ä¸šåŠ¡é€»è¾‘å±æ€§æµ‹è¯•"""
    
    @given(
        price=st.floats(min_value=0, max_value=100000, allow_nan=False, allow_infinity=False),
        quantity=st.integers(min_value=1, max_value=1000),
        discount_rate=st.floats(min_value=0, max_value=1, allow_nan=False)
    )
    def test_discount_calculation(self, price, quantity, discount_rate):
        """æµ‹è¯•æŠ˜æ‰£è®¡ç®—å±æ€§"""
        subtotal = price * quantity
        discount = subtotal * discount_rate
        total = subtotal - discount
        
        # å±æ€§1ï¼šæŠ˜æ‰£ä¸åº”è¶…è¿‡å°è®¡
        assert discount <= subtotal
        
        # å±æ€§2ï¼šæœ€ç»ˆé‡‘é¢ä¸åº”ä¸ºè´Ÿ
        assert total >= 0
        
        # å±æ€§3ï¼šå¦‚æœæŠ˜æ‰£ç‡ä¸º0ï¼Œæœ€ç»ˆé‡‘é¢åº”ç­‰äºå°è®¡
        if discount_rate == 0:
            assert total == subtotal
        
        # å±æ€§4ï¼šå¦‚æœæŠ˜æ‰£ç‡ä¸º1ï¼Œæœ€ç»ˆé‡‘é¢åº”ä¸º0
        if discount_rate == 1:
            assert total == 0
    
    @given(
        principal=st.floats(min_value=100, max_value=1000000),
        annual_rate=st.floats(min_value=0.001, max_value=0.3, allow_nan=False),
        years=st.integers(min_value=1, max_value=30)
    )
    def test_interest_calculation(self, principal, annual_rate, years):
        """æµ‹è¯•åˆ©æ¯è®¡ç®—å±æ€§"""
        # ç®€å•åˆ©æ¯
        simple_interest = principal * annual_rate * years
        simple_total = principal + simple_interest
        
        # å¤åˆ©ï¼ˆå¹´å¤åˆ©ï¼‰
        compound_total = principal * ((1 + annual_rate) ** years)
        compound_interest = compound_total - principal
        
        # å±æ€§1ï¼šå¤åˆ©åº”è¯¥å¤§äºç­‰äºç®€å•åˆ©æ¯ï¼ˆå¯¹äºæ­£åˆ©ç‡ï¼‰
        if annual_rate > 0 and years > 1:
            assert compound_interest >= simple_interest
        
        # å±æ€§2ï¼šæœ€ç»ˆé‡‘é¢åº”è¯¥å¤§äºæœ¬é‡‘
        assert compound_total > principal
```

---

## 4. æ•°æ®åº“æ¨¡ç³Šæµ‹è¯•æŠ€æœ¯

### 4.1 æ¨¡ç³Šæµ‹è¯•æ¦‚è¿°

æ•°æ®åº“æ¨¡ç³Šæµ‹è¯•ï¼ˆFuzz Testingï¼‰æ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æµ‹è¯•æŠ€æœ¯ï¼Œé€šè¿‡å‘ç³»ç»Ÿè¾“å…¥éšæœºã€åŠéšæœºæˆ–å¼‚å¸¸æ•°æ®æ¥å‘ç°æ¼æ´ã€å´©æºƒæˆ–å…¶ä»–é—®é¢˜ã€‚åœ¨æ•°æ®åº“ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡ç³Šæµ‹è¯•å¯ä»¥å¸®åŠ©å‘ç°ï¼š

- SQLæ³¨å…¥æ¼æ´
- æ•°æ®ç±»å‹å¤„ç†é”™è¯¯
- è¾¹ç•Œæ¡ä»¶é—®é¢˜
- å­—ç¬¦ç¼–ç é—®é¢˜
- å¹¶å‘æ§åˆ¶ç¼ºé™·

### 4.2 SQLæ¨¡ç³Šæµ‹è¯•

```python
import random
import string
import sqlite3
from typing import List, Tuple
import re

class SQLFuzzer:
    """SQLæ¨¡ç³Šæµ‹è¯•å™¨"""
    
    def __init__(self, connection):
        self.connection = connection
        self.cursor = connection.cursor()
    
    def generate_random_string(self, length: int = 10) -> str:
        """ç”Ÿæˆéšæœºå­—ç¬¦ä¸²"""
        return ''.join(random.choices(string.ascii_letters + string.digits, k=length))
    
    def generate_random_number(self, min_val: int = 0, max_val: int = 1000000) -> int:
        """ç”Ÿæˆéšæœºæ•°å­—"""
        return random.randint(min_val, max_val)
    
    def generate_malicious_input(self) -> List[str]:
        """ç”Ÿæˆæ¶æ„è¾“å…¥æµ‹è¯•SQLæ³¨å…¥"""
        return [
            "' OR '1'='1",
            "' OR '1'='1' --",
            "' OR '1'='1' /*",
            "'; DROP TABLE users; --",
            "'; DELETE FROM users; --",
            "1' AND '1'='1",
            "1' AND '1'='2",
            "1 OR 1=1",
            "1 OR 1=2",
            "' UNION SELECT * FROM users --",
            "' UNION SELECT NULL, NULL, NULL --",
            "admin' --",
            "admin' #",
            "' OR ''='",
            "' OR 'x'='x",
            "'; EXEC xp_cmdshell('dir'); --",
            "1'; WAITFOR DELAY '0:0:5' --",
        ]
    
    def generate_boundary_values(self) -> List:
        """ç”Ÿæˆè¾¹ç•Œå€¼"""
        return [
            0,
            1,
            -1,
            127,
            128,
            255,
            256,
            32767,
            32768,
            65535,
            65536,
            2147483647,
            2147483648,
            -2147483647,
            -2147483648,
            999999999999999999999999999,
            -999999999999999999999999999,
            0.0,
            -0.0,
            float('inf'),
            float('-inf'),
            float('nan'),
        ]
    
    def generate_unicode_values(self) -> List[str]:
        """ç”ŸæˆUnicodeå­—ç¬¦"""
        return [
            '\u0000',  # ç©ºå­—ç¬¦
            '\u0001',
            '\u007F',  # DEL
            '\u0080',
            '\u00FF',  # Latin-1 Supplement
            '\u0100',
            '\uFFFF',  # Non-private use High Surrogate
            '\U00010000',  # First Supplementary Character
            '\U0010FFFF',  # Maximum Unicode
            'ä½ å¥½ä¸–ç•Œ',
            'ã“ã‚“ã«ã¡ã¯ä¸–ç•Œ',
            'ğŸ‰ğŸš€ğŸ’»',
            '\x00\x01\x02',  # åŸå§‹å­—èŠ‚
        ]
    
    def test_sql_injection(self, table_name: str = 'users'):
        """æµ‹è¯•SQLæ³¨å…¥æ¼æ´"""
        vulnerabilities = []
        
        for malicious_input in self.generate_malicious_input():
            try:
                # å°è¯•åœ¨WHEREå­å¥ä¸­ä½¿ç”¨æ¶æ„è¾“å…¥
                query = f"SELECT * FROM {table_name} WHERE username = '{malicious_input}'"
                self.cursor.execute(query)
                results = self.cursor.fetchall()
                
                # å¦‚æœæŸ¥è¯¢æˆåŠŸä¸”è¿”å›äº†æ•°æ®ï¼Œå¯èƒ½å­˜åœ¨æ³¨å…¥æ¼æ´
                if results:
                    vulnerabilities.append({
                        'type': 'SQL_INJECTION',
                        'input': malicious_input,
                        'query': query,
                        'severity': 'HIGH'
                    })
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­æµ‹è¯•
                pass
        
        return vulnerabilities
    
    def test_boundary_values(self, column: str, table: str):
        """æµ‹è¯•è¾¹ç•Œå€¼å¤„ç†"""
        issues = []
        
        for value in self.generate_boundary_values():
            try:
                query = f"SELECT * FROM {table} WHERE {column} = ?"
                self.cursor.execute(query, (value,))
                self.cursor.fetchall()
            except Exception as e:
                issues.append({
                    'type': 'BOUNDARY_ERROR',
                    'value': str(value),
                    'error': str(e),
                    'severity': 'MEDIUM'
                })
        
        return issues
    
    def test_unicode_handling(self, column: str, table: str):
        """æµ‹è¯•Unicodeå­—ç¬¦å¤„ç†"""
        issues = []
        
        for unicode_value in self.generate_unicode_values():
            try:
                # å°è¯•æ’å…¥Unicodeæ•°æ®
                query = f"INSERT INTO {table} (name) VALUES (?)"
                self.cursor.execute(query, (unicode_value,))
                self.connection.commit()
                
                # å°è¯•è¯»å–
                self.cursor.execute(f"SELECT name FROM {table} WHERE name = ?", (unicode_value,))
                result = self.cursor.fetchone()
                
                # éªŒè¯æ•°æ®ä¸€è‡´æ€§
                if result and result[0] != unicode_value:
                    issues.append({
                        'type': 'UNICODE_CORRUPTION',
                        'input': repr(unicode_value),
                        'output': repr(result[0]),
                        'severity': 'HIGH'
                    })
            except Exception as e:
                issues.append({
                    'type': 'UNICODE_ERROR',
                    'input': repr(unicode_value),
                    'error': str(e),
                    'severity': 'MEDIUM'
                })
        
        return issues
    
    def test_null_handling(self, table: str):
        """æµ‹è¯•NULLå€¼å¤„ç†"""
        issues = []
        
        try:
            # æµ‹è¯•NULLæ’å…¥
            query = f"INSERT INTO {table} (name) VALUES (NULL)"
            self.cursor.execute(query)
            self.connection.commit()
            
            # æµ‹è¯•NULLæŸ¥è¯¢
            self.cursor.execute(f"SELECT * FROM {table} WHERE name IS NULL")
            results = self.cursor.fetchall()
            
            # æµ‹è¯•IS NULL vs = NULL
            self.cursor.execute(f"SELECT * FROM {table} WHERE name = NULL")
            results2 = self.cursor.fetchall()
            
            # åœ¨æ ‡å‡†SQLä¸­ï¼Œ= NULLæ°¸è¿œä¸ä¸ºTRUE
            if results and not results2:
                issues.append({
                    'type': 'NULL_HANDLING_CORRECT',
                    'description': 'IS NULL works correctly, = NULL works as expected'
                })
            else:
                issues.append({
                    'type': 'NULL_HANDLING_UNEXPECTED',
                    'severity': 'LOW'
                })
        except Exception as e:
            issues.append({
                'type': 'NULL_ERROR',
                'error': str(e),
                'severity': 'LOW'
            })
        
        return issues


class DatabaseFuzzerIntegration:
    """æ•°æ®åº“æ¨¡ç³Šæµ‹è¯•é›†æˆ"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.connection = None
        self.fuzzer = None
    
    def setup_test_database(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®åº“"""
        self.connection = sqlite3.connect(self.db_path)
        self.fuzzer = SQLFuzzer(self.connection)
        
        # åˆ›å»ºæµ‹è¯•è¡¨
        self.connection.execute("""
            CREATE TABLE IF NOT EXISTS users (
                id INTEGER PRIMARY KEY,
                username TEXT UNIQUE NOT NULL,
                email TEXT,
                age INTEGER,
                balance REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)
        self.connection.commit()
    
    def run_all_fuzz_tests(self) -> dict:
        """è¿è¡Œæ‰€æœ‰æ¨¡ç³Šæµ‹è¯•"""
        results = {
            'sql_injection': [],
            'boundary': [],
            'unicode': [],
            'null_handling': []
        }
        
        # SQLæ³¨å…¥æµ‹è¯•
        results['sql_injection'] = self.fuzzer.test_sql_injection('users')
        
        # è¾¹ç•Œå€¼æµ‹è¯•
        results['boundary'] = self.fuzzer.test_boundary_values('age', 'users')
        
        # Unicodeæµ‹è¯•
        results['unicode'] = self.fuzzer.test_unicode_handling('username', 'users')
        
        # NULLå¤„ç†æµ‹è¯•
        results['null_handling'] = self.fuzzer.test_null_handling('users')
        
        return results
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.connection:
            self.connection.close()
```

### 4.3 æ··æ²Œå·¥ç¨‹ä¸æ•°æ®åº“

```python
import random
import time
import threading
from contextlib import contextmanager
from typing import List, Callable

class DatabaseChaosEngine:
    """æ•°æ®åº“æ··æ²Œå·¥ç¨‹å¼•æ“"""
    
    def __init__(self, connection):
        self.connection = connection
        self.active_chaos = []
    
    @contextmanager
    def network_latency(self, delay_ms: int = 1000, probability: float = 0.1):
        """æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ"""
        chaos = {
            'type': 'NETWORK_LATENCY',
            'delay_ms': delay_ms,
            'probability': probability
        }
        self.active_chaos.append(chaos)
        
        original_execute = self.connection.cursor().execute
        
        def slow_execute(sql, *args, **kwargs):
            if random.random() < probability:
                time.sleep(delay_ms / 1000)
            return original_execute(sql, *args, **kwargs)
        
        try:
            yield chaos
        finally:
            self.active_chaos.remove(chaos)
    
    @contextmanager
    def connection_failure(self, probability: float = 0.1):
        """æ¨¡æ‹Ÿè¿æ¥å¤±è´¥"""
        chaos = {
            'type': 'CONNECTION_FAILURE',
            'probability': probability
        }
        self.active_chaos.append(chaos)
        
        try:
            if random.random() < probability:
                raise ConnectionError("Simulated connection failure")
            yield chaos
        finally:
            self.active_chaos.remove(chaos)
    
    @contextmanager
    def random_timeout(self, timeout_ms: int = 100, probability: float = 0.05):
        """æ¨¡æ‹Ÿéšæœºè¶…æ—¶"""
        chaos = {
            'type': 'RANDOM_TIMEOUT',
            'timeout_ms': timeout_ms,
            'probability': probability
        }
        self.active_chaos.append(chaos)
        
        try:
            yield chaos
        finally:
            self.active_chaos.remove(chaos)
    
    @contextmanager
    def data_corruption(self, probability: float = 0.01):
        """æ¨¡æ‹Ÿæ•°æ®æŸå"""
        chaos = {
            'type': 'DATA_CORRUPTION',
            'probability': probability
        }
        self.active_chaos.append(chaos)
        
        yield chaos
        
        self.active_chaos.remove(chaos)


class ChaosTestExample:
    """æ··æ²Œå·¥ç¨‹æµ‹è¯•ç¤ºä¾‹"""
    
    def test_transaction_under_chaos(self):
        """æµ‹è¯•æ··æ²Œç¯å¢ƒä¸‹çš„äº‹åŠ¡å¤„ç†"""
        import sqlite3
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
        conn = sqlite3.connect(':memory:')
        conn.execute("CREATE TABLE accounts (id PRIMARY KEY, balance REAL)")
        conn.execute("INSERT INTO accounts VALUES (1, 1000), (2, 1000)")
        conn.commit()
        
        chaos_engine = DatabaseChaosEngine(conn)
        
        # åœ¨ç½‘ç»œå»¶è¿Ÿä¸‹æµ‹è¯•è½¬è´¦
        with chaos_engine.network_latency(delay_ms=100, probability=0.3):
            try:
                cursor = conn.cursor()
                cursor.execute("BEGIN TRANSACTION")
                cursor.execute("UPDATE accounts SET balance = balance - 100 WHERE id = 1")
                cursor.execute("UPDATE accounts SET balance = balance + 100 WHERE id = 2")
                cursor.execute("COMMIT")
                
                # éªŒè¯ç»“æœ
                cursor.execute("SELECT balance FROM accounts WHERE id = 1")
                balance1 = cursor.fetchone()[0]
                
                cursor.execute("SELECT balance FROM accounts WHERE id = 2")
                balance2 = cursor.fetchone()[0]
                
                assert balance1 == 900
                assert balance2 == 1100
                
            except Exception as e:
                # äº‹åŠ¡åº”è¯¥å›æ»š
                conn.rollback()
                print(f"Transaction failed as expected: {e}")
        
        conn.close()
    
    def test_concurrent_operations_under_chaos(self):
        """æµ‹è¯•æ··æ²Œç¯å¢ƒä¸‹çš„å¹¶å‘æ“ä½œ"""
        import sqlite3
        
        conn = sqlite3.connect(':memory:')
        conn.execute("CREATE TABLE counters (id PRIMARY KEY, value INTEGER)")
        conn.execute("INSERT INTO counters VALUES (1, 0)")
        conn.commit()
        
        results = []
        errors = []
        
        def increment_counter(thread_id):
            try:
                for _ in range(10):
                    cursor = conn.cursor()
                    cursor.execute("BEGIN")
                    cursor.execute("SELECT value FROM counters WHERE id = 1")
                    current = cursor.fetchone()[0]
                    time.sleep(random.random() * 0.001)
                    cursor.execute("UPDATE counters SET value = ? WHERE id = 1", (current + 1,))
                    cursor.execute("COMMIT")
                results.append(thread_id)
            except Exception as e:
                errors.append((thread_id, str(e)))
        
        # å¯åŠ¨å¤šä¸ªå¹¶å‘çº¿ç¨‹
        threads = []
        for i in range(5):
            t = threading.Thread(target=increment_counter, args=(i,))
            threads.append(t)
            t.start()
        
        for t in threads:
            t.join()
        
        # éªŒè¯æœ€ç»ˆè®¡æ•°
        cursor = conn.cursor()
        cursor.execute("SELECT value FROM counters WHERE id = 1")
        final_value = cursor.fetchone()[0]
        
        print(f"Successful threads: {len(results)}")
        print(f"Errors: {len(errors)}")
        print(f"Final counter value: {final_value}")
        
        conn.close()
```

---

## 5. Schemaè¿ç§»æµ‹è¯•

### 5.1 Schemaè¿ç§»æµ‹è¯•ç­–ç•¥

Schemaè¿ç§»æ˜¯æ•°æ®åº“æ¼”è¿›ä¸­çš„å…³é”®æ“ä½œï¼Œéœ€è¦å…¨é¢æµ‹è¯•ä»¥ç¡®ä¿æ•°æ®å®Œæ•´æ€§å’Œåº”ç”¨å…¼å®¹æ€§ã€‚

```python
import pytest
from sqlalchemy import create_engine, inspect, text
from sqlalchemy.orm import sessionmaker
import json
import os
from datetime import datetime

class SchemaMigrationTest:
    """Schemaè¿ç§»æµ‹è¯•"""
    
    def __init__(self, source_db_url: str, target_db_url: str):
        self.source_engine = create_engine(source_db_url)
        self.target_engine = create_engine(target_db_url)
        self.source_inspector = inspect(self.source_engine)
        self.target_inspector = inspect(self.target_engine)
    
    def test_all_tables_migrated(self):
        """æµ‹è¯•æ‰€æœ‰è¡¨éƒ½å·²è¿ç§»"""
        source_tables = set(self.source_inspector.get_table_names())
        target_tables = set(self.target_inspector.get_table_names())
        
        missing_tables = source_tables - target_tables
        assert len(missing_tables) == 0, f"Missing tables: {missing_tables}"
    
    def test_all_columns_preserved(self):
        """æµ‹è¯•æ‰€æœ‰åˆ—éƒ½å·²ä¿ç•™"""
        source_tables = self.source_inspector.get_table_names()
        
        for table_name in source_tables:
            source_columns = {
                col['name']: col for col in self.source_inspector.get_columns(table_name)
            }
            target_columns = {
                col['name']: col for col in self.target_inspector.get_columns(table_name)
            }
            
            missing_columns = set(source_columns.keys()) - set(target_columns.keys())
            assert len(missing_columns) == 0, \
                f"Table {table_name} missing columns: {missing_columns}"
    
    def test_column_types_compatible(self):
        """æµ‹è¯•åˆ—ç±»å‹å…¼å®¹"""
        source_tables = self.source_inspector.get_table_names()
        
        type_compatibility_map = {
            'INTEGER': ['INTEGER', 'BIGINT', 'SMALLINT'],
            'BIGINT': ['BIGINT'],
            'VARCHAR': ['VARCHAR', 'TEXT', 'CHAR'],
            'TEXT': ['TEXT', 'VARCHAR'],
            'DATE': ['DATE', 'TIMESTAMP', 'DATETIME'],
            'TIMESTAMP': ['TIMESTAMP', 'DATETIME'],
            'DECIMAL': ['DECIMAL', 'NUMERIC', 'FLOAT', 'DOUBLE'],
        }
        
        for table_name in source_tables:
            source_columns = {
                col['name']: col for col in self.source_inspector.get_columns(table_name)
            }
            target_columns = {
                col['name']: col for col in self.target_inspector.get_columns(table_name)
            }
            
            for col_name, source_col in source_columns.items():
                target_col = target_columns.get(col_name)
                if target_col:
                    source_type = source_col['type'].upper()
                    target_type = target_col['type'].upper()
                    
                    # æ£€æŸ¥ç±»å‹å…¼å®¹æ€§
                    compatible = False
                    for base_type, compatible_types in type_compatibility_map.items():
                        if base_type in source_type and target_type in compatible_types:
                            compatible = True
                            break
                    
                    assert compatible, \
                        f"Column {table_name}.{col_name} type incompatibility: \
                        {source_type} -> {target_type}"
    
    def test_primary_keys_preserved(self):
        """æµ‹è¯•ä¸»é”®ä¿ç•™"""
        source_tables = self.source_inspector.get_table_names()
        
        for table_name in source_tables:
            source_pk = self.source_inspector.get_pk_constraint(table_name)
            target_pk = self.target_inspector.get_pk_constraint(table_name)
            
            assert set(source_pk['constrained_columns']) == set(target_pk['constrained_columns']), \
                f"Primary key mismatch for table {table_name}"
    
    def test_foreign_keys_preserved(self):
        """æµ‹è¯•å¤–é”®ä¿ç•™"""
        source_tables = self.source_inspector.get_table_names()
        
        for table_name in source_tables:
            source_fks = self.source_inspector.get_foreign_keys(table_name)
            target_fks = self.target_inspector.get_foreign_keys(table_name)
            
            # ç®€åŒ–æ¯”è¾ƒï¼ˆå®é™…åœºæ™¯å¯èƒ½éœ€è¦æ›´è¯¦ç»†çš„æ¯”è¾ƒï¼‰
            source_fk_set = set(tuple(fk['constrained_columns']) for fk in source_fks)
            target_fk_set = set(tuple(fk['constrained_columns']) for fk in target_fks)
            
            assert source_fk_set == target_fk_set, \
                f"Foreign key mismatch for table {table_name}"
    
    def test_indexes_preserved(self):
        """æµ‹è¯•ç´¢å¼•ä¿ç•™"""
        source_tables = self.source_inspector.get_table_names()
        
        for table_name in source_tables:
            source_indexes = self.source_inspector.get_indexes(table_name)
            target_indexes = self.target_inspector.get_indexes(table_name)
            
            # æ¯”è¾ƒç´¢å¼•åˆ—ï¼ˆå¿½ç•¥ç´¢å¼•åï¼‰
            source_index_cols = set(
                tuple(idx['column_names']) for idx in source_indexes if not idx['name'].startswith('sqlite_')
            )
            target_index_cols = set(
                tuple(idx['column_names']) for idx in target_indexes if not idx['name'].startswith('sqlite_')
            )
            
            assert source_index_cols == target_index_cols, \
                f"Index mismatch for table {table_name}"
    
    def test_unique_constraints_preserved(self):
        """æµ‹è¯•å”¯ä¸€çº¦æŸä¿ç•™"""
        source_tables = self.source_inspector.get_table_names()
        
        for table_name in source_tables:
            source_unique = self.source_inspector.get_unique_constraints(table_name)
            target_unique = self.target_inspector.get_unique_constraints(table_name)
            
            # æ¯”è¾ƒå”¯ä¸€çº¦æŸåˆ—
            source_unique_cols = set(tuple(uc['column_names']) for uc in source_unique)
            target_unique_cols = set(tuple(uc['column_names']) for uc in target_unique)
            
            assert source_unique_cols == target_unique_cols, \
                f"Unique constraint mismatch for table {table_name}"


class SchemaMigrationDataTest:
    """Schemaè¿ç§»æ•°æ®æµ‹è¯•"""
    
    def __init__(self, source_db_url: str, target_db_url: str):
        self.source_engine = create_engine(source_db_url)
        self.target_engine = create_engine(target_db_url)
    
    def test_row_count_match(self):
        """æµ‹è¯•è¡Œæ•°åŒ¹é…"""
        source_tables = inspect(self.source_engine).get_table_names()
        
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            for table_name in source_tables:
                source_count = source_conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name}")
                ).scalar()
                
                target_count = target_conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name}")
                ).scalar()
                
                assert source_count == target_count, \
                    f"Row count mismatch for {table_name}: {source_count} vs {target_count}"
    
    def test_data_integrity(self):
        """æµ‹è¯•æ•°æ®å®Œæ•´æ€§"""
        source_tables = inspect(self.source_engine).get_table_names()
        
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            for table_name in source_tables:
                # è·å–æºè¡¨æ•°æ®
                source_data = source_conn.execute(
                    text(f"SELECT * FROM {table_name}")
                ).fetchall()
                
                # é€è¡Œæ¯”è¾ƒ
                for source_row in source_data:
                    # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ˆä½¿ç”¨ä¸»é”®æˆ–æ‰€æœ‰åˆ—ï¼‰
                    pk_constraint = inspect(self.source_engine).get_pk_constraint(table_name)
                    pk_columns = pk_constraint['constrained_columns']
                    
                    if pk_columns:
                        # ä½¿ç”¨ä¸»é”®æŸ¥è¯¢ç›®æ ‡
                        where_clause = " AND ".join(
                            f"{col} = :{col}" for col in pk_columns
                        )
                        params = dict(zip(pk_columns, source_row[:len(pk_columns)]))
                    else:
                        # ä½¿ç”¨æ‰€æœ‰åˆ—
                        columns = inspect(self.source_engine).get_columns(table_name)
                        where_clause = " AND ".join(
                            f"{col['name']} = :{col['name']}" for col in columns
                        )
                        params = dict(zip([col['name'] for col in columns], source_row))
                    
                    target_row = target_conn.execute(
                        text(f"SELECT * FROM {table_name} WHERE {where_clause}"),
                        params
                    ).fetchone()
                    
                    assert target_row is not None, \
                        f"Missing row in target: {table_name}, {params}"
    
    def test_null_preservation(self):
        """æµ‹è¯•NULLå€¼ä¿ç•™"""
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            tables = inspect(self.source_engine).get_table_names()
            
            for table_name in tables:
                # æ£€æŸ¥NULLå€¼æ•°é‡
                source_nulls = source_conn.execute(
                    text(f"""
                        SELECT COUNT(*) FROM {table_name} 
                        WHERE {' OR '.join(f'{col} IS NULL' for col in 
                            [c['name'] for c in inspect(self.source_engine).get_columns(table_name)])}
                    """)
                ).scalar()
                
                target_nulls = target_conn.execute(
                    text(f"""
                        SELECT COUNT(*) FROM {table_name} 
                        WHERE {' OR '.join(f'{col} IS NULL' for col in 
                            [c['name'] for c in inspect(self.target_engine).get_columns(table_name)])}
                    """)
                ).scalar()
                
                assert source_nulls == target_nulls, \
                    f"NULL count mismatch in {table_name}: {source_nulls} vs {target_nulls}"
    
    def test_unique_values_preserved(self):
        """æµ‹è¯•å”¯ä¸€å€¼ä¿ç•™"""
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            tables = inspect(self.source_engine).get_table_names()
            
            for table_name in tables:
                # æ£€æŸ¥æ¯ä¸ªå”¯ä¸€çº¦æŸåˆ—çš„å”¯ä¸€å€¼æ•°é‡
                unique_constraints = inspect(self.source_engine).get_unique_constraints(table_name)
                
                for uc in unique_constraints:
                    columns = uc['column_names']
                    cols_str = ", ".join(columns)
                    
                    source_count = source_conn.execute(
                        text(f"SELECT COUNT(DISTINCT {cols_str}) FROM {table_name}")
                    ).scalar()
                    
                    target_count = target_conn.execute(
                        text(f"SELECT COUNT(DISTINCT {cols_str}) FROM {table_name}")
                    ).scalar()
                    
                    assert source_count == target_count, \
                        f"Unique value count mismatch for {table_name}.{columns}: \
                        {source_count} vs {target_count}"
```

### 5.2 å›æ»šæµ‹è¯•

```python
class MigrationRollbackTest:
    """è¿ç§»å›æ»šæµ‹è¯•"""
    
    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
        self.backup = None
    
    def backup_schema(self):
        """å¤‡ä»½å½“å‰Schema"""
        inspector = inspect(self.engine)
        self.backup = {
            'tables': inspector.get_table_names(),
            'table_schemas': {}
        }
        
        for table_name in self.backup['tables']:
            self.backup['table_schemas'][table_name] = {
                'columns': inspector.get_columns(table_name),
                'pk': inspector.get_pk_constraint(table_name),
                'foreign_keys': inspector.get_foreign_keys(table_name),
                'indexes': inspector.get_indexes(table_name),
                'unique_constraints': inspector.get_unique_constraints(table_name)
            }
    
    def test_rollback_add_column(self):
        """æµ‹è¯•æ·»åŠ åˆ—åçš„å›æ»š"""
        with self.engine.connect() as conn:
            # æ·»åŠ åˆ—
            conn.execute(text("ALTER TABLE users ADD COLUMN temp_column VARCHAR(100)"))
            conn.commit()
            
            # éªŒè¯åˆ—å­˜åœ¨
            columns = [col['name'] for col in inspect(self.engine).get_columns('users')]
            assert 'temp_column' in columns
            
            # å›æ»š
            conn.execute(text("ALTER TABLE users DROP COLUMN temp_column"))
            conn.commit()
            
            # éªŒè¯åˆ—å·²åˆ é™¤
            columns = [col['name'] for col in inspect(self.engine).get_columns('users')]
            assert 'temp_column' not in columns
    
    def test_rollback_drop_table(self):
        """æµ‹è¯•åˆ é™¤è¡¨åçš„å›æ»š"""
        with self.engine.connect() as conn:
            # åˆ›å»ºä¸´æ—¶è¡¨
            conn.execute(text("CREATE TABLE temp_table (id INT PRIMARY KEY, name TEXT)"))
            conn.commit()
            
            # å¤‡ä»½æ•°æ®
            result = conn.execute(text("SELECT * FROM temp_table"))
            backup_data = result.fetchall()
            
            # åˆ é™¤è¡¨
            conn.execute(text("DROP TABLE temp_table"))
            conn.commit()
            
            # éªŒè¯è¡¨å·²åˆ é™¤
            tables = inspect(self.engine).get_table_names()
            assert 'temp_table' not in tables
            
            # é‡å»ºè¡¨
            conn.execute(text("CREATE TABLE temp_table (id INT PRIMARY KEY, name TEXT)"))
            
            # æ¢å¤æ•°æ®
            for row in backup_data:
                conn.execute(
                    text("INSERT INTO temp_table (id, name) VALUES (:id, :name)"),
                    {'id': row[0], 'name': row[1]}
                )
            conn.commit()
            
            # éªŒè¯æ•°æ®æ¢å¤
            result = conn.execute(text("SELECT COUNT(*) FROM temp_table"))
            assert result.scalar() == len(backup_data)
```

---

## 6. æ•°æ®è¿ç§»éªŒè¯

### 6.1 æ•°æ®è¿ç§»æµ‹è¯•æ¡†æ¶

```python
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
from enum import Enum
import hashlib

class MigrationStatus(Enum):
    """è¿ç§»çŠ¶æ€"""
    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    FAILED = "failed"
    VALIDATED = "validated"

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    status: MigrationStatus
    total_records: int
    matched_records: int
    missing_records: int
    mismatched_records: int
    errors: List[str]
    warnings: List[str]
    
    @property
    def success_rate(self) -> float:
        if self.total_records == 0:
            return 0.0
        return (self.matched_records / self.total_records) * 100
    
    @property
    def is_valid(self) -> bool:
        return self.missing_records == 0 and self.mismatched_records == 0


class DataMigrationValidator:
    """æ•°æ®è¿ç§»éªŒè¯å™¨"""
    
    def __init__(self, source_db_url: str, target_db_url: str):
        self.source_engine = create_engine(source_db_url)
        self.target_engine = create_engine(target_db_url)
    
    def validate_all_tables(self) -> Dict[str, ValidationResult]:
        """éªŒè¯æ‰€æœ‰è¡¨"""
        source_tables = inspect(self.source_engine).get_table_names()
        results = {}
        
        for table_name in source_tables:
            results[table_name] = self.validate_table(table_name)
        
        return results
    
    def validate_table(self, table_name: str) -> ValidationResult:
        """éªŒè¯å•ä¸ªè¡¨"""
        errors = []
        warnings = []
        
        # 1. æ£€æŸ¥è¡Œæ•°
        row_count_match = self._validate_row_count(table_name)
        if not row_count_match:
            errors.append("Row count mismatch between source and target")
        
        # 2. éªŒè¯æ•°æ®å®Œæ•´æ€§
        missing, mismatched = self._validate_data_integrity(table_name)
        
        # 3. éªŒè¯NULLå€¼
        nulls_match = self._validate_null_preservation(table_name)
        if not nulls_match:
            warnings.append("NULL value count differs between source and target")
        
        # 4. éªŒè¯å”¯ä¸€çº¦æŸ
        unique_match = self._validate_unique_constraints(table_name)
        if not unique_match:
            warnings.append("Unique constraint values differ")
        
        # è®¡ç®—ç»“æœ
        total = self._get_row_count(table_name, self.source_engine)
        matched = total - missing - mismatched
        
        status = MigrationStatus.VALIDATED if len(errors) == 0 else MigrationStatus.FAILED
        
        return ValidationResult(
            status=status,
            total_records=total,
            matched_records=matched,
            missing_records=missing,
            mismatched_records=mismatched,
            errors=errors,
            warnings=warnings
        )
    
    def _validate_row_count(self, table_name: str) -> bool:
        """éªŒè¯è¡Œæ•°"""
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            source_count = source_conn.execute(
                text(f"SELECT COUNT(*) FROM {table_name}")
            ).scalar()
            
            target_count = target_conn.execute(
                text(f"SELECT COUNT(*) FROM {table_name}")
            ).scalar()
            
            return source_count == target_count
    
    def _validate_data_integrity(self, table_name: str) -> Tuple[int, int]:
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        missing = 0
        mismatched = 0
        
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            # è·å–ä¸»é”®åˆ—
            pk_constraint = inspect(self.source_engine).get_pk_constraint(table_name)
            pk_columns = pk_constraint['constrained_columns']
            
            if not pk_columns:
                warnings.append(f"Table {table_name} has no primary key, skipping detailed validation")
                return 0, 0
            
            # è·å–æ‰€æœ‰è¡Œ
            source_data = source_conn.execute(
                text(f"SELECT * FROM {table_name}")
            ).fetchall()
            
            source_columns = [col['name'] for col in inspect(self.source_engine).get_columns(table_name)]
            
            for row in source_data:
                # æ„å»ºä¸»é”®æ¡ä»¶
                pk_values = {col: row[source_columns.index(col)] for col in pk_columns}
                where_clause = " AND ".join(f"{k} = :{k}" for k in pk_values.keys())
                
                # æŸ¥è¯¢ç›®æ ‡è¡¨
                target_row = target_conn.execute(
                    text(f"SELECT * FROM {table_name} WHERE {where_clause}"),
                    pk_values
                ).fetchone()
                
                if target_row is None:
                    missing += 1
                elif not self._compare_rows(row, target_row, source_columns):
                    mismatched += 1
            
            return missing, mismatched
    
    def _compare_rows(self, source_row: tuple, target_row: tuple, columns: List[str]) -> bool:
        """æ¯”è¾ƒä¸¤è¡Œæ•°æ®"""
        for i, col in enumerate(columns):
            source_val = source_row[i]
            target_val = target_row[i]
            
            # å¤„ç†ç‰¹æ®Šç±»å‹æ¯”è¾ƒ
            if isinstance(source_val, (int, float)) and isinstance(target_val, (int, float)):
                if abs(source_val - target_val) > 0.0001:
                    return False
            elif source_val != target_val:
                return False
        
        return True
    
    def _validate_null_preservation(self, table_name: str) -> bool:
        """éªŒè¯NULLå€¼ä¿ç•™"""
        columns = [col['name'] for col in inspect(self.source_engine).get_columns(table_name)]
        
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            for col in columns:
                source_nulls = source_conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name} WHERE {col} IS NULL")
                ).scalar()
                
                target_nulls = target_conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name} WHERE {col} IS NULL")
                ).scalar()
                
                if source_nulls != target_nulls:
                    return False
        
        return True
    
    def _validate_unique_constraints(self, table_name: str) -> bool:
        """éªŒè¯å”¯ä¸€çº¦æŸ"""
        unique_constraints = inspect(self.source_engine).get_unique_constraints(table_name)
        
        with self.source_engine.connect() as source_conn, \
             self.target_engine.connect() as target_conn:
            
            for uc in unique_constraints:
                cols = ", ".join(uc['column_names'])
                
                source_count = source_conn.execute(
                    text(f"SELECT COUNT(DISTINCT {cols}) FROM {table_name}")
                ).scalar()
                
                target_count = target_conn.execute(
                    text(f"SELECT COUNT(DISTINCT {cols}) FROM {table_name}")
                ).scalar()
                
                if source_count != target_count:
                    return False
        
        return True
    
    def _get_row_count(self, table_name: str, engine) -> int:
        """è·å–è¡Œæ•°"""
        with engine.connect() as conn:
            return conn.execute(
                text(f"SELECT COUNT(*) FROM {table_name}")
            ).scalar()


class DataChecksumValidator:
    """æ•°æ®æ ¡éªŒå’ŒéªŒè¯å™¨"""
    
    def __init__(self, source_db_url: str, target_db_url: str):
        self.source_engine = create_engine(source_db_url)
        self.target_engine = create_engine(target_db_url)
    
    def calculate_table_checksum(self, table_name: str, engine) -> str:
        """è®¡ç®—è¡¨çš„æ ¡éªŒå’Œ"""
        with engine.connect() as conn:
            # è·å–æ‰€æœ‰åˆ—
            columns = [col['name'] for col in inspect(engine).get_columns(table_name)]
            
            # æ„å»ºæ ¡éªŒå’ŒæŸ¥è¯¢
            checksum_query = text(f"""
                SELECT MD5(GROUP_CONCAT(
                    CONCAT({', '.join(f"COALESCE(CAST({col} AS CHAR), '')" for col in columns)})
                ORDER BY {columns[0]}))
                AS checksum
                FROM {table_name}
            """)
            
            result = conn.execute(checksum_query)
            return result.fetchone()[0]
    
    def validate_checksum(self, table_name: str) -> bool:
        """éªŒè¯è¡¨çš„æ•°æ®æ ¡éªŒå’Œ"""
        source_checksum = self.calculate_table_checksum(table_name, self.source_engine)
        target_checksum = self.calculate_table_checksum(table_name, self.target_engine)
        
        return source_checksum == target_checksum
    
    def validate_all_checksums(self) -> Dict[str, bool]:
        """éªŒè¯æ‰€æœ‰è¡¨çš„æ ¡éªŒå’Œ"""
        source_tables = inspect(self.source_engine).get_table_names()
        results = {}
        
        for table_name in source_tables:
            results[table_name] = self.validate_checksum(table_name)
        
        return results
```

### 6.2 æ•°æ®è´¨é‡éªŒè¯

```python
class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""
    
    def __init__(self, db_url: str):
        self.engine = create_engine(db_url)
    
    def validate_data_types(self, table_name: str) -> List[Dict]:
        """éªŒè¯æ•°æ®ç±»å‹"""
        issues = []
        columns = inspect(self.engine).get_columns(table_name)
        
        with self.engine.connect() as conn:
            for col in columns:
                col_name = col['name']
                expected_type = str(col['type'])
                
                # é‡‡æ ·æŸ¥è¯¢
                result = conn.execute(
                    text(f"SELECT {col_name} FROM {table_name} LIMIT 100")
                )
                
                for row in result:
                    value = row[0]
                    if value is not None:
                        if not self._check_type_compatibility(value, expected_type):
                            issues.append({
                                'table': table_name,
                                'column': col_name,
                                'value': str(value)[:50],
                                'expected_type': expected_type,
                                'issue': 'Type mismatch'
                            })
                            break
        
        return issues
    
    def _check_type_compatibility(self, value: Any, expected_type: str) -> bool:
        """æ£€æŸ¥ç±»å‹å…¼å®¹æ€§"""
        type_upper = expected_type.upper()
        
        if 'INT' in type_upper:
            return isinstance(value, int)
        elif 'VARCHAR' in type_upper or 'TEXT' in type_upper:
            return isinstance(value, str)
        elif 'DECIMAL' in type_upper or 'NUMERIC' in type_upper or 'FLOAT' in type_upper or 'DOUBLE' in type_upper:
            return isinstance(value, (int, float))
        elif 'DATE' in type_upper or 'TIMESTAMP' in type_upper:
            return isinstance(value, datetime)
        elif 'BOOL' in type_upper:
            return isinstance(value, bool)
        
        return True
    
    def validate_referential_integrity(self, table_name: str) -> List[Dict]:
        """éªŒè¯å¼•ç”¨å®Œæ•´æ€§"""
        issues = []
        foreign_keys = inspect(self.engine).get_foreign_keys(table_name)
        
        with self.engine.connect() as conn:
            for fk in foreign_keys:
                columns = fk['constrained_columns']
                referred_table = fk['referred_table']
                referred_columns = fk['referred_columns']
                
                for col, ref_col in zip(columns, referred_columns):
                    # æŸ¥æ‰¾å­¤ç«‹è®°å½•
                    result = conn.execute(text(f"""
                        SELECT COUNT(*) 
                        FROM {table_name} t
                        WHERE t.{col} IS NOT NULL
                        AND NOT EXISTS (
                            SELECT 1 FROM {referred_table} r
                            WHERE r.{ref_col} = t.{col}
                        )
                    """))
                    
                    orphan_count = result.scalar()
                    
                    if orphan_count > 0:
                        issues.append({
                            'table': table_name,
                            'column': col,
                            'referred_table': referred_table,
                            'orphan_count': orphan_count,
                            'issue': 'Referential integrity violation'
                        })
        
        return issues
    
    def validate_business_rules(self, table_name: str, rules: List[Dict]) -> List[Dict]:
        """éªŒè¯ä¸šåŠ¡è§„åˆ™"""
        issues = []
        
        with self.engine.connect() as conn:
            for rule in rules:
                rule_name = rule['name']
                condition = rule['condition']
                
                result = conn.execute(text(f"""
                    SELECT COUNT(*) 
                    FROM {table_name}
                    WHERE {condition}
                """))
                
                violation_count = result.scalar()
                
                if violation_count > 0:
                    issues.append({
                        'table': table_name,
                        'rule': rule_name,
                        'condition': condition,
                        'violation_count': violation_count,
                        'issue': 'Business rule violation'
                    })
        
        return issues
    
    def generate_data_quality_report(self, table_name: str) -> Dict:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        report = {
            'table': table_name,
            'total_rows': 0,
            'null_counts': {},
            'duplicate_counts': {},
            'data_type_issues': [],
            'referential_issues': [],
            'quality_score': 100.0
        }
        
        with self.engine.connect() as conn:
            # æ€»è¡Œæ•°
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
            report['total_rows'] = result.scalar()
            
            # NULLå€¼ç»Ÿè®¡
            columns = inspect(self.engine).get_columns(table_name)
            for col in columns:
                col_name = col['name']
                result = conn.execute(
                    text(f"SELECT COUNT(*) FROM {table_name} WHERE {col_name} IS NULL")
                )
                null_count = result.scalar()
                if null_count > 0:
                    report['null_counts'][col_name] = null_count
                    report['quality_score'] -= (null_count / report['total_rows']) * 10
            
            # é‡å¤å€¼ç»Ÿè®¡
            for col in columns:
                col_name = col['name']
                result = conn.execute(text(f"""
                    SELECT {col_name}, COUNT(*) as cnt
                    FROM {table_name}
                    GROUP BY {col_name}
                    HAVING COUNT(*) > 1
                """))
                duplicates = result.fetchall()
                if duplicates:
                    report['duplicate_counts'][col_name] = len(duplicates)
                    report['quality_score'] -= 5
        
        # æ•°æ®ç±»å‹é—®é¢˜
        report['data_type_issues'] = self.validate_data_types(table_name)
        if report['data_type_issues']:
            report['quality_score'] -= 20
        
        # å¼•ç”¨å®Œæ•´æ€§é—®é¢˜
        report['referential_issues'] = self.validate_referential_integrity(table_name)
        if report['referential_issues']:
            report['quality_score'] -= 30
        
        report['quality_score'] = max(0, report['quality_score'])
        
        return report
```

---

## 7. æµ‹è¯•æœ€ä½³å®è·µæ€»ç»“

### 7.1 æµ‹è¯•é‡‘å­—å¡”å®è·µ

```
                    /\
                   /E2E\           <- 5-10% æµ‹è¯•æ•°é‡
                  /------\         å…³é”®ç”¨æˆ·åœºæ™¯
                 /Integration\     <- 15-25% æµ‹è¯•æ•°é‡
                /   Service    \   ç»„ä»¶äº¤äº’
               /    Tests       \   
              /------------------\
             /     Unit Tests    \  <- 70-80% æµ‹è¯•æ•°é‡
            /   (Database Mocks)  \  å¿«é€Ÿåé¦ˆ
           /______________________\
```

### 7.2 æµ‹è¯•æ•°æ®ç®¡ç†ç­–ç•¥

| ç­–ç•¥ | é€‚ç”¨åœºæ™¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|------|----------|------|------|
| å†…å­˜æ•°æ®åº“ | å•å…ƒæµ‹è¯•ã€CI | å¿«é€Ÿã€éš”ç¦» | åŠŸèƒ½å—é™ |
| Dockerå®¹å™¨ | é›†æˆæµ‹è¯• | çœŸå®ç¯å¢ƒ | å¯åŠ¨æ…¢ |
| æ•°æ®åº“å¿«ç…§ | å›å½’æµ‹è¯• | å¯é‡å¤ | å ç”¨ç©ºé—´ |
| æµ‹è¯•æ•°æ®å·¥å‚ | åŠ¨æ€æµ‹è¯• | çµæ´» | éœ€ç»´æŠ¤ |

### 7.3 æµ‹è¯•è¦†ç›–çŸ©é˜µ

| æµ‹è¯•ç±»å‹ | è¦†ç›–å†…å®¹ | è‡ªåŠ¨åŒ–ç¨‹åº¦ |
|----------|----------|------------|
| å•å…ƒæµ‹è¯• | ä¸šåŠ¡é€»è¾‘ã€è¾¹ç•Œæ¡ä»¶ | é«˜ |
| é›†æˆæµ‹è¯• | æ•°æ®ä¸€è‡´æ€§ã€çº¦æŸ | é«˜ |
| å±æ€§æµ‹è¯• | éšæœºåœºæ™¯ã€è¾¹ç•Œ | é«˜ |
| æ¨¡ç³Šæµ‹è¯• | å¼‚å¸¸è¾“å…¥ã€å®‰å…¨ | ä¸­ |
| è¿ç§»æµ‹è¯• | æ•°æ®å®Œæ•´æ€§ | é«˜ |
| æ€§èƒ½æµ‹è¯• | å“åº”æ—¶é—´ã€ååé‡ | ä¸­ |

### 7.4 æŒç»­é›†æˆé…ç½®

```yaml
# .github/workflows/database-tests.yml
name: Database Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install pytest pytest-cov pytest-mock
      - name: Run unit tests
        run: |
          pytest tests/unit/ -v --cov --cov-report=xml

  integration-tests:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: testdb
          POSTGRES_USER: testuser
          POSTGRES_PASSWORD: testpass
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Run integration tests
        run: |
          pytest tests/integration/ -v

  property-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Run property-based tests
        run: |
          pytest tests/property/ -v --hypothesis-show-statistics
```

---

## é™„å½•ï¼šå¸¸ç”¨æ•°æ®åº“æµ‹è¯•å‘½ä»¤

### PostgreSQL

```bash
# æŸ¥çœ‹è¡¨ç»“æ„
\d users

# æŸ¥çœ‹ç´¢å¼•
\d index_name

# æŸ¥çœ‹å¤–é”®
\df

# æ‰§è¡ŒEXPLAIN ANALYZE
EXPLAIN ANALYZE SELECT * FROM users WHERE id = 1;

# æŸ¥çœ‹é”ä¿¡æ¯
SELECT * FROM pg_locks;

# æŸ¥çœ‹æ´»åŠ¨è¿æ¥
SELECT * FROM pg_stat_activity;

# æŸ¥çœ‹æ…¢æŸ¥è¯¢
SELECT * FROM pg_stat_statements ORDER BY total_time DESC LIMIT 10;
```

### MySQL

```sql
-- æŸ¥çœ‹è¡¨ç»“æ„
DESCRIBE users;
SHOW CREATE TABLE users;

-- æŸ¥çœ‹ç´¢å¼•
SHOW INDEX FROM users;

-- æ‰§è¡ŒEXPLAIN
EXPLAIN SELECT * FROM users WHERE id = 1;

-- æŸ¥çœ‹è¿›ç¨‹åˆ—è¡¨
SHOW PROCESSLIST;

-- æŸ¥çœ‹ InnoDB çŠ¶æ€
SHOW ENGINE INNODB STATUS;

-- æŸ¥çœ‹æ…¢æŸ¥è¯¢æ—¥å¿—
SHOW VARIABLES LIKE 'slow_query_log';
SELECT * FROM mysql.slow_log;
```

---

*æœ¬æ–‡æ¡£æ˜¯æ•°æ®åº“æµ‹è¯•ç­–ç•¥çš„å®Œæ•´æŒ‡å—ï¼Œæ¶µç›–ä»å•å…ƒæµ‹è¯•åˆ°æ•°æ®è¿ç§»éªŒè¯çš„å„ä¸ªå±‚é¢ã€‚å»ºè®®ç»“åˆé¡¹ç›®å®é™…æƒ…å†µé€‰æ‹©åˆé€‚çš„æµ‹è¯•ç­–ç•¥ç»„åˆã€‚*
