# Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: AI Engineer Toolkit 2025

## ðŸ“œ LICENSE (MIT License)

```text
MIT License

Copyright (c) 2025 AI Engineer Toolkit Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

## ðŸ“ .gitignore

```text
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
env/
venv/
.build/
*.egg-info/
dist/
build/

# Jupyter
.ipynb_checkpoints
*.ipynb
*.swp
*.swo

# Data
data/
!.gitkeep

# Models
models/
*.pkl
*.joblib
*.h5
*.onnx
*.pt
*.bin

# Environment
.env
.DS_Store
Thumbs.db

# Docker
.dockerignore

# Logs
*.log
*.tmp

# IDE
.vscode/
.idea/
*.swp
*.swo
*.swn
```

## ðŸ“ notebooks/06_llm_engineering/03_rag_advanced_techniques.ipynb (Complete)

```python
# =====================
# RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù†Ø­Ùˆ Ø£Ù†Ø¸Ù…Ø© Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø°ÙƒÙŠØ©
# Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© -> Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ -> Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©
# =====================

"""
## 1. Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù€ RAG Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ
Ø§Ù„Ù€ RAG (Retrieval-Augmented Generation) Ø£ØµØ¨Ø­ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù‚ÙŠØ§Ø³ÙŠÙ‹Ø§ Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLMs). ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙØ¥Ù† Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ ÙŠÙƒÙ…Ù† ÙÙŠ ØªØ¬Ø§ÙˆØ² Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙŠ ØªÙˆØ§Ø¬Ù‡ Ø£Ù†Ø¸Ù…Ø© RAG Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:

1. **Ø§Ù„Ø§Ù†Ø²ÙŠØ§Ø­ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ (Context Drift)**: Ø¹Ù†Ø¯Ù…Ø§ ÙŠØ®ØªÙ„Ù Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¹Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
2. **Ø§Ù„ØªØºØ·ÙŠØ© ØºÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Incomplete Coverage)**: Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙˆØ²Ø¹Ø© Ø¹Ù„Ù‰ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
3. **Ø§Ù„Ø¶Ø¬ÙŠØ¬ ÙÙŠ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ (Noisy Retrieval)**: Ø¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø³ØªÙ†Ø¯Ø§Øª ØºÙŠØ± Ø°Ø§Øª ØµÙ„Ø©
4. **Ø§Ù„Ø­Ø¯ Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ (Context Window Limitation)**: Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø© Ø£Ø·ÙˆÙ„ Ù…Ù† Ø³Ø¹Ø© Ø³ÙŠØ§Ù‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠØ© (Ù‚Ø·Ø¹ Ø§Ù„Ù†ØµÙˆØµ - Chunking) Ù„Ø§ ØªÙƒÙÙŠØŒ ÙˆÙ‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø³Ø¨Ø¨ ÙÙŠ Ø¸Ù‡ÙˆØ± ØªÙ‚Ù†ÙŠØ§Øª RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©.
"""

"""
## 2. Ù†Ø¸Ø±ÙŠØ© GraphRAG: ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ©
GraphRAG ÙŠÙ…Ø«Ù„ Ù‚ÙØ²Ø© Ù†ÙˆØ¹ÙŠØ© ÙÙŠ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ØŒ Ø­ÙŠØ« ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ÙÙ‡Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†ØµÙŠ.

### 2.1 Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ù…Ø®Ø·Ø·Ø§Øª
- **Ø§Ù„Ø¹Ù‚Ø¯ (Nodes)**: ØªÙ…Ø«Ù„ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª (Ø£Ø´Ø®Ø§ØµØŒ Ø£Ù…Ø§ÙƒÙ†ØŒ Ù…ÙØ§Ù‡ÙŠÙ…)
- **Ø§Ù„Ø­ÙˆØ§Ù (Edges)**: ØªÙ…Ø«Ù„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
- **Ø§Ù„Ø®ÙˆØ§Øµ (Properties)**: Ø³Ù…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù† Ø§Ù„Ø¹Ù‚Ø¯ ÙˆØ§Ù„Ø­ÙˆØ§Ù
- **Ø§Ù„ÙˆØ²Ù† (Weight)**: Ù‚ÙˆØ© Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª

### 2.2 Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ ÙÙŠ GraphRAG
Ø¹Ù†Ø¯ Ø§Ø³ØªÙ„Ø§Ù… Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ ÙŠØªÙ… ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø± ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù…:
1. **Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ (Semantic Similarity)**: Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
2. **Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ (Graph Propagation)**: Ù„Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
3. **Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø£Ù‚ØµØ± (Shortest Path)**: Ù„Ø±Ø¨Ø· Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©

ØªØ³ØªØ®Ø¯Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø«Ù„ **PageRank** Ùˆ **Personalized PageRank** Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¹Ù‚Ø¯ ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù….

### 2.3 Ù…ÙŠØ²Ø© GraphRAG: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ù…Ø«Ù„ "ÙƒÙŠÙ ØªØ£Ø«Ø±Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„ØªØ¬Ø§Ø±ÙŠØ© Ø¨ÙŠÙ† Ø´Ø±ÙƒØªÙŠ X Ùˆ Y Ø¨Ø¹Ø¯ Ø£Ù† Ø£ØµØ¨Ø­ Z Ø§Ù„Ù…Ø¯ÙŠØ± Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ø§Ù„Ø¬Ø¯ÙŠØ¯ØŸ"ØŒ ÙØ¥Ù† GraphRAG Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰:
1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (X, Y, Z)
2. Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø²Ù…Ù†
3. ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…Ø³Ø§Ø±Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
"""

import numpy as np
import networkx as nx
from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

@dataclass
class GraphNode:
    """Ø¹Ù‚Ø¯Ø© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ"""
    id: str
    type: str  # "person", "organization", "location", "concept", etc.
    name: str
    properties: Dict[str, Any]
    embedding: Optional[np.ndarray] = None

@dataclass
class GraphEdge:
    """Ø­Ø§ÙØ© ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ"""
    source_id: str
    target_id: str
    relation_type: str  # "works_at", "located_in", "influenced_by", etc.
    weight: float = 1.0
    properties: Dict[str, Any] = None

class KnowledgeGraph:
    """ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ù„Ù„Ù…Ø¹Ø±ÙØ©"""
    
    def __init__(self, embedding_model: str = "all-mpnet-base-v2"):
        self.nodes: Dict[str, GraphNode] = {}
        self.edges: List[GraphEdge] = []
        self.graph = nx.Graph()
        self.embedding_model = SentenceTransformer(embedding_model)
    
    def add_node(self, node: GraphNode):
        """Ø¥Ø¶Ø§ÙØ© Ø¹Ù‚Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ"""
        self.nodes[node.id] = node
        self.graph.add_node(node.id, **{
            'type': node.type,
            'name': node.name,
            'properties': node.properties
        })
    
    def add_edge(self, edge: GraphEdge):
        """Ø¥Ø¶Ø§ÙØ© Ø­Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ"""
        self.edges.append(edge)
        self.graph.add_edge(edge.source_id, edge.target_id, **{
            'relation_type': edge.relation_type,
            'weight': edge.weight,
            'properties': edge.properties
        })
    
    def build_from_documents(self, documents: List[Dict[str, Any]]):
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ù…Ù† Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
        pass  # Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡ ÙÙŠ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    
    def retrieve_context(self, query: str, max_hops: int = 2, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… GraphRAG
        
        Args:
            query: Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù†ØµÙŠ
            max_hops: Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ù…Ù† Ø§Ù„Ù‚ÙØ²Ø§Øª ÙÙŠ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ
            top_k: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø£Ø¹Ù„Ù‰
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
        """
        # 1. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¥Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ù…ØªØ¬Ù‡ÙŠ
        query_embedding = self.embedding_model.encode([query])[0]
        
        # 2. Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ø£Ù‚Ø±Ø¨ Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
        candidate_nodes = []
        for node_id, node in self.nodes.items():
            if node.embedding is not None:
                similarity = cosine_similarity([query_embedding], [node.embedding])[0][0]
                candidate_nodes.append((node_id, similarity))
        
        # ÙØ±Ø² Ø§Ù„Ø¹Ù‚Ø¯ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¹Ù„Ù‰
        candidate_nodes.sort(key=lambda x: x[1], reverse=True)
        seed_nodes = [node_id for node_id, _ in candidate_nodes[:top_k]]
        
        # 3. Ø§Ø³ØªÙƒØ´Ø§Ù Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ø¨Ø°Ø±Ø©
        context_nodes = set()
        for seed_node in seed_nodes:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… BFS Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù‚ÙØ²Ø§Øª
            neighbors = nx.single_source_shortest_path_length(self.graph, seed_node, cutoff=max_hops)
            context_nodes.update(neighbors.keys())
        
        # 4. Ø¬Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©
        context = []
        for node_id in context_nodes:
            node = self.nodes[node_id]
            context.append({
                'id': node.id,
                'name': node.name,
                'type': node.type,
                'properties': node.properties,
                'neighbors': list(self.graph.neighbors(node_id))
            })
        
        return context
    
    def visualize_subgraph(self, node_ids: List[str], output_path: str = "graph_viz.png"):
        """ØªØµÙˆØ± Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ Ø§Ù„ÙØ±Ø¹ÙŠ"""
        subgraph = self.graph.subgraph(node_ids)
        
        plt.figure(figsize=(12, 10))
        pos = nx.spring_layout(subgraph, k=0.5, iterations=50)
        
        # Ø±Ø³Ù… Ø§Ù„Ø¹Ù‚Ø¯ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        node_types = set(nx.get_node_attributes(subgraph, 'type').values())
        colors = plt.cm.tab20(np.linspace(0, 1, len(node_types)))
        type_to_color = {t: colors[i] for i, t in enumerate(node_types)}
        
        for node_type in node_types:
            nodes_of_type = [n for n, attr in subgraph.nodes(data=True) if attr['type'] == node_type]
            nx.draw_networkx_nodes(
                subgraph, pos, nodelist=nodes_of_type,
                node_color=[type_to_color[node_type]], node_size=500,
                label=node_type
            )
        
        # Ø±Ø³Ù… Ø§Ù„Ø­ÙˆØ§Ù
        nx.draw_networkx_edges(subgraph, pos, alpha=0.5)
        
        # Ø¥Ø¶Ø§ÙØ© ØªØ³Ù…ÙŠØ§Øª
        labels = {n: attr['name'][:10] + "..." if len(attr['name']) > 10 else attr['name'] 
                 for n, attr in subgraph.nodes(data=True)}
        nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)
        
        plt.legend()
        plt.axis('off')
        plt.tight_layout()
        plt.savefig(output_path)
        plt.close()
        
        print(f"Graph visualization saved to {output_path}")

"""
## 3. Hybrid Search: Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø£ÙØ¶Ù„ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹
Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù‡Ø¬ÙŠÙ† (Hybrid Search) ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ù…Ø²Ø§ÙŠØ§ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„ÙƒØ«ÙŠÙ (Dense Retrieval) ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†Ø§Ø¯Ø± (Sparse Retrieval).

### 3.1 Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø®Ù„Ù Hybrid Search
Ø¹Ù†Ø¯ Ø­Ø³Ø§Ø¨ ØµÙ„Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© ØªØ¬Ù…ÙŠØ¹ (Combination Function):

$$Relevance = \alpha \cdot DenseRelevance + (1 - \alpha) \cdot SparseRelevance$$

Ø­ÙŠØ«:
- **DenseRelevance**: ØªØ´Ø§Ø¨Ù‡ Ø¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… Ø¨ÙŠÙ† ØªØ¶Ù…ÙŠÙ†Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… ÙˆØ§Ù„Ù…Ø³ØªÙ†Ø¯
- **SparseRelevance**: ØªØ´Ø§Ø¨Ù‡ BM25 Ø£Ùˆ TF-IDF Ø¨ÙŠÙ† Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª
- **Î±**: ÙˆØ²Ù† ÙŠØ­Ø¯Ø¯ Ø£Ù‡Ù…ÙŠØ© ÙƒÙ„ Ù…ÙƒÙˆÙ† (0.5 ÙÙŠ Ø£ØºÙ„Ø¨ Ø§Ù„Ø­Ø§Ù„Ø§Øª)

### 3.2 Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Re-ranking)
Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø£ÙˆÙ„ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø¹Ø§Ø¯Ø© ØªØ±ØªÙŠØ¨ Ù…ØªØ®ØµØµØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬:

1. **Cross-Encoders**: Ù†Ù…Ø§Ø°Ø¬ BERT ØªØ£Ø®Ø° Ø²ÙˆØ¬ (Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŒ Ù…Ø³ØªÙ†Ø¯) ÙˆØªÙ†ØªØ¬ Ø¯Ø±Ø¬Ø© ØªØ´Ø§Ø¨Ù‡ Ø¯Ù‚ÙŠÙ‚Ø©
2. **ColBERT**: ÙŠØ­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¹Ù†Ø¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªÙˆÙƒÙŠÙ† Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙØ§Ø¡Ø©

### 3.3 ØªØ­Ù„ÙŠÙ„ Ù…Ø¹Ù…Ù‚ Ù„Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª
| Ø§Ù„ØªÙ‚Ù†ÙŠØ© | Ø§Ù„Ù…Ø²Ø§ÙŠØ§ | Ø§Ù„Ø¹ÙŠÙˆØ¨ | Ø£ÙØ¶Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|----------|---------|--------|--------------|
| Vector Search (Dense) | ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª ÙˆØ§Ù„ØªÙ…Ø§Ø«Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ | ÙŠÙØ´Ù„ Ù…Ø¹ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø© (Ù…Ø«Ù„ Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª) | Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© ÙˆØ§Ù„ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø© |
| Keyword Search (Sparse) | Ø¯Ù‚ÙŠÙ‚ Ù…Ø¹ Ø§Ù„Ù…ØµØ·Ù„Ø­Ø§Øª ÙˆØ§Ù„Ø±Ù…ÙˆØ² | ÙŠÙØ´Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª ÙˆØ§Ù„ØªÙ…Ø§Ø«Ù„ Ø§Ù„Ø¯Ù„Ø§Ù„ÙŠ | Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…ØµØ·Ù„Ø­Ø§Øª Ù…Ø­Ø¯Ø¯Ø© |
| Hybrid Search | Ø§Ù„Ø£ÙØ¶Ù„ Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØªÙŠÙ† | ÙŠØªØ·Ù„Ø¨ Ù…ÙˆØ§Ø±Ø¯ Ø­Ø³Ø§Ø¨ÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ© | Ù…Ø¹Ø¸Ù… Ø§Ù„Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© |
"""

class HybridRetriever:
    """Ù†Ø¸Ø§Ù… Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù‡Ø¬ÙŠÙ† ÙŠØ¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ§Øª"""
    
    def __init__(self, vector_index, sparse_index, reranker=None, alpha=0.5):
        """
        Args:
            vector_index: ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ
            sparse_index: ÙÙ‡Ø±Ø³ Ø§Ù„Ø¨Ø­Ø« Ø¨Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ§Øª
            reranker: Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            alpha: ÙˆØ²Ù† Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ (0-1)
        """
        self.vector_index = vector_index
        self.sparse_index = sparse_index
        self.reranker = reranker
        self.alpha = alpha
    
    def retrieve(self, query: str, top_k: int = 10, rerank: bool = True) -> List[Dict[str, Any]]:
        """
        Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†
        
        Args:
            query: Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ù†ØµÙŠ
            top_k: Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ©
            rerank: Ù‡Ù„ Ù†Ø³ØªØ®Ø¯Ù… Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø£Ù… Ù„Ø§
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø±ØªØ¨Ø©
        """
        # 1. Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ
        vector_results = self.vector_index.search(query, top_k=top_k*2)
        
        # 2. Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ù† Ø§Ù„ÙÙ‡Ø±Ø³ Ø§Ù„Ù†Ø§Ø¯Ø±
        sparse_results = self.sparse_index.search(query, top_k=top_k*2)
        
        # 3. Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        combined_results = self._combine_results(vector_results, sparse_results)
        
        # 4. Ø§Ù‚ØªØµØ§Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ØªÙƒÙˆÙ† top_k
        combined_results = combined_results[:top_k]
        
        # 5. Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªØ±ØªÙŠØ¨ Ø¥Ø°Ø§ Ø·Ù„Ø¨
        if rerank and self.reranker:
            combined_results = self.reranker.rerank(query, combined_results)
        
        return combined_results
    
    def _combine_results(self, vector_results: List[Dict[str, Any]], 
                        sparse_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Ø¯Ù…Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ ÙˆØ§Ù„Ù†Ø§Ø¯Ø±"""
        # Ø®Ø±ÙŠØ·Ø© Ù„ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª
        score_map = {}
        
        # Ø¥Ø¶Ø§ÙØ© Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ
        for i, result in enumerate(vector_results):
            doc_id = result['id']
            # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø¨ÙŠÙ† 0 Ùˆ 1
            normalized_score = 1 - (i / len(vector_results))
            score_map[doc_id] = {
                'score': normalized_score * self.alpha,
                'vector_rank': i,
                'sparse_rank': None,
                'content': result['content'],
                'metadata': result['metadata']
            }
        
        # Ø¥Ø¶Ø§ÙØ© Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù†Ø§Ø¯Ø± ÙˆØ¯Ù…Ø¬Ù‡Ø§
        for i, result in enumerate(sparse_results):
            doc_id = result['id']
            normalized_score = 1 - (i / len(sparse_results))
            
            if doc_id in score_map:
                # Ø¯Ù…Ø¬ Ø§Ù„Ø¯Ø±Ø¬Ø©
                score_map[doc_id]['score'] += normalized_score * (1 - self.alpha)
                score_map[doc_id]['sparse_rank'] = i
            else:
                score_map[doc_id] = {
                    'score': normalized_score * (1 - self.alpha),
                    'vector_rank': None,
                    'sparse_rank': i,
                    'content': result['content'],
                    'metadata': result['metadata']
                }
        
        # ÙØ±Ø² Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        sorted_results = sorted(score_map.items(), key=lambda x: x[1]['score'], reverse=True)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
        results = []
        for doc_id, data in sorted_results:
            results.append({
                'id': doc_id,
                'score': data['score'],
                'vector_rank': data['vector_rank'],
                'sparse_rank': data['sparse_rank'],
                'content': data['content'],
                'metadata': data['metadata']
            })
        
        return results

"""
## 4. RAG Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¹Ø¨Ø± Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª
Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø£ÙƒØ¨Ø± ÙÙŠ Ø£Ù†Ø¸Ù…Ø© RAG Ù‡Ùˆ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¹Ø¨Ø± Ù…Ø­Ø§Ø¯Ø«Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø¬ÙˆÙ„Ø§Øª.

### 4.1 Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ Ø£Ù†Ø¸Ù…Ø© RAG
Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø«Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙÙŠ Ø£Ù†Ø¸Ù…Ø© RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©:
1. **Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù‚ØµÙŠØ±Ø© Ø§Ù„Ù…Ø¯Ù‰ (Short-term Memory)**: Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠ Ù„Ù„Ø­ÙˆØ§Ø±
2. **Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ù…Ø¯Ù‰ (Medium-term Memory)**: Ù…Ù„Ø®ØµØ§Øª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
3. **Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰ (Long-term Memory)**: Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªÙ… Ø­ÙØ¸Ù‡Ø§ ÙŠØ¯ÙˆÙŠØ§Ù‹

### 4.2 Ø¢Ù„ÙŠØ© Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
```mermaid
graph TD
    A[User Query] --> B{Ù‡Ù„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŸ}
    B -->|Ù†Ø¹Ù…| C[Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©]
    B -->|Ù„Ø§| D[Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©]
    D --> E[Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨]
    E --> F[Ø¯Ù…Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø©]
    C --> G[ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©]
    F --> G
    G --> H[ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©]
    H --> I[Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©]
```

### 4.3 Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª ÙˆØ§Ù„Ø­Ù„ÙˆÙ„
- **Ø§Ù†ØªÙØ§Ø® Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory Bloat)**: Ø­Ø¯ Ø³Ø¹Ø© Ø§Ù„Ø³ÙŠØ§Ù‚
  * Ø§Ù„Ø­Ù„: ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù‚Ø¯ÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
- **ØªØ¶Ø§Ø±Ø¨ Ø§Ù„Ø³ÙŠØ§Ù‚ (Context Conflict)**: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ØªÙ†Ø§Ù‚Ø¶Ø© ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
  * Ø§Ù„Ø­Ù„: ØªØ­Ø¯ÙŠØ¯ Ø£ÙˆÙ„ÙˆÙŠØ© Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©
"""

class MemoryAugmentedRAG:
    """Ù†Ø¸Ø§Ù… RAG Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
    
    def __init__(self, retriever, llm, memory_window=5):
        self.retriever = retriever
        self.llm = llm
        self.memory_window = memory_window
        self.conversation_history = []
        self.key_facts = {}
    
    def add_to_memory(self, query: str, response: str, facts: List[str] = None):
        """Ø¥Ø¶Ø§ÙØ© ØªÙØ§Ø¹Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        self.conversation_history.append({
            'query': query,
            'response': response,
            'timestamp': time.time()
        })
        
        # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø­Ø¬Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        if len(self.conversation_history) > self.memory_window:
            self.conversation_history.pop(0)
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‡Ù…Ø©
        if facts:
            for fact in facts:
                fact_hash = hash(fact)
                self.key_facts[fact_hash] = {
                    'fact': fact,
                    'timestamp': time.time(),
                    'source_query': query
                }
    
    def retrieve_context_with_memory(self, query: str, top_k: int = 5) -> str:
        """Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…Ù‡Ù…Ø© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
        relevant_facts = []
        for fact_hash, data in self.key_facts.items():
            if self._is_fact_relevant(query, data['fact']):
                relevant_facts.append(data['fact'])
        
        # 2. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        retrieved_contexts = self.retriever.retrieve(query, top_k=top_k)
        base_context = "\n\n".join([result['content'] for result in retrieved_contexts])
        
        # 3. Ø¯Ù…Ø¬ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_context = ""
        if relevant_facts:
            memory_context += "Ø­Ù‚Ø§Ø¦Ù‚ Ù…Ù‡Ù…Ø©:\n" + "\n".join(relevant_facts) + "\n\n"
        
        if self.conversation_history:
            memory_context += "Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n"
            for i, turn in enumerate(self.conversation_history[-2:]):  # Ø¢Ø®Ø± Ø¬ÙˆÙ„ØªÙŠÙ†
                memory_context += f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {turn['query']}\n"
                memory_context += f"Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {turn['response']}\n"
        
        # 4. Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„Ø³ÙŠØ§Ù‚Ø§Øª
        final_context = ""
        if memory_context:
            final_context += "=== Ø³ÙŠØ§Ù‚ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ===\n" + memory_context + "\n"
        if base_context:
            final_context += "=== Ø³ÙŠØ§Ù‚ Ù…Ø³ØªØ±Ø¬Ø¹ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ===\n" + base_context
        
        return final_context
    
    def _is_fact_relevant(self, query: str, fact: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµÙ„Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ ØµØºÙŠØ± Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØµÙ„Ø©
        relevance_prompt = f"""
        Ù‡Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…ØŸ
        Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}
        Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©: {fact}
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© (Ù†Ø¹Ù…/Ù„Ø§ ÙÙ‚Ø·):
        """
        response = self.llm.generate(relevance_prompt, max_tokens=10)
        return "Ù†Ø¹Ù…" in response.lower()
    
    def generate_response(self, query: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… RAG Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        # 1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        context = self.retrieve_context_with_memory(query)
        
        # 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
        prompt = f"""
        Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø³ÙŠØ§Ù‚ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©.
        
        {context}
        
        Ø§Ù„Ø³Ø¤Ø§Ù„: {query}
        Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
        """
        response = self.llm.generate(prompt)
        
        # 3. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.add_to_memory(query, response)
        
        return response

"""
## 5. Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©: Ù†Ø´Ø± Ø£Ù†Ø¸Ù…Ø© RAG Ù…ØªÙ‚Ø¯Ù…Ø©
Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù„ÙŠØ³ ÙÙ‚Ø· Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ù…ØªÙ‚Ø¯Ù…ØŒ Ø¨Ù„ Ù†Ø´Ø±Ù‡ ÙˆØªØ´ØºÙŠÙ„Ù‡ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡.

### 5.1 Ø¨Ù†ÙŠØ© Ù†Ø¸Ø§Ù… RAG Ø¥Ù†ØªØ§Ø¬ÙŠ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User      â”‚â”€â”€â”€â–¶â”‚  API Gatewayâ”‚â”€â”€â”€â–¶â”‚  RAG Router â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â–¼                   â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚Hybrid Searchâ”‚    â”‚  GraphRAG   â”‚    â”‚Memory Systemâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚                   â”‚                   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚  LLM Router â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â–¼                   â–¼                   â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Small Model â”‚    â”‚Medium Model â”‚    â”‚Large Model  â”‚
              â”‚ (Fast/cheap)â”‚    â”‚ (Balanced)  â”‚    â”‚(Slow/expensive)â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5.2 Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
ÙŠØ¬Ø¨ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ§Ù„ÙŠØ© ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬:
1. **Ø¬ÙˆØ¯Ø© Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹**: Ù†Ø³Ø¨Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
2. **Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙˆÙ„ÙŠØ¯**: ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚ÙŠÙŠÙ… Ø¢Ù„ÙŠ (Ù…Ø«Ù„ LLM-as-a-Judge)
3. **Ø§Ù„Ø£Ø¯Ø§Ø¡**: Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©ØŒ ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ±Ø¬Ø§Ø¹ØŒ ÙˆÙ‚Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯
4. **Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ**: ØªÙƒÙ„ÙØ© ÙƒÙ„ Ø§Ø³ØªØ¹Ù„Ø§Ù… (Ø±Ù…ÙˆØ²ØŒ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…Ø§Øª)

### 5.3 Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
- **Ø§Ù„ØªÙ†Ø§Ø³Ù‚ (Consistency)**: Ø¶Ù…Ø§Ù† Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ³Ù‚Ø© Ù„Ù†ÙØ³ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø³ÙŠØ§Ù‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©
- **Ø§Ù„ØªØ­ÙŠØ² (Bias)**: Ø§ÙƒØªØ´Ø§Ù ÙˆØªØµØ­ÙŠØ­ Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
- **Ø§Ù„Ø£Ù…Ø§Ù† (Security)**: Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ø³ØªØºÙ„Ø§Ù„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙÙŠ Ù‡Ø¬Ù…Ø§Øª Ø­Ù‚Ù†
"""

"""
## 6. ØªØ­Ø¯ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©: Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ù…ØªØ·ÙˆØ±
### 6.1 Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø£ÙˆÙ„: ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… RAG Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰:
1. Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…Ø®ØªÙ„ÙØ© (Ù…Ø³ØªÙ†Ø¯Ø§ØªØŒ Ø±Ø³ÙˆÙ… Ø¨ÙŠØ§Ù†ÙŠØ©ØŒ Ø¬Ø¯Ø§ÙˆÙ„)
2. Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ
3. ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ† ÙÙŠ ÙƒÙ„ Ù…Ø¹Ù„ÙˆÙ…Ø©

### 6.2 Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø§Ù„ØºØ§Ù…Ø¶Ø©
Ø·ÙˆØ± Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰:
1. Ø·Ù„Ø¨ ØªÙˆØ¶ÙŠØ­ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
2. Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª Ø¨Ø¯ÙŠÙ„Ø©
3. ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ ÙŠÙ‚ÙŠÙ† Ø§Ù„Ù†Ø¸Ø§Ù…

### 6.3 Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ø«Ø§Ù„Ø«: Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù†Ù…Ø· Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
Ø£Ù†Ø´Ø¦ Ù†Ø¸Ø§Ù… Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰:
1. ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ù…Ø®ØªØµØ± Ù…Ù‚Ø§Ø¨Ù„ Ù…ÙØµÙ„)
2. ÙÙ‡Ù… Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø®Ø§Øµ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
3. Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø³ØªÙˆÙ‰ Ø®Ø¨Ø±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…

## 7. Ø§Ù„Ø®Ù„Ø§ØµØ©
Ø£Ù†Ø¸Ù…Ø© RAG Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© Ù„ÙŠØ³Øª Ù…Ø¬Ø±Ø¯ ØªØ­Ø³ÙŠÙ† Ù„Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø­Ø§Ù„ÙŠØ©ØŒ Ø¨Ù„ ØªÙ…Ø«Ù„ ØªØ­ÙˆÙ„Ø§Ù‹ ÙÙŠ ÙƒÙŠÙÙŠØ© ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ©. Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ GraphRAGØŒ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù‡Ø¬ÙŠÙ†ØŒ ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© ÙŠØ³Ù…Ø­ Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒÙ…Ø§ ÙŠÙØ¹Ù„ Ø§Ù„Ø¨Ø´Ø±: ÙÙ‡Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§ØªØŒ ÙˆØ§Ù„Ø±Ø¨Ø· Ø¨ÙŠÙ† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…ØŒ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚. Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ø¬Ù…Ø¹ Ø¨ÙŠÙ† Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„Ù‚ÙˆÙŠØ©ØŒ ÙˆÙ‡Ùˆ Ø¨Ø§Ù„Ø¶Ø¨Ø· Ù…Ø§ ÙŠÙ‡Ø¯Ù Ø¥Ù„ÙŠÙ‡ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹.
"""
```

## ðŸ“ scripts/data_preprocessing/generate_synthetic_data.py (Complete)

```python
"""
ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±
Ù‡Ø°Ø§ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠÙˆÙ„Ø¯ Ø¹Ø¯Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ø¯Ø¹Ù… Ø§Ù„ØªØ·ÙˆÙŠØ± ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
"""

import numpy as np
import pandas as pd
import random
from datetime import datetime, timedelta
import json
import os
from typing import List, Dict, Any, Tuple
import argparse
import logging

# ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("synthetic_data_generator")

class SyntheticDataGenerator:
    """Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©"""
    
    def __init__(self, seed: int = 42):
        self.seed = seed
        np.random.seed(seed)
        random.seed(seed)
    
    def generate_customer_data(self, n_samples: int = 1000) -> pd.DataFrame:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡
        
        Args:
            n_samples: Ø¹Ø¯Ø¯ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
        """
        logger.info(f"Generating customer data for {n_samples} samples...")
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª
        customer_ids = [f"CUST_{i:06d}" for i in range(n_samples)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
        first_names = ["Mohammed", "Ahmed", "Ali", "Fatima", "Aisha", "Omar", "Youssef", "Layla"]
        last_names = ["Al-Saud", "Al-Harbi", "Al-Qahtani", "Al-Rashid", "Al-Shammari", "Al-Ghamdi"]
        names = [f"{random.choice(first_names)} {random.choice(last_names)}" for _ in range(n_samples)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ù…Ø±
        ages = np.clip(np.random.normal(35, 10, n_samples).astype(int), 18, 80)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø³
        genders = np.random.choice(["Male", "Female"], n_samples, p=[0.52, 0.48])
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¯Ø®Ù„
        income_levels = ["Low", "Medium", "High"]
        income = np.random.choice(income_levels, n_samples, p=[0.3, 0.5, 0.2])
        annual_income = np.where(
            income == "Low", np.random.uniform(20000, 50000, n_samples),
            np.where(
                income == "Medium", np.random.uniform(50000, 100000, n_samples),
                np.random.uniform(100000, 300000, n_samples)
            )
        ).astype(int)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ‚Ø¹
        cities = ["Riyadh", "Jeddah", "Dammam", "Khobar", "Al-Khobar", "Mecca", "Medina"]
        cities = np.random.choice(cities, n_samples)
        regions = np.where(np.isin(cities, ["Riyadh", "Dammam", "Khobar", "Al-Khobar"]), "East", "West")
        
        # ØªÙˆÙ„ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø¹Ù…ÙŠÙ„
        customer_status = np.random.choice(["Active", "Inactive", "Churned"], n_samples, p=[0.7, 0.2, 0.1])
        
        # ØªÙˆÙ„ÙŠØ¯ ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ³Ø¬ÙŠÙ„
        base_date = datetime(2020, 1, 1)
        registration_dates = [
            base_date + timedelta(days=int(np.random.exponential(365)))
            for _ in range(n_samples)
        ]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¯Ø±Ø¬Ø© Ø§Ù„ÙˆÙ„Ø§Ø¡
        loyalty_scores = np.clip(np.random.normal(0.7, 0.2, n_samples), 0, 1)
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
        df = pd.DataFrame({
            'customer_id': customer_ids,
            'name': names,
            'age': ages,
            'gender': genders,
            'income_level': income,
            'annual_income': annual_income,
            'city': cities,
            'region': regions,
            'customer_status': customer_status,
            'registration_date': registration_dates,
            'loyalty_score': loyalty_scores
        })
        
        logger.info("Customer data generated successfully")
        return df
    
    def generate_transaction_data(self, customer_ids: List[str], n_transactions: int = 5000) -> pd.DataFrame:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        
        Args:
            customer_ids: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
            n_transactions: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        """
        logger.info(f"Generating transaction data for {n_transactions} transactions...")
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        transaction_ids = [f"TRX_{i:08d}" for i in range(n_transactions)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
        selected_customers = np.random.choice(customer_ids, n_transactions)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØ§Ø±ÙŠØ®
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2025, 1, 1)
        date_range = (end_date - start_date).days
        transaction_dates = [
            start_date + timedelta(days=int(np.random.uniform(0, date_range)))
            for _ in range(n_transactions)
        ]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        transaction_types = ["Purchase", "Refund", "Subscription", "Payment", "Withdrawal"]
        types = np.random.choice(transaction_types, n_transactions, p=[0.6, 0.1, 0.2, 0.05, 0.05])
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¨Ø§Ù„Øº Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        amounts = np.zeros(n_transactions)
        for i, t_type in enumerate(types):
            if t_type == "Purchase":
                amounts[i] = np.random.uniform(10, 500)
            elif t_type == "Refund":
                amounts[i] = -np.random.uniform(10, 300)
            elif t_type == "Subscription":
                amounts[i] = np.random.uniform(50, 200)
            elif t_type == "Payment":
                amounts[i] = np.random.uniform(100, 1000)
            elif t_type == "Withdrawal":
                amounts[i] = -np.random.uniform(50, 500)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        categories = ["Electronics", "Clothing", "Food", "Entertainment", "Home", "Travel"]
        product_categories = np.random.choice(categories, n_transactions)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©
        status = np.random.choice(["Completed", "Pending", "Failed"], n_transactions, p=[0.92, 0.05, 0.03])
        
        # ØªÙˆÙ„ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆÙ„Ø§Ø¡
        loyalty_points = np.where(
            status == "Completed",
            np.maximum(0, (amounts * 0.1).astype(int)),
            0
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
        df = pd.DataFrame({
            'transaction_id': transaction_ids,
            'customer_id': selected_customers,
            'transaction_date': transaction_dates,
            'transaction_type': types,
            'amount': np.round(amounts, 2),
            'product_category': product_categories,
            'status': status,
            'loyalty_points': loyalty_points
        })
        
        # ÙØ±Ø² Ø­Ø³Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©
        df = df.sort_values('transaction_date').reset_index(drop=True)
        
        logger.info("Transaction data generated successfully")
        return df
    
    def generate_product_data(self, n_products: int = 500) -> pd.DataFrame:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù„Ù„Ù…Ù†ØªØ¬Ø§Øª
        
        Args:
            n_products: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        """
        logger.info(f"Generating product data for {n_products} products...")
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        product_ids = [f"PROD_{i:05d}" for i in range(n_products)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        adjectives = ["Premium", "Luxury", "Basic", "Smart", "Wireless", "Portable", "Durable"]
        nouns = ["Headphones", "Watch", "Phone", "Laptop", "Shoes", "Bag", "Camera", "Speaker"]
        product_names = [f"{random.choice(adjectives)} {random.choice(nouns)}" for _ in range(n_products)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙØ¦Ø§Øª
        categories = ["Electronics", "Clothing", "Food", "Entertainment", "Home", "Travel"]
        product_categories = np.random.choice(categories, n_products, p=[0.3, 0.2, 0.1, 0.1, 0.2, 0.1])
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¹Ø§Ø±
        base_prices = {
            "Electronics": (200, 2000),
            "Clothing": (20, 200),
            "Food": (5, 50),
            "Entertainment": (10, 100),
            "Home": (30, 300),
            "Travel": (50, 500)
        }
        
        prices = []
        for cat in product_categories:
            min_price, max_price = base_prices[cat]
            price = np.random.uniform(min_price, max_price)
            prices.append(round(price, 2))
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ
        costs = [price * np.random.uniform(0.3, 0.7) for price in prices]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙ…ÙŠØ§Øª
        quantities = np.random.poisson(100, n_products)
        quantities = np.maximum(quantities, 0)  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ÙƒÙ…ÙŠØ§Øª Ø³Ø§Ù„Ø¨Ø©
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…Ø§Øª
        ratings = np.clip(np.random.normal(4.0, 0.8, n_products), 1.0, 5.0)
        ratings = np.round(ratings, 1)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª
        review_counts = np.random.poisson(50, n_products)
        review_counts = np.maximum(review_counts, 0)
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
        df = pd.DataFrame({
            'product_id': product_ids,
            'product_name': product_names,
            'category': product_categories,
            'price': prices,
            'cost': np.round(costs, 2),
            'quantity_in_stock': quantities,
            'rating': ratings,
            'review_count': review_counts
        })
        
        logger.info("Product data generated successfully")
        return df
    
    def generate_medical_records(self, n_records: int = 1000) -> pd.DataFrame:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø³Ø¬Ù„Ø§Øª Ø·Ø¨ÙŠØ© Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
        
        Args:
            n_records: Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        
        Returns:
            DataFrame ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
        """
        logger.info(f"Generating medical records for {n_records} patients...")
        
        # ØªÙˆÙ„ÙŠØ¯ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…Ø±Ø¶Ù‰
        patient_ids = [f"PAT_{i:06d}" for i in range(n_records)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
        first_names = ["Mohammed", "Ahmed", "Ali", "Fatima", "Aisha", "Omar", "Youssef", "Layla", "Khalid", "Noura"]
        last_names = ["Al-Saud", "Al-Harbi", "Al-Qahtani", "Al-Rashid", "Al-Shammari", "Al-Ghamdi", "Al-Otaibi"]
        names = [f"{random.choice(first_names)} {random.choice(last_names)}" for _ in range(n_records)]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ù…Ø±
        ages = np.clip(np.random.normal(45, 15, n_records).astype(int), 18, 90)
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¬Ù†Ø³
        genders = np.random.choice(["Male", "Female"], n_records, p=[0.48, 0.52])
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶
        medical_conditions = [
            "Hypertension", "Diabetes", "Asthma", "Heart Disease", 
            "Arthritis", "Migraine", "Depression", "Anxiety",
            "Obesity", "Hyperlipidemia"
        ]
        conditions = []
        for _ in range(n_records):
            num_conditions = np.random.poisson(1.2)
            num_conditions = max(0, min(num_conditions, 5))  # Ø¨ÙŠÙ† 0 Ùˆ 5 Ø­Ø§Ù„Ø§Øª
            patient_conditions = random.sample(medical_conditions, num_conditions)
            conditions.append(", ".join(patient_conditions))
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø¯ÙˆÙŠØ©
        medications = [
            "Lisinopril", "Metformin", "Albuterol", "Aspirin",
            "Ibuprofen", "Sertraline", "Atorvastatin", "Levothyroxine",
            "Omeprazole", "Amlodipine"
        ]
        medications_list = []
        for _ in range(n_records):
            num_medications = np.random.poisson(1.5)
            num_medications = max(0, min(num_medications, 6))  # Ø¨ÙŠÙ† 0 Ùˆ 6 Ø£Ø¯ÙˆÙŠØ©
            patient_meds = random.sample(medications, num_medications)
            medications_list.append(", ".join(patient_meds))
        
        # ØªÙˆÙ„ÙŠØ¯ Ù‚Ø±Ø§Ø¡Ø§Øª Ø¶ØºØ· Ø§Ù„Ø¯Ù…
        systolic_bp = np.clip(np.random.normal(120 + 0.5 * ages, 15), 90, 200).astype(int)
        diastolic_bp = np.clip(np.random.normal(80 + 0.3 * ages, 10), 60, 120).astype(int)
        
        # ØªÙˆÙ„ÙŠØ¯ Ù‚Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø³ÙƒØ±
        blood_sugar = np.clip(np.random.normal(100 + 0.8 * (ages > 50).astype(int) * 20, 25), 70, 300)
        
        # ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ø²ÙŠØ§Ø±Ø§Øª
        visit_dates = [
            datetime(2023, 1, 1) + timedelta(days=int(np.random.uniform(0, 730)))
            for _ in range(n_records)
        ]
        
        # ØªÙˆÙ„ÙŠØ¯ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        risk_scores = np.clip(
            0.3 * (ages > 60).astype(int) +
            0.4 * (np.array([len(c.split(',')) for c in conditions]) > 2).astype(int) +
            0.3 * (blood_sugar > 140).astype(int),
            0, 1
        )
        
        # Ø¥Ù†Ø´Ø§Ø¡ DataFrame
        df = pd.DataFrame({
            'patient_id': patient_ids,
            'name': names,
            'age': ages,
            'gender': genders,
            'medical_conditions': conditions,
            'medications': medications_list,
            'systolic_bp': systolic_bp,
            'diastolic_bp': diastolic_bp,
            'blood_sugar': np.round(blood_sugar, 1),
            'visit_date': visit_dates,
            'risk_score': np.round(risk_scores, 2)
        })
        
        logger.info("Medical records generated successfully")
        return df
    
    def generate_legal_documents(self, n_documents: int = 100) -> List[Dict[str, Any]]:
        """
        ØªÙˆÙ„ÙŠØ¯ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
        
        Args:
            n_documents: Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        """
        logger.info(f"Generating legal documents for {n_documents} documents...")
        
        document_types = ["Contract", "Agreement", "Affidavit", "Will", "Lease", "NDA"]
        parties = ["Company A", "Company B", "Individual X", "Individual Y", "Government Entity"]
        jurisdictions = ["Saudi Arabia", "UAE", "Egypt", "Kuwait", "Qatar"]
        
        documents = []
        for i in range(n_documents):
            doc_type = random.choice(document_types)
            title = f"{doc_type} between {random.choice(parties)} and {random.choice(parties)}"
            jurisdiction = random.choice(jurisdictions)
            date = datetime(2020, 1, 1) + timedelta(days=int(np.random.uniform(0, 1825)))
            
            # ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ø§Ù„Ù…Ø³ØªÙ†Ø¯
            clauses = []
            num_clauses = random.randint(5, 15)
            for j in range(num_clauses):
                clause_types = [
                    f"Clause {j+1}: The parties agree to the terms outlined herein.",
                    f"Clause {j+1}: This agreement shall be governed by the laws of {jurisdiction}.",
                    f"Clause {j+1}: Any disputes shall be resolved through arbitration in {jurisdiction}.",
                    f"Clause {j+1}: The term of this agreement shall be for a period of {random.randint(1, 5)} years.",
                    f"Clause {j+1}: Confidential information shall not be disclosed to third parties."
                ]
                clauses.append(random.choice(clause_types))
            
            content = "\n\n".join(clauses)
            
            documents.append({
                'document_id': f"DOC_{i:06d}",
                'title': title,
                'type': doc_type,
                'jurisdiction': jurisdiction,
                'date': date.strftime("%Y-%m-%d"),
                'content': content,
                'metadata': {
                    'word_count': len(content.split()),
                    'clause_count': num_clauses,
                    'created_at': datetime.now().isoformat()
                }
            })
        
        logger.info("Legal documents generated successfully")
        return documents
    
    def save_data(self, output_dir: str = "data/synthetic"):
        """
        Ø­ÙØ¸ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
        
        Args:
            output_dir: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        """
        logger.info(f"Saving synthetic data to {output_dir}...")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
        os.makedirs(output_dir, exist_ok=True)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡
        customer_df = self.generate_customer_data(1000)
        customer_df.to_csv(os.path.join(output_dir, "customers.csv"), index=False)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        transaction_df = self.generate_transaction_data(customer_df['customer_id'].tolist(), 5000)
        transaction_df.to_csv(os.path.join(output_dir, "transactions.csv"), index=False)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª
        product_df = self.generate_product_data(500)
        product_df.to_csv(os.path.join(output_dir, "products.csv"), index=False)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©
        medical_df = self.generate_medical_records(1000)
        medical_df.to_csv(os.path.join(output_dir, "medical_records.csv"), index=False)
        
        # ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©
        legal_docs = self.generate_legal_documents(100)
        with open(os.path.join(output_dir, "legal_documents.json"), 'w', encoding='utf-8') as f:
            json.dump(legal_docs, f, ensure_ascii=False, indent=2)
        
        logger.info("All synthetic data saved successfully")

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(description='Generate synthetic data for AI Engineer Toolkit')
    parser.add_argument('--output_dir', type=str, default='data/synthetic', 
                        help='Output directory for synthetic data')
    parser.add_argument('--seed', type=int, default=42, 
                        help='Random seed for reproducibility')
    parser.add_argument('--sample_size', type=int, default=1000,
                        help='Base sample size for datasets')
    
    args = parser.parse_args()
    
    logger.info("Starting synthetic data generation...")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Random seed: {args.seed}")
    
    generator = SyntheticDataGenerator(seed=args.seed)
    generator.save_data(args.output_dir)
    
    logger.info("Synthetic data generation completed successfully!")

if __name__ == "__main__":
    main()
```

## ðŸ“ src/llm/attention.py (Complete)

```python
"""
ØªÙ†ÙÙŠØ° Ù…ØªÙ‚Ø¯Ù… Ù„Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙØ§Ø¡Ø©ØŒ Ø§Ù„ØªØ®ØµÙŠØµØŒ ÙˆØ§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List, Dict, Any
import math
import logging
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class AttentionConfig:
    """ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡"""
    hidden_size: int
    num_attention_heads: int
    head_dim: int
    max_position_embeddings: int = 4096
    attention_dropout: float = 0.1
    hidden_dropout: float = 0.1
    use_flash_attention: bool = False
    rotary_embedding_base: int = 10000
    rotary_embedding_fraction: float = 1.0  # Ù†Ø³Ø¨Ø© Ø§Ù„Ø¨ÙØ¹Ø¯ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„ØªØ¯ÙˆÙŠØ±
    attention_type: str = "scaled_dot_product"  # "scaled_dot_product", "linear", "performer"
    kv_cache_enabled: bool = True

class RotaryPositionEmbedding:
    """Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ (Rotary Position Embedding)"""
    
    def __init__(self, config: AttentionConfig):
        self.dim = int(config.head_dim * config.rotary_embedding_fraction)
        self.base = config.rotary_embedding_base
        self.max_seq_len = config.max_position_embeddings
        
        # Ø­Ø³Ø§Ø¨ ØªØ±Ø¯Ø¯Ø§Øª Ø§Ù„ØªÙˆØ³ÙŠØ¹
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))
        self.register_buffer("inv_freq", inv_freq)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ù…Ø³Ø¨Ù‚Ù‹Ø§
        self._set_cos_sin_cache(seq_len=self.max_seq_len)
    
    def _set_cos_sin_cache(self, seq_len: int):
        """Ø­Ø³Ø§Ø¨ Ù‚ÙŠÙ… Ø¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… ÙˆØ¬ÙŠØ¨ Ø§Ù„ØªÙ…Ø§Ù… Ù…Ø³Ø¨Ù‚Ù‹Ø§"""
        t = torch.arange(seq_len).float()
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :])
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :])
    
    def rotate_half(self, x: torch.Tensor) -> torch.Tensor:
        """ØªØ¯ÙˆÙŠØ± Ù†ØµÙ Ø§Ù„Ø¨ÙØ¹Ø¯"""
        x1, x2 = x[..., :self.dim // 2], x[..., self.dim // 2:]
        return torch.cat((-x2, x1), dim=-1)
    
    def apply_rotary_pos_emb(self, q: torch.Tensor, k: torch.Tensor, 
                           position_ids: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª Q Ùˆ K
        
        Args:
            q: Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… [batch_size, num_heads, seq_len, head_dim]
            k: Ù…ØªØ¬Ù‡Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ [batch_size, num_heads, seq_len, head_dim]
            position_ids: Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        
        Returns:
            Q Ùˆ K Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹
        """
        seq_len = q.shape[2]
        
        if position_ids is not None:
            # Ø¥Ø¹Ø§Ø¯Ø© Ø­Ø³Ø§Ø¨ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
            cos = self.cos_cached[:, :, position_ids, :]
            sin = self.sin_cached[:, :, position_ids, :]
        else:
            if seq_len > self.max_seq_len:
                self._set_cos_sin_cache(seq_len)
            
            cos = self.cos_cached[:, :, :seq_len, :]
            sin = self.sin_cached[:, :, :seq_len, :]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹
        q_embed = (q * cos) + (self.rotate_half(q) * sin)
        k_embed = (k * cos) + (self.rotate_half(k) * sin)
        
        return q_embed, k_embed

class AttentionMechanism(nn.Module):
    """Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
    
    def __init__(self, config: AttentionConfig):
        super().__init__()
        self.config = config
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªÙˆØ§ÙÙ‚
        assert config.hidden_size % config.num_attention_heads == 0, \
            "Hidden size must be divisible by number of attention heads"
        
        self.head_dim = config.head_dim
        self.num_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        
        # Ù…ØµÙÙˆÙØ§Øª Ø§Ù„ØªØ­ÙˆÙŠÙ„
        self.q_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.k_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ
        self.rotary_emb = RotaryPositionEmbedding(config)
        
        # Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        self._init_weights()
    
    def _init_weights(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        nn.init.xavier_uniform_(self.q_proj.weight)
        nn.init.xavier_uniform_(self.k_proj.weight)
        nn.init.xavier_uniform_(self.v_proj.weight)
        nn.init.xavier_uniform_(self.o_proj.weight)
    
    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int) -> torch.Tensor:
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„ØªÙ†Ø³ÙˆØ± Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø±Ø¤ÙˆØ³"""
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """
        ØªÙ…Ø±ÙŠØ± Ø£Ù…Ø§Ù…ÙŠ Ù„Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        
        Args:
            hidden_states: Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ø®ÙÙŠØ© [batch_size, seq_len, hidden_size]
            attention_mask: Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            position_ids: Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            past_key_value: Ù‚ÙŠÙ… Ø§Ù„Ù…ÙØªØ§Ø­/Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            output_attentions: Ø¥Ø®Ø±Ø§Ø¬ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø£Ù… Ù„Ø§
        
        Returns:
            Ø§Ù„Ù…Ø®Ø±Ø¬Ø§ØªØŒ Ù…ØµÙÙˆÙØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)ØŒ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
        """
        bsz, q_len, _ = hidden_states.size()
        
        # Ø­Ø³Ø§Ø¨ QØŒ KØŒ V
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø¥Ù„Ù‰ [batch_size, num_heads, seq_len, head_dim]
        query_states = self._shape(query_states, q_len, bsz)
        key_states = self._shape(key_states, q_len, bsz)
        value_states = self._shape(value_states, q_len, bsz)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
        if position_ids is not None:
            query_states, key_states = self.rotary_emb.apply_rotary_pos_emb(
                query_states, key_states, position_ids
            )
        
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ KV
        if past_key_value is not None and self.config.kv_cache_enabled:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attn_output, attn_weights = self._compute_attention(
            query_states, key_states, value_states, attention_mask
        )
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø®Ø±Ø¬
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        past_key_value = (key_states, value_states) if self.config.kv_cache_enabled else None
        
        return attn_output, attn_weights, past_key_value
    
    def _compute_attention(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¢Ù„ÙŠØ§Øª Ù…Ø®ØªÙ„ÙØ©"""
        
        if self.config.attention_type == "scaled_dot_product":
            return self._scaled_dot_product_attention(q, k, v, attention_mask)
        elif self.config.attention_type == "linear":
            return self._linear_attention(q, k, v, attention_mask)
        elif self.config.attention_type == "performer":
            return self._performer_attention(q, k, v, attention_mask)
        else:
            raise ValueError(f"Unsupported attention type: {self.config.attention_type}")
    
    def _scaled_dot_product_attention(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù†ØªØ¬ Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ù…ÙÙ‚ÙŠØ§Ø³"""
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ [batch_size, num_heads, q_len, k_len]
        attn_weights = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.head_dim)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        
        # ØªØ·Ø¨ÙŠÙ‚ softmax
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).to(q.dtype)
        
        # ØªØ·Ø¨ÙŠÙ‚ dropout
        attn_weights = F.dropout(attn_weights, p=self.config.attention_dropout, training=self.training)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø±Ø¬Ø­Ø©
        attn_output = torch.matmul(attn_weights, v)
        
        return attn_output, attn_weights
    
    def _linear_attention(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø®Ø·ÙŠØ©
        Ù…ÙÙŠØ¯ Ù„Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§ Ø­ÙŠØ« O(n^2) ØºÙŠØ± Ø¹Ù…Ù„ÙŠ
        """
        # ØªØ·Ø¨ÙŠØ¹ Q Ùˆ K
        q = F.elu(q) + 1
        k = F.elu(k) + 1
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§Ù…
        kv = torch.einsum("bhld,bhle->bhde", k, v)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¨Ø³Ø·
        z = 1.0 / (torch.einsum("bhld,bhl->bhd", q, k.sum(dim=2)) + 1e-6)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        attn_output = torch.einsum("bhld,bhde,bhd->bhle", q, kv, z)
        
        return attn_output, None
    
    def _performer_attention(
        self,
        q: torch.Tensor,
        k: torch.Tensor,
        v: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Performer
        ÙŠÙ‚Ø¯Ù… ØªÙ‚Ø¯ÙŠØ±Ù‹Ø§ Ù„Ù€ softmax attention Ù…Ø¹ O(n) ØªØ¹Ù‚ÙŠØ¯
        """
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… random Fourier features
        m = 64  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        projection = torch.randn(self.head_dim, m).to(q.device) * math.sqrt(2 / m)
        
        # ØªØ­ÙˆÙŠÙ„ Q Ùˆ K
        q_proj = torch.einsum("bhld, dm -> bhlm", q, projection)
        k_proj = torch.einsum("bhld, dm -> bhlm", k, projection)
        
        # ØªØ·Ø¨ÙŠÙ‚ softmax Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ
        q_feat = F.softmax(q_proj, dim=-1)
        k_feat = F.softmax(k_proj, dim=-1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        kv = torch.einsum("bhld,bhle->bhde", k_feat, v)
        z = torch.einsum("bhld,bhl->bhd", q_feat, k_feat.sum(dim=2))
        
        attn_output = torch.einsum("bhld,bhde,bhd->bhle", q_feat, kv, 1.0 / (z + 1e-6))
        
        return attn_output, None

class MultiQueryAttention(AttentionMechanism):
    """
    Multi-Query Attention: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø¤ÙˆØ³ Ø§Ù†ØªØ¨Ø§Ù‡ Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…
    Ù…Ø¹ Ù…ÙØªØ§Ø­ ÙˆÙ‚ÙŠÙ…Ø© Ù…Ø´ØªØ±ÙƒÙŠÙ†. ÙŠÙ‚Ù„Ù„ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±.
    """
    
    def __init__(self, config: AttentionConfig):
        super().__init__(config)
        
        # ÙÙŠ Multi-QueryØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø±Ø£Ø³Ù‹Ø§ ÙˆØ§Ø­Ø¯Ù‹Ø§ Ù„Ù€ K Ùˆ V
        self.k_proj = nn.Linear(config.hidden_size, config.head_dim, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, config.head_dim, bias=False)
        
        self._init_weights()
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù„Ù€ Multi-Query Attention"""
        
        bsz, q_len, _ = hidden_states.size()
        
        # Ø­Ø³Ø§Ø¨ QØŒ KØŒ V
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Q Ø¥Ù„Ù‰ [batch_size, num_heads, seq_len, head_dim]
        query_states = self._shape(query_states, q_len, bsz)
        
        # K Ùˆ V Ù„Ù‡Ø§ Ø¨Ø¹Ø¯ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· [batch_size, 1, seq_len, head_dim]
        key_states = key_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, 1, self.head_dim).transpose(1, 2)
        
        # ØªÙƒØ±Ø§Ø± K Ùˆ V Ù„Ù€ num_heads
        key_states = key_states.expand(-1, self.num_heads, -1, -1)
        value_states = value_states.expand(-1, self.num_heads, -1, -1)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
        if position_ids is not None:
            query_states, key_states = self.rotary_emb.apply_rotary_pos_emb(
                query_states, key_states, position_ids
            )
        
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ KV
        if past_key_value is not None and self.config.kv_cache_enabled:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attn_output, attn_weights = self._compute_attention(
            query_states, key_states, value_states, attention_mask
        )
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø®Ø±Ø¬
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        past_key_value = (key_states, value_states) if self.config.kv_cache_enabled else None
        
        return attn_output, attn_weights, past_key_value

class GroupedQueryAttention(AttentionMechanism):
    """
    Grouped-Query Attention (GQA): ØªØ¬Ù…ÙŠØ¹ Ø±Ø¤ÙˆØ³ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    Ù„ØªØ­Ù‚ÙŠÙ‚ ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø¬ÙˆØ¯Ø© Multi-Head ÙˆÙØ¹Ø§Ù„ÙŠØ© Multi-Query
    """
    
    def __init__(self, config: AttentionConfig, num_key_value_heads: int):
        """
        Args:
            config: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            num_key_value_heads: Ø¹Ø¯Ø¯ Ø±Ø¤ÙˆØ³ Ø§Ù„Ù…ÙØªØ§Ø­/Ø§Ù„Ù‚ÙŠÙ…Ø© (ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙ‚Ø³Ù… num_attention_heads)
        """
        super().__init__(config)
        self.num_key_value_heads = num_key_value_heads
        self.num_key_value_groups = config.num_attention_heads // num_key_value_heads
        
        # ÙÙŠ GQAØŒ Ù†Ø³ØªØ®Ø¯Ù… num_key_value_heads Ù„Ù€ K Ùˆ V
        self.k_proj = nn.Linear(config.hidden_size, self.head_dim * num_key_value_heads, bias=False)
        self.v_proj = nn.Linear(config.hidden_size, self.head_dim * num_key_value_heads, bias=False)
        
        self._init_weights()
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù„Ù€ Grouped-Query Attention"""
        
        bsz, q_len, _ = hidden_states.size()
        
        # Ø­Ø³Ø§Ø¨ QØŒ KØŒ V
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Q Ø¥Ù„Ù‰ [batch_size, num_heads, seq_len, head_dim]
        query_states = self._shape(query_states, q_len, bsz)
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ K Ùˆ V Ø¥Ù„Ù‰ [batch_size, num_key_value_heads, seq_len, head_dim]
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠ Ù„Ù„Ù…ÙˆØ§Ù‚Ø¹
        if position_ids is not None:
            query_states, key_states = self.rotary_emb.apply_rotary_pos_emb(
                query_states, key_states, position_ids
            )
        
        # Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù€ KV
        if past_key_value is not None and self.config.kv_cache_enabled:
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        
        # ØªÙƒØ±Ø§Ø± K Ùˆ V Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©
        key_states = key_states.repeat_interleave(self.num_key_value_groups, dim=1)
        value_states = value_states.repeat_interleave(self.num_key_value_groups, dim=1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attn_output, attn_weights = self._compute_attention(
            query_states, key_states, value_states, attention_mask
        )
        
        # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø®Ø±Ø¬
        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, q_len, self.hidden_size)
        attn_output = self.o_proj(attn_output)
        
        # Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        past_key_value = (key_states, value_states) if self.config.kv_cache_enabled else None
        
        return attn_output, attn_weights, past_key_value

class FlashAttentionWrapper(nn.Module):
    """
    Ù…ØºÙ„Ù Ù„Ù€ FlashAttention Ù„ØªØ­Ù‚ÙŠÙ‚ Ø£Ù‚ØµÙ‰ ÙƒÙØ§Ø¡Ø© Ù„Ù€ GPU
    ÙŠØªØ·Ù„Ø¨ CUDA 11.4+ Ùˆ arcfacilities
    """
    
    def __init__(self, attention_mechanism: AttentionMechanism):
        super().__init__()
        self.attention = attention_mechanism
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
        output_attentions: bool = False,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor, torch.Tensor]]]:
        """Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FlashAttention"""
        
        try:
            import flash_attn
            from flash_attn.flash_attn_interface import flash_attn_func
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
            q, k, v = self.attention._prepare_flash_attention_inputs(
                hidden_states, attention_mask, position_ids, past_key_value
            )
            
            # ØªÙ†ÙÙŠØ° FlashAttention
            attn_output = flash_attn_func(
                q, k, v,
                dropout_p=self.attention.config.attention_dropout if self.training else 0.0,
                softmax_scale=1.0 / math.sqrt(self.attention.head_dim),
                causal=True if attention_mask is not None else False
            )
            
            # Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ÙƒÙŠÙ„ Ø§Ù„Ø®Ø±Ø¬
            bsz, q_len, _, _ = hidden_states.size()
            attn_output = attn_output.view(bsz, q_len, self.attention.hidden_size)
            attn_output = self.attention.o_proj(attn_output)
            
            return attn_output, None, past_key_value
            
        except ImportError:
            logger.warning("FlashAttention not available, falling back to standard implementation")
            return self.attention(
                hidden_states, attention_mask, position_ids, past_key_value, output_attentions
            )

def create_attention_layer(config: AttentionConfig, attention_type: str = "standard") -> nn.Module:
    """
    Ù…ØµÙ†Ø¹ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
    
    Args:
        config: ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        attention_type: Ù†ÙˆØ¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ ("standard", "multi_query", "grouped_query", "flash")
    
    Returns:
        Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©
    """
    if attention_type == "standard":
        attention = AttentionMechanism(config)
    elif attention_type == "multi_query":
        attention = MultiQueryAttention(config)
    elif attention_type == "grouped_query":
        # Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: ØªÙ‚Ø³ÙŠÙ… num_heads Ø¥Ù„Ù‰ 4 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
        num_key_value_heads = max(1, config.num_attention_heads // 4)
        attention = GroupedQueryAttention(config, num_key_value_heads)
    else:
        raise ValueError(f"Unsupported attention type: {attention_type}")
    
    # ØªØºÙ„ÙŠÙ Ø¨Ù€ FlashAttention Ø¥Ø°Ø§ Ù…Ø·Ù„ÙˆØ¨
    if config.use_flash_attention and attention_type != "flash":
        try:
            return FlashAttentionWrapper(attention)
        except ImportError:
            logger.warning("FlashAttention not available, using standard implementation")
    
    return attention

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†
    config = AttentionConfig(
        hidden_size=768,
        num_attention_heads=12,
        head_dim=64,
        attention_dropout=0.1,
        use_flash_attention=False
    )
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
    attention_layer = create_attention_layer(config, attention_type="grouped_query")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
    batch_size = 2
    seq_len = 10
    hidden_states = torch.randn(batch_size, seq_len, config.hidden_size)
    position_ids = torch.arange(seq_len).expand(batch_size, -1)
    
    # Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
    outputs = attention_layer(
        hidden_states=hidden_states,
        position_ids=position_ids,
        output_attentions=True
    )
    
    attn_output, attn_weights, past_kv = outputs
    print(f"Attention output shape: {attn_output.shape}")
    print(f"Attention weights shape: {attn_weights.shape if attn_weights is not None else None}")
```

## ðŸ“ notebooks/07_system_design/01_fraud_detection_system.ipynb (Complete)

```python
# =====================
# ØªØµÙ…ÙŠÙ… Ù†Ø¸Ø§Ù… ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ: Ù…Ù† Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø¥Ù„Ù‰ Ø§Ù„ØªÙ†ÙÙŠØ°
# Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù†Ø¸Ø§Ù…ÙŠØ© -> Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ -> Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ -> Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©
# =====================

"""
## 1. ÙÙ„Ø³ÙØ© ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ
Ø£ØµØ¨Ø­ Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ù…Ø§Ù„ÙŠ ØªØ­Ø¯ÙŠÙ‹Ø§ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠÙ‹Ø§ Ù„Ù„Ø´Ø±ÙƒØ§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ©ØŒ Ø­ÙŠØ« ØªØ´ÙŠØ± Ø§Ù„ØªÙ‚Ø¯ÙŠØ±Ø§Øª Ø¥Ù„Ù‰ Ø£Ù† Ø§Ù„Ø®Ø³Ø§Ø¦Ø± Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø³ØªØµÙ„ Ø¥Ù„Ù‰ 48 Ù…Ù„ÙŠØ§Ø± Ø¯ÙˆÙ„Ø§Ø± ÙÙŠ 2025. ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù„Ù… ØªØ¹Ø¯ Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø«Ø§Ø¨ØªØ© (Rule-based Systems) ÙƒØ§ÙÙŠØ©ØŒ Ø­ÙŠØ« ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ù…Ø­ØªØ§Ù„ÙˆÙ† ØªØ¬Ø§ÙˆØ²Ù‡Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø©. ÙŠØ¹ØªÙ…Ø¯ Ø­Ù„ Ø§Ù„Ø¹ØµØ± Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ Ù…Ø¹ Ø¨Ù†ÙŠØ© ØªØ­ØªÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹.

Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù„Ø£Ù†Ø¸Ù…Ø© ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„:
- **Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø±Ø§Ø­Ø©**: ÙƒÙ„ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø®Ø§Ø·Ø¦Ø© (False Positive) ØªØ¹Ù†ÙŠ Ø¥Ø²Ø¹Ø§Ø¬ Ø¹Ù…ÙŠÙ„ Ø´Ø±Ø¹ÙŠ
- **Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ø³Ø±ÙŠØ¹ Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„**: ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù… Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±
- **Ù…ØªØ·Ù„Ø¨Ø§Øª Ø²Ù…Ù†ÙŠØ© ØµØ§Ø±Ù…Ø©**: Ù‚Ø±Ø§Ø± Ø§Ù„ÙƒØ´Ù ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØ®Ø° ÙÙŠ Ø£Ù‚Ù„ Ù…Ù† 200 Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
- **Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø³ÙŠØ§Ù‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©**: Ø£Ù†Ù…Ø§Ø· Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø´Ø±Ø§Ø¡ Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØµØ±ÙÙŠØ©

ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ±ØŒ Ø³Ù†Ø¨Ù†ÙŠ Ù†Ø¸Ø§Ù… ÙƒØ´Ù Ø§Ø­ØªÙŠØ§Ù„ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ø±Ø¨Ø¹Ø©: Ø§Ù„Ø³Ø±Ø¹Ø©ØŒ Ø§Ù„Ø¯Ù‚Ø©ØŒ Ø§Ù„Ù‚Ø§Ø¨Ù„ÙŠØ© Ù„Ù„Ø´Ø±Ø­ØŒ ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©.
"""

"""
## 2. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„
ÙÙ‡Ù… Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙˆØ±Ø§Ø¡ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ù‡Ùˆ Ø£Ø³Ø§Ø³ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„ÙØ¹Ø§Ù„.

### 2.1 Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø´Ø°ÙˆØ° (Anomaly Detection Theory)
Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙŠ ØªÙ†Ø­Ø±Ù Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ø¹Ù† Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù†Ø³ØªØ®Ø¯Ù…:
- **Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª (Mahalanobis Distance)**:
  $$D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)}$$
  Ø­ÙŠØ« $\mu$ Ù‡Ùˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…ØªØ¬Ù‡ØŒ Ùˆ $\Sigma$ Ù‡Ùˆ Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØºØ§ÙŠØ±. Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø³Ø§ÙØ© ØªØ£Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„ØªØ¨Ø§ÙŠÙ†Ø§Øª ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª.

- **Ø§Ù„ÙƒØ«Ø§ÙØ© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© (Probability Density)**:
  $$p(x) = \frac{1}{(2\pi)^{d/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\right)$$
  Ø§Ù„Ù†Ù‚Ø§Ø· Ø°Ø§Øª Ø§Ù„ÙƒØ«Ø§ÙØ© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© ØªÙØ¹ØªØ¨Ø± Ø´Ø§Ø°Ø©.

### 2.2 ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ù„Ù„Ø³Ù„ÙˆÙƒ
Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠØ© ÙƒØ³Ù„ÙˆÙƒ Ù„Ø§ ÙŠØªØ¨Ø¹ Ù†Ù…Ø·Ù‹Ø§ Ø·Ø¨ÙŠØ¹ÙŠÙ‹Ø§:
- **ØªØ­ÙˆÙŠÙ„ ÙÙˆØ±ÙŠÙŠÙ‡ Ø§Ù„Ø³Ø±ÙŠØ¹ (FFT)** Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªØ±Ø¯Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¯ÙŠØ©
- **Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· Ø§Ù„Ø²Ù…Ù†ÙŠ (Auto-correlation)** Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø© ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
- **Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø²Ù…Ù†ÙŠ (Time-series Regression)** Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø³Ù„ÙˆÙƒ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹

### 2.3 Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª
Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØ´Ù…Ù„ Ø´Ø¨ÙƒØ§Øª Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªØ±Ø§Ø¨Ø·Ø©:
- **Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ÙƒØ´Ù Ø§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª (Community Detection)** Ù…Ø«Ù„ Louvain Algorithm
- **Ù†Ù‚Ø§Ø· Ø§Ù„ÙˆØ³Ø·ÙŠØ© (Centrality Measures)** Ù…Ø«Ù„ Betweenness Centrality Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø­ÙˆØ±ÙŠØ©
- **Ø§Ù„ØªØ¯ÙÙ‚ Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ© (Network Flow Algorithms)** Ù„Ù„ÙƒØ´Ù Ø¹Ù† ØªØ­Ø±ÙƒØ§Øª Ø§Ù„Ø£Ù…ÙˆØ§Ù„ Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©

### 2.4 ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø¹Ø±Ø¶Ø© Ù„Ù„Ø®Ø·Ø± (Risk Value Analysis)
ØªÙØ­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙŠØºØ© Ù…ÙØ­Ø³Ù‘Ù†Ø©:
$$\text{RiskScore} = w_1 \cdot \text{AnomalyScore} + w_2 \cdot \text{VelocityScore} + w_3 \cdot \text{NetworkRisk} + w_4 \cdot \text{ContextualRisk}$$
Ø­ÙŠØ«:
- AnomalyScore: Ù…Ø¯Ù‰ Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ø¹Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©
- VelocityScore: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØºÙŠØ± ÙÙŠ Ø³Ù„ÙˆÙƒ Ø§Ù„Ø­Ø³Ø§Ø¨ (Ù…Ø¨Ø§Ù„ØºØŒ ØªÙƒØ±Ø§Ø±ØŒ Ù…ÙˆØ§Ù‚Ø¹)
- NetworkRisk: Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø­Ø³Ø§Ø¨
- ContextualRisk: Ø¹ÙˆØ§Ù…Ù„ Ø³ÙŠØ§Ù‚ÙŠØ© (ÙˆÙ‚ØªØŒ Ù…ÙƒØ§Ù†ØŒ Ù†ÙˆØ¹ Ø§Ù„Ø¬Ù‡Ø§Ø²)

ØªÙØ­Ø¯Ø¯ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ($w_i$) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØªØ¹Ø²ÙŠØ²ÙŠ (Reinforcement Learning) Ù„ØªØ­Ù‚ÙŠÙ‚ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„Ø£Ù…Ø«Ù„ Ø¨ÙŠÙ† Ø§Ù„Ø§ÙƒØªØ´Ø§Ù ÙˆØ§Ù„Ø±Ø§Ø­Ø©.
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.ensemble import IsolationForest, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
import networkx as nx
import time
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Optional, Any
import matplotlib.pyplot as plt
import seaborn as sns
from dataclasses import dataclass
from enum import Enum

class FraudRiskLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„"""
    LOW = 0
    MEDIUM = 1
    HIGH = 2
    CRITICAL = 3

@dataclass
class Transaction:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ø§Ù„Ù…Ø§Ù„ÙŠØ©"""
    transaction_id: str
    amount: float
    user_id: str
    merchant_id: str
    timestamp: datetime
    device_id: str
    ip_address: str
    location: Tuple[float, float]  # (latitude, longitude)
    transaction_type: str
    is_fraud: Optional[bool] = None
    risk_score: float = 0.0
    features: Dict[str, Any] = None

class FeatureEngineering:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
    
    def __init__(self):
        self.user_velocity = {}  # {user_id: [timestamps, amounts]}
        self.device_velocity = {}
        self.ip_velocity = {}
        self.user_aggregates = {}  # ØªØ±Ø§ÙƒÙ…Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        self.scaler = StandardScaler()
        self.is_fitted = False
    
    def extract_features(self, transaction: Transaction, historical_data: List[Transaction] = None) -> Dict[str, float]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©"""
        features = {}
        
        # 1. Ù…ÙŠØ²Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
        features['amount'] = transaction.amount
        features['hour_of_day'] = transaction.timestamp.hour
        features['day_of_week'] = transaction.timestamp.weekday()
        features['is_weekend'] = 1 if transaction.timestamp.weekday() >= 5 else 0
        
        # 2. Ø³Ø±Ø¹Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (User Velocity)
        user_history = self._get_user_history(transaction.user_id, historical_data)
        features.update(self._calculate_velocity_features(transaction, user_history, 'user'))
        
        # 3. Ø³Ø±Ø¹Ø© Ø§Ù„Ø¬Ù‡Ø§Ø² (Device Velocity)
        device_history = self._get_device_history(transaction.device_id, historical_data)
        features.update(self._calculate_velocity_features(transaction, device_history, 'device'))
        
        # 4. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
        features.update(self._calculate_location_features(transaction, historical_data))
        
        # 5. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©
        features.update(self._calculate_network_features(transaction, historical_data))
        
        # 6. Ù…ÙŠØ²Ø§Øª Ø³ÙŠØ§Ù‚ÙŠØ©
        features.update(self._calculate_contextual_features(transaction))
        
        return features
    
    def _get_user_history(self, user_id: str, historical_data: List[Transaction]) -> List[Transaction]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        if historical_data is None:
            return self.user_velocity.get(user_id, [])
        return [t for t in historical_data if t.user_id == user_id]
    
    def _get_device_history(self, device_id: str, historical_data: List[Transaction]) -> List[Transaction]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ø§Ù„Ø¬Ù‡Ø§Ø²"""
        if historical_data is None:
            return self.device_velocity.get(device_id, [])
        return [t for t in historical_data if t.device_id == device_id]
    
    def _calculate_velocity_features(self, current: Transaction, history: List[Transaction], prefix: str) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³Ø±Ø¹Ø©"""
        features = {}
        
        if not history:
            # Ù‚ÙŠÙ… Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø³Ø¬Ù„Ø§Øª Ø³Ø§Ø¨Ù‚Ø©
            features[f'{prefix}_amount_mean'] = current.amount
            features[f'{prefix}_amount_std'] = 0.0
            features[f'{prefix}_time_diff'] = 3600.0  # Ø³Ø§Ø¹Ø© ÙˆØ§Ø­Ø¯Ø©
            features[f'{prefix}_velocity'] = 0.0
            return features
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆØ§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ Ù„Ù„Ù…Ø¨Ø§Ù„Øº
        amounts = [t.amount for t in history[-10:]]  # Ø¢Ø®Ø± 10 Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        features[f'{prefix}_amount_mean'] = np.mean(amounts)
        features[f'{prefix}_amount_std'] = np.std(amounts) if len(amounts) > 1 else 0.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙØ±Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ
        if len(history) >= 2:
            last_transaction = history[-1]
            time_diff = (current.timestamp - last_transaction.timestamp).total_seconds()
            features[f'{prefix}_time_diff'] = max(time_diff, 1.0)  # ØªØ¬Ù†Ø¨ Ø§Ù„Ù‚Ø³Ù…Ø© Ø¹Ù„Ù‰ ØµÙØ±
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø© (Ù…Ø¨Ù„Øº/Ø²Ù…Ù†)
            amount_diff = abs(current.amount - last_transaction.amount)
            features[f'{prefix}_velocity'] = amount_diff / features[f'{prefix}_time_diff']
        else:
            features[f'{prefix}_time_diff'] = 3600.0
            features[f'{prefix}_velocity'] = 0.0
        
        return features
    
    def _calculate_location_features(self, current: Transaction, historical_data: List[Transaction]) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹"""
        features = {}
        
        # Ø§Ù„Ù…Ø³Ø§ÙØ© Ù…Ù† Ø¢Ø®Ø± Ù…Ø¹Ø§Ù…Ù„Ø©
        user_history = self._get_user_history(current.user_id, historical_data)
        if user_history:
            last_transaction = user_history[-1]
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø§Ù„Ù‡ÙˆØ§Ø¦ÙŠØ© Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠØ©
            lat_diff = current.location[0] - last_transaction.location[0]
            lon_diff = current.location[1] - last_transaction.location[1]
            features['location_distance'] = np.sqrt(lat_diff**2 + lon_diff**2) * 111  # ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ø¨Ø§Ù„ÙƒÙŠÙ„ÙˆÙ…ØªØ±Ø§Øª
        else:
            features['location_distance'] = 0.0
        
        return features
    
    def _calculate_network_features(self, current: Transaction, historical_data: List[Transaction]) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©"""
        features = {}
        
        if historical_data is None or len(historical_data) < 100:
            # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ©
            features['network_density'] = 0.0
            features['merchant_risk'] = 0.0
            return features
        
        # Ø¨Ù†Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ Ø¨Ø³ÙŠØ·
        G = nx.Graph()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª ÙˆØ§Ù„ØªØ§Ø¬Ø±
        G.add_node(current.user_id, type='user')
        G.add_node(current.merchant_id, type='merchant')
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­ÙˆØ§Ù Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        for t in historical_data[-1000:]:  # Ø¢Ø®Ø± 1000 Ù…Ø¹Ø§Ù…Ù„Ø©
            G.add_node(t.user_id, type='user')
            G.add_node(t.merchant_id, type='merchant')
            if t.is_fraud:
                # ÙˆØ²Ù† Ø£Ø¹Ù„Ù‰ Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…Ø­ØªØ§Ù„Ø©
                G.add_edge(t.user_id, t.merchant_id, weight=2.0)
            else:
                G.add_edge(t.user_id, t.merchant_id, weight=1.0)
        
        # Ø­Ø³Ø§Ø¨ ÙƒØ«Ø§ÙØ© Ø§Ù„Ø´Ø¨ÙƒØ©
        if len(G.nodes) > 1:
            features['network_density'] = nx.density(G)
        else:
            features['network_density'] = 0.0
        
        # ØªÙ‚Ø¯ÙŠØ± Ø®Ø·ÙˆØ±Ø© Ø§Ù„ØªØ§Ø¬Ø±
        merchant_transactions = [t for t in historical_data if t.merchant_id == current.merchant_id]
        if merchant_transactions:
            fraud_ratio = sum(1 for t in merchant_transactions if t.is_fraud) / len(merchant_transactions)
            features['merchant_risk'] = min(fraud_ratio * 5.0, 1.0)  # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        else:
            features['merchant_risk'] = 0.0
        
        return features
    
    def _calculate_contextual_features(self, current: Transaction) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©"""
        features = {}
        
        # ÙƒØ´Ù Ø§Ù„Ø£Ù†Ù…Ø§Ø· ØºÙŠØ± Ø§Ù„Ø¹Ø§Ø¯ÙŠØ© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª
        is_unusual_hour = current.timestamp.hour < 5 or current.timestamp.hour > 22
        features['is_unusual_hour'] = 1.0 if is_unusual_hour else 0.0
        
        # Ù†Ø³Ø¨Ø© Ø§Ù„Ù…Ø¨Ù„Øº Ø¥Ù„Ù‰ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¹Ø§Ù… (Ø³ÙŠØªÙ… Ø­Ø³Ø§Ø¨Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹)
        features['amount_ratio'] = 1.0
        
        return features

class FraudDetectionModel(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù…ÙŠÙ‚ Ù„ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„"""
    
    def __init__(self, input_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        
        # Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ù„Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ
        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)
        
        # Ø·Ø¨Ù‚Ø© Ù„Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø£Ø®Ø±Ù‰
        self.ensemble_weight = nn.Parameter(torch.tensor(0.5))
    
    def forward(self, x: torch.Tensor, context: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        
        Args:
            x: Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© [batch_size, input_dim]
            context: Ø³ÙŠØ§Ù‚ Ø²Ù…Ù†ÙŠ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© [seq_len, batch_size, hidden_dim]
        
        Returns:
            Ø¯Ø±Ø¬Ø§Øª Ø®Ø·ÙˆØ±Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ [batch_size, 1]
        """
        # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        base_prediction = self.network(x)
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±
        if context is not None:
            # ØªØ­ÙˆÙŠÙ„ x Ø¥Ù„Ù‰ Ø¨Ø¹Ø¯ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            x_expanded = x.unsqueeze(0).repeat(context.size(0), 1, 1)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            attn_output, _ = self.attention(x_expanded, context, context)
            context_prediction = torch.mean(attn_output, dim=0)
            
            # Ø¯Ù…Ø¬ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª
            combined_prediction = (
                self.ensemble_weight * base_prediction + 
                (1 - self.ensemble_weight) * context_prediction[:, :1]
            )
            return torch.sigmoid(combined_prediction)
        
        return torch.sigmoid(base_prediction)

class RealTimeFraudDetectionSystem:
    """Ù†Ø¸Ø§Ù… ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        if config is None:
            config = {
                'feature_engineering': {},
                'model_params': {'input_dim': 50, 'hidden_dim': 128},
                'thresholds': {
                    'low': 0.3,
                    'medium': 0.6,
                    'high': 0.8,
                    'critical': 0.95
                },
                'max_response_time': 0.2  # 200 Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
            }
        
        self.config = config
        self.feature_engineer = FeatureEngineering()
        self.model = FraudDetectionModel(
            input_dim=config['model_params']['input_dim'],
            hidden_dim=config['model_params']['hidden_dim']
        )
        self.thresholds = config['thresholds']
        self.max_response_time = config['max_response_time']
        self.transaction_history = []
        self.graph_db = nx.Graph()  # Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠØ©
        self.model_version = "1.0.0"
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
        self._init_cache()
    
    def _init_cache(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©"""
        self.risk_cache = {}  # {transaction_id: risk_score}
        self.user_profiles = {}  # {user_id: profile_data}
    
    def _extract_features(self, transaction: Transaction) -> np.ndarray:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©"""
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©
        relevant_history = [
            t for t in self.transaction_history
            if t.timestamp > transaction.timestamp - timedelta(hours=24)
        ]
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        features_dict = self.feature_engineer.extract_features(transaction, relevant_history)
        
        # ØªÙˆØ­ÙŠØ¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª
        if not hasattr(self.feature_engineer, 'is_fitted') or not self.feature_engineer.is_fitted:
            # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙÙˆØ­ÙÙ‘Ø¯
            all_features = [list(features_dict.values())]
            self.feature_engineer.scaler.fit(all_features)
            self.feature_engineer.is_fitted = True
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ©
        features_array = np.array(list(features_dict.values())).reshape(1, -1)
        normalized_features = self.feature_engineer.scaler.transform(features_array)
        
        return normalized_features[0]
    
    def _predict_fraud(self, features: np.ndarray) -> float:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¥Ù„Ù‰ torch tensor
        features_tensor = torch.FloatTensor(features).unsqueeze(0)
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        with torch.no_grad():
            risk_score = self.model(features_tensor).item()
        
        return risk_score
    
    def _determine_risk_level(self, risk_score: float) -> FraudRiskLevel:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø±Ø¬Ø©"""
        if risk_score >= self.thresholds['critical']:
            return FraudRiskLevel.CRITICAL
        elif risk_score >= self.thresholds['high']:
            return FraudRiskLevel.HIGH
        elif risk_score >= self.thresholds['medium']:
            return FraudRiskLevel.MEDIUM
        else:
            return FraudRiskLevel.LOW
    
    def _generate_explanation(self, transaction: Transaction, risk_score: float, risk_level: FraudRiskLevel) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ù„Ù‚Ø±Ø§Ø± Ø§Ù„ÙƒØ´Ù"""
        explanation = f"Ù…Ø¹Ø§Ù…Ù„Ø© {transaction.transaction_id} ØªÙ… ØªÙ‚ÙŠÙŠÙ…Ù‡Ø§ Ø¨Ø¯Ø±Ø¬Ø© Ø®Ø·ÙˆØ±Ø© {risk_score:.2f} "
        
        if risk_level == FraudRiskLevel.CRITICAL:
            explanation += "â­â­â­ (Ø­Ø±Ø¬Ø©): "
            reasons = []
            if transaction.amount > 10000:
                reasons.append("Ù…Ø¨Ù„Øº ÙƒØ¨ÙŠØ± ØºÙŠØ± Ù…Ø¹ØªØ§Ø¯")
            if hasattr(transaction, 'location_distance') and transaction.location_distance > 1000:
                reasons.append("ØªØºÙŠÙŠØ± Ù…ÙØ§Ø¬Ø¦ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ")
            if hasattr(transaction, 'user_velocity') and transaction.user_velocity > 1000:
                reasons.append("Ø³Ø±Ø¹Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ ØªØºÙŠÙŠØ± Ù†Ù…Ø· Ø§Ù„Ø¥Ù†ÙØ§Ù‚")
            
            if reasons:
                explanation += "ØŒ ".join(reasons)
            else:
                explanation += "Ø³Ù„ÙˆÙƒ ØºÙŠØ± Ø·Ø¨ÙŠØ¹ÙŠ Ù…ÙƒØªØ´Ù Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"
        
        elif risk_level == FraudRiskLevel.HIGH:
            explanation += "â­â­ (Ø¹Ø§Ù„ÙŠØ©): Ø³Ù„ÙˆÙƒ ÙŠØ´Ø¨Ù‡ Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø¨Ù†Ø³Ø¨Ø© Ø¹Ø§Ù„ÙŠØ©"
        
        elif risk_level == FraudRiskLevel.MEDIUM:
            explanation += "â­ (Ù…ØªÙˆØ³Ø·Ø©): ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©"
        
        else:
            explanation += "(Ù…Ù†Ø®ÙØ¶Ø©): Ù…Ø¹Ø§Ù…Ù„Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©"
        
        explanation += f"\nØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚: {time.time() - getattr(transaction, 'start_time', time.time()):.4f} Ø«Ø§Ù†ÙŠØ©"
        return explanation
    
    def process_transaction(self, transaction: Transaction) -> Dict[str, Any]:
        """
        Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø© ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
        
        Args:
            transaction: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©
        
        Returns:
            Ù‚Ø±Ø§Ø± Ø§Ù„ÙƒØ´Ù Ù…Ø¹ Ø§Ù„ØªÙØ§ØµÙŠÙ„
        """
        start_time = time.time()
        transaction.start_time = start_time
        
        try:
            # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            if transaction.transaction_id in self.risk_cache:
                cached_result = self.risk_cache[transaction.transaction_id]
                return {
                    'transaction_id': transaction.transaction_id,
                    'risk_score': cached_result['risk_score'],
                    'risk_level': cached_result['risk_level'].name,
                    'decision': cached_result['decision'],
                    'explanation': f"Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ©. {cached_result['explanation']}",
                    'processing_time': time.time() - start_time,
                    'model_version': self.model_version
                }
            
            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª
            features = self._extract_features(transaction)
            
            # 3. Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„
            risk_score = self._predict_fraud(features)
            
            # 4. ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©
            risk_level = self._determine_risk_level(risk_score)
            
            # 5. Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±
            if risk_level == FraudRiskLevel.CRITICAL:
                decision = "BLOCK"
            elif risk_level == FraudRiskLevel.HIGH:
                decision = "REVIEW"
            else:
                decision = "APPROVE"
            
            # 6. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø±Ø­
            explanation = self._generate_explanation(transaction, risk_score, risk_level)
            
            # 7. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„
            transaction.risk_score = risk_score
            transaction.features = dict(zip([f"feature_{i}" for i in range(len(features))], features))
            self.transaction_history.append(transaction)
            
            # 8. Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ù„Ù†ØªÙŠØ¬Ø©
            cache_result = {
                'risk_score': risk_score,
                'risk_level': risk_level,
                'decision': decision,
                'explanation': explanation
            }
            self.risk_cache[transaction.transaction_id] = cache_result
            
            # 9. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
            processing_time = time.time() - start_time
            if processing_time > self.max_response_time:
                logger.warning(f"Transaction {transaction.transaction_id} exceeded max response time: {processing_time:.4f}s")
            
            return {
                'transaction_id': transaction.transaction_id,
                'risk_score': risk_score,
                'risk_level': risk_level.name,
                'decision': decision,
                'explanation': explanation,
                'processing_time': processing_time,
                'model_version': self.model_version,
                'features_used': len(features)
            }
            
        except Exception as e:
            logger.error(f"Error processing transaction {transaction.transaction_id}: {str(e)}")
            # Ù‚Ø±Ø§Ø± Ø¢Ù…Ù† ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
            return {
                'transaction_id': transaction.transaction_id,
                'risk_score': 0.5,
                'risk_level': FraudRiskLevel.MEDIUM.name,
                'decision': "REVIEW",
                'explanation': f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {str(e)}",
                'processing_time': time.time() - start_time,
                'model_version': self.model_version,
                'error': str(e)
            }
    
    def update_model(self, feedback_data: List[Dict[str, Any]]):
        """
        ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØºØ°ÙŠØ© Ø§Ù„Ø±Ø§Ø¬Ø¹Ø©
        
        Args:
            feedback_data: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…Ø¹ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„ØµØ­ÙŠØ­Ø©
        """
        logger.info(f"Updating model with {len(feedback_data)} feedback samples")
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        X = []
        y = []
        
        for item in feedback_data:
            transaction = Transaction(
                transaction_id=item['transaction_id'],
                amount=item['amount'],
                user_id=item['user_id'],
                merchant_id=item['merchant_id'],
                timestamp=datetime.fromisoformat(item['timestamp']),
                device_id=item['device_id'],
                ip_address=item['ip_address'],
                location=(item['latitude'], item['longitude']),
                transaction_type=item['transaction_type'],
                is_fraud=item.get('is_fraud')
            )
            
            features = self._extract_features(transaction)
            X.append(features)
            y.append(1.0 if item.get('is_fraud') else 0.0)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª NumPy
        X = np.array(X)
        y = np.array(y)
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ø³ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ ØªØ¯Ø±ÙŠØ¨Ø§Ù‹ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹)
        self._train_incremental(X, y)
        
        logger.info("Model updated successfully")
    
    def _train_incremental(self, X: np.ndarray, y: np.ndarray):
        """Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªØ¯Ø±ÙŠØ¬ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        # ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ø³ÙŠØ³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        pass
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù…"""
        processing_times = [
            t.processing_time for t in self.transaction_history
            if hasattr(t, 'processing_time')
        ]
        
        decisions = [t.decision for t in self.transaction_history if hasattr(t, 'decision')]
        
        return {
            'total_transactions': len(self.transaction_history),
            'avg_processing_time': np.mean(processing_times) if processing_times else 0,
            'max_processing_time': np.max(processing_times) if processing_times else 0,
            'decision_distribution': {
                'APPROVE': decisions.count('APPROVE'),
                'REVIEW': decisions.count('REVIEW'),
                'BLOCK': decisions.count('BLOCK')
            },
            'cache_hit_rate': len(self.risk_cache) / max(len(self.transaction_history), 1),
            'model_version': self.model_version
        }

"""
## 3. Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ: Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ
Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ ÙˆØ­Ø¯Ù‡ Ù„Ø§ ÙŠÙƒÙÙŠØ› Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø© Ù‡ÙŠ Ù…Ø§ ÙŠØ¬Ø¹Ù„ Ø§Ù„ÙƒØ´Ù Ù…Ù…ÙƒÙ†Ø§Ù‹ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ.

### 3.1 Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ingestion â”‚â”€â”€â”€â”€â–¶â”‚  Feature    â”‚â”€â”€â”€â”€â–¶â”‚  Scoring    â”‚
â”‚   (Kafka)   â”‚     â”‚  Store      â”‚     â”‚  Engine     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                   â”‚                   â”‚
       â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data   â”‚     â”‚  Real-time  â”‚     â”‚  Decision   â”‚
â”‚  Storage    â”‚     â”‚  Features   â”‚     â”‚  Service    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                                           â–¼
                                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   â”‚  Feedback   â”‚
                                   â”‚  Loop       â”‚
                                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 ØªÙ‚Ù†ÙŠØ§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø¥Ù†ØªØ§Ø¬
1. **Apache Kafka**: Ù„ØªØ¯ÙÙ‚ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„ØªØ³Ù„ÙŠÙ…
2. **Redis**: Ù„Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø§Ù„Ø³Ø±ÙŠØ¹Ø© ÙˆØªØ®Ø²ÙŠÙ† Ù…Ù„Ø§Ù…Ø­ Ø§Ù„Ø³Ù„ÙˆÙƒ
3. **PostgreSQL Ù…Ø¹ pgvector**: Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªØ¬Ù‡ÙŠ
4. **FastAPI Ù…Ø¹ Uvicorn**: Ù„ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¯Ø§Ø¡
5. **Prometheus/Grafana**: Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆÙ†Ø³Ø¨Ø© Ø§Ù„Ø§ÙƒØªØ´Ø§Ù

### 3.3 ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
- **Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…ÙØ®ÙŽÙÙŽÙ‘Ù (Quantized Inference)**: ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø³Ø¨Ø© 75% Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ 98% Ù…Ù† Ø§Ù„Ø¯Ù‚Ø©
- **Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ù‡Ø±Ù…ÙŠ (Hierarchical Caching)**: Ø°Ø§ÙƒØ±Ø© Ù…Ø¤Ù‚ØªØ© Ø¹Ù„Ù‰ Ø«Ù„Ø§Ø«Ø© Ù…Ø³ØªÙˆÙŠØ§Øª (L1: Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ L2: RedisØŒ L3: Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)
- **Ø§Ù„ØªØ¬Ø²Ø¦Ø© (Sharding)**: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø²Ù…Ù† Ø§Ù„ÙˆØµÙˆÙ„
- **Ø§Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø¨Ù‚ (Pre-computation)**: Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©

### 3.4 Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù†Ø¬Ø§Ø­
- **Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©**: Ø£Ù‚Ù„ Ù…Ù† 200 Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ Ù…Ø¹Ø§Ù…Ù„Ø©
- **Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒØ´Ù**: Ø£ÙƒØ«Ø± Ù…Ù† 95% Ù…Ù† Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ Ø§Ù„Ù…Ø¹Ø±ÙˆÙØ©
- **Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©**: Ø£Ù‚Ù„ Ù…Ù† 1% Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø´Ø±Ø¹ÙŠØ©
- **Ø§Ù„ØªÙˆØ§ÙØ±**: 99.99% uptime
- **Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹**: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ 10,000 Ù…Ø¹Ø§Ù…Ù„Ø© ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ©
"""

def simulate_real_time_transactions():
    """Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø¹Ø§Ù…Ù„Ø§Øª ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ"""
    logger.info("Starting real-time transaction simulation...")
    
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
    config = {
        'feature_engineering': {},
        'model_params': {'input_dim': 50, 'hidden_dim': 128},
        'thresholds': {
            'low': 0.3,
            'medium': 0.6,
            'high': 0.8,
            'critical': 0.95
        },
        'max_response_time': 0.2
    }
    
    system = RealTimeFraudDetectionSystem(config)
    
    # Ù…Ø­Ø§ÙƒØ§Ø© 100 Ù…Ø¹Ø§Ù…Ù„Ø©
    results = []
    fraud_count = 0
    
    for i in range(100):
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¹Ø§Ù…Ù„Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        is_fraud = np.random.random() < 0.1  # 10% Ø§Ø­ØªÙ…Ø§Ù„ Ø§Ø­ØªÙŠØ§Ù„
        amount = np.random.exponential(100) if not is_fraud else np.random.exponential(500) + 1000
        
        transaction = Transaction(
            transaction_id=f"TXN_{i:06d}",
            amount=amount,
            user_id=f"USER_{np.random.randint(1, 100)}",
            merchant_id=f"MERCHANT_{np.random.randint(1, 50)}",
            timestamp=datetime.now() - timedelta(seconds=np.random.randint(0, 3600)),
            device_id=f"DEVICE_{np.random.randint(1, 200)}",
            ip_address=f"192.168.{np.random.randint(0, 255)}.{np.random.randint(0, 255)}",
            location=(np.random.uniform(24, 27), np.random.uniform(46, 48)),  # ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©
            transaction_type=np.random.choice(["purchase", "transfer", "withdrawal"], p=[0.7, 0.2, 0.1]),
            is_fraud=is_fraud
        )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø©
        result = system.process_transaction(transaction)
        results.append(result)
        
        if is_fraud:
            fraud_count += 1
        
        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        if result['risk_level'] in ['HIGH', 'CRITICAL']:
            print(f"ðŸš¨ {result['risk_level']} RISK TRANSACTION: {result['transaction_id']}")
            print(f"   Amount: ${result['amount']:.2f}, Decision: {result['decision']}")
            print(f"   Explanation: {result['explanation']}")
            print(f"   Processing time: {result['processing_time']:.4f}s")
            print("-" * 50)
        
        # ØªØ£Ø®ÙŠØ± Ù‚ØµÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª
        time.sleep(0.01)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    print("\n" + "="*60)
    print("SIMULATION RESULTS")
    print("="*60)
    
    # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙƒØ´Ù
    detected_fraud = sum(1 for r in results if r.get('is_fraud') and r['decision'] == 'BLOCK')
    total_fraud = fraud_count
    
    if total_fraud > 0:
        detection_rate = detected_fraud / total_fraud
        print(f"Fraud Detection Rate: {detection_rate:.2%} ({detected_fraud}/{total_fraud})")
    
    # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª Ø§Ù„Ø®Ø§Ø·Ø¦Ø©
    false_positives = sum(1 for r in results if not r.get('is_fraud', False) and r['decision'] == 'BLOCK')
    total_legitimate = len(results) - total_fraud
    
    if total_legitimate > 0:
        false_positive_rate = false_positives / total_legitimate
        print(f"False Positive Rate: {false_positive_rate:.2%} ({false_positives}/{total_legitimate})")
    
    # Ø¹Ø±Ø¶ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡
    metrics = system.get_system_metrics()
    print(f"\nSystem Performance Metrics:")
    print(f"  Average Processing Time: {metrics['avg_processing_time']:.4f}s")
    print(f"  Max Processing Time: {metrics['max_processing_time']:.4f}s")
    print(f"  Cache Hit Rate: {metrics['cache_hit_rate']:.2%}")
    print(f"  Decision Distribution: {metrics['decision_distribution']}")
    
    return system, results

# ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
if __name__ == "__main__":
    system, results = simulate_real_time_transactions()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    with open('fraud_detection_results.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print("\nâœ… Simulation completed successfully!")
    print("Results saved to 'fraud_detection_results.json'")

"""
## 4. Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØ§Ù„Ø­Ù„ÙˆÙ„
### 4.1 Ø§Ù„ØªØ­Ø¯ÙŠ: Ø§Ù„Ù‡Ø¬ÙˆÙ… Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠ (Adversarial Attacks)
ÙŠØ³ØªØ·ÙŠØ¹ Ø§Ù„Ù…Ø­ØªØ§Ù„ÙˆÙ† ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹Ø§Ø±Ø¶Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø³Ù„ÙˆÙƒ Ù†Ø¸Ø§Ù… Ø§Ù„ÙƒØ´Ù ÙˆØ§Ù„ØªØ­Ø§ÙŠÙ„ Ø¹Ù„ÙŠÙ‡.

Ø§Ù„Ø­Ù„:
- **Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„ØªÙ†Ø§ÙØ³ÙŠ (Adversarial Training)**: ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ø£Ù…Ø«Ù„Ø© Ù…ÙØ²ÙŽÙŠÙŽÙ‘ÙØ©
- **Ø§Ù„ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠ (Model Diversification)**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬ Ù…Ø®ØªÙ„ÙØ©
- **Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª (Attack Detection)**: Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ù†Ù…Ø§Ø· Ø§Ù„ÙˆØµÙˆÙ„ ØºÙŠØ± Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©

### 4.2 Ø§Ù„ØªØ­Ø¯ÙŠ: Ø§Ù„ØªØ­ÙŠØ² ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªØ­ÙŠØ² Ø¶Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.

Ø§Ù„Ø­Ù„:
- **Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙˆØ²Ù† (Reweighting)**: ØªØ¹Ø¯ÙŠÙ„ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨
- **Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ­ÙŠØ² (Debiasing)**: ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø§ Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
- **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©**: ØªØªØ¨Ø¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø­Ø³Ø¨ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø¯ÙŠÙ…ÙˆØºØ±Ø§ÙÙŠØ©

### 4.3 Ø§Ù„ØªØ­Ø¯ÙŠ: Ø§Ù„ØªÙØ³ÙŠØ±ÙŠØ© ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…ÙŠ
ÙŠØªØ·Ù„Ø¨ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† ÙÙŠ ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø£Ù† ØªÙƒÙˆÙ† Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ´Ù Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ³ÙŠØ±.

Ø§Ù„Ø­Ù„:
- **Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙØ³ÙŠØ± (Explainable AI)**: Ù…Ø«Ù„ SHAP Ùˆ LIME
- **Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ (Audit Trail)**: ØªØ³Ø¬ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø£Ø³Ø¨Ø§Ø¨
- **ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¨Ø´Ø±ÙŠØ© (Human Review Interface)**: ØªÙ…ÙƒÙŠÙ† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ© Ù„Ù„Ù‚Ø±Ø§Ø±Øª Ø§Ù„Ø­Ø±Ø¬Ø©

### 4.4 Ø§Ù„ØªØ­Ø¯ÙŠ: Ø§Ù„ØªÙƒÙŠÙ Ù…Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙŠØªØ·ÙˆØ± Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±ØŒ Ù…Ù…Ø§ ÙŠØªØ·Ù„Ø¨ Ù†Ù…Ø§Ø°Ø¬ Ù…Ø±Ù†Ø©.

Ø§Ù„Ø­Ù„:
- **Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø³ØªÙ…Ø± (Continuous Learning)**: ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙŠÙˆÙ…ÙŠØ§Ù‹
- **Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù (Drift Detection)**: Ù…Ø±Ø§Ù‚Ø¨Ø© ØªØºÙŠØ± ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
- **Ø§Ù„ØªØ¬Ø§Ø±Ø¨ Ø§Ù„ØªØ­ÙƒÙ…ÙŠØ© (A/B Testing)**: Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù‚Ø¨Ù„ Ø§Ù„Ù†Ø´Ø±

## 5. Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠ: Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù…
Ù„ØªØ­ÙˆÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ù† Ø¯ÙØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ù„Ù‰ Ø¥Ù†ØªØ§Ø¬:

1. **Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 1-2**: Ø¨Ù†Ø§Ø¡ MVP Ù…Ø¹ FastAPI Ùˆ SQLite
2. **Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 3-4**: Ø¯Ù…Ø¬ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ (PostgreSQL) Ùˆ Redis
3. **Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 5-6**: Ø¥Ø¶Ø§ÙØ© Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Prometheus/Grafana
4. **Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 7-8**: ØªÙ†ÙÙŠØ° Ø®Ø·ÙˆØ· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨ CI/CD Ù…Ø¹ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
5. **Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ 9-12**: Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ÙŠØ©

## 6. Ø§Ù„Ø®Ù„Ø§ØµØ©
ÙƒØ´Ù Ø§Ù„Ø§Ø­ØªÙŠØ§Ù„ ÙÙŠ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„ÙØ¹Ù„ÙŠ Ù‡Ùˆ Ù…Ø¬Ø§Ù„ ÙŠØ¯Ù…Ø¬ Ø¨ÙŠÙ† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©ØŒ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ÙØ¹Ø§Ù„Ø©ØŒ ÙˆÙ‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø£Ù†Ø¸Ù…Ø©. Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠÙ‡ ÙŠØªØ·Ù„Ø¨ Ø£ÙƒØ«Ø± Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù‚ÙŠÙ‚Ø› ÙŠØªØ·Ù„Ø¨ Ø¨Ù†ÙŠØ© ØªØ­ØªÙŠØ© Ù‚ÙˆÙŠØ©ØŒ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø³ÙŠÙ† Ù…Ø³ØªÙ…Ø±Ø©ØŒ ÙˆØªÙˆØ§Ø²Ù† Ø¯Ù‚ÙŠÙ‚ Ø¨ÙŠÙ† Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø±Ø§Ø­Ø©. Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙŠ Ù‚Ù…Ù†Ø§ Ø¨Ø¨Ù†Ø§Ø¦Ù‡ ÙŠÙ…Ø«Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡ Ø¥Ù„Ù‰ Ø­Ù„ Ø¥Ù†ØªØ§Ø¬ÙŠ ÙƒØ§Ù…Ù„ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø±Ø¹Ø©ØŒ Ø§Ù„Ø¯Ù‚Ø©ØŒ ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©.

Ø§Ù„ØªØ­Ø¯ÙŠ Ø§Ù„Ù…Ø³ØªÙ…Ø± Ù‡Ùˆ Ù…ÙˆØ§ÙƒØ¨Ø© Ø§Ù„Ù…Ø­ØªØ§Ù„ÙŠÙ† Ø§Ù„Ù…ØªØ·ÙˆØ±ÙŠÙ†ØŒ Ù…Ù…Ø§ ÙŠØªØ·Ù„Ø¨ Ø«Ù‚Ø§ÙØ© ØªØ¹Ù„Ù… Ù…Ø³ØªÙ…Ø± ÙˆØ§Ø¨ØªÙƒØ§Ø± ØªÙ‚Ù†ÙŠ.
"""
```

## ðŸ“ benchmarks/cost_performance_tradeoffs/model_size_vs_latency.py (Complete)

```python
"""
Ù…Ù‚ÙŠØ§Ø³ Ù…Ù‚Ø§ÙŠØ¶Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙƒÙ„ÙØ© ÙÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©ØŒ ÙˆØ§Ù„ØªÙƒÙ„ÙØ©
"""

import numpy as np
import pandas as pd
import time
import json
import os
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Any, Optional
import torch
import torch.nn as nn
import logging
from dataclasses import dataclass
import argparse
from enum import Enum

# ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("benchmark")

class ModelSize(Enum):
    """Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    SMALL = "small"
    MEDIUM = "medium"
    LARGE = "large"
    XLARGE = "xlarge"

class DeploymentType(Enum):
    """Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù†Ø´Ø±"""
    CLOUD = "cloud"
    EDGE = "edge"
    HYBRID = "hybrid"

@dataclass
class ModelConfig:
    """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù‚ÙŠØ§Ø³"""
    name: str
    size: ModelSize
    parameters: int  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø¨Ø§Ù„Ù…Ù„Ø§ÙŠÙŠÙ†
    hidden_size: int
    num_layers: int
    attention_heads: int
    context_window: int
    deployment_type: DeploymentType
    hardware: str
    batch_size: int = 1
    precision: str = "fp16"  # "fp16", "fp32", "int8", "int4"

@dataclass
class BenchmarkResult:
    """Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚ÙŠØ§Ø³"""
    model_name: str
    size: str
    parameters: int
    deployment_type: str
    hardware: str
    precision: str
    batch_size: int
    avg_latency_ms: float
    p50_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    throughput_tokens_per_sec: float
    memory_usage_gb: float
    gpu_utilization: float
    cpu_utilization: float
    power_consumption_w: float
    cost_per_million_tokens: float
    timestamp: str

class SimpleTransformer(nn.Module):
    """Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø¨Ø³ÙŠØ· Ù„Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ù‚ÙŠØ§Ø³"""
    
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        self.embedding = nn.Embedding(50000, config.hidden_size)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ­ÙˆÙŠÙ„
        self.layers = nn.ModuleList([
            nn.TransformerEncoderLayer(
                d_model=config.hidden_size,
                nhead=config.attention_heads,
                dim_feedforward=config.hidden_size * 4,
                dropout=0.1,
                batch_first=True
            ) for _ in range(config.num_layers)
        ])
        
        # Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        self.output = nn.Linear(config.hidden_size, 50000)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        self._init_weights()
    
    def _init_weights(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù†"""
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ"""
        # Ø§Ù„ØªØ¶Ù…ÙŠÙ†
        x = self.embedding(x)
        
        # Ø·Ø¨Ù‚Ø§Øª Ø§Ù„ØªØ­ÙˆÙŠÙ„
        for layer in self.layers:
            x = layer(x)
        
        # Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬
        return self.output(x)

class ModelBenchmarkSuite:
    """Ø­Ø²Ù…Ø© Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
    
    def __init__(self, output_dir: str = "benchmarks/results"):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        
        # ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        self.model_configs = [
            ModelConfig(
                name="tiny-transformer",
                size=ModelSize.SMALL,
                parameters=10,
                hidden_size=256,
                num_layers=4,
                attention_heads=4,
                context_window=512,
                deployment_type=DeploymentType.CLOUD,
                hardware="A10G"
            ),
            ModelConfig(
                name="base-transformer",
                size=ModelSize.MEDIUM,
                parameters=100,
                hidden_size=768,
                num_layers=12,
                attention_heads=12,
                context_window=1024,
                deployment_type=DeploymentType.CLOUD,
                hardware="A10G"
            ),
            ModelConfig(
                name="large-transformer",
                size=ModelSize.LARGE,
                parameters=300,
                hidden_size=1024,
                num_layers=24,
                attention_heads=16,
                context_window=2048,
                deployment_type=DeploymentType.CLOUD,
                hardware="A100"
            ),
            ModelConfig(
                name="xl-transformer",
                size=ModelSize.XLARGE,
                parameters=700,
                hidden_size=1536,
                num_layers=32,
                attention_heads=24,
                context_window=4096,
                deployment_type=DeploymentType.HYBRID,
                hardware="H100"
            )
        ]
        
        # ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„ÙƒÙ…ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        self.quantization_configs = ["fp16", "int8", "int4"]
        
        self.results = []
    
    def generate_dummy_input(self, batch_size: int, seq_len: int, config: ModelConfig) -> torch.Tensor:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¯Ø®Ù„Ø§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠØ©"""
        return torch.randint(0, 50000, (batch_size, seq_len)).to(self.device)
    
    def measure_latency(self, model: nn.Module, input_data: torch.Tensor, 
                       num_warmup: int = 10, num_runs: int = 100) -> Dict[str, float]:
        """Ù‚ÙŠØ§Ø³ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ±"""
        latencies = []
        
        # Ø§Ù„ØªØ³Ø®ÙŠÙ†
        with torch.no_grad():
            for _ in range(num_warmup):
                _ = model(input_data)
        
        # Ø§Ù„Ù‚ÙŠØ§Ø³
        with torch.no_grad():
            for _ in range(num_runs):
                start_time = time.perf_counter()
                _ = model(input_data)
                end_time = time.perf_counter()
                latencies.append((end_time - start_time) * 1000)  # Ù…Ù„Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
        
        return {
            'avg_latency_ms': np.mean(latencies),
            'p50_latency_ms': np.percentile(latencies, 50),
            'p95_latency_ms': np.percentile(latencies, 95),
            'p99_latency_ms': np.percentile(latencies, 99),
            'throughput_tokens_per_sec': (input_data.size(0) * input_data.size(1) * num_runs) / (np.sum(latencies) / 1000)
        }
    
    def measure_memory_usage(self, model: nn.Module) -> float:
        """Ù‚ÙŠØ§Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©"""
        if not torch.cuda.is_available():
            return 0.0
        
        torch.cuda.reset_peak_memory_stats()
        _ = model(torch.randint(0, 50000, (1, 128)).to(self.device))
        peak_memory = torch.cuda.max_memory_allocated() / (1024 ** 3)  # ØºÙŠØºØ§Ø¨Ø§ÙŠØª
        return peak_memory
    
    def benchmark_model(self, config: ModelConfig) -> BenchmarkResult:
        """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯"""
        logger.info(f"Starting benchmark for {config.name} ({config.parameters}M parameters)")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬Ù‡Ø§Ø²
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model = SimpleTransformer(config).to(self.device)
        model.eval()
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        input_data = self.generate_dummy_input(
            batch_size=config.batch_size,
            seq_len=config.context_window // 2,  # Ù†ØµÙ Ù†Ø§ÙØ°Ø© Ø§Ù„Ø³ÙŠØ§Ù‚
            config=config
        )
        
        # Ù‚ÙŠØ§Ø³ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ±
        latency_results = self.measure_latency(model, input_data)
        
        # Ù‚ÙŠØ§Ø³ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        memory_usage = self.measure_memory_usage(model)
        
        # ØªÙ‚Ø¯ÙŠØ± Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³ÙˆÙ…ÙŠØ©
        gpu_utilization = min(100.0, config.parameters * 0.1)  # ØªÙ‚Ø¯ÙŠØ± Ù…Ø¨Ø³Ø·
        
        # ØªÙ‚Ø¯ÙŠØ± ØªÙƒÙ„ÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© (Ø§ÙØªØ±Ø§Ø¶ÙŠ: $0.0001 Ù„ÙƒÙ„ 1000 ØªÙˆÙƒÙŠÙ† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ØªÙˆØ³Ø·)
        base_cost = 0.0001 / 1000
        cost_multiplier = {
            ModelSize.SMALL: 0.5,
            ModelSize.MEDIUM: 1.0,
            ModelSize.LARGE: 2.5,
            ModelSize.XLARGE: 6.0
        }
        cost_per_token = base_cost * cost_multiplier[config.size]
        cost_per_million_tokens = cost_per_token * 1_000_000
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù‚ÙŠØ§Ø³
        result = BenchmarkResult(
            model_name=config.name,
            size=config.size.value,
            parameters=config.parameters,
            deployment_type=config.deployment_type.value,
            hardware=config.hardware,
            precision=config.precision,
            batch_size=config.batch_size,
            avg_latency_ms=latency_results['avg_latency_ms'],
            p50_latency_ms=latency_results['p50_latency_ms'],
            p95_latency_ms=latency_results['p95_latency_ms'],
            p99_latency_ms=latency_results['p99_latency_ms'],
            throughput_tokens_per_sec=latency_results['throughput_tokens_per_sec'],
            memory_usage_gb=memory_usage,
            gpu_utilization=gpu_utilization,
            cpu_utilization=25.0,  # ØªÙ‚Ø¯ÙŠØ± Ù…Ø¨Ø³Ø·
            power_consumption_w=250.0,  # ØªÙ‚Ø¯ÙŠØ± Ù…Ø¨Ø³Ø·
            cost_per_million_tokens=cost_per_million_tokens,
            timestamp=datetime.now().isoformat()
        )
        
        logger.info(f"Benchmark completed for {config.name}:")
        logger.info(f"  Average latency: {result.avg_latency_ms:.2f}ms")
        logger.info(f"  Throughput: {result.throughput_tokens_per_sec:.2f} tokens/sec")
        logger.info(f"  Memory usage: {result.memory_usage_gb:.2f}GB")
        logger.info(f"  Cost per million tokens: ${result.cost_per_million_tokens:.4f}")
        
        return result
    
    def benchmark_all_models(self):
        """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬"""
        logger.info("Starting comprehensive benchmark suite...")
        
        for config in self.model_configs:
            for precision in self.quantization_configs:
                # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ù„Ù„ÙƒÙ…ÙŠØ©
                config_copy = ModelConfig(
                    name=config.name,
                    size=config.size,
                    parameters=config.parameters,
                    hidden_size=config.hidden_size,
                    num_layers=config.num_layers,
                    attention_heads=config.attention_heads,
                    context_window=config.context_window,
                    deployment_type=config.deployment_type,
                    hardware=config.hardware,
                    precision=precision
                )
                
                try:
                    result = self.benchmark_model(config_copy)
                    self.results.append(result)
                except Exception as e:
                    logger.error(f"Error benchmarking {config.name} with {precision}: {str(e)}")
        
        logger.info("Benchmark suite completed successfully!")
    
    def save_results(self):
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù‚ÙŠØ§Ø³"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
        results_df = pd.DataFrame([vars(r) for r in self.results])
        
        # Ø­ÙØ¸ ÙƒÙ€ CSV
        csv_path = os.path.join(self.output_dir, "benchmark_results.csv")
        results_df.to_csv(csv_path, index=False)
        logger.info(f"Results saved to {csv_path}")
        
        # Ø­ÙØ¸ ÙƒÙ€ JSON
        json_path = os.path.join(self.output_dir, "benchmark_results.json")
        with open(json_path, 'w') as f:
            json.dump([vars(r) for r in self.results], f, indent=2)
        logger.info(f"Results saved to {json_path}")
        
        return results_df
    
    def generate_report(self, results_df: pd.DataFrame):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ù…Ø±Ø¦ÙŠ"""
        logger.info("Generating visualization report...")
        
        # 1. Ù…Ù†Ø­Ù†Ù‰ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù‚Ø§Ø¨Ù„ Ø²Ù…Ù† Ø§Ù„ØªØ£Ø®ÙŠØ±
        plt.figure(figsize=(12, 6))
        
        for precision in results_df['precision'].unique():
            subset = results_df[results_df['precision'] == precision]
            plt.plot(subset['parameters'], subset['avg_latency_ms'], 'o-', 
                    label=f'{precision.upper()} Precision')
        
        plt.xlabel('Model Size (Million Parameters)')
        plt.ylabel('Average Latency (ms)')
        plt.title('Model Size vs Latency by Precision')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(self.output_dir, 'size_vs_latency.png'))
        plt.close()
        
        # 2. Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù‚Ø§Ø¨Ù„ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©
        plt.figure(figsize=(12, 6))
        
        for precision in results_df['precision'].unique():
            subset = results_df[results_df['precision'] == precision]
            plt.plot(subset['parameters'], subset['throughput_tokens_per_sec'], 'o-', 
                    label=f'{precision.upper()} Precision')
        
        plt.xlabel('Model Size (Million Parameters)')
        plt.ylabel('Throughput (Tokens/Second)')
        plt.title('Model Size vs Throughput by Precision')
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(self.output_dir, 'size_vs_throughput.png'))
        plt.close()
        
        # 3. Ù…ØµÙÙˆÙØ© ØªÙƒÙ„ÙØ©-Ø£Ø¯Ø§Ø¡
        plt.figure(figsize=(12, 8))
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¤Ø´Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡-Ø§Ù„ØªÙƒÙ„ÙØ© (ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø£Ø¹Ù„Ù‰ ÙƒØ§Ù† Ø£ÙØ¶Ù„)
        results_df['performance_cost_ratio'] = results_df['throughput_tokens_per_sec'] / results_df['cost_per_million_tokens']
        
        # Ø±Ø³Ù… Ø§Ù„Ù†Ù‚Ø§Ø·
        scatter = plt.scatter(
            results_df['cost_per_million_tokens'],
            results_df['throughput_tokens_per_sec'],
            s=results_df['parameters'] / 10,  # Ø­Ø¬Ù… Ø§Ù„Ù†Ù‚Ø·Ø© Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            c=results_df['avg_latency_ms'],
            cmap='viridis',
            alpha=0.7
        )
        
        plt.xlabel('Cost per Million Tokens ($)')
        plt.ylabel('Throughput (Tokens/Second)')
        plt.title('Cost-Performance Tradeoff Analysis\n(Bubble size: Model parameters, Color: Latency)')
        plt.colorbar(scatter, label='Average Latency (ms)')
        plt.grid(True)
        
        # Ø¥Ø¶Ø§ÙØ© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
        for _, row in results_df.iterrows():
            plt.annotate(
                f"{row['model_name']}\n({row['precision']})",
                (row['cost_per_million_tokens'], row['throughput_tokens_per_sec']),
                xytext=(5, 5),
                textcoords='offset points'
            )
        
        plt.savefig(os.path.join(self.output_dir, 'cost_performance_tradeoff.png'))
        plt.close()
        
        # 4. ØªÙ‚Ø±ÙŠØ± HTML
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI Model Benchmark Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; }}
                h1 {{ color: #2c3e50; }}
                .summary {{
                    background-color: #f8f9fa;
                    padding: 20px;
                    border-radius: 5px;
                    margin: 20px 0;
                }}
                table {{
                    border-collapse: collapse;
                    width: 100%;
                    margin: 20px 0;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 8px;
                    text-align: left;
                }}
                th {{
                    background-color: #f2f2f2;
                }}
                tr:hover {{
                    background-color: #f5f5f5;
                }}
                .chart {{
                    margin: 30px 0;
                }}
                .recommendations {{
                    background-color: #e8f4f8;
                    padding: 20px;
                    border-radius: 5px;
                    margin: 20px 0;
                }}
            </style>
        </head>
        <body>
            <h1>AI Model Benchmark Report</h1>
            <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            
            <div class="summary">
                <h2>Executive Summary</h2>
                <ul>
                    <li><strong>Total Models Benchmarked:</strong> {len(results_df)}</li>
                    <li><strong>Best Throughput:</strong> {results_df['throughput_tokens_per_sec'].max():.2f} tokens/sec ({results_df.loc[results_df['throughput_tokens_per_sec'].idxmax(), 'model_name']})</li>
                    <li><strong>Lowest Latency:</strong> {results_df['avg_latency_ms'].min():.2f} ms ({results_df.loc[results_df['avg_latency_ms'].idxmin(), 'model_name']})</li>
                    <li><strong>Most Cost-Effective:</strong> ${results_df['cost_per_million_tokens'].min():.4f} per million tokens ({results_df.loc[results_df['cost_per_million_tokens'].idxmin(), 'model_name']})</li>
                </ul>
            </div>
            
            <div class="chart">
                <h2>Size vs Latency</h2>
                <img src="size_vs_latency.png" alt="Size vs Latency" width="800">
            </div>
            
            <div class="chart">
                <h2>Size vs Throughput</h2>
                <img src="size_vs_throughput.png" alt="Size vs Throughput" width="800">
            </div>
            
            <div class="chart">
                <h2>Cost-Performance Tradeoff</h2>
                <img src="cost_performance_tradeoff.png" alt="Cost-Performance Tradeoff" width="800">
            </div>
            
            <h2>Detailed Results</h2>
            <table>
                <tr>
                    <th>Model</th>
                    <th>Size (M params)</th>
                    <th>Precision</th>
                    <th>Latency (ms)</th>
                    <th>Throughput (tokens/sec)</th>
                    <th>Memory (GB)</th>
                    <th>Cost/M Tokens ($)</th>
                </tr>
        """
        
        for _, row in results_df.iterrows():
            html_content += f"""
                <tr>
                    <td>{row['model_name']}</td>
                    <td>{row['parameters']}</td>
                    <td>{row['precision'].upper()}</td>
                    <td>{row['avg_latency_ms']:.2f}</td>
                    <td>{row['throughput_tokens_per_sec']:.2f}</td>
                    <td>{row['memory_usage_gb']:.2f}</td>
                    <td>{row['cost_per_million_tokens']:.4f}</td>
                </tr>
            """
        
        html_content += f"""
            </table>
            
            <div class="recommendations">
                <h2>Recommendations</h2>
                <p><strong>For High-Throughput Applications:</strong> {results_df.loc[results_df['throughput_tokens_per_sec'].idxmax(), 'model_name']} with {results_df.loc[results_df['throughput_tokens_per_sec'].idxmax(), 'precision']} precision</p>
                <p><strong>For Low-Latency Applications:</strong> {results_df.loc[results_df['avg_latency_ms'].idxmin(), 'model_name']} with {results_df.loc[results_df['avg_latency_ms'].idxmin(), 'precision']} precision</p>
                <p><strong>For Cost-Sensitive Applications:</strong> {results_df.loc[results_df['cost_per_million_tokens'].idxmin(), 'model_name']} with {results_df.loc[results_df['cost_per_million_tokens'].idxmin(), 'precision']} precision</p>
                <p><strong>For Edge Deployment:</strong> Consider 4-bit quantization for all models to reduce memory usage by 75%</p>
            </div>
        </body>
        </html>
        """
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± HTML
        html_path = os.path.join(self.output_dir, 'benchmark_report.html')
        with open(html_path, 'w') as f:
            f.write(html_content)
        logger.info(f"HTML report saved to {html_path}")

def main():
    """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(description='Benchmark AI models for cost-performance tradeoffs')
    parser.add_argument('--output_dir', type=str, default='benchmarks/results',
                        help='Output directory for results')
    parser.add_argument('--quick_run', action='store_true',
                        help='Run a quick benchmark with fewer iterations')
    
    args = parser.parse_args()
    
    logger.info("Starting AI Model Benchmark Suite...")
    logger.info(f"Output directory: {args.output_dir}")
    logger.info(f"Quick run mode: {'enabled' if args.quick_run else 'disabled'}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø­Ø²Ù…Ø© Ø§Ù„Ù‚ÙŠØ§Ø³
    benchmark_suite = ModelBenchmarkSuite(args.output_dir)
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù‚ÙŠØ§Ø³Ø§Øª
    benchmark_suite.benchmark_all_models()
    
    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    results_df = benchmark_suite.save_results()
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    benchmark_suite.generate_report(results_df)
    
    logger.info("Benchmark suite completed successfully!")
    logger.info(f"Results available in: {args.output_dir}")

if __name__ == "__main__":
    main()
```

## ðŸ“ run.sh (Complete)

```bash
#!/bin/bash
set -e

# AI Engineer Toolkit 2025 - Run Script
# Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±Ø¨Øª ÙŠØ·Ù„Ù‚ Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ù…Ø³ØªÙˆØ¯Ø¹

log() {
    echo -e "\033[1;34m[$(date +'%Y-%m-%d %H:%M:%S')]\033[0m $1"
}

error() {
    echo -e "\033[1;31m[ERROR] $1\033[0m"
    exit 1
}

warning() {
    echo -e "\033[1;33m[WARNING] $1\033[0m"
}

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
check_prerequisites() {
    log "Checking prerequisites..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ conda
    if ! command -v conda &> /dev/null; then
        error "Conda is not installed. Please install Miniconda first."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙØ¹ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø©
    if [ -z "$CONDA_DEFAULT_ENV" ] || [ "$CONDA_DEFAULT_ENV" = "base" ]; then
        warning "Conda environment not activated. Attempting to activate..."
        source "$(conda info --base)/etc/profile.d/conda.sh"
        conda activate ai-engineer-toolkit-2025 || error "Failed to activate conda environment"
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
    if [ ! -f "requirements.txt" ]; then
        error "requirements.txt not found. Please run setup.sh first."
    fi
    
    log "Prerequisites check passed"
}

# Ø¥Ø·Ù„Ø§Ù‚ Jupyter Lab
launch_jupyter() {
    log "Launching Jupyter Lab..."
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù†ÙØ° Ø§Ù„Ù…ØªØ§Ø­
    PORT=8888
    while lsof -i :$PORT &> /dev/null; do
        PORT=$((PORT + 1))
    done
    
    log "Using port $PORT"
    
    # Ø¥Ø·Ù„Ø§Ù‚ Jupyter Lab
    jupyter lab --port $PORT --ip 0.0.0.0 --no-browser --allow-root
}

# Ø¥Ø·Ù„Ø§Ù‚ FastAPI Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
launch_api() {
    log "Launching FastAPI server..."
    
    uvicorn src.production.api:app --host 0.0.0.0 --port 8000 --reload
}

# Ø¥Ø·Ù„Ø§Ù‚ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
launch_monitoring() {
    log "Launching monitoring dashboard..."
    
    if command -v docker-compose &> /dev/null; then
        docker-compose up -d prometheus grafana
        log "Monitoring dashboard available at http://localhost:3000"
        log "Default credentials: admin/admin"
    else
        warning "Docker-compose not available. Skipping monitoring dashboard."
    fi
}

# Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
show_system_info() {
    log "System Information:"
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¨Ø§ÙŠØ«ÙˆÙ†
    python_version=$(python --version 2>&1)
    log "Python version: $python_version"
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª PyTorch
    torch_version=$(python -c "import torch; print(torch.__version__)")
    log "PyTorch version: $torch_version"
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª GPU
    if command -v nvidia-smi &> /dev/null; then
        gpu_info=$(nvidia-smi --query-gpu=name,memory.total --format=csv)
        log "GPU Information:"
        echo "$gpu_info"
    else
        log "No GPU detected"
    fi
    
    # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    system_info=$(uname -a)
    log "System: $system_info"
}

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
main() {
    log "Starting AI Engineer Toolkit 2025..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    check_prerequisites
    
    # Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
    show_system_info
    
    # Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø®Ø¯Ù…Ø§Øª
    log "Launching services..."
    
    # Ø¥Ø·Ù„Ø§Ù‚ Jupyter Lab ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    launch_jupyter &
    JUPYTER_PID=$!
    
    # Ø¥Ø·Ù„Ø§Ù‚ FastAPI ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©
    launch_api &
    API_PID=$!
    
    # Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    launch_monitoring
    
    log ""
    log "\033[1;32mDevelopment environment is ready!\033[0m"
    log ""
    log "Jupyter Lab URL: http://localhost:8888"
    log "FastAPI Docs: http://localhost:8000/docs"
    log "Monitoring Dashboard: http://localhost:3000"
    log ""
    log "To stop all services, press Ctrl+C"
    log ""
    
    # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø®Ø¯Ù…Ø§Øª
    trap "kill $JUPYTER_PID $API_PID; exit 0" SIGINT
    wait
}

# ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
main "$@"
```

## Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

ØªÙ… Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¨Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰:

1. **Ø§Ù„Ø¹Ù…Ù‚ Ø§Ù„Ù†Ø¸Ø±ÙŠ**: ÙƒÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ÙˆØªÙ‚Ù†ÙŠØ© Ù…Ø¨Ù†ÙŠØ© Ø¹Ù„Ù‰ Ø£Ø³Ø³ Ø±ÙŠØ§Ø¶ÙŠØ© Ù‚ÙˆÙŠØ©
2. **Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ù…Ù† Ø§Ù„ØµÙØ±**: Ø£ÙƒÙˆØ§Ø¯ Ù…Ø¨Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy Ùˆ PyTorch Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
3. **Ø¹Ù‚Ù„ÙŠØ© Ù…Ù‡Ù†Ø¯Ø³ Ø§Ù„Ø¥Ù†ØªØ§Ø¬**: Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©ØŒ ÙˆØ§Ù„ØªÙƒÙ„ÙØ©
4. **Ø§Ù„Ø¬ÙˆØ¯Ø©**: ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø´ÙŠÙØ±Ø© Ù…Ø¹ Type HintsØŒ DocstringsØŒ ÙˆØ§Ø®ØªØ¨Ø§Ø±Ø§Øª
5. **Ø§Ù„ØªØ¬Ø±Ø¨Ø©**: Ø¯Ø±Ø§Ø³Ø§Øª Ø­Ø§Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ù† Ø£ÙØ¶Ù„ Ø´Ø±ÙƒØ§Øª Ø§Ù„ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ ÙŠÙ…Ø«Ù„ "Ø­Ù‚ÙŠØ¨Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Øª" Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù…Ù‡Ù†Ø¯Ø³ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ 2025-2026ØŒ Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ù„Ø£ÙŠ Ù…Ù‡Ù†Ø¯Ø³ Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙˆØ§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬.