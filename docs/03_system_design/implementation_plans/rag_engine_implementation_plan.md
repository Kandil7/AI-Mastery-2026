



   


ïº—ï»¤ïºŽÙ… â€” ïº§ï» Ù‘ï»´ï»¨ï»² Ø£ï»—ïºªÙ‘Ù… ï»Ÿï»š Ø´Ø±ïº£Ù‹ïºŽ ï»«ï»¨ïºªïº³ï»´Ù‹ïºŽ ï»£Ùï» ïº¨Ù‘ïº¼Ù‹ïºŽ Ùˆï»Ÿï»œï»¦ ï»‹ï»¤ï»´ï»˜Ù‹ïºŽ ï»Ÿï»¤Ø´Ø±ÙˆØ¹ :mini-RAG
ï»£ïºŽ ï»«ï»®ØŸ ï»›ï»´ï»’ ï»³Ùïº’ï»¨ï»° ï»£ï»¦ Ø§ï»Ÿïº¼ï»”ïº®ØŸ Ùˆï»£ïºŽ Ø§ï»Ÿïº˜ïº¤ïº´ï»´ï»¨ïºŽØª Ø§ï»Ÿïº˜ï»² ïº—ïº ï»Œï» ï»ª Production-grade ï»“ï»ŒÙ„Ù‹Ø§.
 
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­
 
)Ø§ï»ŸØ´Ø±Ø­ ï»£ïº´ïº˜ï»¨ïºª ï»£ïº’ïºŽØ´Ø±Ø© Ø¥ï»Ÿï»° Ø§ï»Ÿï»¤ï» ï»”ïºŽØª Ø§ï»Ÿïº˜ï»² Ø±ï»“ï»Œïº˜ï»¬ïºŽ(
  Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿïº´ï» ïº´ï» ïº” mini-RAG - ï»£ï»¦â€¦
 


 

(1 ï»£ïºŽ ï»«ï»® ï»£Ø´Ø±ÙˆØ¹ mini-RAG ï»“ï»ŒÙ„Ù‹Ø§ØŸ
mini-RAG ï»«ï»® ï»£Ø´Ø±ÙˆØ¹ ïº—ï»Œï» ï»´ï»¤ï»²â€“Ø§ïº£ïº˜ïº®Ø§ï»“ï»² ï»³ï»Œï» Ù‘ï»¤ï»š ï»›ï»´ï»’ ïº—ïº¤ï»®Ù‘Ù„ ïº—ïº ïºŽØ±á»‹ Ø§ï»ŸÙ€ Notebooks Ø¥ï»Ÿï»° ï»§ï»ˆïºŽ  RAG ïºŸïºŽï»«ïº° ï»ŸÙ„Ø¥ï»§ïº˜ïºŽØ¬ ïº‘ïºŽïº³ïº˜ïº¨ïºªØ§Ù…:
API Layer ï»›Ù€ FastAPI	 
(PGVector Ø£Ùˆ Qdrant) Vector DB	  LLM Provider (OpenAI / Ollama / HF)	  Background Workers (Celery)	  Docker + Monitoring	 
Ø§ï»Ÿï»¬ïºªÙ Ø§ï»Ÿïº¤ï»˜ï»´ï»˜ï»²: ïº³ïºª Ø§ï»Ÿï»”ïº ï»®Ø© ïº‘ï»´ï»¦ Science Data Ùˆ Engineering SoftwareØŒ Ùˆïº‘ï»¨ïºŽØ¡ ï»§ï»ˆïºŽÙ… ï»³ï»¤ï»œï»¦ ïº—ïº¸ï»ï»´ï» ï»ª Ùˆïº—ï»®ïº³ï»´ï»Œï»ª Ùˆï»£ïº®Ø§ï»—ïº’ïº˜ï»ª ï»“ï»² Ø§Ù„Ø¥ï»§ïº˜ïºŽØ¬.


(Architecture Overview) Ø§ï»Ÿï»¤ï»Œï»¤ïºŽØ±ï»³ïº” Ø§ï»Ÿï»”ï»œïº®Ø© (2
Ø§ï»Ÿïº˜ïºªï»“Ù‘ï»– Ø§Ù„Ø£ïº³ïºŽØ³ÙŠ Ù„Ø£ÙŠ ïº³ïº†Ø§Ù„:

mathematica

User Question
â†’ API (FastAPI)
â†’ Embedding(question)
â†’ Vector Search (Top-K Chunks)
â†’ Prompt = Context + Question
 
â†’ LLM
â†’ Answer + Sources

 
Copy code  
 
ï»ƒïº’ï»˜ïºŽØª Ø§ï»Ÿï»¨ï»ˆïºŽÙ…:
API Layer: upload / query / health	 
Business Logic: chunking, retrieval, prompt assembly	 
:Data Layer	 
Postgres (users, docs, chats)	  Vector DB (embeddings)	 
Redis (cache)	  Ø§ï»Ÿïºœï»˜ï»´ï» ïº” ÙˆØ§ï»Ÿï»¤ï»ŒïºŽï»Ÿïº ïº” indexing ï»Ÿï» Ù€ Async Layer: Celery	 


(3 ï»«ï»´ï»œï»ž Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ )ï»›ï»¤ïºŽ ï»“ï»² Ø§ï»Ÿïº®ï»³ïº’ï»®(
 

css

mini-rag/
â”œâ”€â”€ src/
â”‚	â”œâ”€â”€ main.py
â”‚	â”œâ”€â”€ config.py
â”‚	â”œâ”€â”€ routes/
â”‚	â”œâ”€â”€ services/
â”‚	â”œâ”€â”€ models/
â”‚	â””â”€â”€ workers/
â”œâ”€â”€ docker/
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

.(checkpoint) ï»£ïº´ïº˜ï»˜ï» ïº” ïº—ï»Œï» ï»´ï»¤ï»´ïº” ï»£ïº®ïº£ï» ïº” = GitHub ï»“ï»² branch ï»›ï»ž


 

Copy code  
 
(4 Ø§ï»Ÿï»¤ï»œï»®ï»§ïºŽØª Ø§Ù„Ø£ïº³ïºŽïº³ï»´ïº” â€” ï»£ïºŽØ°Ø§ ï»³ï»”ï»Œï»ž ï»›ï»ž ïºŸïº°Ø¡ØŸ
A)	FastAPI
:endpoints ïº—ï»Œïº®ï»³ï»’	 
   upload/ Ø±ï»“ï»Š ï»£ï» ï»”ïºŽØª
 
   query/  ï»ƒïº®Ø­ Ø£ïº³ïºŒï» ïº”    health/ ï»“ïº¤ïºº Ø§ï»Ÿïº¨ïºªï»£ïº”
   Ø§ïº³ïº˜ïº¨ïºªØ§Ù… APIRouter ï»Ÿï» ïº˜ï»¨ï»ˆï»´ï»¢
validation ï»Ÿï» Ù€ Pydantic	 

B)	File Processing
PDF / DOCX / TXT :Ø¯ï»‹ï»¢	 
  Ø§ïº³ïº˜ïº¨ïº®Ø§Ø¬ Ø§ï»Ÿï»¨ïºº   ïº—ï»¨ï»ˆï»´ï»”ï»ª
(overlap ï»£ï»Š) Chunking	 
Ø§ï»Ÿï»¨ï»˜ï»„ïº” Ø§ï»Ÿïº ï»®ï»«ïº®ï»³ïº”: ïºŸï»®Ø¯Ø© Ø§ï»ŸÙ€ chunking = ïºŸï»®Ø¯Ø© Ø§Ù„Ø¥ïºŸïºŽïº‘ïº”.

C)	Embeddings + Vector DB
embedding Ø¥ï»Ÿï»° chunk ï»›ï»ž ïº—ïº¤ï»®ï»³ï»ž	  PGVector Ø£Ùˆ Qdrant ï»“ï»² ïº—ïº¨ïº°ï»³ï»¨ï»ª	  cosine similarity ïº‘Ù€ Ø§ï»Ÿïº’ïº¤ïºš	 
D)	RAG Logic
ï»Ÿï» ïº´ïº†Ø§Ù„ embedding .1
semantic search .2
.3 Ø¯ï»£ïºž Ø§ï»Ÿï»¨ïº˜ïºŽïº‹ïºž ï»“ï»² Prompt .4 Ø¥Ø±ïº³ïºŽï»Ÿï»¬ïºŽ ï»Ÿï» Ù€ LLM
.5 Ø¥Ø±ïºŸïºŽØ¹ Ø¥ïºŸïºŽïº‘ïº” ï»£ïºªï»‹ï»®ï»£ïº” ïº‘ïºŽï»Ÿïº´ï»´ïºŽÙ‚
E)	Background Workers
indexing	  parsing	 
     OCR )Ù„Ø§ïº£ï»˜Ù‹ïºŽ(    ï»³ï»¤ï»¨ï»Š ïº£ïº á¹‡ Ø§ï»ŸÙ€ API


(5 ï»›ï»´ï»’ ïº—ïº’ï»¨ï»´ï»ª ï»£ï»¦ Ø§ï»Ÿïº¼ï»”ïº® Blueprint) ï»‹ï»¤ï» ï»²(
Skeleton :1 Ø§ï»Ÿïº¨ï»„ï»®Ø©
FastAPI + config + routers	 
 
Health endpoint	 

Upload & Parsing :2 Ø§ï»Ÿïº¨ï»„ï»®Ø©
File validation		  Extract text		  Chunking	 
Vector Store :3 Ø§ï»Ÿïº¨ï»„ï»®Ø©
PGVector Ø£Ùˆ Qdrant Ø§ïº§ïº˜ï»´ïºŽØ±	 
init collection	  upsert/search	 
RAG Pipeline :4 Ø§ï»Ÿïº¨ï»„ï»®Ø© ÙˆØ§ïº¿ïº¢ Prompt template	  Context-only answering		 
Async :5 Ø§ï»Ÿïº¨ï»„ï»®Ø©
Celery + broker	  workers ï»‹ï»¦ API ï»“ïº¼ï»ž	 
Docker :6 Ø§ï»Ÿïº¨ï»„ï»®Ø© docker-compose	  env management		 


(6 Ø§ï»Ÿïº˜ïº¤ïº´ï»´ï»¨ïºŽØª Ø§ï»Ÿï»¤ï»¬ï»¤ïº” )Ø§ï»Ÿïº˜ï»² ïº—ï»”ïº®Ù‘Ù‚ Demo ï»‹ï»¦ (Production
Ø°ï»›ï»² Chunking .1 
Recursive chunking
ïº£ïº´á¹‡ Ø§ï»Ÿï»Œï»¨ïºŽÙˆï»³ï»¦ / Ø§ï»Ÿï»”ï»˜ïº®Ø§Øª
(chars ï»£ïº¶) Token-aware

Hybrid Search .2 ðŸ”¥
Semantic (Embeddings) Keyword (BM25)
Recall Ø£ï»“ï»€ï»ž	 
 
Re-Ranking .3 ðŸ”¥
Top-20 â†’ Reranker â†’ Top-3	  ïºŸÙ‹ïºªØ§ hallucination ï»³ï»˜Ù‘ï» ï»ž	 
Multi-Tenancy .4 ðŸ”¥
payload ï»“ï»² user_id	  vector search ï»“ï»² filter	 
   ï»‹ïº°Ù„ ïº‘ï»´ïºŽï»§ïºŽØª ïº£ï»˜ï»´ï»˜ï»²

Caching .5 ðŸ”¥
embeddings ï»Ÿï» Ù€ Redis	 
   Redis ï»ŸÙ„Ø¥ïºŸïºŽïº‘ïºŽØª Ø§ï»Ÿï»¤ïº˜ï»œïº®Ø±Ø©    ïº—ï»˜ï» ï»´ï»ž Ø§ï»Ÿïº˜ï»œï» ï»”ïº” ÙˆØ§ï»ŸÙ€ latency
Guardrails .6 ðŸ”¥
â€If answer not in context â†’ say I donâ€™t knowâ€œ	 
ï»£ï»¨ïº¨ï»”ïº¾ temperature	  ï»£ï»€ïº’ï»®Ø· max tokens	 
Observability .7 ðŸ”¥
Prometheus metrics		  stage ï»Ÿï»œï»ž latency		  token usage	  retrieval hit rate	 


(7 ï»£ïºŽØ°Ø§ ïº—ïº˜ï»Œï» Ù‘ï»¢ ï»“ï»Œï» ï»´Ù‹ïºŽ ï»£ï»¦ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ØŸ
ïº‘ï»Œïºª Ø¥ïº—ï»¤ïºŽÙ… mini-RAG Ø£ï»§ïº–:
   ïº—ï»”ï»¬ï»¢ RAG ï»›Ù€ System Ùˆï»Ÿï»´ïº² ï»›ï»®Ø¯    ï»—ïºŽØ¯Ø± ïº—ïº’ï»¨ï»²:
ChatPDF-like systems	  Internal Knowledge Bots	 
ïº£ï»˜ï»´ï»˜ï»´ïº” AI APIs	 
Senior AI / Backend Engineer ï»Ÿï»¤ïº´ïº˜ï»®Ù‰ ïºŸïºŽï»«ïº°	 

 
(8 Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” )ï»Ÿï»® ïº£ïºŽïº‘á»± ï»§ï»œï»¤Ù‘ï»ž(
ï»§ï»˜ïºªØ± ï»§ï»¨ï»˜ï»ž Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ø¥ï»Ÿï»°:
Enterprise RAG	  Auth + Billing	  UI (Next.js)	 
Evaluation Framework	 
Agentic RAG	 
ï»Ÿï»® ïº—ïº¤:á¹‡
   Ø£Ø´Ø±Ø­ ï»›ï»ž Branch ïº³ï»„ïº® ïº‘ïº´ï»„ïº®
Clean Architecture ï»§ïº´ïº¨ïº” Ø£ï»‹ï»¤ï»ž Ø£Ùˆ	 
Starter Template Production-Ready ï»§ïº¤ï»®ï»Ÿï»ª Ø£Ùˆ	 
ï»—ï»®ï»ŸÙ‘ï»² ïº—ïº¤á¹‡ ï»§ï»œï»¤Ù‘ï»ž ï»“ï»² Ø£ÙŠ Ø§ïº—ïº ïºŽÙ‡ 





Clean Architecture ï»§ïº´ïº¨ïº” Ø£ï»‹ï»¤ï»ž Starter Template Production-Ready ï»§ïº¤ï»®ï»Ÿï»ª Ø£Ùˆ

ï»£ïº ïº®Ø¯ ï»£ïº¶) Clean Architecture + Starter Template Production-Ready ï»›Ù€ ï»«ï»¨ï»Œï»¤ï» ï»¬ïºŽ â€” ïº—ï»¤ïºŽÙ…
.(refactor
ï»«ïº˜ï»„ï» ï»Š ï»“ï»² Ø§Ù„Ø¢ïº§ïº® ïº‘Ù€ Template Repo ïº—ï»˜ïºªØ± ïº—ïº’ïºªØ£ ï»£ï»¨ï»ª Ø£ÙŠ backend RAG/AI ïº‘Ø³Ø±ï»‹ïº”ØŒ Ùˆïº—ïº’ïºªÙ‘Ù„ Providers
ï»£ïº’ïºŽØ´Ø± Ø§ï»£ïº˜ïºªØ§Ø¯ Ø¯Ù‡) .Use Cases Ø§ï»ŸÙ€ ïº—ï» ï»¤ïº² ï»£ïºŽ ïº‘ïºªÙˆÙ† Vector DB (Qdrant/PGVector)Ùˆ (OpenAI/Ollama)
 
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­
 
ï»Ÿï»”ï» ïº´ï»”ïº” Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ø§Ù„Ø£ïº»ï» ï»²: ï»“ïº¼ï»ž Ø§ï»ŸÙ€ API/DB/LLM ï»‹ï»¦ Ø§ï»ŸÙ€ (Core
  Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿïº´ï» ïº´ï» ïº” mini-RAG - ï»£ï»¦â€¦
 


 

(1 Ø§ï»Ÿï»¬ïºªÙ Ø§ï»Ÿï»¤ï»Œï»¤ïºŽØ±ÙŠ
ÙˆÙ„Ø§ Postgres ÙˆÙ„Ø§ FastAPI ï»‹ï»¦ ïº£ïºŽïºŸïº” Ø£ÙŠ ï»£ïºŽï»³ï»Œïº®ï»“ïº¶ Core (Entities + Use Cases) Ø§ï»ŸÙ€ :Ø°ï»«ïº’ï»´ïº” ï»—ïºŽï»‹ïºªØ©
.OpenAI ÙˆÙ„Ø§ Qdrant
 
(Document, Chunk, Query, Answer) Ùˆï»—ï»®Ø§ï»‹ïºª ï»›ï»´ïºŽï»§ïºŽØª :Domain	  Application: Use Cases (UploadDocument, IndexDocument, AskQuestion)	 
Ports: Interfaces (LLMProvider, EmbeddingsProvider, VectorStore, DocRepo, Cache,	 
FileStore)
Adapters: Implementations (OpenAI/Ollama, Qdrant/PGVector, Postgres, Redis,	 
S3/local)
Delivery: FastAPI Routes + DTOs	 
Infra: DB sessions, logging, tracing, Celery, config, migrations	 


Repo Structure (Production-Ready) (2
Ø¯Ù‡ ï»«ï»´ï»œï»ž ï»£ï»˜ïº˜ïº®Ø­ â€starterâ€œ ï»§ï»ˆï»´ï»’ Ùˆï»—ïºŽïº‘ï»ž ï»Ÿï» ïº˜ï»®ïº³ï»Œïº”:

text

rag-starter/
â”œâ”€â”€ app/
â”‚	â”œâ”€â”€ main.py	# FastAPI app + DI bootstrap
â”‚	â”œâ”€â”€ api/
â”‚	â”‚	â”œâ”€â”€ v1/
â”‚	â”‚	â”‚	â”œâ”€â”€ routes_documents.py
â”‚	â”‚	â”‚	â”œâ”€â”€ routes_queries.py
â”‚	â”‚	â”‚	â””â”€â”€ routes_health.py
â”‚	â”‚	â””â”€â”€ deps.py	# auth, request-scoped deps
â”‚	â”œâ”€â”€ core/
â”‚	â”‚	â”œâ”€â”€ config.py	# Settings (pydantic-settings)
â”‚	â”‚	â”œâ”€â”€ logging.py	# structlog/loguru setup
â”‚	â”‚	â””â”€â”€ observability.py	# metrics/tracing wiring
â”‚	â”œâ”€â”€ domain/
â”‚	â”‚	â”œâ”€â”€ entities.py	# Document, Chunk, Query, Answer
â”‚	â”‚	â””â”€â”€ errors.py	# Domain errors
â”‚	â”œâ”€â”€ application/
â”‚	â”‚	â”œâ”€â”€ dto.py	# Request/Response models (internal)
â”‚	â”‚	â”œâ”€â”€ ports/

â”‚	â”‚	â”‚	â”œâ”€â”€	llm.py	
â”‚	â”‚	â”‚	â”œâ”€â”€	embeddings.py	
â”‚	â”‚	â”‚	â”œâ”€â”€	vector_store.py	
â”‚	â”‚	â”‚	â”œâ”€â”€	repos.py	#	documents/chats
â”‚	â”‚	â”‚	â”œâ”€â”€	cache.py		
 

â”‚	â”‚	â”‚	â””â”€â”€ file_store.py	
â”‚	â”‚	â”œâ”€â”€	use_cases/	
â”‚	â”‚	â”‚	â”œâ”€â”€ upload_document.py	
â”‚	â”‚	â”‚	â”œâ”€â”€ index_document.py	
â”‚	â”‚	â”‚	â””â”€â”€ ask_question.py	
â”‚	â”‚	â””â”€â”€	services/	
â”‚	â”‚		â”œâ”€â”€ chunking.py	#	pure logic
â”‚	â”‚		â”œâ”€â”€ prompt_builder.py	#	pure logic
â”‚	â”‚		â””â”€â”€ rerank.py	#	optional
â”‚	â”œâ”€â”€ adapters/
â”‚	â”‚	â”œâ”€â”€ llm/
â”‚	â”‚	â”‚	â”œâ”€â”€ openai_llm.py
â”‚	â”‚	â”‚	â””â”€â”€ ollama_llm.py
â”‚	â”‚	â”œâ”€â”€ embeddings/
â”‚	â”‚	â”‚	â”œâ”€â”€ openai_embeddings.py
â”‚	â”‚	â”‚	â””â”€â”€ local_embeddings.py
â”‚	â”‚	â”œâ”€â”€ vector/
â”‚	â”‚	â”‚	â”œâ”€â”€ qdrant_store.py
â”‚	â”‚	â”‚	â””â”€â”€ pgvector_store.py
â”‚	â”‚	â”œâ”€â”€ persistence/
â”‚	â”‚	â”‚	â”œâ”€â”€ postgres/
â”‚	â”‚	â”‚	â”‚	â”œâ”€â”€ models.py
â”‚	â”‚	â”‚	â”‚	â”œâ”€â”€ repo_documents.py
â”‚	â”‚	â”‚	â”‚	â””â”€â”€ repo_chats.py
â”‚	â”‚	â”‚	â””â”€â”€ migrations/	# Alembic
â”‚	â”‚	â”œâ”€â”€ cache/
â”‚	â”‚	â”‚	â””â”€â”€ redis_cache.py
â”‚	â”‚	â””â”€â”€ filestore/
â”‚	â”‚	â”œâ”€â”€ local_store.py
â”‚	â”‚	â””â”€â”€ s3_store.py
â”‚	â”œâ”€â”€ workers/
â”‚	â”‚	â”œâ”€â”€ celery_app.py
â”‚	â”‚	â””â”€â”€ tasks.py	# index pipeline background
â”‚	â””â”€â”€ tests/
â”‚	â”œâ”€â”€ unit/
â”‚	â””â”€â”€ integration/
â”œâ”€â”€ docker/
â”‚	â”œâ”€â”€ docker-compose.yml
â”‚	â”œâ”€â”€ Dockerfile
â”‚	â””â”€â”€ prometheus_grafana/	# optional
 
â”œâ”€â”€ scripts/
â”‚	â”œâ”€â”€ dev.sh
â”‚	â””â”€â”€ lint.sh
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ .env.example
â””â”€â”€ README.md



Clean Architecture á»±ï»—ï»  â€” Ports (Interfaces) Ø§ï»ŸÙ€ (3
Ø£ï»£ïºœï» ïº” ï»Ÿï» Ù€ interfaces Ø§ï»Ÿï» ï»² â€œïº‘ïº˜ï»˜ï»”ï»žâ€ Ø§ï»Ÿïº˜ïº’ï»Œï»´ïºŽØª ï»Ÿï» ïº¨ïºŽØ±Ø¬:

VectorStore Port
upsert(chunks)
search(query_embedding, filters, top_k)
delete(document_id, tenant_id)	 

EmbeddingsProvider Port
embed_text(text) -> List[float]
embed_batch(texts) -> List[List[float]]	 

LLMProvider Port
generate(prompt, params) -> str	 

DocumentRepository Port (Postgres)
create_document(meta)
save_chunks_metadata(doc_id, chunk_ids, ...)
update_status(doc_id, status)

 





Copy code
 
list_documents(tenant_id)	 

Cache Port (Redis)
get_embedding_cache(hash)	  set_embedding_cache(hash, vector, ttl)	 
.ïº—ïº˜ïº„ïº›ïº® Use Cases ï»£ïºŽ ïº‘ïºªÙˆÙ† Ollama ïº‘Ù€ OpenAI Ø£Ùˆ PGVector ïº‘Ù€ Qdrant ïº—ï»Ù‘ï»´ïº® ïº—ï»˜ïºªØ± ï»›ïºªÙ‡
 


 
 
Use Cases (Application Layer) â€” Where the Business Lives (4
UC1: UploadDocument
Input: file + tenant_id
Output: document_id + status

:Flow

validate file .1
store file (FileStore port) .2 create doc row (DocumentRepo port) .3 enqueue indexing task (Celery) .4
UC2: IndexDocument (Worker)
:Flow
load file .1 extract text (adapter: pdf/docx/txt) .2 chunking (pure service) .3
embeddings (Embeddings port) + caching .4 upsert to vector store (VectorStore port) .5 persist chunk metadata/status (Repo port) .6
UC3: AskQuestion
:Flow
embed question (cache-first) .1
vector search (tenant filter) .2
optional rerank .3
build prompt (pure) .4
call LLM .5
return answer + sources .6


FastAPI ï»“ï»² ï»‹ï»¤ï» ï»² ïº‘ïº¸ï»œï»ž â€” DI (Dependency Injection) (5
:Settings á¹‡ïº£ïº´ ports ï»Ÿï» Ù€ implementations ï»³ïº®Ù‘ïºŸï»Š bootstrap.py Ø£Ùˆ container.py ï»«ïº˜ï»Œï»¤ï»ž	 
VECTOR_BACKEND=qdrant|pgvector	 
 
LLM_BACKEND=openai|ollama
EMBEDDINGS_BACKEND=openai|local	 
.startup ï»“ï»² ÙˆØ§ïº£ïºª adapters + wiring Ø§ï»ŸÙ€ ï»“ï»² Factory :Ø§ï»Ÿï»”ï»œïº®Ø©


(Starter Ø§ï»ŸÙ€ ï»“ï»²  Ù„Ø§Ø²) Production-Ready Checklist (6
Ø¯Ù‡ Ø§ï»Ÿï»”ïº®Ù‚ Ø§ï»Ÿïº¤ï»˜ï»´ï»˜ï»² ïº‘ï»´ï»¦ â€œï»£Ø´Ø±ÙˆØ¹ ïº—ï»Œï» ï»´ï»¤ï»²â€ Ùˆ Productionâ€œ :â€Template

Security API Key / JWT		  Rate limiting	 
(Postgres ï»“ï»² vector + row-level constraints ï»“ï»² filters) Tenant isolation end-to-end	 

Reliability
LLM ï»ŸÙ€ Retries + circuit breakers	  upload/index ï»Ÿï» Ù€ Idempotency keys	  workers ï»Ÿï» Ù€ Dead letter queue	 
Observability
Structured logs	  Metrics: latency per stage (embed/search/llm)	 
Tracing: request_id	 

Cost/Latency
embeddings + answers ï»Ÿï» Ù€ Redis caching	 
Batch embeddings	  Prompt budget + truncation	 
Quality
Hybrid search (BM25 + vector)	 
Reranking	  Eval harness (golden Q/A, retrieval hit rate)	 


(Ø³Ø±ï»³ï»Š Mapping) ï»‹ï»¤ï» Ù‹ï»´ïºŽ â€mini-rag ïº—ïº¤ï»®ï»³ï»žâ€œ (7
 



Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿïº´ï» ïº´ï» ïº” mini-RAG - ï»£ï»¦â€¦
 
Ø§ï»Ÿï» ï»² ï»‹ï»¨ïºªÙƒ ï»“ï»² Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ø§ï»Ÿïº¤ïºŽï»Ÿï»²:
FastAPI endpoints + env + routing	 
 
upload â†’ chunk â†’ embedding â†’ vector search â†’ prompt â†’ LLM :ï»£ïº®Ø§ïº£ï»ž
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­  
ï»«ï»¨ï»Œï»¤ï»ž:
application/services/* (pure) Ø¥ï»Ÿï»° chunking/prompt logic ï»§ï»˜ï»ž	 
*/adapters Ø¥ï»Ÿï»° Qdrant/OpenAI ï»§ï»˜ï»ž	  DTO â†’ call use case â†’ response :(thin) â€Ø±ï»“ï»´ï»Œïº”â€œ routes Ø§ï»ŸÙ€ ïº—ïº¨ï» ï»²	 


(8 ï»£ïº¨ïº®ïºŸïºŽØª ïºŸïºŽï»«ïº°Ø© Ø£ï»—ïºªØ± Ø£ï»›ïº˜ïº’ï»¬ïºŽ ï»Ÿï»š ï»“ï»®Ø±Ù‹Ø§ )ïº‘ïºªÙˆÙ† Ø£ïº³ïºŒï» ïº” Ø¥ïº¿ïºŽï»“ï»´ïº”(
ï»Ÿï»® ïº—ïº¤á¹‡ ïº—ï»¤Ø´ÙŠ ïº‘ïº„Ø³Ø±Ø¹ ï»ƒïº®ï»³ï»–ØŒ Ø£ï»§ïºŽ Ø£ï»›ïº˜á¹‡ ï»Ÿï»š ï»«ï»¨ïºŽ ï»“ï»² ï»§ï»”ïº² Ø§ï»Ÿïº¸ïºŽØª )ï»‹ï» ï»° Ø¯ï»“ï»ŒïºŽØª:(
(interfaces) ï»›ïºŽï»£ï» ïº” Ports .1
Use Cases (Upload/Index/Ask) 3 .2 Adapters: OpenAI + Ollama + Qdrant + PGVector + Redis .3 FastAPI routes (v1) + dependency wiring .4
Celery tasks + docker compose (api + worker + db + qdrant + redis) .5


Ø§ïº§ïº˜ïºŽØ± ï»£ïº´ïºŽØ± Ø§ï»Ÿïº˜ï»¨ï»”ï»´ïº¬ )ÙˆØ£ï»§ïºŽ ï»«ïº’ïºªØ£ ï»“ï»®Ø±Ù‹Ø§ ïº‘ïºŽï»Ÿï»¬ï»´ï»œï»ž ÙˆØ§ï»Ÿï»œï»®Ø¯(
A) Starter â€œMinimal Productionâ€: (OpenAI + Qdrant + Postgres + Redis + Celery)	 
OpenAI/Ollama,) ï»Ÿï» ïº˜ïº’ïºªï»³ï»ž A + factories ï»§ï»”ïº² :â€B) Starter â€œProvider-Agnostic	 
(Qdrant/PGVector
Ø£ï»§ïºŽ Ø£Ø±ïº·ïº¢ B Ù„Ø£ï»§ï»¬ïºŽ Ø§ï»Ÿï»˜ï»´ï»¤ïº” Ø§ï»Ÿïº¤ï»˜ï»´ï»˜ï»´ïº” ï»Ÿï» Ù€ Architecture CleanØŒ Ùˆïº—ïº¨ïºªï»£ï»š ï»“ï»² ï»£ïº¸ïºŽØ±ï»³ï»Š Ø§ï»Ÿï»Œï»¤Ù„Ø§Ø¡.





clean archticture and clean code and SOLID and ready for production
 
ïº—ï»¤ïºŽÙ… â€” ï»«ï»¨ï»„ï» ï»Š Template Starter ï»³ïº¤ï»˜ï»– 4 ïº£ïºŽïºŸïºŽØª ï»£ï»Š ïº‘ï»Œïº¾:
(FastAPI/DB/LLM ï»‹ï»¦ ïº—ï»¤ïºŽÙ‹ï»£ïºŽ ï»£ï»¨ï»”ïº¼ï» ï»´ï»¦ Domain/Application) Clean Architecture .1
Clean Code (Naming, boundaries, small functions, no god classes) .2
(DIP + ISP + SRP ïº§ïº¼ï»®Ù‹ïº»ïºŽ) SOLID .3
Production-ready (Auth, multi-tenant, async workers, caching, observability, .4
migrations, docker)
mini-RAG: upload â†’ chunk â†’ embed â†’ vector search Ø§ï»ŸÙ€ ï»Ÿïº¨ï»‚ Ø§ï»Ÿï»„ïº’ï»´ï»Œï»² Ø§ï»Ÿïº˜ï»„ï»®ï»³ïº® ïº‘ïºŽï»Ÿï»€ïº’ï»‚ ÙˆØ¯Ù‡
 
Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿïº´ï» ïº´ï» ïº” mini-RAG - ï»£ï»¦â€¦
 
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­
 
LLM â†’ prompt â†’ØŒ ï»Ÿï»œï»¦ ïº‘ï»¤ïº´ïº˜ï»®Ù‰ ï»§ï»ˆïºŽÙ… Ø¥ï»§ïº˜ïºŽïºŸï»²
 

 

(ïºŸïºŽï»«ïº° ï»£ï»˜ïº˜ïº®Ø­) Structure: Clean Architecture (1

text

rag-starter/
â”œâ”€â”€ app/
â”‚	â”œâ”€â”€ main.py	# FastAPI bootstrap

â”‚	â”œâ”€â”€	api/v1/	#	Controllers (thin)
â”‚	â”œâ”€â”€	core/	#	config, logging, metrics
â”‚	â”œâ”€â”€	domain/	#	Entities + domain rules
â”‚	â”œâ”€â”€	application/		
â”‚	â”‚	â”œâ”€â”€ ports/	#	Interfaces (DIP)
â”‚	â”‚	â”œâ”€â”€ use_cases/	#	Orchestration (SRP)
â”‚	â”‚	â””â”€â”€ services/	#	pure logic (chunking/prompt)
â”‚	â”œâ”€â”€	adapters/	#	Implementations (DB/LLM/Vector/Cache)
â”‚	â”œâ”€â”€	workers/	#	Celery
â”‚	â””â”€â”€	tests/	#	unit + integration
â”œâ”€â”€ docker/
â”œâ”€â”€ pyproject.toml
â””â”€â”€ .env.example

ï»—ï»®Ø§ï»‹ïºª ïº»ïºŽØ±ï»£ïº”
.FastAPI/Qdrant/OpenAI/SQLAlchemy ï»³ïº´ïº˜ï»®Ø±Ø¯ÙˆØ§ ï»£ï»¤ï»¨ï»®Ø¹ /application Ùˆ /domain	 
   /api Ùˆ /adapters ï»«ï»¢ Ø§ï»Ÿï» ï»² â€œï»³ïº˜ïº´ïº¨ï»®Ø§â€ ïº‘ïºŽï»Ÿïº˜ï»˜ï»¨ï»´ïºŽØª.    ï»›ï»ž Case Use = ï»£ï» ï»’ ï»£ïº´ïº˜ï»˜ï»ž = ï»£ïº´ïº†Ùˆï»Ÿï»´ïº” ÙˆØ§ïº£ïºªØ© .(SRP)
 
(2 SOLID ï»‹ï» ï»° Ø£Ø±Ø¶ Ø§ï»Ÿï»®Ø§ï»—ï»Š )ï»£ïº¶ ïº·ï»ŒïºŽØ±Ø§Øª(
S â€” SRP
.DB session ÙˆÙ„Ø§ chunking ÙˆÙ„Ø§ parsing ï»³ï»Œï»¤ï»ž Ù„Ø§ AskQuestionUseCase	 
*/application/services ï»“ï»² parsing/chunking/prompt	 
*/adapters/persistence ï»“ï»² DB	 

O â€” OCP
ïº—ï»€ï»´ï»’ Provider ïºŸïºªï»³ïºª Anthropic) ï»£ïºœÙ„Ù‹Ø§( ïº‘ïºˆï»§ïº¸ïºŽØ¡ Adapter ïºŸïºªï»³ïºª ï»³ï»„ïº’ï»– LLMPort ïº‘ïºªÙˆÙ† ïº—ï»Œïºªï»³ï»ž Ø§ï»ŸÙ€ Use
.Case

L â€” LSP
Ø£ÙŠ Adapter ï»³ï»„ïº’ï»– ï»§ï»”ïº² Port Ù„Ø§Ø²Ù… ï»³ïº¸ïº˜ï»ï»ž ïº‘ï»¨ï»”ïº² Ø§ï»Ÿï»Œï»˜ïºª (contract) ÙˆØ¥Ù„Ø§ ï»«ïº˜ï»œØ³Ø± Ø§ï»ŸÙ€ .Core

I â€” ISP
Ports ïº»ï»ï»´ïº®Ø© Ùˆï»£ïº®ï»›ïº°Ø©:
EmbeddingsPort
VectorStorePort
LLMPort
DocumentRepoPort
CachePort	 
ïº‘ïºªÙ„ Port ÙˆØ§ïº£ïºª ïº¿ïº¨ï»¢.

(Ø§Ù„Ø£ï»«ï»¢) D â€” DIP
.ï»“ï»˜ï»‚ Interfaces (Ports) ï»‹ï» ï»° ïº—ï»Œïº˜ï»¤ïºª Use Cases


ÙˆØ§ïº¿ïº¢ + Domain (Entities) â€” minimal (3
.ï»«ï»¨ïºŽ Pydantic Ù„Ø§ .ï»«ï»¨ïºŽ ORM Ù„Ø§

python

# app/domain/entities.py
from dataclasses import dataclass
from typing import Optional, Sequence
 
@dataclass(frozen=True) class TenantId:
value: str

@dataclass(frozen=True) class DocumentId:
value: str

@dataclass(frozen=True) class Chunk:
id: str
document_id: DocumentId tenant_id: TenantId
text: str

@dataclass(frozen=True) class Answer:
text: str
sources: Sequence[str] # chunk ids or doc refs



Ø§ï»Ÿï»¤ï»Œï»¤ïºŽØ±ï»³ïº” á»±ï»—ï»  â€” Application Ports (Interfaces) (4


 
class LLMPort(Protocol):
def generate(self, prompt: str, *, temperature: float, max_tokens: int) -> st


Copy code  



 
Copy code  
 
(Orchestration) â€Ø§ï»Ÿïº˜ï»¨ï»”ï»´ïº¬ÙŠ Ø§ï»Ÿï»¤ï»¨ï»„ï»–â€œ â€” Use Cases (5
AskQuestionUseCase (Production-grade)
cache embeddings	  tenant isolation	 
(â€I donâ€™t know ï»—ï»®Ù„ Ø§ï»Ÿïº´ï»´ïºŽÙ‚ ï»“ï»² ï»£ïº¶ ï»Ÿï»®â€œ) prompt guardrail	  (metrics ï»Ÿï» Ù€ ïºŸïºŽï»«ïº°Ø©) latency hooks ï»—ï»´ïºŽïº³ïºŽØª	 
 
python

# app/application/use_cases/ask_question.py
import hashlib
from dataclasses import dataclass
from app.application.ports.embeddings import EmbeddingsPort
from app.application.ports.vector_store import VectorStorePort from app.application.ports.llm import LLMPort
 
from app.application.ports.cache import CachePort
from app.application.services.prompt_builder import build_rag_prompt from app.domain.entities import TenantId, Answer

@dataclass
class AskQuestionRequest: tenant_id: str
question: str top_k: int = 5

class AskQuestionUseCase: def  init (
self,
embeddings: EmbeddingsPort,
vector_store: VectorStorePort, llm: LLMPort,
cache: CachePort,
) -> None:
self._embeddings = embeddings self._vector = vector_store
self._llm = llm
self._cache = cache

def execute(self, req: AskQuestionRequest) -> Answer: tenant = TenantId(req.tenant_id)

# ---- Embedding cache (cost + latency control)
key = "emb:q:" + hashlib.md5(req.question.encode("utf-8")).hexdigest() cached = self._cache.get_json(key)
if cached and "vector" in cached: q_vec = cached["vector"]
else:
q_vec = self._embeddings.embed_one(req.question)
self._cache.set_json(key, {"vector": q_vec}, ttl_seconds=7 * 24 * 360

# ---- Multi-tenant retrieval
chunks = self._vector.search(q_vec, tenant_id=tenant, top_k=req.top_k) prompt = build_rag_prompt(question=req.question, chunks=chunks)
 
text = self._llm.generate(prompt, temperature=0.2, max_tokens=600) sources = [c.id for c in chunks]
return Answer(text=text, sources=sources)


Prompt Builder (pure function)

.SRP + pure function + testable :Ù„Ø§ïº£ï»†



 

Copy code  
 
Adapters (Infra) â€” Qdrant/OpenAI/Postgres/Redis (6
:(Ùï»£ïº¨ïº˜ØµØ±) Qdrant ï»£ïºœïºŽÙ„ .ports ï»Ÿï» Ù€ implementations á¹‡ïº—ï»œïº˜ ï»«ï»¨ïºŽ
 

python

# app/adapters/vector/qdrant_store.py
from typing import Sequence
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue, PointStruct from app.application.ports.vector_store import VectorStorePort
from app.domain.entities import Chunk, TenantId class QdrantVectorStore(VectorStorePort):
 
def   init  (self, client: QdrantClient, collection: str) -> None: self._client = client
self._collection = collection

def upsert(self, chunks: Sequence[Chunk], vectors: Sequence[list[float]]) -> points = []
for c, v in zip(chunks, vectors): points.append(PointStruct(
id=c.id, vector=v, payload={
"text": c.text,
"tenant_id": c.tenant_id.value,
"document_id": c.document_id.value,
}
))
self._client.upsert(collection_name=self._collection, points=points)

def search(self, query_vector: list[float], tenant_id: TenantId, top_k: int) flt = Filter(
must=[FieldCondition(key="tenant_id", match=MatchValue(value=tenant_i
)
res = self._client.search(
collection_name=self._collection, query_vector=query_vector,
query_filter=flt, limit=top_k
)
out: list[Chunk] = [] for r in res:
payload = r.payload or {} out.append(Chunk(
id=str(r.id),
document_id= docid(payload.get("document_id")), tenant_id=TenantId(payload.get("tenant_id")),
text=payload.get("text", ""),
))
return out

def   docid(v: str | None):
 
from app.domain.entities import DocumentId return DocumentId(v or "unknown")




API Layer (FastAPI) â€” Thin Controllers (7
Ø§ï»ŸÙ€ route ï»³ï»Œï»¤ï»ž:
auth â†’ tenant_id	  validate request DTO	 
call use case	  return response	 
ï»£ï»¤ï»¨ï»®Ø¹ ï»£ï»¨ï»„ï»– RAG ïºŸï»®Ù‡ .route



 






Copy code  
 
Production Readiness (Minimum Set) (8
Ø¯Ù‡ â€œØ§ï»Ÿïº¤ïºª Ø§Ù„Ø£Ø¯ï»§ï»°â€ Ø§ï»Ÿï» ï»² Ù„Ø§Ø²Ù… ï»³ï»œï»®Ù† ï»“ï»² Ø§ï»Ÿïº´ïº˜ïºŽØ±ïº—ïº®:

Security
(JWT Ø£Ùˆ) API Key header	 
Rate limiting	  (body ï»£ï»¦ ï»£ïº¶) auth layer ï»£ïº¼ïºªØ±Ù‡ Ø¯Ø§ïº‹Ù‹ï»¤ïºŽ tenant_id	 
Async & Scalability
Upload triggers Celery indexing task	  heavy processing ï»³ï»Œï»¤ï»ž Ù„Ø§ API	 
Reliability
LLM calls ï»Ÿï» Ù€ Retries	 
Timeouts	  upload/index (document hash) ï»Ÿï» Ù€ Idempotency	 
Observability
structured logs + request_id	  metrics: embed/search/llm latency	 
 
tracing (optional)	 

Data
Postgres + Alembic	  Vector DB + migration/init	 


(9 Ø§ï»Ÿï» ï»² ï»«ïºŽï»‹ï»¤ï» ï»ª â€œØ§ï»Ÿïº˜ïºŽï»Ÿï»² ï»£ïº’ïºŽØ´Ø±Ø©â€ ï»«ï»¨ïºŽ ï»“ï»² Ø§ï»Ÿïº¸ïºŽØª )ïº‘ïºªÙˆÙ† ï»Ÿï»’ ÙˆØ¯ÙˆØ±Ø§Ù†(
ï»Ÿï»® ï»«ï»¨ï»œï»¤ï»ž ïº‘ï»¨ï»”ïº² Ø§Ù„Ø£ïº³ï» ï»®:á»‹ ï»«ï»œïº˜á¹‡ ï»Ÿï»š ï»—ïºŽï»Ÿá»± repo ï»›ïºŽï»£ï»ž ïº‘ïºŽï»Ÿï»œï»®Ø¯ ï»‹ï» ï»° Ø£ïºŸïº°Ø§Ø¡:
core/config.py + .env.example + logging .1
auth deps + tenant extraction .2
UploadDocumentUseCase + IndexDocument worker pipeline .3
Postgres repos + Alembic models .4
Redis cache adapter .5
Docker compose: api + worker + postgres + redis + qdrant .6
Ùˆïº—ïº’ï»˜ï»° ï»‹ï»¨ïºªÙƒ Starter Production ïº£ï»˜ï»´ï»˜ï»² ï»£ï»„ïºŽïº‘ï»– ï»Ÿï»”ï»œïº®Ø© mini-RAG ï»Ÿï»œï»¦ â€œï»£Ùï»¬ï»¨ïºªØ³ ïº»ïº¢â€
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­  

Ø¥Ø°Ø§ ï»‹ïºŽï»³ïº°ï»§ï»² Ø£ïº‘ïºªØ£ ï»“ï»®Ø±Ù‹Ø§:
ï»«ïº’ïºªØ£ ïº‘ïºŽï»Ÿïº ïº°Ø¡ (1) Ùˆ(2) Ùˆ(3) ï»“ï»² Ø§ï»Ÿïº®Ø¯ Ø§ï»Ÿïº ïºŽÙŠ )ï»›ï»®Ø¯ ï»›ïºŽï»£ï»ž(ØŒ ïº›ï»¢ ï»§ï»œï»¤ï»ž ïº‘ïºŽï»—ï»² Ø§ï»Ÿï»¤ï» ï»”ïºŽØª ïº—ïº’ïºŽï»‹Ù‹ïºŽ.





ï»›ï»¤ï»ž


ïº—ï»¤ïºŽÙ… â€” ï»§ï»œï»¤Ù‘ï»ž ï»‹ï»¤ï» ï»´Ù‹ïºŽ Ùˆï»§ïº’ï»¨ï»² Ø§ï»Ÿïº ïº°Ø¡ (1) + (2) + :(3)
Config + Logging + Auth/Tenant + Upload Use Case + Index Worker Pipeline (Celery)
)ÙˆØ¯Ù‡ ï»§ï»”ïº² Ø§ï»ŸÙ€ flow Ø§ï»Ÿï» ï»² ï»“ï»² mini-RAG ï»Ÿï»œï»¦ ï»£Ùï»”ïº¼ï»®Ù„ Architecture Clean + ïºŸïºŽï»«ïº° ï»ŸÙ„Ø¥ï»§ïº˜ïºŽØ¬(
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­  

ï»£Ù„Ø§ïº£ï»ˆïº”: Ø§ï»Ÿï»œï»®Ø¯ ï»«ï»¨ïºŽ â€œï»—ïºŽïº‘ï»ž ï»Ÿï» ï»¨ïº´ïº¦â€ ï»›Ù€ .Starter ï»«ï»¨ï»”ïº˜ïº®Ø¶ 3.11 +Python Ùˆ pyproject.toml
 
.Ù„Ø§ïº£Ù‹ï»˜ïºŽ ï»§ïº¤Ù‘ï»®ï»Ÿï»ªâ€”ï»‹ïºŽØ¯ÙŠ requirements.txt ï»‹ï» ï»° Ø£ï»§ïº– ï»Ÿï»® .(uv Ø£Ùˆ Poetry)


Dependencies (pyproject / requirements) (0
Ø§ï»Ÿï»¤ï»¬ï»¤ï»´ï»¦ Ø§Ù„Ø¢Ù†:
fastapi, uvicorn	  pydantic-settings	 
structlog	  python-multipart	 
aiofiles	  celery + redis	 
python-docx + (PDF ï»ŸÙ€) pypdf	 


app/core/config.py (Production Settings) (1

python

# app/core/config.py
from pydantic_settings import BaseSettings, SettingsConfigDict from pydantic import Field

class Settings(BaseSettings):
model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")

# App
app_name: str = "rag-starter"
env: str = Field(default="dev", description="dev|staging|prod") debug: bool = False

# Security
api_key_header: str = "X-API-KEY"

# Providers
llm_backend: str = "openai"	# openai|ollama
embeddings_backend: str = "openai"	# openai|local
vector_backend: str = "qdrant"	# qdrant|pgvector
 


# OpenAI
openai_api_key: str | None = None
openai_chat_model: str = "gpt-4o-mini"
openai_embed_model: str = "text-embedding-3-small"

# Ollama
ollama_base_url: str = "http://localhost:11434" ollama_chat_model: str = "llama3.1"
ollama_embed_model: str = "nomic-embed-text"

# Qdrant
qdrant_host: str = "localhost" qdrant_port: int = 6333
qdrant_collection: str = "chunks"
embedding_dim: int = 1536 # must match embeddings backend

# Redis / Celery
redis_url: str = "redis://localhost:6379/0"
celery_broker_url: str = "redis://localhost:6379/1"
celery_result_backend: str = "redis://localhost:6379/2"

# Files
upload_dir: str = "./uploads" max_upload_mb: int = 20

settings = Settings()


env.example.

bash

APP_NAME=rag-starter ENV=dev
DEBUG=true

# Security
API_KEY_HEADER=X-API-KEY

# Providers
 

 


app/core/logging.py (Structured Logging) (2

 
structlog.processors.TimeStamper(fmt="iso"), structlog.processors.add_log_level,
structlog.processors.StackInfoRenderer(), structlog.processors.format_exc_info,
structlog.processors.JSONRenderer(),
],
wrapper_class=structlog.make_filtering_bound_logger(level), logger_factory=structlog.stdlib.LoggerFactory(),
cache_logger_on_first_use=True,
)



Domain + Ports + Use Cases (Upload + Index) (3
app/domain/entities.py 3.1

 
tenant_id: TenantId
document_id: DocumentId text: str

@dataclass(frozen=True) class UploadResult:
document_id: DocumentId
status: str # "queued" | "processing" | "indexed" | "failed"

Ports 3.2
app/application/ports/file_store.py

app/application/ports/document_repo.py

app/application/ports/task_queue.py

 

# app/application/ports/task_queue.py
from typing import Protocol
from app.domain.entities import DocumentId, TenantId

class TaskQueuePort(Protocol):
def enqueue_index_document(self, *, tenant_id: TenantId, document_id: Documen


C	C


Use Case: UploadDocument 3.3

python

# app/application/use_cases/upload_document.py
from dataclasses import dataclass
from app.domain.entities import TenantId, UploadResult
from app.application.ports.file_store import FileStorePort
from app.application.ports.document_repo import DocumentRepoPort from app.application.ports.task_queue import TaskQueuePort

@dataclass
class UploadDocumentRequest: tenant_id: str
filename: str
content_type: str data: bytes

class UploadDocumentUseCase:
def   init  (self, file_store: FileStorePort, repo: DocumentRepoPort, queue:
self._file_store = file_store self._repo = repo
self._queue = queue

async def execute(self, req: UploadDocumentRequest) -> UploadResult: tenant = TenantId(req.tenant_id)

stored = await self._file_store.save_upload( tenant_id=tenant.value,
upload_filename=req.filename,
 
content_type=req.content_type, data=req.data,
)

doc_id = self._repo.create_document(tenant_id=tenant, stored_file=stored) self._repo.set_status(tenant_id=tenant, document_id=doc_id, status="queue

# async indexing
self._queue.enqueue_index_document(tenant_id=tenant, document_id=doc_id) return UploadResult(document_id=doc_id, status="queued")



Adapters (LocalFileStore + InMemoryRepo + (4
Ø³Ø±ï»³ï»Š ïº—ïº¸ï»ï»´ï»ž â€” CeleryQueue)
ï»«ï»¨ïº´ïº˜ïº¨ïºªÙ… InMemoryRepo ï»£ïº†ï»—ïº˜Ù‹ïºŽ ï»‹ïº¸ïºŽÙ† template ï»³ïº¸ïº˜ï»ï»ž ï»“ï»®Ø±Ù‹Ø§. ï»“ï»² Ø§ï»Ÿïº®ïº³ïºŽï»Ÿïº” Ø§ï»Ÿïº ïºŽï»³ïº” ï»«ï»¨ïº’ïºªÙ‘ï»Ÿï»ª ïº‘Ù€
.Postgres + Alembic

app/adapters/filestore/local_store.py 4.1

 
# unique name (idempotency-friendly-ish)
digest = hashlib.md5(data).hexdigest()[:10]
safe_name = upload_filename.replace("/", "_").replace("\\", "_") name = f"{int(time.time())}_{tenant_id}_{digest}_{safe_name}"
path = os.path.join(self._dir, name)

with open(path, "wb") as f: f.write(data)

return StoredFile( path=path,
filename=upload_filename, content_type=content_type, size_bytes=len(data),
)


app/adapters/persistence/inmemory/document_repo.py 4.2

 
def set_status(self, *, tenant_id: TenantId, document_id: DocumentId, status: doc = self._docs.get(document_id.value)
if not doc or doc["tenant_id"] != tenant_id.value: return
doc["status"] = status doc["error"] = error

# helper (used by worker in this minimal phase)
def get(self, document_id: DocumentId) -> dict | None: return self._docs.get(document_id.value)


app/adapters/queue/celery_queue.py 4.3



Workers: Celery app + Task Skeleton (IndexDocument) (5

 
Copy code  
 
app/workers/celery_app.py 5.1
 

python
 

# app/workers/celery_app.py
from celery import Celery
from app.core.config import settings

celery_app = Celery( "rag_starter",
broker=settings.celery_broker_url,
backend=settings.celery_result_backend,
)

celery_app.conf.update( task_acks_late=True,
worker_prefetch_multiplier=1, task_routes={
"index_document": {"queue": "indexing"},
},
)

(â€ï»«ï»´ï»œï»žâ€œ Index pipeline) app/workers/tasks.py 5.2
status transitions :Ø§Ù„Ø¢Ù† .Ø§ï»Ÿïº ïºŽï»³ïº” Ø§ï»Ÿïº®ïº³ïºŽï»Ÿïº” ï»“ï»² parsing/chunking/embeddings/vector ï»«ï»¨ïº¤ï»‚ ï»«ï»¨ïºŽ
.+ safe error handling
Copy code  
python

# app/workers/tasks.py
import structlog
from app.workers.celery_app import celery_app
from app.domain.entities import TenantId, DocumentId
from app.core.bootstrap import get_container	# DI container (below)

log = structlog.get_logger()

@celery_app.task(name="index_document", bind=True, autoretry_for=(Exception,), re def index_document(self, *, tenant_id: str, document_id: str) -> dict:
c = get_container() # container with ports/adapters
repo = c["document_repo"]

tenant = TenantId(tenant_id)
doc_id = DocumentId(document_id)
 


repo.set_status(tenant_id=tenant, document_id=doc_id, status="processing") try:
# TODO next: parse -> chunk -> embed -> upsert -> persist chunk metadata # For now: mark indexed
repo.set_status(tenant_id=tenant, document_id=doc_id, status="indexed") log.info("indexed_document", tenant_id=tenant_id, document_id=document_id return {"ok": True}
except Exception as e:
repo.set_status(tenant_id=tenant, document_id=doc_id, status="failed", er log.exception("index_failed", tenant_id=tenant_id, document_id=document_i raise




Auth + Tenant Isolation (FastAPI deps) (6
app/api/v1/deps.py 6.1
.Postgres + JWT ï»§ï»®Ù‘ïº»ï» ï»ª Ù„Ø§ïº£Ù‹ï»˜ïºŽ .tenant/user ï»³ïº¤ïºªØ¯ Production pattern: API Key Ø£ïº‘ïº´ï»‚



 
Copy code  
 
API Routes (Upload) â€” Thin Controller (7
app/api/v1/routes_documents.py 7.1
 
python

# app/api/v1/routes_documents.py
from fastapi import APIRouter, UploadFile, File, Depends, HTTPException from app.api.v1.deps import get_tenant_id
from app.core.bootstrap import get_container
from app.application.use_cases.upload_document import UploadDocumentUseCase, Uplo router = APIRouter(prefix="/v1/documents", tags=["documents"])
@router.post("/upload")
async def upload_document(
file: UploadFile = File(...),
tenant_id: str = Depends(get_tenant_id),
):
if not file.filename:
raise HTTPException(400, "Missing filename") data = await file.read()
c = get_container()
uc: UploadDocumentUseCase = c["upload_use_case"]

result = await uc.execute( UploadDocumentRequest(
tenant_id=tenant_id,
filename=file.filename,
content_type=file.content_type or "application/octet-stream", data=data,
)
)
return {"document_id": result.document_id.value, "status": result.status}


C	C


(ÙˆÙˆØ§ïº¿ïº¢ ï»£ïº®ï»›ïº°ÙŠ) Bootstrap / DI Container (8
app/core/bootstrap.py

python
 

# app/core/bootstrap.py
from functools import lru_cache
from qdrant_client import QdrantClient # used later
from app.core.config import settings
Copy code
from app.adapters.filestore.local_store import LocalFileStore
from app.adapters.persistence.inmemory.document_repo import InMemoryDocumentRepo from app.workers.celery_app import celery_app
from app.adapters.queue.celery_queue import CeleryTaskQueue
from app.application.use_cases.upload_document import UploadDocumentUseCase

@lru_cache(maxsize=1)
def get_container() -> dict:
# Ports implementations
document_repo = InMemoryDocumentRepo()
file_store = LocalFileStore(upload_dir=settings.upload_dir, max_mb=settings.m queue = CeleryTaskQueue(celery_app)

# Use cases
upload_uc = UploadDocumentUseCase(file_store=file_store, repo=document_repo,

return {
"document_repo": document_repo, "file_store": file_store,
"task_queue": queue,
"upload_use_case": upload_uc,
# next: embeddings, vector_store, llm, cache
}


C	C


FastAPI App Entry (9
app/main.py

 
from app.core.logging import setup_logging
from app.api.v1.routes_documents import router as documents_router

def create_app() -> FastAPI:
setup_logging(settings.debug)

app = FastAPI(title=settings.app_name, debug=settings.debug) app.include_router(documents_router)

@app.get("/health") def health():
return {"status": "ok", "env": settings.env} return app
app = create_app()

ïº—ïº¸ï»ï»´ï»ž:

bash

uvicorn app.main:app --reload --host 0.0.0.0 --port 5000

Copy code  

Worker (Celery) ïº—ïº¸ï»ï»´ï»ž (10

bash

# terminal 1: API
uvicorn app.main:app --reload --host 0.0.0.0 --port 5000

# terminal 2: worker
celery -A app.workers.celery_app.celery_app worker -Q indexing --loglevel=INFO

:upload Ø§ïº§ïº˜ïº’ïºŽØ±
 
-F "file=@./sample.pdf"

Copy code  


Ø§ï»Ÿï» ï»² ïºŸïºŽÙŠ )Ø§ï»Ÿïº®ïº³ïºŽï»Ÿïº” Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” ï»£ïº’ïºŽØ´Ø±Ø©(
.upload â†’ create doc â†’ enqueue task â†’ worker marks indexed :ïº·Ù‘ï»ïºŽÙ„ template Ø§ï»ŸÙ€ Ø¯ï»Ÿï»®ï»—ïº˜ï»²

ï»“ï»² Ø§ï»Ÿïº®Ø¯ Ø§ï»Ÿïº ïºŽÙŠ ï»«ï»¨ï»Œï»¤ï»ž Ø§ï»Ÿïº ïº°Ø¡ â€œØ§ï»Ÿï»¤ï»¬ï»¢â€ RAG :Production

Text extraction (PDF/DOCX/TXT) .1 Chunking (token-aware + overlap) .2 Embeddings (OpenAI + cache Redis) .3
Vector store Qdrant (init + upsert + search with tenant filter) .4
AskQuestion endpoint + UseCase .5
Guardrails + prompt budgeting .6
ï»—ï»®ï»ŸÙ‘ï»² â€œï»›ï»¤ï»ž Ø§ï»Ÿïº ïº°Ø¡ RAG Ø§ï»Ÿï»œïºŽï»£ï»žâ€ ÙˆØ£ï»§ïºŽ ï»«ï»œïº˜á¹‡ ï»Ÿï»š Ø§ï»Ÿï»¤ï» ï»”ïºŽØª ï»›ïºŽï»£ï» ïº” ïº‘ï»¨ï»”ïº² Ø§Ù„Ø£ïº³ï» ï»®.á»‹





ï»›ï»¤ï»ž Ø§ï»Ÿïº ïº°Ø¡ RAG Ø§ï»Ÿï»œïºŽï»£ï»ž


Starter Template (Clean Architecture + SOLID + Ø§ï»ŸÙ€ ï»§ï»”ïº² ï»‹ï» ï»° Ø§ï»Ÿï»œïºŽï»£ï»ž RAG Ø§ï»Ÿïº ïº°Ø¡ ï»«ï»¨ï»œÙ‘ï»¤ï»ž â€” ïº—ï»¤ïºŽÙ…
:Production-ready patterns)
Extraction â†’ Chunking (token-aware) â†’ Embeddings (OpenAI) + Redis Cache â†’ Qdrant
.Vector Store â†’ AskQuestion Use Case + Endpoint
mini-RAG (upload â†’ chunk â†’ embeddings â†’ vector ï»“ï»² Ø§ïº—Ø´Ø±Ø­ Ø§ï»Ÿï» ï»² pipeline Ø§ï»ŸÙ€ ï»§ï»”ïº² ÙˆØ¯Ù‡
 
mini-rag ï»£Ø´Ø±ÙˆØ¹ Ø´Ø±Ø­
 
LLM) â†’ prompt â†’ search ï»Ÿï»œï»¦ ïº‘ïº˜ï»˜ïº´ï»´ï»¢ ï»§ï»ˆï»´ï»’ Ùˆï»—ïºŽïº‘ï»ž ï»Ÿï» ïº˜ïº’ïºªï»³ï»ž
  Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿïº´ï» ïº´ï» ïº” mini-RAG - ï»£ï»¦â€¦

ï»«ï»”ïº˜ïº®Ø¶ Ø¥ï»§ï»š ï»£ï»œï»¤ï»ž ï»‹ï» ï»° Ø§ï»Ÿï»¬ï»´ï»œï»ž Ø§ï»Ÿï» ï»² ï»›ïº˜ïº’ï»¨ïºŽÙ‡ ï»“ï»² Ø§ï»Ÿïº®Ø¯ Ø§ï»Ÿïº´ïºŽïº‘ï»–.
 


 
 
(1 Ø¥ïº¿ïºŽï»“ïº” Dependencies )Ø§ï»Ÿïº¤ïºª Ø§Ù„Ø£Ø¯ï»§ï»°(
: requirements.txt ï»Ÿï»®

txt

fastapi
uvicorn[standard] pydantic-settings python-multipart aiofiles
structlog

celery redis

qdrant-client openai

pypdf
python-docx tiktoken



Extraction Service (PDF/DOCX/TXT) â€” Pure-ish Service (2
app/application/services/text_extraction.py

 
class TextExtractor(Protocol):
def extract(self, file_path: str, content_type: str) -> ExtractedText: ...

app/adapters/extraction/default_extractor.py
Copy code	python

# app/adapters/extraction/default_extractor.py
from pypdf import PdfReader import docx
from app.application.services.text_extraction import TextExtractor, ExtractedText

class DefaultTextExtractor(TextExtractor):
def extract(self, file_path: str, content_type: str) -> ExtractedText:
if content_type == "application/pdf" or file_path.lower().endswith(".pdf" return self._extract_pdf(file_path)
if file_path.lower().endswith(".docx") or content_type in ("application/v return self._extract_docx(file_path)
# fallback txt
return self._extract_txt(file_path)

def _extract_pdf(self, file_path: str) -> ExtractedText: reader = PdfReader(file_path)
pages = []
for i, page in enumerate(reader.pages, start=1): t = page.extract_text() or ""
if t.strip():
pages.append(f"\n--- Page {i} ---\n{t}") text = "\n".join(pages).strip()
return ExtractedText(text=text, metadata={"pages": len(reader.pages), "ty

def _extract_docx(self, file_path: str) -> ExtractedText: d = docx.Document(file_path)
paras = [p.text for p in d.paragraphs if p.text.strip()] text = "\n".join(paras).strip()
return ExtractedText(text=text, metadata={"paragraphs": len(paras), "type

def _extract_txt(self, file_path: str) -> ExtractedText: for enc in ("utf-8", "utf-8-sig", "latin-1"):
try:
 
with open(file_path, "r", encoding=enc) as f: text = f.read().strip()
return ExtractedText(text=text, metadata={"encoding": enc, "type" except UnicodeDecodeError:
continue
raise ValueError("Unable to decode text file")





Chunking (Token-aware + overlap + safe) â€” Pure (3
Service
app/application/services/chunking.py

 

enc = tiktoken.get_encoding(encoding_name) tokens = enc.encode(text)

chunks: List[str] = [] start = 0
max_t = max(50, spec.max_tokens)
overlap = min(spec.overlap_tokens, max_t - 1)

while start < len(tokens):
end = min(start + max_t, len(tokens)) chunk_tokens = tokens[start:end]
chunk = enc.decode(chunk_tokens).strip() if chunk:
chunks.append(chunk) if end == len(tokens):
break
start = end - overlap return chunks



Embeddings Port + OpenAI Adapter + Redis Cache (4
app/application/ports/embeddings.py

python

# app/application/ports/embeddings.py
from typing import Protocol, Sequence

class EmbeddingsPort(Protocol):
def embed_one(self, text: str) -> list[float]: ...
def embed_many(self, texts: Sequence[str]) -> list[list[float]]: ...


 

# app/application/ports/cache.py
from typing import Protocol, Optional

class CachePort(Protocol):
def get_json(self, key: str) -> Optional[dict]: ...
def set_json(self, key: str, value: dict, ttl_seconds: int) -> None: ...

 
Copy code  
 
app/adapters/cache/redis_cache.py
 

python

# app/adapters/cache/redis_cache.py
import json import redis
from app.application.ports.cache import CachePort

class RedisCache(CachePort):
def   init  (self, redis_url: str) -> None:
self._r = redis.Redis.from_url(redis_url, decode_responses=True)

def get_json(self, key: str) -> dict | None: v = self._r.get(key)
return json.loads(v) if v else None

def set_json(self, key: str, value: dict, ttl_seconds: int) -> None: self._r.setex(key, ttl_seconds, json.dumps(value))

app/adapters/embeddings/openai_embeddings.py

 

def embed_one(self, text: str) -> list[float]:
resp = self._client.embeddings.create(model=self._model, input=text) return resp.data[0].embedding

def embed_many(self, texts: Sequence[str]) -> list[list[float]]:
resp = self._client.embeddings.create(model=self._model, input=list(texts return [d.embedding for d in resp.data]


app/application/services/embedding_cache.py (pure-ish utility)



VectorStore Port + Qdrant Adapter (init + upsert + (5
search)
 

Copy code  
 
app/application/ports/vector_store.py
python
 

# app/application/ports/vector_store.py
from typing import Protocol, Sequence
from app.domain.entities import Chunk, TenantId

class VectorStorePort(Protocol):
def ensure_collection(self) -> None: ...
def upsert(self, chunks: Sequence[Chunk], vectors: Sequence[list[float]]) -> def search(self, query_vector: list[float], tenant_id: TenantId, top_k: int)

C	C

app/adapters/vector/qdrant_store.py
Copy code	python

# app/adapters/vector/qdrant_store.py
from typing import Sequence, List
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, Fie from app.application.ports.vector_store import VectorStorePort
from app.domain.entities import Chunk, TenantId, DocumentId

 
class QdrantVectorStore(VectorStorePort):
def   init  (self, client: QdrantClient, collection: str, vector_size: self._client = client
self._collection = collection self._size = vector_size
 

int) -
 

def ensure_collection(self) -> None:
# create if not exists
exists = self._client.collection_exists(self._collection) if not exists:
self._client.create_collection(
collection_name=self._collection,
vectors_config=VectorParams(size=self._size, distance=Distance.CO
)

def upsert(self, chunks: Sequence[Chunk], vectors: Sequence[list[float]]) ->
 
points: List[PointStruct] = []
for c, v in zip(chunks, vectors): points.append(
PointStruct(
id=c.id, vector=v, payload={
"text": c.text,
"tenant_id": c.tenant_id.value,
"document_id": c.document_id.value,
},
)
)
self._client.upsert(collection_name=self._collection, points=points)

def search(self, query_vector: list[float], tenant_id: TenantId, top_k: int) flt = Filter(
must=[
FieldCondition(key="tenant_id", match=MatchValue(value=tenant_id.
]
)
res = self._client.search(
collection_name=self._collection, query_vector=query_vector,
query_filter=flt, limit=top_k,
)

out: list[Chunk] = [] for r in res:
payload = r.payload or {} out.append(
Chunk(
id=str(r.id),
tenant_id=TenantId(payload.get("tenant_id", "")),
document_id=DocumentId(payload.get("document_id", "")), text=payload.get("text", ""),
)
 
)
return out





LLM Port + OpenAI Adapter (6
app/application/ports/llm.py

python

# app/application/ports/llm.py
from typing import Protocol

class LLMPort(Protocol):
def generate(self, prompt: str, *, temperature: float, max_tokens: int) -> st


C	C

 
Copy code  
 
app/adapters/llm/openai_llm.py
 

 
 
)
return resp.choices[0].message.content or ""





 
Copy code  
 
Use Case: AskQuestion (RAG) (7
 
app/application/services/prompt_builder.py (guardrails + citations)

app/application/use_cases/ask_question.py

 
tenant_id: str question: str top_k: int = 5

 
class AskQuestionUseCase:
def  init (self, cached_embeddings: self._emb = cached_embeddings
self._vector = vector_store self._llm = llm
 

CachedEmbeddings, vector_store:
 

VectorS
 

def execute(self, req: AskQuestionRequest) -> Answer: tenant = TenantId(req.tenant_id)
q_vec = self._emb.embed_one(req.question)

chunks = self._vector.search(q_vec, tenant_id=tenant, top_k=req.top_k) prompt = build_rag_prompt(question=req.question, chunks=chunks)

text = self._llm.generate(prompt, temperature=0.2, max_tokens=700) sources = [c.id for c in chunks]
return Answer(text=text, sources=sources)





Parse â†’ Chunk â†’) Ø§ï»Ÿïº¤ï»˜ï»´ï»˜ï»² Worker: IndexDocument (8
(Embed â†’ Upsert
app/workers/tasks.py ïº—ïº¤ïºªï»³ïºš

 

@celery_app.task(
name="index_document", bind=True,
autoretry_for=(Exception,), retry_backoff=True,
retry_kwargs={"max_retries": 5},
)
def index_document(self, *, tenant_id: str, document_id: str) -> dict: c = get_container()
repo = c["document_repo"]
extractor = c["text_extractor"]
cached_emb = c["cached_embeddings"] vector_store = c["vector_store"]

tenant = TenantId(tenant_id)
doc_id = DocumentId(document_id)

repo.set_status(tenant_id=tenant, document_id=doc_id, status="processing")

try:
# For this minimal phase, repo is in-memory, so get stored_file from it
doc = repo.get(doc_id) if not doc:
raise ValueError("Document not found") stored = doc["stored_file"]
extracted = extractor.extract(stored.path, stored.content_type) if not extracted.text.strip():
raise ValueError("No text extracted from file")

# chunking
chunks_text = chunk_text_token_aware(extracted.text, spec=ChunkSpec(max_t

# embed + upsert
vector_store.ensure_collection()

chunks: list[Chunk] = []
vectors: list[list[float]] = []
 
for t in chunks_text:
cid = str(uuid.uuid4())
chunks.append(Chunk(id=cid, tenant_id=tenant, document_id=doc_id, tex vectors.append(cached_emb.embed_one(t))

vector_store.upsert(chunks, vectors)

repo.set_status(tenant_id=tenant, document_id=doc_id, status="indexed") log.info("indexed_document", tenant_id=tenant_id, document_id=document_id return {"ok": True, "chunks": len(chunks)}

except Exception as e:
repo.set_status(tenant_id=tenant, document_id=doc_id, status="failed", er log.exception("index_failed", tenant_id=tenant_id, document_id=document_i raise



API: Ask Endpoint (Thin) + DTO (9
app/api/v1/routes_queries.py

 

 
router Ù„Ø¥ïº¿ïºŽï»“ïº” app/main.py ïº—ïº¤ïºªï»³ïºš



Adapters/UseCases ï»›ï»ž ï»Ÿïº®ïº‘ï»‚ DI Container ïº—ïº¤ïºªï»³ïºš (10
app/core/bootstrap.py )Ø§ïº³ïº˜ïº’ïºªØ§Ù„ Ø§ï»Ÿï»¨ïº´ïº¨ïº” Ø§ï»Ÿïº´ïºŽïº‘ï»˜ïº”(

 

from app.core.config import settings

from app.adapters.filestore.local_store import LocalFileStore
from app.adapters.persistence.inmemory.document_repo import InMemoryDocumentRepo from app.adapters.queue.celery_queue import CeleryTaskQueue

from app.adapters.extraction.default_extractor import DefaultTextExtractor from app.adapters.cache.redis_cache import RedisCache

from app.adapters.embeddings.openai_embeddings import OpenAIEmbeddings from app.application.services.embedding_cache import CachedEmbeddings

from app.adapters.vector.qdrant_store import QdrantVectorStore from app.adapters.llm.openai_llm import OpenAILLM

from app.workers.celery_app import celery_app

from app.application.use_cases.upload_document import UploadDocumentUseCase from app.application.use_cases.ask_question import AskQuestionUseCase

@lru_cache(maxsize=1)
def get_container() -> dict:
# --- repositories / stores
document_repo = InMemoryDocumentRepo()
file_store = LocalFileStore(upload_dir=settings.upload_dir, max_mb=settings.m queue = CeleryTaskQueue(celery_app)

# --- extraction
text_extractor = DefaultTextExtractor()

# --- cache
cache = RedisCache(settings.redis_url)

# --- embeddings (OpenAI for now)
if not settings.openai_api_key:
# In production: raise at startup. Here: let it fail on first call if mis
pass
embeddings = OpenAIEmbeddings(api_key=settings.openai_api_key or "", model=se cached_embeddings = CachedEmbeddings(embeddings=embeddings, cache=cache, ttl_
 


# --- vector store
qdrant = QdrantClient(host=settings.qdrant_host, port=settings.qdrant_port) vector_store = QdrantVectorStore(
client=qdrant,
collection=settings.qdrant_collection, vector_size=settings.embedding_dim,
)

# --- llm
llm = OpenAILLM(api_key=settings.openai_api_key or "", model=settings.openai_

# --- use cases
upload_uc = UploadDocumentUseCase(file_store=file_store, repo=document_repo, ask_uc = AskQuestionUseCase(cached_embeddings=cached_embeddings, vector_store

return {
"document_repo": document_repo, "file_store": file_store,
"task_queue": queue,
"text_extractor": text_extractor, "cache": cache,
"embeddings": embeddings,
"cached_embeddings": cached_embeddings, "vector_store": vector_store,
"llm": llm,
"upload_use_case": upload_uc, "ask_use_case": ask_uc,
}





(API + Redis + Qdrant + Worker) ï»›ïºŽï»£ï»ž ïº—ïº¸ï»ï»´ï»ž (11
(ï»Ÿï» ïº˜ïº¸ï»ï»´ï»ž ïºŸÙ‹ïºªØ§ ï»£ï»”ï»´ïºª) docker-compose
: docker/docker-compose.yml

yaml
 

version: "3.9" services:
redis:
image: redis:7-alpine ports: ["6379:6379"]

qdrant:
image: qdrant/qdrant:latest ports: ["6333:6333"]
volumes:
- qdrant_data:/qdrant/storage

volumes:
qdrant_data:

ïº—ïº¸ï»ï»´ï»ž:


 
Copy code  
 
ïº›ï»¢:
 

bash

# API
uvicorn app.main:app --reload --host 0.0.0.0 --port 5000

# Worker
celery -A app.workers.celery_app.celery_app worker -Q indexing --loglevel=INFO



 
Copy code  
 
End-to-End Ø§ïº§ïº˜ïº’ïºŽØ± (12
Upload
 

bash
 

curl -X POST "http://localhost:5000/v1/documents/upload" \
-H "X-API-KEY=tenant_demo_12345678" \
-F "file=@./sample.pdf"
Copy code

Ask

bash

curl -X POST "http://localhost:5000/v1/queries/ask" \
-H "Content-Type: application/json" \
-H "X-API-KEY=tenant_demo_12345678" \
","top_k":5}'ï»£ïºŽ Ø§ï»Ÿï®­ïºªÙ ï»£ï»¦ Ø§ï»Ÿï»¤ïº¸ïº®ÙˆØ¹ØŸ'{"question":" -d


(13 ï»£Ù„Ø§ïº£ï»ˆïºŽØª Production )ï»£ï»¬ï»¢ ïºŸïºªÙ‹Ø§(
Ø§ï»Ÿï»¨ïº´ïº¨ïº” Ø¯ÙŠ skeleton Production-readyØŒ ï»Ÿï»œï»¦ ï»Ÿïº´ï»ª ï»§ïºŽï»—ïºº 3 ïº—ïº®ï»—ï»´ïºŽØª â€œÙ„Ø§Ø²Ù…â€ ï»—ïº’ï»ž Ø£ÙŠ ï»§Ø´Ø± ïº£ï»˜ï»´ï»˜ï»²:
Users/Docs/Chunks/Chats + multi-) Postgres + Alembic ïº‘Ù€ InMemoryRepo Ø§ïº³ïº˜ïº’ïºªØ§Ù„ .1
(ïº£ï»˜ï»´ï»˜ï»² tenant (Rate limiting/Ø¯ÙˆØ±/ï»£ïº¨ïº°ï»§ïº” API keys) ïº£ï»˜ï»´ï»˜ï»² Auth .2
Observability (metrics/tracing + structured logs request_id) .3


Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº ïºŽï»³ïº”
:ï»Ÿï»š á¹‡ï»«ï»œïº˜ â€Postgres + Alembic ï»›ï»¤ï»žâ€œ ïº—ï»˜ï»®Ù„ ï»Ÿï»® SQLAlchemy models (users, documents, chunks, chats)	 
DocumentRepoPort ïº—ï»„ïº’ï»– repos	 
ïºŸïºŽï»«ïº°Ø© migrations	  DB ï»£ï»¦ API key lookup ïº‘Ù€ auth Ø±ïº‘ï»‚	 
ï»£ï»€ïº’ï»®Ø· chunk metadata + status transitions ïº£ï»”ï»†	 
Ø£ÙˆÙ‹Ù„Ø§ØŸ Hybrid search + rerank ÙˆÙ„Ø§ Postgres + Alembic ïº‘Ù€ ï»£ïº’ïºŽØ´Ø±Ø© Ø£ïº‘ïºªØ£ á¹‡ïº—ïº¤


 


Postgres + Alembic ï»›ï»¤ï»ž


Clean Architecture / ïº‘Ù€ Ùˆï»£ï» ïº˜ïº°Ù… Production-ready ïº‘ïº¸ï»œï»ž Postgres + Alembic ï»§ï»œÙ‘ï»¤ï»ž â€” ïº—ï»¤ïºŽÙ…
:SOLID
Auth Ø§ï»ŸÙ€ ï»§ïº®ïº‘ï»‚ Ùˆï»›ï»¤ïºŽÙ† ØŒSQLAlchemy Repo + Migrations ïº‘Ù€ InMemoryDocumentRepo ï»«ï»¨ïº´ïº˜ïº’ïºªÙ„
.(ïº£ï»˜ï»´ï»˜ï»² multi-tenant) Users ïºŸïºªÙˆÙ„ ïº‘Ù€ (API Key)
Ø§ï»Ÿï»¨ïº˜ï»´ïº ïº”:
ï»³ïº¤Ù‘ïºªØ« â†’ DB ï»£ï»¦ stored_file ï»³ï»˜ïº®Ø£ Document record â†’ Celery worker ï»³ïº¤ï»”ï»† â†’ Upload
(á¹‡ïº—ïº¤ ï»Ÿï»® chunks metadata ï»§ï»€ï»´ï»’ Ù„Ø§ïº£Ù‹ï»˜ïºŽ) â†’ status
.(body ï»£ï»¦ ï»£ïº¶) User.api_key ï»£ï»¦ ï»³ïº„ïº—ï»² TenantId ÙˆØ§ï»ŸÙ€


Dependencies (1
Ø£ïº¿ï»’:

txt

sqlalchemy>=2.0
psycopg[binary]>=3.1 alembic>=1.13

Settings: DATABASE_URL (2
: env.example. ï»“ï»²

bash

DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/rag

 
Copy code  
 

:Ø£ïº¿ï»’ app/core/config.py Ùˆï»“ï»²
 

python
 
Copy code
database_url: str = "postgresql+psycopg://postgres:postgres@localhost:5432/rag"


C	C


SQLAlchemy Base + Engine + Session (3
app/adapters/persistence/postgres/db.py

python

# app/adapters/persistence/postgres/db.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, DeclarativeBase from app.core.config import settings

class Base(DeclarativeBase): pass

engine = create_engine(settings.database_url, pool_pre_ping=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)



 


Copy code  
 
ORM Models (Users + Documents) (4
app/adapters/persistence/postgres/models.py
 

python

# app/adapters/persistence/postgres/models.py
from sqlalchemy import String, Text, Integer, DateTime, ForeignKey, func, Index from sqlalchemy.orm import Mapped, mapped_column, relationship
from app.adapters.persistence.postgres.db import Base

class User(Base):
  tablename	 = "users"

id: Mapped[str] = mapped_column(String(36), primary_key=True)
email: Mapped[str] = mapped_column(String(320), unique=True, nullable=False)
 
api_key: Mapped[str] = mapped_column(String(128), unique=True, nullable=False created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve documents = relationship("Document", back_populates="user", cascade="all, del
Index("ix_users_api_key", User.api_key)



class Document(Base):
  tablename	 = "documents"

id: Mapped[str] = mapped_column(String(36), primary_key=True)
user_id: Mapped[str] = mapped_column(String(36), ForeignKey("users.id", ondel

filename: Mapped[str] = mapped_column(String(512), nullable=False)
content_type: Mapped[str] = mapped_column(String(128), nullable=False) file_path: Mapped[str] = mapped_column(Text, nullable=False)
size_bytes: Mapped[int] = mapped_column(Integer, nullable=False)

status: Mapped[str] = mapped_column(String(32), nullable=False, default="crea error: Mapped[str | None] = mapped_column(Text, nullable=True)

created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve updated_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve

user = relationship("User", back_populates="documents")

Index("ix_documents_user_id", Document.user_id) Index("ix_documents_status", Document.status)


.user_id ïº‘Ù€ ï»§ï»”ï» ïº˜ïº® Ù„Ø§Ø²Ù… query ï»›ï»ž ï»“ï»² . user_id = tenant boundary :Ù„Ø§ïº£ï»†


Alembic Setup (5
5.1 Ø¥ï»§ïº¸ïºŽØ¡ Alembic )ï»£ïº®Ø© ÙˆØ§ïº£ïºªØ©(
:root ï»£ï»¦
 
bash

alembic init app/adapters/persistence/postgres/migrations

 
Copy code   Copy code  
 
alembic.ini 5.2
.(practice Ø£ï»“ï»€ï»ž) env ï»£ï»¦ ï»Ÿï»´ï»˜ïº®Ø£ sqlalchemy.url ï»‹Ù‘ïºªÙ„
.env.py ï»“ï»² env ï»£ï»¦ ï»«ï»¨ïº¤ï»˜ï»¨ï»ª ÙˆØ¥ïº£ï»¨ïºŽ ØŒplaceholder ïº§ï» ï»´ï»ª alembic.ini ï»“ï»²
migrations/env.py 5.3
Ø§ïº³ïº˜ïº’ïºªÙ„ ï»£ïº¤ïº˜ï»®Ù‰ env.py ïº‘Ù€:
 

python

# app/adapters/persistence/postgres/migrations/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool from alembic import context
import os

from app.adapters.persistence.postgres.db import Base
from app.adapters.persistence.postgres import models # noqa: F401	(ensure model

config = context.config

if config.config_file_name is not None: fileConfig(config.config_file_name)

target_metadata = Base.metadata

def get_url() -> str:
# Prefer DATABASE_URL from env url = os.getenv("DATABASE_URL") if not url:
raise RuntimeError("DATABASE_URL is not set") return url

def run_migrations_offline() -> None: context.configure(
url=get_url(),
 
target_metadata=target_metadata, literal_binds=True,
dialect_opts={"paramstyle": "named"}, compare_type=True,
)
with context.begin_transaction(): context.run_migrations()

def run_migrations_online() -> None:
configuration = config.get_section(config.config_ini_section) or {} configuration["sqlalchemy.url"] = get_url()

connectable = engine_from_config( configuration,
prefix="sqlalchemy.",
poolclass=pool.NullPool,
)

with connectable.connect() as connection: context.configure(
connection=connection,
target_metadata=target_metadata, compare_type=True,
)

with context.begin_transaction(): context.run_migrations()

if context.is_offline_mode(): run_migrations_offline()
else:
run_migrations_online()


Migration Ø£ÙˆÙ„ 5.4

bash

alembic revision --autogenerate -m "create users and documents" alembic upgrade head
 

 
Postgres Repository Adapter (implements (6
DocumentRepoPort)
.worker ï»Ÿï» Ù€ document + stored_file ï»§ï»˜ïº®Ø£ ï»›ï»¤ïºŽÙ† ï»§ïº¤ïº˜ïºŽØ¬
app/adapters/persistence/postgres/repo_documents.py

python

# app/adapters/persistence/postgres/repo_documents.py
import uuid
from sqlalchemy import select, update
from app.adapters.persistence.postgres.db import SessionLocal from app.adapters.persistence.postgres.models import Document
Copy code
from app.application.ports.document_repo import DocumentRepoPort
from app.domain.entities import DocumentId, TenantId, StoredFile

class PostgresDocumentRepo(DocumentRepoPort):
def create_document(self, *, tenant_id: TenantId, stored_file: StoredFile) -> doc_id = DocumentId(str(uuid.uuid4()))
with SessionLocal() as db: db.add(
Document(
id=doc_id.value,
user_id=tenant_id.value,	# tenant_id == user_id in o
filename=stored_file.filename,
content_type=stored_file.content_type, file_path=stored_file.path,
size_bytes=stored_file.size_bytes, status="created",
)
)
db.commit() return doc_id

def set_status(self, *, tenant_id: TenantId, document_id: DocumentId, status: with SessionLocal() as db:
stmt = (
update(Document)
.where(Document.id == document_id.value, Document.user_id == tena
 
.values(status=status, error=error)
)
db.execute(stmt) db.commit()

def get_stored_file(self, *, tenant_id: TenantId, document_id: DocumentId) -> with SessionLocal() as db:
stmt = select(Document).where(Document.id == document_id.value, Docum doc = db.execute(stmt).scalar_one_or_none()
if not doc:
return None
return StoredFile(
path=doc.file_path,
filename=doc.filename,
content_type=doc.content_type, size_bytes=doc.size_bytes,
)


Clean ï»“ï»² . get_stored_file ï»“ï»´ï»ª ï»›ïºŽï»§ïº¶ ï»£ïºŽ Ø§ï»Ÿïº´ïºŽïº‘ï»– ï»“ï»² DocumentRepoPort :ï»£Ù„Ø§ïº£ï»ˆïº”
:Ø§Ù„Ø£ï»“ï»€ï»ž Architecture

DocumentReadPort ï»£ï»¨ï»”ïº¼ï»ž Port ï»§ï»€ï»´ï»’ Ø¥ï»£ïºŽ	 
  Ø£Ùˆ ï»§ï»®ïº³Ù‘ï»Š Ø§ï»ŸÙ€ DocumentRepoPort ïº‘ïº¸ï»œï»ž ï»£ïº¤ïºªÙˆØ¯. Ø£ï»§ïºŽ ï»«ï»Œï»¤ï» ï»ª ïº‘ïº¸ï»œï»ž SOLID Ø£ï»“ï»€ï»ž: Port ïºŸïºªï»³ïºª.


(7 Ø¥ïº¿ïºŽï»“ïº” Port ï»—ïº®Ø§Ø¡Ø© (ISP)
app/application/ports/document_reader.py

 
class DocumentReaderPort(Protocol):
def get_stored_file(self, *, tenant_id: TenantId, document_id: DocumentId) ->
Copy code  

.(Ø§Ù„Ø§ïº—ï»¨ï»´ï»¦ implements ÙˆØ§ïº£ïºªØ© class ïº‘ïº’ïº´ïºŽï»ƒïº”) Ø§Ù„Ø§ïº›ï»¨ï»´ï»¦ ï»³ï»„ïº’ï»– PostgresDocumentRepo ÙˆØ§ïºŸï»Œï»ž

Auth: API Key â†’ user_id (tenant_id) (8
ïº‘ïºªÙ„ ï»£ïºŽ ï»§ï»ŒïºŽï»£ï»ž Ø§ï»ŸÙ€ api_key ï»«ï»® tenant ï»£ïº’ïºŽØ´Ø±Ø©ØŒ ï»§ï»Œï»¤ï»ž lookup ï»“ï»² .Postgres
C	app/adapters/persistence/postgres/repo_users.pCy

python

# app/adapters/persistence/postgres/repo_users.py
from sqlalchemy import select
from app.adapters.persistence.postgres.db import SessionLocal from app.adapters.persistence.postgres.models import User

class UserLookupRepo:
def get_user_id_by_api_key(self, api_key: str) -> str | None: with SessionLocal() as db:
stmt = select(User.id).where(User.api_key == api_key) return db.execute(stmt).scalar_one_or_none()

(ïº—ïº¤ïºªï»³ïºš) app/api/v1/deps.py

 
if not user_id:
raise HTTPException(status_code=401, detail="Invalid API key")

# tenant_id == user_id
return user_id





InMemory ïº‘ïºªÙ„ DB ï»£ï»¦ StoredFile ï»Ÿï»˜ïº®Ø§Ø¡Ø© Worker ïº—ïº¤ïºªï»³ïºš (9
:doc á¹‡ïºŸï»  ïºŸïº°Ø¡ ï»‹Ù‘ïºªÙ„ app/workers/tasks.py ï»“ï»²












Postgres Repos + ïº‘Ù€ InMemory Ø§ïº³ïº˜ïº’ïºªØ§Ù„ :Bootstrap (10
UserLookup
(Ø§Ù„Ø£ïº³ïºŽïº³ï»´ïº” Ø§ï»Ÿïº˜ï»ï»´ï»´ïº®Ø§Øª) app/core/bootstrap.py

python

# app/core/bootstrap.py (only show changed parts)
from app.adapters.persistence.postgres.repo_documents import PostgresDocumentRepo from app.adapters.persistence.postgres.repo_users import UserLookupRepo

@lru_cache(maxsize=1)
def get_container() -> dict:
# --- postgres repos
document_repo = PostgresDocumentRepo()
document_reader = document_repo # implements DocumentReaderPort
user_lookup_repo = UserLookupRepo()

# ... Ø§ï»ŸÙ€ ïº‘ïºŽï»—ï»² adapters (file_store, queue, extractor, cache, embeddings, vecto
 


upload_uc = UploadDocumentUseCase(file_store=file_store, repo=document_repo, ask_uc = AskQuestionUseCase(cached_embeddings=cached_embeddings, vector_store

return {
"document_repo": document_repo,
"document_reader": document_reader,
"user_lookup_repo": user_lookup_repo,
# ... rest
"upload_use_case": upload_uc, "ask_use_case": ask_uc,
}





Docker Compose: Postgres + Redis + Qdrant (11
docker/docker-compose.yml

 
volumes:
- qdrant_data:/qdrant/storage

volumes:
pg_data:
qdrant_data:

ïº—ïº¸ï»ï»´ï»ž:

bash

cd docker
docker compose up -d

 
Copy code  
 
:migrations ïº›ï»¢









ï»ŸÙ„Ø§ïº§ïº˜ïº’ïºŽØ± Ø³Ø±ï»³ï»Š Seed User (API Key) (12
Ø§ï»›ïº˜á¹‡ ïº³ï»œïº®ï»³ïº’ïº– ïº‘ïº´ï»´ï»‚:
scripts/seed_user.py
 

python

# scripts/seed_user.py
import uuid
from app.adapters.persistence.postgres.db import SessionLocal from app.adapters.persistence.postgres.models import User

def main():
user_id = str(uuid.uuid4())
api_key = "tenant_demo_12345678" # test key
email = "demo@example.com"
 
with SessionLocal() as db:
db.add(User(id=user_id, email=email, api_key=api_key)) db.commit()

print("Seeded:", {"user_id": user_id, "api_key": api_key, "email": email})

if  name	 == " main ": main()

ïº—ïº¸ï»ï»´ï»ž:

bash

python scripts/seed_user.py

Postgres ïº‘ï»Œïºª Test Flow (13
Seed user .1 X-API-KEY=tenant_demo_12345678 ïº‘Ù€Ù€ Upload .2 DB ï»£ï»¦ indexing ï»³ïº¸ïº˜ï»ï»ž Worker .3
results á¹‡ï»³ïº ï»´ Ask endpoint  .4


Ø§ï»Ÿï» ï»² ï»§ïºŽï»—ïºº ï»Ÿï»´ïº’ï»˜ï»° Production-gradeâ€œ ïº—ï»¤ïºŽï»£Ù‹ïºŽâ€
.ï»§ï»ˆï»´ï»’ ïº‘ïº¸ï»œï»ž â€Postgres + Alembicâ€ + â€œAuth lookupâ€œ ï»§ï»”ïº¬Øª ï»›ïºªÙ‡ Ø£ï»§ïºŽ
Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” )Ø£ï»—ï»®Ù‰ ïº—ïº®ï»—ï»´ïº” ï»Ÿï» ïº ï»®Ø¯Ø©/Ø§ï»Ÿïº˜ïº˜ïº’ï»Š:(
Chunks table (metadata + chunk_id + offsets + hash) .1
Chat history table (question/answer/sources/tokens/latency) .2
.3 hash document Idempotency: ï»³ï»¤ï»¨ï»Š Ø¥ï»‹ïºŽØ¯Ø© Ø§ï»Ÿï»”ï»¬ïº®ïº³ïº” ï»Ÿï»¨ï»”ïº² Ø§ï»Ÿï»¤ï» ï»’
(Ø£ï»›ïºœïº® multi-tenant ïº—ïº¸ïºªØ¯ ï»‹ïºŽï»³ïº° ï»Ÿï»®) Row-level policies .4
Ùˆ worker Ø§ï»ŸÙ€ ï»“ï»² ÙˆØ§ïº³ïº˜ïº¨ïºªØ§ï»£ï»¬ï»¢ reposÙˆ migrations ï»«ï»¨ï»€ï»´ï»’ â€chunks + chat tables ï»›ï»¤ï»žâ€œ ïº—ï»˜ï»®Ù„ ï»Ÿï»®
.ask

 


Hybrid search + rerank


Ùˆïº‘Ù€ ØŒClean Architecture Ø§ï»ŸÙ€ ï»§ï»”ïº² Ø¯Ø§ïº§ï»ž ï»§ï»ˆï»´ï»’ ïº‘ïº¸ï»œï»ž Hybrid Search + Rerank ï»«ï»¨ï»€ï»´ï»’ â€” ïº—ï»¤ïºŽÙ…
.Production-grade knobs (latency/cost/quality)

ï»«ï»¨ïº’ï»¨ï»´ï»¬ïºŽ ï»‹ï» ï»° 3 ï»ƒïº’ï»˜ïºŽØª:

Candidate Generation (Recall) .1
Vector Search (Qdrant / PGVector)	  (Ùˆï»‹ï»¤ï» ï»² Ø³Ø±ï»³ï»Š) tsvector ïº‘ïºŽïº³ïº˜ïº¨ïºªØ§Ù… Postgres Ø¯Ø§ïº§ï»ž Keyword Search (BM25-ish)	 
(weighted Ø£Ùˆ RRF :Ø§ï»Ÿï»¨ïº˜ïºŽïº‹ïºž Ø¯ï»£ïºž) Fusion .2
Rerank (Precision) .3
(Ø£ï»ï» ï»° ï»Ÿï»œï»¦ Ø£ïº³ï»¬ï»ž) LLM-as-reranker Ø£Ùˆ (Ø£ï»“ï»€ï»ž) Cross-Encoder	 

(1 Ø§ï»Ÿï»¤ï»Œï»¤ïºŽØ±ï»³ïº” Ø§ï»Ÿï»¤ïº´ïº˜ï»¬ïºªï»“ïº”

pgsql

question
â†’ embed(question)
â†’ vector_candidates = VectorStore.search(top_k=K_vec)
â†’ keyword_candidates = KeywordStore.search(top_k=K_kw)
â†’ fused = Fusion(vector_candidates, keyword_candidates)
â†’ reranked = Reranker.rank(question, fused)	# top_n
â†’ build_prompt(question, reranked)
â†’ LLM answer

Ø¥ï»‹ïºªØ§Ø¯Ø§Øª ÙˆØ§ï»—ï»Œï»´ïº”:

 

Copy code  
 
K_vec = 30
K_kw = 30
 
fused_limit = 40
(ïº£ïº´ïºŽØ³ latency ï»Ÿï»® 5 Ø£Ùˆ) rerank_top_n = 8


 
(SOLID/ISP) ïºŸïºªï»³ïºªØ© Ports (2
Keyword Search Port 2.1

python

# app/application/ports/keyword_store.py
from typing import Protocol, Sequence
from app.domain.entities import Chunk, TenantId

class KeywordStorePort(Protocol):
def search(self, *, query: str, tenant_id: TenantId, top_k: int) -> Sequence[


C	C

 

Copy code  
 
Fusion Service (pure) 2.2
 

python

# app/application/services/fusion.py
from dataclasses import dataclass from typing import Sequence
from app.domain.entities import Chunk

@dataclass(frozen=True) class ScoredChunk:
chunk: Chunk score: float

def rrf_fusion(
*,
vector_hits: Sequence[ScoredChunk], keyword_hits: Sequence[ScoredChunk], k: int = 60,
out_limit: int = 40,
) -> list[ScoredChunk]: """
Reciprocal Rank Fusion (RRF): robust, no score calibration needed. score = Î£ 1 / (k + rank)
"""
acc: dict[str, float] = {}
 
def add(hits: Sequence[ScoredChunk]):
for rank, h in enumerate(hits, start=1):
acc[h.chunk.id] = acc.get(h.chunk.id, 0.0) + 1.0 / (k + rank)

add(vector_hits) add(keyword_hits)

# keep best
scored = sorted(acc.items(), key=lambda x: x[1], reverse=True)[:out_limit]
# reconstruct chunks by id
by_id = {h.chunk.id: h.chunk for h in list(vector_hits) + list(keyword_hits)} return [ScoredChunk(chunk=by_id[cid], score=s) for cid, s in scored if cid in


Reranker Port 2.3

python

# app/application/ports/reranker.py from typing import Protocol, Sequence from app.domain.entities import Chunk

class RerankerPort(Protocol):
def rerank(self, *, query: str, chunks: Sequence[Chunk], top_n: int) -> Seque


C	C


 
Copy code  
 
Postgres Keyword Search Adapter (tsvector) (3
 
.ïºŸÙ‹ïºªØ§ ï»‹ï»¤ï» ï»² BM25-ish ï»›Ù€ Postgres Full-Text Search (FTS) ï»§ïº´ïº˜ïº¨ïºªÙ… ØŒ(ï»›ïº’ï»´ïº®) Elasticsearch ïº‘ïºªÙ„
( Ù„Ø§Ø²) chunks ïºŸïºªÙˆÙ„ :DB 3.1
.tenant isolation Ùˆïº‘Ù€ ïº‘Ø³Ø±ï»‹ïº” ï»³ïº¸ïº˜ï»ï»ž keyword search ï»‹ïº¸ïºŽÙ† Postgres ï»“ï»² chunks ï»§ïº¨ïº°Ù† Ù„Ø§Ø²Ù…

:(ï»£ïº’ïºªïº‹Ù‹ï»´ïºŽ) Migration

C	chunks(id, document_id, user_id, text, tsv, created_at)	C
tsv ï»‹ï» ï»° GIN index	 

ORM model
 
python

# app/adapters/persistence/postgres/models_chunks.py
from sqlalchemy import String, Text, DateTime, ForeignKey, func, Index from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.dialects.postgresql import TSVECTOR
from app.adapters.persistence.postgres.db import Base

class ChunkRow(Base):
  tablename	 = "chunks"

id: Mapped[str] = mapped_column(String(36), primary_key=True)
document_id: Mapped[str] = mapped_column(String(36), ForeignKey("documents.id user_id: Mapped[str] = mapped_column(String(36), ForeignKey("users.id", ondel

text: Mapped[str] = mapped_column(Text, nullable=False)
tsv: Mapped[object] = mapped_column(TSVECTOR, nullable=False)

created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve

Index("ix_chunks_user_id", ChunkRow.user_id)
Index("ix_chunks_tsv", ChunkRow.tsv, postgresql_using="gin")


C	C

ïº—ï»”ï»Œï»´ï»ž tsv ïº—ï» ï»˜ïºŽïº‹ï»´Ù‹ïºŽ )Ø£ï»“ï»€ï»ž (practice
:migration SQL ï»“ï»²

sql

 
-- tsv = to_tsvector('simple', text)

 


.insert/update code ï»“ï»² Ø£Ùˆ
Keyword repo 3.2
 

python

# app/adapters/persistence/postgres/keyword_store.py
from sqlalchemy import select, text as sql_text
from app.adapters.persistence.postgres.db import SessionLocal
 
from app.adapters.persistence.postgres.models_chunks import ChunkRow from app.application.ports.keyword_store import KeywordStorePort
from app.domain.entities import Chunk, TenantId, DocumentId

class PostgresKeywordStore(KeywordStorePort):
def search(self, *, query: str, tenant_id: TenantId, top_k: int):
# websearch_to_tsquery Ø§ï»Ÿï»„ïº’ï¯¿ï»Œï¯¿ïº” ï»Ÿï» ï»¤ïºªïº§ï»¼Øª Ø£ï»“ï»€ï»ž
tsq = sql_text("websearch_to_tsquery('simple', :q)")
rank = sql_text("ts_rank_cd(chunks.tsv, websearch_to_tsquery('simple', :q

with SessionLocal() as db: stmt = (
select(ChunkRow.id, ChunkRow.document_id, ChunkRow.user_id, Chunk
.where(ChunkRow.user_id == tenant_id.value)
.where(ChunkRow.tsv.op("@@")(tsq))
.order_by(rank.desc())
.limit(top_k)
)
rows = db.execute(stmt, {"q": query}).all()

out = []
for cid, doc_id, uid, txt in rows: out.append(
Chunk(id=cid, tenant_id=TenantId(uid), document_id=DocumentId(doc
)
return out





 



C












Copy code
 
ï»“ï»² chunks ïº§Ù‘ïº°Ù† Qdrant ï»ŸÙ€ upsert ïº‘ï»Œïºª :Worker ïº—ï»Œïºªï»³ï»ž (4
PostgreCs
:worker indexing ï»“ï»² Postgres (id + text + tsv + user_id + ï»“ï»² chunks ï»Ÿï» Ù€ insert Ø§ï»‹ï»¤ï»ž :chunking ïº‘ï»Œïºª	 
document_id)
.tracing Ùˆ keyword search ï»³ïº¨ïºªÙ… Ø¯Ù‡	 
Port ï»Ÿï» ï»œïº˜ïºŽïº‘ïº”:
 

python
 

# app/application/ports/chunk_repo.py from typing import Protocol, Sequence from app.domain.entities import Chunk

class ChunkRepoPort(Protocol):
def upsert_chunks(self, *, chunks: Sequence[Chunk]) -> None: ...

:Adapter Postgres

 

Copy code
 
python
 
# app/adapters/persistence/postgres/repo_chunks.py
from sqlalchemy import insert, text as sql_text
from app.adapters.persistence.postgres.db import SessionLocal
from app.adapters.persistence.postgres.models_chunks import ChunkRow from app.application.ports.chunk_repo import ChunkRepoPort
from app.domain.entities import Chunk

class PostgresChunkRepo(ChunkRepoPort): def upsert_chunks(self, *, chunks):
values = []
for c in chunks:
values.append({ "id": c.id,
"document_id": c.document_id.value, "user_id": c.tenant_id.value,
"text": c.text,
# tsv computed in SQL for consistency
})

with SessionLocal() as db:
# Insert rows
db.execute(insert(ChunkRow), values)
# Update tsv in one shot (or define generated column via migration) db.execute(sql_text("UPDATE chunks SET tsv = to_tsvector('simple', te db.commit()

C	C

.update ï»‹ï»¦ ï»“ïº˜ïº´ïº˜ï»ï»¨ï»° ØŒmigration ï»“ï»² tsv Generated Column ïº—ïº¨ï» ï»² :ï»›ïºªÙ‡ ï»£ï»¦ Ø£ï»“ï»€ï»ž
 

 
(fusion ï»‹ïº¸ïºŽÙ†) scores ï»§ïº¤ïº˜ïºŽØ¬ :Vector Search (5
.scored hits ï»³ïº®Ù‘ïºŸï»Š VectorStore adapter ï»§ïº¤Ù‘ïºªØ« .score (similarity) ïº‘ï»´ïº®ïºŸï»Š Qdrant

ïº‘ïºªÙ„ ï»£ïºŽ ï»§ï»œØ³Ø± Port Ø§ï»Ÿï»˜ïºªï»³ï»¢ØŒ ï»§ï»€ï»´ï»’ Port ïºŸïºªï»³ïºª Ø£Ùˆ :DTO


 



Copy code  
 
. (...)VectorStore adapter method search_scored ïº›ï»¢

:Port
 

python

# app/application/ports/vector_store.py
from typing import Protocol, Sequence
from app.domain.entities import TenantId
from app.application.services.scoring import ScoredChunk

class VectorStorePort(Protocol):
def ensure_collection(self) -> None: ...
def search_scored(self, query_vector: list[float], tenant_id: TenantId, top_k


C	C

:(ï»£ïº¨ïº˜ØµØ±) Qdrant implementation

 
def search_scored(...):
res = self._client.search(..., limit=top_k) out=[]
for r in res:
p = r.payload or {}
c = Chunk(... text=p.get("text","") ...)
out.append(ScoredChunk(chunk=c, score=float(r.score))) return out



 

Copy code  
 
(ïº§ï»´ïºŽØ±Ø§Øª 3) Rerank Adapter (6
(ïºŸï»®Ø¯Ø© Ø£ï»“ï»€ï»ž) A: Cross-Encoder ïº§ï»´ïºŽØ±
(bge-reranker ï»£ïºœï»ž) reranker ï»§ï»¤ï»®Ø°Ø¬ + sentence-transformers	 
   Ø³Ø±ï»³ï»Š ï»§ïº´ïº’ï»´Ù‹ïºŽ ï»‹ï» ï»° GPUØŒ Ùˆï»‹ï» ï»° CPU ï»£ï»¤ï»œï»¦ ï»³ïº’ï»˜ï»° ïº‘ï»„ïºŠ.
.Port stays same
(Ø£ïº³ï»¬ï»ž) B: LLM Rerank ïº§ï»´ïºŽØ±
ï»§ïº¨ï» ï»² Ø§ï»ŸÙ€ LLM ï»³ïº®ïºŸÙ‘ï»Š ïº—ïº®ïº—ï»´á¹‡ IDs ï»“ï»˜ï»‚. )ï»Ÿï»œï»¦ cost Ø£ï»‹ï» ï»°(

:Adapter ï»£ïºœïºŽÙ„
 

python

# app/adapters/rerank/llm_reranker.py
from app.application.ports.reranker import RerankerPort from app.application.ports.llm import LLMPort
from app.domain.entities import Chunk

class LLMReranker(RerankerPort):
def   init  (self, llm: LLMPort) -> None: self._llm = llm

def rerank(self, *, query: str, chunks: list[Chunk], top_n: int) -> list[Chun items = "\n".join([f"{i}. [{c.id}] {c.text[:400]}" for i,c in enumerate(c prompt = (
"Rank the passages by relevance to the query. Return ONLY the top ids f"Query: {query}\n\nPassages:\n{items}\n"
 
)
raw = self._llm.generate(prompt, temperature=0.0, max_tokens=200)
# parsing safely omitted ï»“ï»²â€”Ú¾ï»¨ïºŽ prod ï»»Ø²Ù… json parse + fallback # fallback: return first top_n
return chunks[:top_n]


C: Lightweight heuristic rerank (fallback) ïº§ï»´ïºŽØ±
overlap keywords + cosine score + length penalty	 
.fallback ï»›Ù€ ïºŸÙ‹ïºªØ§ Ø³Ø±ï»³ï»Š	 


 
Copy code  
 
Use Case: AskQuestion â€” Hybrid + Rerank (7
ïº‘ïºªÙ„ ask Ø§ï»Ÿïº¤ïºŽï»Ÿï»²ØŒ ï»«ï»¨ï»Œï»¤ï»ž:
 

 
 
cached_embeddings: CachedEmbeddings, vector_store: VectorStorePort,
keyword_store: KeywordStorePort, reranker: RerankerPort,
llm,
) -> None:
self._emb = cached_embeddings self._vec = vector_store
self._kw = keyword_store self._rerank = reranker self._llm = llm

def execute(self, req: AskHybridRequest) -> Answer: tenant = TenantId(req.tenant_id)
q_vec = self._emb.embed_one(req.question)

vec_hits = self._vec.search_scored(q_vec, tenant_id=tenant, top_k=req.k_v kw_chunks = self._kw.search(query=req.question, tenant_id=tenant, top_k=r

# Convert kw to scored by rank (RRF doesn't need calibrated scores)
from app.application.services.scoring import ScoredChunk
kw_hits = [ScoredChunk(chunk=c, score=1.0) for c in kw_chunks]

fused = rrf_fusion(vector_hits=vec_hits, keyword_hits=kw_hits, out_limit= fused_chunks = [s.chunk for s in fused]

reranked = self._rerank.rerank(query=req.question, chunks=fused_chunks, t

prompt = build_rag_prompt(question=req.question, chunks=reranked) text = self._llm.generate(prompt, temperature=0.2, max_tokens=700)

return Answer(text=text, sources=[c.id for c in reranked])





ïºŸïºªï»³ïºª API Endpoint (8
v1/queries/ask-hybrid/

python
 

 

Bootstrap wiring (9
:Ø£ïº¿ï»’ ()get_container ï»“ï»²

 




Copy code  
 
()keyword_store = PostgresKeywordStore	 
(worker ï»³ïº´ïº˜ïº¨ïºªï»£ï»ª) ()chunk_repo = PostgresChunkRepo	  (CrossEncoderReranker Ø£Ùˆ LLMReranker(llm)) = reranker	 
(...)ask_hybrid_use_case = AskQuestionHybridUseCase	 



(Ø§ï»Ÿï»¨ï»˜ïºŽØ· Ø£ï»«ï»¢) Production tuning (10
:Latency	 
   Ø±ï»³ïº®ï»§ï»š top-8 â†’ top-40 ï»£ï»¤ïº˜ïºŽØ².
 
Ø£Ùˆ Ø£ï»—ï»ž rerank_top_n ïº§ï» ï»² â†’ bottleneck ï»³ïº’ï»˜ï»° ï»£ï»¤ï»œï»¦ CPU ï»‹ï» ï»° Cross-encoder	 
.low-confidence ï»‹ï»¨ïºª ï»“ï»˜ï»‚ LLM rerank Ø§ïº³ïº˜ïº¨ïºªÙ…
:Quality	 
   Hybrid ï»³ïº®ï»“ï»Š Recall ïºŸïºªÙ‹Ø§ ïº§ïº¼ï»®ïº»Ù‹ïºŽ ï»ŸÙ„Ø£ïº³ï»¤ïºŽØ¡/Ø§Ù„Ø£Ø±ï»—ïºŽÙ…/Ø§ï»Ÿïº˜ï»®Ø§Ø±ï»³ïº¦.
.hallucinationÙˆ â€context driftâ€œ ï»³ï»˜ï» ï»ž Rerank	 
:Multi-tenant	  Keyword: WHERE user_id = tenant_id	 
Vector: filter payload tenant_id	 
:Cost control	 
(chunks + ïº³ïº†Ø§Ù„) Cache embeddings	  (ï»›ïº’ï»´ïº® ïº—ïº¤ïº´ï»´ï»¦) embed_one per chunk ïº‘ïºªÙ„ indexing ï»“ï»² Batch embeddings	 


â€chunks + generated tsv ïº‘Ù€ Ø§ï»Ÿïº¨ïºŽïº»ïº” migrations ï»›ï»¤ï»žâ€œ ïº—ï»˜ï»®Ù„ ï»Ÿï»®
Ø£ï»§ïºŽ Ø£ï»›ïº˜á¹‡ ï»Ÿï»š:
(chunks table + generated tsv + gin index) ï»›ïºŽï»£ï» ïº” Alembic migration	  indexing Ø£ïº›ï»¨ïºŽØ¡ Postgres ï»“ï»² chunks á¹‡ï»Ÿï»´ï»œïº˜ worker ïº—ï»Œïºªï»³ï»ž	 
Ø§Ù„Ø§ïº—ïº´ïºŽÙ‚ ïº‘ï»¨ï»”ïº² chunk_id/doc_id/user_id ï»³ïº¤ïº˜ï»®ÙŠ ïº‘ïº¤ï»´ïºš Qdrant payload ïº—ï»Œïºªï»³ï»ž	 
Ø³Ø±ï»³ï»ŠØŸ ï»›ïº¤ï»ž LLM Rerank ÙˆÙ„Ø§ ï»£ïº¤ï» ï»² Cross-Encoder ï»³ï»œï»®Ù† reranker á¹‡ïº—ïº¤




chunks ïº‘Ù€ Ø§ï»Ÿïº¨ïºŽïº»ïº” migrations ï»›ï»¤ï»ž chunks + chat tables ï»›ï»¤ï»ž
+ generated tsv


Thought for 5s  
ïº‘ïºªÙˆÙ†) generated tsvector ï»£ï»Š Chunks + Chat Tables + Alembic migrations ï»«ï»¨ï»œÙ‘ï»¤ï»ž â€” ïº—ï»¤ïºŽÙ…
.hybrid keyword search ÙˆØ§ï»ŸÙ€ worker ï»Ÿï» Ù€ Wiring + (ï»³ïºªÙˆÙŠ update

ï»«ï»¨ïºŽ ï»«ïºªï»³ï»š:

ORM Models (ChunkRow + ChatSession + ChatTurn) .1
(generated tsv + indexes) ïºŸïºŽï»«ïº°Ø© Alembic Migration .2
 
Repos/Ports (ChunkRepo + ChatRepo) .3 (indexing Ø£ïº›ï»¨ïºŽØ¡ Postgres ï»“ï»² chunks ï»³ïº¤ï»”ï»†) Worker modifications .4 tsv generated ï»‹ï» ï»° ï»³ï»Œïº˜ï»¤ïºª KeywordStore (Postgres FTS) .5


ORM Models (1
app/adapters/persistence/postgres/models_chunks.py 1.1
python

from sqlalchemy import String, Text, DateTime, ForeignKey, func, Index from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.dialects.postgresql import TSVECTOR
from app.adapters.persistence.postgres.db import Base

class ChunkRow(Base):
  tablename	 = "chunks"

id: Mapped[str] = mapped_column(String(36), primary_key=True)

document_id: Mapped[str] = mapped_column( String(36),
ForeignKey("documents.id", ondelete="CASCADE"), nullable=False,
)
user_id: Mapped[str] = mapped_column( String(36),
ForeignKey("users.id", ondelete="CASCADE"), nullable=False,
)

text: Mapped[str] = mapped_column(Text, nullable=False)

# NOTE: this will be GENERATED ALWAYS AS ... STORED via migration. tsv: Mapped[object] = mapped_column(TSVECTOR, nullable=False)

created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve

Index("ix_chunks_user_id", ChunkRow.user_id)
 
Index("ix_chunks_document_id", ChunkRow.document_id)
Index("ix_chunks_tsv", ChunkRow.tsv, postgresql_using="gin")





Chat Tables (Session + Turn) 1.2

app/adapters/persistence/postgres/models_chat.py

python

from sqlalchemy import String, Text, Integer, DateTime, ForeignKey, func, Index from sqlalchemy.orm import Mapped, mapped_column
from app.adapters.persistence.postgres.db import Base

CcolpayscsodCehatSession(Base):
  tablename	 = "chat_sessions"

id: Mapped[str] = mapped_column(String(36), primary_key=True)
user_id: Mapped[str] = mapped_column(String(36), ForeignKey("users.id", ondel title: Mapped[str | None] = mapped_column(String(200), nullable=True)
created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve Index("ix_chat_sessions_user_id", ChatSession.user_id)

class ChatTurn(Base):
  tablename	 = "chat_turns"

id: Mapped[str] = mapped_column(String(36), primary_key=True)
session_id: Mapped[str] = mapped_column(String(36), ForeignKey("chat_sessions
C	C
user_id: Mapped[str] = mapped_column(String(36), ForeignKey("users.id", ondel

question: Mapped[str] = mapped_column(Text, nullable=False) answer: Mapped[str] = mapped_column(Text, nullable=False)

# store sources as a compact JSON string (we can move to JSONB later)
sources_json: Mapped[str] = mapped_column(Text, nullable=False, default="[]")
 


# observability/cost fields (optional but useful)
retrieval_k: Mapped[int] = mapped_column(Integer, nullable=False, default=5) embed_ms: Mapped[int | None] = mapped_column(Integer, nullable=True)
search_ms: Mapped[int | None] = mapped_column(Integer, nullable=True) llm_ms: Mapped[int | None] = mapped_column(Integer, nullable=True)
prompt_tokens: Mapped[int | None] = mapped_column(Integer, nullable=True)
completion_tokens: Mapped[int | None] = mapped_column(Integer, nullable=True) created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve
Index("ix_chat_turns_user_id", ChatTurn.user_id)
Index("ix_chat_turns_session_id", ChatTurn.session_id) Index("ix_chat_turns_created_at", ChatTurn.created_at)




Alembic Migration (generated tsv + chat tables) (2
2.1 ïº—ïº„ï»›ïºª Ø¥Ù† env.py ïº‘ï»´ïº´ïº˜ï»®Ø±Ø¯ ï»›ï»ž models
:imports Ø£ïº¿ï»’ app/adapters/persistence/postgres/migrations/env.py ï»“ï»²

python

from app.adapters.persistence.postgres import models # users/documents
from app.adapters.persistence.postgres import models_chunks	# chunks
from app.adapters.persistence.postgres import models_chat # chat

 
Copy code  
 
Create revision 2.2





2.3 ï»£ïº¤ïº˜ï»®Ù‰ migration )Ø§ï»§ïº´ïº¨ï»ª ï»›ï»¤ïºŽ ï»«ï»®(
: migrations/versions/*.py Ø¯Ø§ïº§ï»ž Ø§ï»Ÿïº ïºªï»³ïºª revision ï»£ï» ï»’ ï»“ï»² ïº¿ï»Œï»ª
 

python
 

from alembic import op import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision = "xxxx_add_chunks_chat"
down_revision = "<<<PUT_PREVIOUS_REVISION_ID_HERE>>>" branch_labels = None
depends_on = None



def upgrade() -> None: # --- chunks table op.create_table(
"chunks",
sa.Column("id", sa.String(length=36), primary_key=True),
sa.Column("document_id", sa.String(length=36), sa.ForeignKey("documents.i sa.Column("user_id", sa.String(length=36), sa.ForeignKey("users.id", onde sa.Column("text", sa.Text(), nullable=False),

# Create column first; we'll convert to GENERATED using raw SQL
sa.Column("tsv", postgresql.TSVECTOR(), nullable=False),

sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.tex
)

op.create_index("ix_chunks_user_id", "chunks", ["user_id"])
op.create_index("ix_chunks_document_id", "chunks", ["document_id"])
op.create_index("ix_chunks_tsv", "chunks", ["tsv"], postgresql_using="gin")

# Make tsv a generated stored column
# Postgres syntax: GENERATED ALWAYS AS (...) STORED
# Need to drop & recreate if Alembic can't alter generated; simplest: ALTER C
op.execute("""
ALTER TABLE chunks ALTER COLUMN tsv
SET DATA TYPE tsvector
USING to_tsvector('simple', coalesce(text, '')); """)
 
# convert to generated (requires dropping default; we use raw SQL) # Approach: drop column and re-add as generated (reliable).
op.execute("""
ALTER TABLE chunks DROP COLUMN tsv; """)
op.execute("""
ALTER TABLE chunks
ADD COLUMN tsv tsvector GENERATED ALWAYS AS (to_tsvector('simple', coales """)
op.execute("""
CREATE INDEX ix_chunks_tsv ON chunks USING gin (tsv); """)

# --- chat_sessions
op.create_table(
"chat_sessions",
sa.Column("id", sa.String(length=36), primary_key=True),
sa.Column("user_id", sa.String(length=36), sa.ForeignKey("users.id", onde sa.Column("title", sa.String(length=200), nullable=True),
sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.tex
)
op.create_index("ix_chat_sessions_user_id", "chat_sessions", ["user_id"])

# --- chat_turns
op.create_table( "chat_turns",
sa.Column("id", sa.String(length=36), primary_key=True),
sa.Column("session_id", sa.String(length=36), sa.ForeignKey("chat_session sa.Column("user_id", sa.String(length=36), sa.ForeignKey("users.id", onde

sa.Column("question", sa.Text(), nullable=False), sa.Column("answer", sa.Text(), nullable=False),
sa.Column("sources_json", sa.Text(), nullable=False, server_default=sa.te

sa.Column("retrieval_k", sa.Integer(), nullable=False, server_default="5" sa.Column("embed_ms", sa.Integer(), nullable=True),
sa.Column("search_ms", sa.Integer(), nullable=True), sa.Column("llm_ms", sa.Integer(), nullable=True),
sa.Column("prompt_tokens", sa.Integer(), nullable=True),
sa.Column("completion_tokens", sa.Integer(), nullable=True),
 


sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.tex
)
op.create_index("ix_chat_turns_user_id", "chat_turns", ["user_id"])
op.create_index("ix_chat_turns_session_id", "chat_turns", ["session_id"]) op.create_index("ix_chat_turns_created_at", "chat_turns", ["created_at"])


def downgrade() -> None:
op.drop_index("ix_chat_turns_created_at", table_name="chat_turns") op.drop_index("ix_chat_turns_session_id", table_name="chat_turns") op.drop_index("ix_chat_turns_user_id", table_name="chat_turns")
op.drop_table("chat_turns")

op.drop_index("ix_chat_sessions_user_id", table_name="chat_sessions") op.drop_table("chat_sessions")

op.drop_index("ix_chunks_tsv", table_name="chunks")
op.drop_index("ix_chunks_document_id", table_name="chunks") op.drop_index("ix_chunks_user_id", table_name="chunks")
op.drop_table("chunks")



.(users/documents ï»‹ï»¤ï» ïº– Ø§ï»Ÿï» ï»² Ø§ï»Ÿïº´ïºŽïº‘ï»˜ïº” migration ïº‘ïº˜ïºŽØ¹ ID) Ø§ï»Ÿïº¼ïº¤ï»´ïº¢ down_revision ïº£ï»‚ :ï»£ï»¬ï»¢

ïº›ï»¢:

bash

alembic upgrade head

Copy code  


Ports + Repos (3
ChunkRepo Port 3.1
app/application/ports/chunk_repo.py

python
 

from typing import Protocol, Sequence from app.domain.entities import Chunk

class ChunkRepoPort(Protocol):
def insert_many(self, *, chunks: Sequence[Chunk]) -> None: ...
Copy code  
PostgresChunkRepo 3.2
app/adapters/persistence/postgres/repo_chunks.py



ChatRepo Port 3.3
 
app/application/ports/chat_repo.py


PostgresChatRepo 3.4
app/adapters/persistence/postgres/repo_chat.py

 
sid = str(uuid.uuid4())
with SessionLocal() as db:
db.execute(insert(ChatSession).values(id=sid, user_id=tenant_id.value db.commit()
return sid

def add_turn( self,
*,
tenant_id: TenantId, session_id: str,
question: str, answer: str,
sources,
retrieval_k: int,
embed_ms: int | None = None, search_ms: int | None = None, llm_ms: int | None = None,
prompt_tokens: int | None = None,
completion_tokens: int | None = None,
) -> str:
tid = str(uuid.uuid4())
with SessionLocal() as db: db.execute(
insert(ChatTurn).values( id=tid,
session_id=session_id,
user_id=tenant_id.value, question=question,
answer=answer,
sources_json=json.dumps(list(sources)), retrieval_k=retrieval_k,
embed_ms=embed_ms,
search_ms=search_ms, llm_ms=llm_ms,
prompt_tokens=prompt_tokens,
completion_tokens=completion_tokens,
)
)
 
db.commit() return tid




generated tsv ï»‹ï» ï»° ï»³ï»Œïº˜ï»¤ïºª KeywordStore (4
app/adapters/persistence/postgres/keyword_store.py

python

from sqlalchemy import select, text as sql_text
from app.adapters.persistence.postgres.db import SessionLocal
from app.adapters.persistence.postgres.models_chunks import ChunkRow from app.application.ports.keyword_store import KeywordStorePort
from app.domain.entities import Chunk, TenantId, DocumentId

class PostgresKeywordStore(KeywordStorePort):
def search(self, *, query: str, tenant_id: TenantId, top_k: int): tsq = sql_text("websearch_to_tsquery('simple', :q)")
rank = sql_text("ts_rank_cd(chunks.tsv, websearch_to_tsquery('simple', :q
Copy code
with SessionLocal() as db: stmt = (
select(ChunkRow.id, ChunkRow.document_id, ChunkRow.user_id, Chunk
.where(ChunkRow.user_id == tenant_id.value)
.where(ChunkRow.tsv.op("@@")(tsq))
.order_by(rank.desc())
.limit(top_k)
)
rows = db.execute(stmt, {"q": query}).all()

return [
Chunk(
id=cid,
tenant_id=TenantId(uid),
document_id=DocumentId(doc_id), text=txt,
)
 
for (cid, doc_id, uid, txt) in rows
]





Postgres + upsert Qdrant ï»“ï»² chunks ïº§Ù‘ïº°Ù† indexing Ø£ïº›ï»¨ïºŽØ¡ :Worker (5
: chunks list ïº—ïº’ï»¨ï»² ï»£ïºŽ ïº‘ï»Œïºª app/workers/tasks.py ï»“ï»²

Ùˆïº—ï»˜ïºªØ± status failed á¹‡ïº—ïº´ï»´ ï»“ïº¸ï»ž Qdrant Ùˆï»Ÿï»® ØŒQdrant ïº›ï»¢ Ø£ÙˆÙ‹Ù„Ø§ Postgres á»±Ø§ï»›ïº˜ :ï»‹ïºŽØ¯Ø© Ø£ï»“ï»€ï»ž á¹‡ïº—ïº®ïº—ï»´
 
Copy code  
 
.retry ïº—ï»Œï»´ïºª
 


 

(repos Ø£ïº¿ï»’) Bootstrap wiring (6
: app/core/bootstrap.py ï»“ï»²

 
"keyword_store": keyword_store,
}





 

Copy code  





























Copy code  
 
(7 )Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ ï»Ÿï»œï»¦ ï»—ï»®ÙŠ( ïº—ïº´ïº ï»´ï»ž Turn Chat ïº‘ï»Œïºª Ø§Ù„Ø¥ïºŸïºŽïº‘ïº”
:answer ïº—ï»Œï»¤ï»ž ï»£ïºŽ ïº‘ï»Œïºª AskQuestionHybridUseCase ï»“ï»²
.turn ïº§Ù‘ïº°Ù† :request ï»“ï»² session_id ï»‹ï»¨ïºªÙƒ ï»Ÿï»®	 
:ïº›ï»¢ ØŒDTO ï»Ÿï» Ù€ session_id: str | None Ø£ïº¿ï»’



















Quick sanity checks (8
: alembic upgrade head ïº‘ï»Œïºª	 
tsv GENERATED STORED ï»³ï»ˆï»¬ïº® Ù„Ø§Ø²Ù… psql ï»“ï»² d chunks\	 
:keyword Ø§ïº§ïº˜ïº’ïº®	 
   Ø§Ø±ï»“ï»Š ï»£ï» ï»’
chunks á¹‡ï»›ïº˜ worker ïº—ïº„ï»›ïºª	 
 
   ïºŸïº®Ù‘á»‹ ask-hybrid/ ïº‘ïº´ïº†Ø§Ù„ ï»³ïº¤ïº˜ï»®ÙŠ ï»›ï» ï»¤ïºŽØª ïº£ïº®ï»“ï»´Ù‹ïºŽ ï»£ï»¦ Ø§ï»Ÿï»¨ïºº â†’ recall keyword ï»³ïº˜ïº¤ïº´ï»¦ ïºŸïºªÙ‹Ø§


ï»Ÿï»® ïº—ïº¤á¹‡ØŒ Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” Ø§ï»Ÿï»¤ï»¨ï»„ï»˜ï»´ïº”:
(indexing ïº—ï»œïº®Ø§Ø± ï»£ï»¨ï»Š) ï»Ÿï» ï»¤ï» ï»’ Idempotency hashing .1
(chunk ï»Ÿï»œï»ž hash) Chunk de-dup .2
 
(Ø§ï»Ÿï»„ï»®ï»³ï»ž Ø§ï»Ÿï»¤ïºªÙ‰ ï»‹ï» ï»° ÙˆØ£Ø±ïº§ïºº LLM rerank ï»£ï»¦ Ø£ï»“ï»€ï»ž) Reranker Cross-Encoder local .3
ï»—ï»®ï»ŸÙ‘ï»² Ø£ÙŠ Ø§ïº—ïº ïºŽÙ‡ ï»§ï»œï»¤ï»ž.




Idempotency hashing ï»Ÿï» ï»¤ï» ï»’ (ïº—ï»œïº®Ø§Ø± ï»£ï»¨ï»Š indexing) Chunk de-dup (hash ï»Ÿï»œï»ž chunk)
Reranker Cross-Encoder local (ï»£ï»¦ Ø£ï»“ï»€ï»ž LLM rerank ÙˆØ£Ø±ïº§ïºº
)ï»‹ï» ï»° Ø§ï»Ÿï»¤ïºªÙ‰ Ø§ï»Ÿï»„ï»®ï»³ï»ž


Thought for 21s  
ïº—ï»¤ïºŽÙ… â€” ï»«ï»¨ï»€ï»´ï»’ Ø§ï»ŸïºœÙ„Ø§Ø« ïº—ïº®ï»—ï»´ïºŽØª Ø¯ÙŠ â€œïº»ïº¢â€ Ùˆïº‘ïº¸ï»œï»ž Production-gradeØŒ ï»£ï»¦ ï»ï»´ïº® ï»£ïºŽ ï»§ï»œØ³Ø± Clean
:Architecture
.1 hashing Idempotency ï»Ÿï» ï»¤ï» ï»’ )ï»£ï»¨ï»Š ïº—ï»œïº®Ø§Ø± indexing ï»Ÿï»¨ï»”ïº² Ø§ï»Ÿï»¤ïº¤ïº˜ï»®Ù‰ Ø¯Ø§ïº§ï»ž ï»§ï»”ïº² Ø§ï»ŸÙ€ (tenant
(mapping docâ†’chunks + ï»£ï»®Ù‘ïº£ïºª ïº—ïº¨ïº°ï»³ï»¦ + chunk ï»Ÿï»œï»ž hash) Chunk de-dup .2
(LLM rerank ï»£ï»¦ ÙˆØ£ïº›ïº’ïº– Ø£Ø±ïº§ïºº) ï»£ïº¤ï» ï»² Reranker Cross-Encoder .3
.DB schema + migrations + adapters + worker changes + wiring + knobs :ï»«ïºªï»³ï»š


ï»Ÿï» ï»¤ï» ï»’ Idempotency hashing (1
Ø§ï»Ÿï»”ï»œïº®Ø©
.upload Ø§ï»ŸÙ€ ï»‹ï»¨ïºª Ø§ï»Ÿï»¤ï» ï»’ bytes ï»‹ï» ï»° file_sha256 á¹‡ï»§ïº¤ïº´	 
. documents.file_sha256 ï»“ï»² ï»§ïº¨Ù‘ïº°ï»§ï»ª	 
: (user_id, file_sha256) ï»‹ï» ï»° unique index ï»§ï»Œï»¤ï»ž	 
   ï»Ÿï»® ï»§ï»”ïº² Ø§ï»Ÿï»¤ï» ï»’ Ø§ïº—ïº®ï»“ï»Š ïº—ïºŽï»§ï»² ï»Ÿï»¨ï»”ïº² Ø§ï»Ÿï»¤ïº´ïº˜ïº¨ïºªÙ… â†’ ï»§ïº®ïºŸÙ‘ï»Š ï»§ï»”ïº² document_id ïº‘ïºªÙ„ Ø¥ï»§ïº¸ïºŽØ¡
.re-index ï»£ï»¨ï»Š + ïºŸïºªï»³ïºª Document

Migration: add file_sha256 + unique index 1.1
:revision Ø§ï»‹ï»¤ï»ž

bash
 

alembic revision -m "add document file hash idempotency"

 
Copy code  
 
:migration ï»“ï»² ïº¿ï»Š
 

python

from alembic import op import sqlalchemy as sa

revision = "xxxx_doc_hash" down_revision = "<<<PREV>>>" branch_labels = None
depends_on = None

def upgrade():
op.add_column("documents", sa.Column("file_sha256", sa.String(length=64), nul op.create_index(
"uq_documents_user_file_sha256", "documents",
["user_id", "file_sha256"], unique=True,
)

def downgrade():
op.drop_index("uq_documents_user_file_sha256", table_name="documents") op.drop_column("documents", "file_sha256")

C	C

ïº›ï»¢:

bash

alembic upgrade head

 
Copy code   Copy code  
 

(ï»³ïº˜ï»ï»´ïº® ï»£ïº¤ïº˜ïºŽØ¬ ï»£ïº¶ StoredFile + UploadResult) Update Domain 1.2
.â€create or get existingâ€œ á¹‡Ùˆï»§ï»„ï»  hash á¹‡ï»§ïº¤ïº´ Ù„Ø§Ø²Ù… UseCase upload ï»“ï»² ï»Ÿï»œï»¦

idempotency ï»³ïºªï»‹ï»¢ Port: DocumentRepo 1.3
ï»‹ïºªÙ‘Ù„/Ø£ïº¿ï»’ port ïº»ï»ï»´ïº® :(ISP)
 
python

# app/application/ports/document_idempotency.py
from typing import Protocol
from app.domain.entities import TenantId, DocumentId

class DocumentIdempotencyPort(Protocol):
def get_by_file_hash(self, *, tenant_id: TenantId, file_sha256: str) -> Docum def create_document_with_hash(self, *, tenant_id: TenantId, stored_file, file

C	C

Postgres repo implementation 1.4
Copy code	python

# app/adapters/persistence/postgres/repo_documents_idempotency.py
import uuid
from sqlalchemy import select
from sqlalchemy.exc import IntegrityError
from app.adapters.persistence.postgres.db import SessionLocal from app.adapters.persistence.postgres.models import Document
from app.domain.entities import TenantId, DocumentId, StoredFile
from app.application.ports.document_idempotency import DocumentIdempotencyPort

class PostgresDocumentIdempotencyRepo(DocumentIdempotencyPort):
def get_by_file_hash(self, *, tenant_id: TenantId, file_sha256: str) -> Docum with SessionLocal() as db:
stmt = select(Document.id).where(Document.user_id == tenant_id.value, v = db.execute(stmt).scalar_one_or_none()
return DocumentId(v) if v else None

def create_document_with_hash(self, *, tenant_id: TenantId, stored_file: Stor doc_id = DocumentId(str(uuid.uuid4()))
with SessionLocal() as db: db.add(
Document(
id=doc_id.value,
user_id=tenant_id.value,
filename=stored_file.filename,
content_type=stored_file.content_type,
 
file_path=stored_file.path,
size_bytes=stored_file.size_bytes, status="created",
file_sha256=file_sha256,
)
)
try:
db.commit()
return doc_id
except IntegrityError: db.rollback()
# someone inserted same hash concurrently â†’ fetch existing
stmt = select(Document.id).where(Document.user_id == tenant_id.va existing = db.execute(stmt).scalar_one()
return DocumentId(existing)


Update UploadDocumentUseCase (idempotent) 1.5

python

# app/application/use_cases/upload_document.py (replace create logic)
import hashlib
from app.application.ports.document_idempotency import DocumentIdempotencyPort

class UploadDocumentUseCase:
def   init  (self, file_store, repo: DocumentRepoPort, queue, idem_repo: Docu
self._file_store = file_store self._repo = repo
self._queue = queue
self._idem = idem_repo

Copy code
async def execute(self, req: UploadDocumentRequest) -> UploadResult:
tenant = TenantId(req.tenant_id)

file_sha256 = hashlib.sha256(req.data).hexdigest()

existing = self._idem.get_by_file_hash(tenant_id=tenant, file_sha256=file if existing:
# already known â†’ do NOT enqueue again
return UploadResult(document_id=existing, status="already_exists")
 


stored = await self._file_store.save_upload( tenant_id=tenant.value,
upload_filename=req.filename, content_type=req.content_type, data=req.data,
)

doc_id = self._idem.create_document_with_hash(tenant_id=tenant, stored_fi self._repo.set_status(tenant_id=tenant, document_id=doc_id, status="queue self._queue.enqueue_index_document(tenant_id=tenant, document_id=doc_id)

return UploadResult(document_id=doc_id, status="queued")





Ø§ï»Ÿïº¼ïº¤ï»´ïº¢ Ø§ï»Ÿïº˜ïº¼ï»¤ï»´ï»¢ â€” (chunk ï»Ÿï»œï»ž hash) Chunk de-dup (2
ï»Ÿï»´ï»ª Ø§ï»Ÿïº˜ïº¼ï»¤ï»´ï»¢ Ø§ï»Ÿïº¼ïº¢ Ù„Ø§Ø²  ï»³ï»œï»®Ù† ïºŸïºªÙˆï»Ÿï»´ï»¦ØŸ
ï»Ÿï»® ïº§ïº°ï»§Ù‘ïºŽ chunks Ø¯Ø§ïº§ï»ž documents ï»£ïº’ïºŽØ´Ø±Ø©ØŒ Ø§ï»ŸïºªÙŠ-Ø¯á»‹ ï»«ï»´ïº’ï»˜ï»° â€œØ¯Ø§ïº§ï»ž doc ï»“ï»˜ï»‚.â€
Ø£ï»§ïº– ï»ƒï» ïº’ïº– de-dup ïº£ï»˜ï»´ï»˜ï»² )ï»³ï»®ï»“ïº® ï»£ïº´ïºŽïº£ïº” + ï»³ïºœïº’ïº– search Keyword + ï»³ïº¤ïº´ï»¦ (re-useØŒ ï»“ïºŽÙ„Ø£ï»“ï»€ï»ž:
chunk_store : chunk unique per tenant via (user_id, chunk_hash)	 
position/á¹‡ïº—ïº®ïº—ï»´ + document_chunks : mapping doc â†’ chunk	 

Migration: chunk_store + document_chunks + generated tsv 2.1
)ïº‘ïºªÙ„ ïºŸïºªÙˆÙ„ chunks Ø§ï»Ÿï»˜ïºªï»³ï»¢ ï»Ÿï»® ï»›ï»¨ïº– ï»‹ïºŽï»£ï» Ù‘ï»ª(                            code Copy ï»Ÿï»® ï»‹ï»¨ïºªÙƒ ïºŸïºªÙˆÙ„ chunks ïº‘ïºŽï»Ÿï»”ï»Œï»ž: Ø¥ï»£Ù‘ïºŽ ï»§ï»Œï»¤ï»ž migration Ø§ï»§ïº˜ï»˜ïºŽï»Ÿï»´ïº”. ï»Ÿï»® ï»Ÿïº´ï»ª ï»“ï»² ï»£ïº®ïº£ï» ïº” templateØŒ Ø§Ù„Ø£ïº³ï»¬ï»ž: ï»§ï»Œï»¤ï»ž tables Ø§ï»Ÿïº ïºªï»³ïºªØ© Ùˆï»§ïº´ï»´á¹‡ Ø§ï»Ÿï»˜ïºªï»³ï»¤ïº” Ø£Ùˆ ï»§ïº°ï»³ï» ï»¬ïºŽ ï»“ï»² .downgrade/cleanup
:Revision


python
 

from alembic import op import sqlalchemy as sa
from sqlalchemy.dialects import postgresql

revision = "xxxx_chunk_dedup" down_revision = "<<<PREV>>>" branch_labels = None
depends_on = None

def upgrade():
# chunk_store
op.create_table(
"chunk_store",
sa.Column("id", sa.String(36), primary_key=True),
sa.Column("user_id", sa.String(36), sa.ForeignKey("users.id", ondelete="C sa.Column("chunk_hash", sa.String(64), nullable=False),
sa.Column("text", sa.Text(), nullable=False),
sa.Column("tsv", postgresql.TSVECTOR(), nullable=False),
sa.Column("created_at", sa.DateTime(timezone=True), server_default=sa.tex
)
op.create_index("uq_chunk_store_user_hash", "chunk_store", ["user_id", "chunk op.create_index("ix_chunk_store_user_id", "chunk_store", ["user_id"])

# generated tsv
op.execute("ALTER TABLE chunk_store DROP COLUMN tsv;") op.execute("""
ALTER TABLE chunk_store
ADD COLUMN tsv tsvector GENERATED ALWAYS AS (to_tsvector('simple', coales """)
op.execute("CREATE INDEX ix_chunk_store_tsv ON chunk_store USING gin (tsv);")

# document_chunks mapping
op.create_table(
"document_chunks",
sa.Column("document_id", sa.String(36), sa.ForeignKey("documents.id", ond sa.Column("chunk_id", sa.String(36), sa.ForeignKey("chunk_store.id", onde sa.Column("ord", sa.Integer(), nullable=False),
sa.PrimaryKeyConstraint("document_id", "ord"),
)
op.create_index("ix_document_chunks_document_id", "document_chunks", ["docume
 
op.create_index("ix_document_chunks_chunk_id", "document_chunks", ["chunk_id"

def downgrade():
op.drop_index("ix_document_chunks_chunk_id", table_name="document_chunks")
op.drop_index("ix_document_chunks_document_id", table_name="document_chunks") op.drop_table("document_chunks")

op.execute("DROP INDEX IF EXISTS ix_chunk_store_tsv;")
op.drop_index("ix_chunk_store_user_id", table_name="chunk_store")
op.drop_index("uq_chunk_store_user_hash", table_name="chunk_store") op.drop_table("chunk_store")


ORM Models 2.2
app/adapters/persistence/postgres/models_chunk_store.py

python

from sqlalchemy import String, Text, DateTime, ForeignKey, Integer, func, Index, from sqlalchemy.orm import Mapped, mapped_column
from sqlalchemy.dialects.postgresql import TSVECTOR
from app.adapters.persistence.postgres.db import Base

class ChunkStoreRow(Base):
  tablename	 = "chunk_store"
  table_args	 = (
UniqueConstraint("user_id", "chunk_hash", name="uq_chunk_store_user_hash"
)

id: Mapped[str] = mapped_column(String(36), primary_key=True)
user_id: Mapped[str] = mapped_column(String(36), ForeignKey("users.id", ondel
Copy ccohduenk_hash: Mapped[str] = mapped_column(String(64), nullable=False) text: Mapped[str] = mapped_column(Text, nullable=False)
tsv: Mapped[object] = mapped_column(TSVECTOR, nullable=False) # generated in
created_at: Mapped["DateTime"] = mapped_column(DateTime(timezone=True), serve

Index("ix_chunk_store_user_id", ChunkStoreRow.user_id)
Index("ix_chunk_store_tsv", ChunkStoreRow.tsv, postgresql_using="gin")
 
class DocumentChunkRow(Base):
  tablename	 = "document_chunks"
document_id: Mapped[str] = mapped_column(String(36), ForeignKey("documents.id ord: Mapped[int] = mapped_column(Integer, primary_key=True)
chunk_id: Mapped[str] = mapped_column(String(36), ForeignKey("chunk_store.id"

Index("ix_document_chunks_document_id", DocumentChunkRow.document_id) Index("ix_document_chunks_chunk_id", DocumentChunkRow.chunk_id)


Chunk hashing + de-dup repo 2.3
:Port

:Adapter

 
stmt = select(ChunkStoreRow.id).where(ChunkStoreRow.user_id == tenant existing = db.execute(stmt).scalar_one_or_none()
if existing:
return existing

cid = str(uuid.uuid4())
db.add(ChunkStoreRow(id=cid, user_id=tenant_id.value, chunk_hash=chun try:
db.commit() return cid
except IntegrityError: db.rollback()
# concurrent insert
existing = db.execute(stmt).scalar_one() return existing

def replace_document_chunks(self, *, tenant_id: TenantId, document_id: str, c # tenant_id not stored in mapping table; tenant isolation enforced via do with SessionLocal() as db:
db.execute(delete(DocumentChunkRow).where(DocumentChunkRow.document_i rows = [{"document_id": document_id, "ord": i, "chunk_id": cid} for i if rows:
db.execute(insert(DocumentChunkRow), rows) db.commit()


Worker changes: hashing + de-dup + mapping + Qdrant 2.4
:chunking ïº‘ï»Œïºª index_document Ø¯Ø§ïº§ï»ž

 
chunk_ids_in_order = [] chunks_for_qdrant = [] vectors = []

for t in chunks_text: h = chunk_sha256(t)
chunk_id = dedup_repo.upsert_chunk_store(tenant_id=tenant, chunk_hash=h, text chunk_ids_in_order.append(chunk_id)

# Qdrant uses chunk_id as point id (stable + dedup)
chunks_for_qdrant.append(Chunk(id=chunk_id, tenant_id=tenant, document_id=doc vectors.append(cached_emb.embed_one(t))

dedup_repo.replace_document_chunks( tenant_id=tenant,
document_id=doc_id.value,
chunk_ids_in_order=chunk_ids_in_order,
)

vector_store.ensure_collection()
vector_store.upsert(chunks_for_qdrant, vectors)


ï»£ï»´ïº°Ø©: ï»Ÿï»® ï»§ï»”ïº² chunk Ø§ïº—ï»œïº®Ø± ï»“ï»² ï»£ï» ï»’ Ø£Ùˆ ï»£ï» ï»”ïºŽØª Ø£ïº§ïº®Ù‰ ï»Ÿï»¨ï»”ïº² Ø§ï»Ÿï»¤ïº´ïº˜ïº¨ïºªÙ…ØŒ ï»§ï»”ïº² chunk_id ï»«ï»´ïº˜ï»ŒïºŽØ¯ Ø§ïº³ïº˜ïº¨ïºªØ§ï»£ï»ª.


chunk_store (generated tsv) ï»‹ï» ï»° Hybrid Keyword Search (3
:(Ø§ï»Ÿï»˜ïºªï»³ï»¤ïº” chunks ï»£ïº¶) chunk_store ï»£ï»¦ ï»Ÿï»´ï»œï»®Ù† KeywordStore query ïº‘Ù‘ïºªÙ„

 
class PostgresKeywordStore(KeywordStorePort):
def search(self, *, query: str, tenant_id: TenantId, top_k: int): tsq = sql_text("websearch_to_tsquery('simple', :q)")
rank = sql_text("ts_rank_cd(chunk_store.tsv, websearch_to_tsquery('simple

with SessionLocal() as db: stmt = (
select(ChunkStoreRow.id, ChunkStoreRow.user_id, ChunkStoreRow.tex
.where(ChunkStoreRow.user_id == tenant_id.value)
.where(ChunkStoreRow.tsv.op("@@")(tsq))
.order_by(rank.desc())
.limit(top_k)
)
rows = db.execute(stmt, {"q": query}).all()

ï»£ïº¶ ï»£ïº®ïº‘ï»®Ø· ïº‘ïºªÙˆÙƒ ÙˆØ§ïº£ïºª chunk_store Ú¾ï»¨ïºŽ Ø§ïº§ïº˜ï¯¿ïºŽØ±ÙŠØ› document_id #
# ï»£ïº¤ïº˜ïºŽØ¬ ï»Ÿï»® doc filter: join document_chunks (ï»»ïº£Ù‹ï»˜ïºŽ ïº‘ï»¨ï»€ï¯¿ï»”ï»ª).
return [Chunk(id=cid, tenant_id=TenantId(uid), document_id=DocumentId("")





Reranker Cross-Encoder Local (Production-grade) (4
Dependencies 4.1


 
Copy code  
 
Settings knobs 4.2
: app/core/config.py ï»“ï»²
 

python

rerank_backend: str = "cross_encoder" # cross_encoder|none
cross_encoder_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2" cross_encoder_device: str = "cpu" # cpu|cuda


Copy code  
 

 
ï»£ï»¤ï»œï»¦ ïº—ïº´ïº˜ïº¨ïºªÙ… models Ø£ï»—ï»®Ù‰ Ù„Ø§ïº£ï»˜Ù‹ïºŽ )Ø²ÙŠ (bge-reranker ï»Ÿï»® ï»‹ï»¨ïºªÙƒ .GPU

Adapter implements RerankerPort 4.3

python

# app/adapters/rerank/cross_encoder_reranker.py
from typing import Sequence
from sentence_transformers import CrossEncoder
from app.application.ports.reranker import RerankerPort from app.domain.entities import Chunk

class CrossEncoderReranker(RerankerPort):
def   init  (self, model_name: str, device: str = "cpu") -> None:
self._model = CrossEncoder(model_name, device=device)

def rerank(self, *, query: str, chunks: Sequence[Chunk], top_n: int) -> Seque if not chunks:
return []
pairs = [(query, c.text) for c in chunks]
scores = self._model.predict(pairs)	# numpy array
ranked = sorted(zip(chunks, scores), key=lambda x: float(x[1]), reverse=T return [c for c, _ in ranked[:top_n]]

C	C

bootstrap ï»“ï»² Wiring 4.4



 
Copy code  
 
(5 logging Chat + ïºŸï»®Ø¯Ø© )Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ ï»Ÿï»œï»¦ ï»—ï»®ÙŠ(
ïº‘ï»Œïºª ï»£ïºŽ ïº—ï»¨ïº˜ïºž Ø§Ù„Ø¥ïºŸïºŽïº‘ïº” ï»“ï»² :AskHybridUseCase
 
chat_turns (question/answer/sources + timings) ï»“ï»² turn ïº§Ù‘ïº°Ù†	 
.Eval-ready history Ùˆ Observability ïº‘ï»´ïºªÙŠ Ø¯Ù‡	 
)Ø£ï»§ïº– ï»‹ï»¨ïºªÙƒ ChatRepo ïº³ïºŽïº‘ï»˜Ù‹ïºŽâ€”ï»£ï»¤ï»œï»¦ ï»§ï»®ïº»ï» ï»ª ï»£ïº’ïºŽØ´Ø±Ø©(.


(container ï»Ÿï» Ù€ ïº—ï»€ï»´ï»”ï»ª Ø§ï»Ÿï» ï»² Ø¥ï»³ï»ª) Bootstrapping summary (6
:Ø£ïº¿ï»’ ()get_container ï»“ï»²
()document_idem_repo = PostgresDocumentIdempotencyRepo
()chunk_dedup_repo = PostgresChunkDedupRepo
(...)reranker = CrossEncoderReranker	 
(chunk_store ï»‹ï» ï»°) ()keyword_store = PostgresKeywordStore	 
Update UploadDocumentUseCase(..., idem_repo=document_idem_repo)	 



(7 ï»£Ù„Ø§ïº£ï»ˆïºŽØª Production ï»£ï»¬ï»¤ïº” ïºŸïºªÙ‹Ø§
   Ø¥ï»‹ïºŽØ¯Ø© Ø±ï»“ï»Š ï»§ï»”ïº² Ø§ï»Ÿï»¤ï» ï»’: â€already_existsâ€œ ï»³ïº®ïºŸï»Š document_id Ø§ï»Ÿïº¤ïºŽï»Ÿï»² â†’ ï»£ï»”ï»´ïº¶ queue ÙˆÙ„Ø§
.indexing race ïº‘ïº˜ï»˜ï»”ï»ž Ø§ï»Ÿï» ï»² ï»«ï»² unique indexes + IntegrityError handling Ø§ï»ŸÙ€ :Concurrency	 
.conditions
:Chunk de-dup	 
   ï»³ï»˜ï» ï»ž DB Ø§ï»Ÿïº¤ïº ï»¢
(chunk_id point id Ø§ïº³ïº˜ïº¨ïºªï»£ïº– ï»Ÿï»®) Qdrant points ï»³ï»˜ï» ï»ž	 
ïº›ïºŽïº‘ïº– ïº‘ïº¸ï»œï»ž keyword recall Ùˆï»³ïº¤ïº´ï»¦	 
:Cross-Encoder	 
.latency á¹‡Ø±Ø§ï»— ï»Ÿï»œï»¦ ØŒrerank 40 passages â†’ top 8 ï»ŸÙ€ ï»£ï»¤ïº˜ïºŽØ² :CPU ï»‹ï» ï»°	 
   ï»‹ï» ï»° :GPU Ø£Ø³Ø±Ø¹ ïº‘ï»œïºœï»´ïº® Ùˆï»³ï»”ïº˜ïº¢ ï»Ÿï»š top_k Ø£ï»›ïº’ïº®.


Ø¥Ø°Ø§ ïº—ïº¤á¹‡ØŒ Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿï» ï»² ïº‘ï»Œïºªï»«ïºŽ ï»£ïº’ïºŽØ´Ø±Ø© ï»‹ïº¸ïºŽÙ† ï»³ïº’ï»˜ï»° :â€Enterprise-readyâ€œ
Doc-filtered keyword search (join document_chunks + chunk_store ) .1 (Qdrant storage ïº—ï»®ï»“ï»´ïº®) Vector payload minimal + fetch text from DB .2 Ùˆï»›ï» ï»”ïº” Ùˆï»—ïº– ï»Ÿïº˜ï»˜ï» ï»´ï»ž (embed_one ïº‘ïºªÙ„) indexing ï»“ï»² Batch embeddings .3
ï»—ï»®ï»ŸÙ‘ï»² ïº—ïº¤á¹‡ ï»§ï»œï»¤ï»ž ïº‘ïº„ÙŠ ïº—ïº®ïº—ï»´á¹‡ØŸ
 

 	 	 	 	 	 



Doc-filtered keyword search (join document_chunks + chunk_store)

Vector payload minimal + fetch text from DB (ïº—ï»®ï»“ï»´ïº® Qdrant storage)

Batch embeddings ï»“ï»² indexing (ïº‘ïºªÙ„ embed_one) ï»Ÿïº˜ï»˜ï» ï»´ï»ž
Ùˆï»—ïº– Ùˆï»›ï» ï»”ïº”


:Clean Architecture Ø±ÙˆØ­ ïº‘ï»¨ï»”ïº² â€Enterprise-gradeâ€œ ïº—ïº®ï»—ï»´ïºŽØª 3 ï»«ï»¨ï»€ï»´ï»’ â€” ïº—ï»¤ïºŽÙ…
(ï»£ïº¤ïºªØ¯ document ï»‹ï» ï»° ï»£ï»˜ï»´ïºª ï»Ÿï»œï»¦ FTS) Doc-filtered keyword search .1
(Postgres ï»£ï»¦ á¹‡Ùï»³ïº ï»  Ø§ï»Ÿï»¨ïºº + payload Ø£ï»—ï»ž ï»³ïº¨ïº°Ù† Qdrant) Vector payload minimal .2
(Ø£ï»—ï»ž ïº—ï»œï» ï»”ïº” + Ø³Ø±ï»‹ïº”) indexing ï»“ï»² Batch embeddings .3
.Retrieval/Ask ïº—ï»Œïºªï»³Ù„Ø§Øª + Worker ïº—ï»Œïºªï»³Ù„Ø§Øª + Ports + Adapters + SQL :ï»«ïºªï»³ï»š


Doc-filtered keyword search ( join document_chunks + (1
chunk_store )
1.1 Port )Ø¥ïº¿ïºŽï»“ïº” ï»“ï» ïº˜ïº® document_id Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ(
ïº‘ïºªÙ„ ï»£ïºŽ ï»§ï»Œï»¤ï»ž Port ïºŸïºªï»³ïºªØŒ ï»§ïº¨ï» ï»² method ïº—ï»˜ïº’ï»ž None | str document_id: ISP) ï»£ï»˜ïº’ï»®Ù„ Ù„Ø£ï»§ï»ª ï»§ï»”ïº² Ø§ï»Ÿï»¤ïº´ïº†Ùˆï»Ÿï»´ïº”.(
python

# app/application/ports/keyword_store.py
from typing import Protocol, Sequence
from app.domain.entities import Chunk, TenantId

class KeywordStorePort(Protocol): def search(
self,
*,
 
query: str,
tenant_id: TenantId, top_k: int,
document_id: str | None = None,
) -> Sequence[Chunk]: ...

 







Copy code  
 
PostgresKeywordStore (doc-filtered) 1.2
Ø§ï»Ÿï»¤ïº’ïºªØ£:
:ï»£ï»®ïºŸï»®Ø¯ document_id ï»Ÿï»®	  join: document_chunks dc â†’ chunk_store cs ï»§ï»Œï»¤ï»ž	 
dc.document_id = :doc_id + cs.user_id = :user_id ï»§ï»”ï» ïº˜ïº®	 
ts_rank_cd(cs.tsv, tsquery) ïº‘Ù€ á¹‡ï»§ïº®ïº—	  (ï»£ï»Œïº®ÙˆÙ document_id) Chunk id/text Ùˆï»§ïº®Ù‘ïºŸï»Š	 
 
python

# app/adapters/persistence/postgres/keyword_store.py
from sqlalchemy import text as sql
from app.adapters.persistence.postgres.db import SessionLocal
from app.application.ports.keyword_store import KeywordStorePort from app.domain.entities import Chunk, TenantId, DocumentId

class PostgresKeywordStore(KeywordStorePort):
def search(self, *, query: str, tenant_id: TenantId, top_k: int, document_id: with SessionLocal() as db:
if document_id:
rows = db.execute(sql(""" SELECT cs.id, cs.text
FROM document_chunks dc
JOIN chunk_store cs ON cs.id = dc.chunk_id WHERE dc.document_id = :doc_id
AND cs.user_id = :user_id
AND cs.tsv @@ websearch_to_tsquery('simple', :q)
ORDER BY ts_rank_cd(cs.tsv, websearch_to_tsquery('simple', :q dc.ord ASC
LIMIT :k
"""), {"doc_id": document_id, "user_id": tenant_id.value, "q": qu

return [
 
Chunk(
id=cid,
tenant_id=tenant_id,
document_id=DocumentId(document_id), text=txt,
)
for (cid, txt) in rows
]

# tenant-wide keyword search
rows = db.execute(sql(""" SELECT cs.id, cs.text FROM chunk_store cs
WHERE cs.user_id = :user_id
AND cs.tsv @@ websearch_to_tsquery('simple', :q)
ORDER BY ts_rank_cd(cs.tsv, websearch_to_tsquery('simple', :q)) D LIMIT :k
"""), {"user_id": tenant_id.value, "q": query, "k": top_k}).all()

return [
Chunk(
id=cid,
tenant_id=tenant_id,
document_id=DocumentId(""), text=txt,
)
for (cid, txt) in rows
]


âœ… ï»›ïºªÙ‡ ïº—ï»˜ïºªØ± ïº—ï»Œï»¤ï»ž â€œïº‘ïº¤ïºš Ø¯Ø§ïº§ï»ž document ÙˆØ§ïº£ïºªâ€ useful) ïºŸïºªÙ‹Ø§ ï»Ÿï»®Ø§ïºŸï»¬ïº” .(ChatPDF


Vector payload minimal + fetch text from DB (2
Ø§ï»Ÿï»¤ïº¸ï»œï» ïº”
:ÙˆØ¯Ù‡ ØŒ text ï»“ï»´ï»ª Qdrant payload ïº£ïºŽï»ŸÙ‹ï»´ïºŽ
storage ï»³ïº°Ù‘ÙˆØ¯	 
   ï»³ï»œïº®Ø± Ø§ï»Ÿïº’ï»´ïºŽï»§ïºŽØª ï»£ï»Š Postgres
 













Copy code  
 
  ï»³ïº¼ï»ŒÙ‘á¹‡ ïº—ïº¤ïºªï»³ïºš Ø§ï»Ÿï»¨ïºº/Ø§ï»Ÿïº˜ï»¨ï»ˆï»´ï»’
Ø§ï»Ÿïº¤ï»ž Ø§ï»Ÿïº¼ïº¤ï»´ïº¢
:ï»³ïº¨ïº°Ù† Qdrant	 
tenant_id	 
(Ø£ïº»Ù‹Ù„Ø§ id) chunk_id	 
(doc-filter vector-side ï»£ïº¤ïº˜ïºŽØ¬ ï»Ÿï»® Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ) document_id ï»£ï»¤ï»œï»¦	 
. ChunkTextReaderPort ï»‹ïº’ïº® Postgres ï»£ï»¦ á»±ï»³ïº ï»  Ø§ï»Ÿï»¨ïºº	 
Port: ChunkTextReader 2.1
 

python

# app/application/ports/chunk_text_reader.py
from typing import Protocol, Sequence
from app.domain.entities import TenantId

class ChunkTextReaderPort(Protocol):
def get_texts_by_ids(self, *, tenant_id: TenantId, chunk_ids: Sequence[str])


C	C

Adapter: PostgresChunkTextReader 2.2
 
AND id = ANY(:ids)
"""), {"user_id": tenant_id.value, "ids": list(chunk_ids)}).all() return {cid: txt for cid, txt in rows}

ï»£Ù„Ø§ïº£ï»ˆïº”: ANY(:ids) ï»³ï»Œï»¤ï»ž ï»“ï»² psycopg ï»›ï»®ï»³ïº²ØŒ ï»Ÿï»® ÙˆØ§ïºŸï»¬ïº˜ï»š ï»£ïº¸ï»œï» ïº” ï»§ïº’ïºªÙ‘ï»Ÿï»¬ïºŽ Ø¥ï»Ÿï»° = id WHERE
. ANY(CAST(:ids AS text[]))

Qdrant upsert: payload minimal ïº—ï»Œïºªï»³ï»ž 2.3


 


Copy code  
 
ï»“ï»˜ï»‚ IDs ï»³ïº®Ù‘ïºŸï»Š Qdrant search ïº—ï»Œïºªï»³ï»ž 2.4
ï»«ï»¨ïº®ïºŸï»Š objects Chunk ïº‘ïºŽï»Ÿï»¨ïºº ï»“ïºŽØ±ØºØŒ Ùˆïº‘ï»Œïºªï»³ï»¦ ï»§ï»¤Ù„Ø£Ù‡ ï»£ï»¦ .DB
 

python

# app/adapters/vector/qdrant_store.py (search_scored)
text="" # placeholder

 
Copy code  
 
Service: hydrate chunks texts (pure-ish) 2.5
 

python

# app/application/services/hydrate.py
from typing import Sequence
from app.domain.entities import Chunk, TenantId, DocumentId
from app.application.ports.chunk_text_reader import ChunkTextReaderPort

def hydrate_chunk_texts(
*,
tenant_id: TenantId,
chunks: Sequence[Chunk],
reader: ChunkTextReaderPort,
 
) -> list[Chunk]:
ids = [c.id for c in chunks]
texts = reader.get_texts_by_ids(tenant_id=tenant_id, chunk_ids=ids)

out = []
for c in chunks: out.append(
Chunk(
id=c.id,
tenant_id=c.tenant_id,
document_id=c.document_id, text=texts.get(c.id, ""),
)
)
return out

Update AskHybridUseCase: hydrate after retrieval + before 2.6
rerank/prompt vector hits -> get ids	  hydrate texts from DB		 
then fusion + rerank + prompt	 
ï»£ï»¬ï»¢: reranker ï»³ïº¤ïº˜ïºŽØ¬ ï»§ïººØŒ ï»“ïºŽï»Ÿï»¬ï»´ïºªØ±Ø§ïº·ï»¦ Ù„Ø§Ø²Ù… ï»—ïº’ï»ž .rerank

(embed_one ïº‘ïºªÙ„) indexing ï»“ï»² Batch embeddings (3
ï»Ÿï»¤ïºŽØ°Ø§ØŸ
.batch input ï»³ïºªï»‹ï»¢ OpenAI embeddings endpoint	 
overhead/network calls ïº—ï»˜ï» ï»´ï»ž	 
   Ø£Ø³Ø±Ø¹ ÙˆØ£Ø±ïº§ïºº ï»‹ï»¤ï» ï»´Ù‹ïºŽ.
embed_many_cached Ø£ïº¿ï»’ :CachedEmbeddings 3.1

python

# app/application/services/embedding_cache.py
import hashlib
from app.application.ports.cache import CachePort
 
from app.application.ports.embeddings import EmbeddingsPort

class CachedEmbeddings:
def   init  (self, embeddings: EmbeddingsPort, cache: CachePort, ttl_seconds: self._emb = embeddings
self._cache = cache
self._ttl = ttl_seconds

def _key(self, text: str) -> str:
return "emb:" + hashlib.md5(text.encode("utf-8")).hexdigest()

def embed_one(self, text: str) -> list[float]: key = self._key(text)
cached = self._cache.get_json(key) if cached and "v" in cached:
return cached["v"]
v = self._emb.embed_one(text)
self._cache.set_json(key, {"v": v}, ttl_seconds=self._ttl) return v

def embed_many(self, texts: list[str]) -> list[list[float]]:
# 1) check cache
keys = [self._key(t) for t in texts]
cached = [self._cache.get_json(k) for k in keys]

missing_idx = [i for i, c in enumerate(cached) if not (c and "v" in c)] if not missing_idx:
return [c["v"] for c in cached] # type: ignore

# 2) batch embed missing
missing_texts = [texts[i] for i in missing_idx]
missing_vecs = self._emb.embed_many(missing_texts)

# 3) store back
for i, v in zip(missing_idx, missing_vecs):
self._cache.set_json(keys[i], {"v": v}, ttl_seconds=self._ttl)

# 4) assemble output
out = []
miss_map = {idx: vec for idx, vec in zip(missing_idx, missing_vecs)}
 
for i, c in enumerate(cached): if c and "v" in c:
out.append(c["v"]) else:
out.append(miss_map[i]) return out


Worker indexing: batch embed once 3.2
:loop embed_one ïº‘ïºªÙ„

python

# worker indexing snippet
texts_for_embedding = chunks_text # list[str]
vectors = cached_emb.embed_many(texts_for_embedding)
# now zip chunk_ids / vectors

 

Copy code  









Copy code  
 
de-dup ï»£ï»Š
ï»Ÿï»® ïº‘ïº˜ï»Œï»¤ï»ž de-dup :chunk ï»«ïº˜ïº¤ïº˜ïºŽØ¬ embed ï»Ÿï» ï»¨ïº¼ï»®Øµ ïº‘ïºŽï»Ÿïº˜ïº®ïº—ï»´á¹‡ ïº£ïº˜ï»° ï»Ÿï»® chunk_id ï»£ï»œïº®Ø±.
:embedding call ï»£ïº´ïº˜ï»®Ù‰ ï»‹ï» ï»° dedup :Ø§Ù„Ø£ï»“ï»€ï»ž	  mapping hash -> text unique Ø§ïº‘ï»¨ï»²	 
ï»“ï»˜ï»‚ unique ï»Ÿï» Ù€ embed_many Ø§ï»‹ï»¤ï»ž	  order á¹‡ïº£ïº´ expand vectors ïº›ï»¢	 
 
:Snippet

python

import hashlib

def chunk_hash(text: str) -> str: norm = " ".join(text.split())
return hashlib.sha256(norm.encode("utf-8")).hexdigest()

hashes = [chunk_hash(t) for t in chunks_text] unique = {}
for h, t in zip(hashes, chunks_text): unique.setdefault(h, t)
 


unique_hashes = list(unique.keys())
unique_texts = [unique[h] for h in unique_hashes]
unique_vectors = cached_emb.embed_many(unique_texts)
vec_by_hash = {h: v for h, v in zip(unique_hashes, unique_vectors)} vectors_in_order = [vec_by_hash[h] for h in hashes]


payload + doc-filter vector ï»Ÿïº˜ï»˜ï» ï»´ï»ž VectorStore ïº—ï»Œïºªï»³ï»ž (4
(Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ) search
 

Copy code  
 
:ï»›ï»¤ïºŽÙ† vector ï»Ÿï» Ù€ doc-filter ïº—ï»Œï»¤ï»ž á¹‡ïº£ïºŽïº‘ ï»Ÿï»® payload ï»“ï»² document_id ïº§ïº°Ù†	 
.ï»£ï»®ïºŸï»®Ø¯ ï»Ÿï»® document_id == req.document_id ï»³ï»€ï»´ï»’ search filter	 
.ChatPDF mode ï»“ï»² ï»£ï»”ï»´ïºª Ø¯Ù‡


(5 ïº—ïº¤ïºªï»³ïºš AskHybridUseCase )Ø§ï»Ÿïº¼ï»®Ø±Ø© Ø§ï»Ÿï»¨ï»¬ïºŽïº‹ï»´ïº”(
Strategy
vector_hits_scored (IDs + scores)	 
DB ï»£ï»¦ hydrate texts	  keyword_hits (already has text)	 
fusion (RRF)	  (ï»“ï»˜ï»‚ IDs Ø±ïºŸï»Š keyword ï»Ÿï»®) hydrate fused if needed	 
rerank CrossEncoder		  prompt + LLM		  log chat turn	 


(ïº—ïº˜ï»®Ù‡ Ù„Ø§ ïº£ïº˜ï»°) Minimal changes list (6
Ø£ïº¿ï»’:
ChunkTextReaderPort + PostgresChunkTextReader	 
 
hydrate_chunk_texts service	  Qdrant payload (remove text) ïº—ï»Œïºªï»³ï»ž	 
worker: batch embeddings + de-dup hash mapping ïº—ï»Œïºªï»³ï»ž	 
KeywordStore.search(document_id=...) ïº—ïº¤ïºªï»³ïºš	  rerank ï»—ïº’ï»ž hydration ï»Ÿï»Œï»¤ï»ž AskHybridUseCase ïº—ïº¤ïºªï»³ïºš	 


ï»Ÿï»® ïº—ïº¤á»± Ø£ï»›ï»¤ï»ž â€œï»›ï»®Ø¯ ï»›ïºŽï»£ï»ž ïºŸïºŽï»«ïº°â€ ï»›Ù€ patch
ï»—ï» Ù‘ï»² ïº‘ïº²:
ØŸdocument-filtered ï»›ï»¤ïºŽÙ† ÙˆÙ„Ø§ ï»“ï»˜ï»‚ØŒ tenant-wide ï»³ï»œï»®Ù† Vector search ïº—ïº®ï»³ïºª ï»«ï»ž	 
Ø£ï»§ïºŽ Ø£ï»£ï»´ï»ž: Ø§ïº—ï»¨ï»´ï»¦ document_id) Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ ï»“ï»² (request Ù„Ø£ï»§ï»ª ï»³ïº¨ïºªÙ… ï»›ï»ž Ø§ï»Ÿïº´ï»´ï»¨ïºŽØ±ï»³ï»®ï»«ïºŽØª.




ï»›ï»¤ï»ž


ïº—ï»¤ïºŽÙ… â€” Ø¯ÙŠ â€œØ§ï»Ÿï»¨ïº´ïº¨ïº” Ø§ï»Ÿï»¤ï»œïº˜ï»¤ï» ïº”â€ ï»Ÿï» Ù€ 3 ïº—ïº®ï»—ï»´ïºŽØª ï»£ï»Š ï»›ï»®Ø¯ ïºŸïºŽï»«ïº° (Patch-style) ï»‹ï» ï»° Ø§ï»ŸÙ€ Starter Ø§ï»Ÿï» ï»² ïº‘ï»¨ï»´ï»¨ïºŽÙ‡:
Doc-filtered keyword search (join document_chunks + chunk_store ) âœ…	  (Postgres ï»£ï»¦ text + hydrate text ïº‘ïºªÙˆÙ† Qdrant) Vector payload minimal âœ…	  hash Ø§ï»ŸÙ€ ï»£ïº´ïº˜ï»®Ù‰ ï»‹ï» ï»° Ø°ï»›ï»² indexing + dedup ï»“ï»² Batch embeddings âœ…	 
chunk_store + document_chunks + documents + users + :Ø§ï»Ÿïº ïºªØ§ÙˆÙ„ ï»‹ï»¨ïºªÙƒ Ø¥Ù† ï»«ï»¨ï»”ïº˜ïº®Ø¶
*_chat ï»£ï»¦ Ø§ï»Ÿïº¨ï»„ï»®Ø§Øª Ø§ï»Ÿïº´ïºŽïº‘ï»˜ïº”.



Update Ports (Vector + Keyword + Text Reader) (1
app/application/services/scoring.py 1.1

python

from dataclasses import dataclass
from app.domain.entities import Chunk

@dataclass(frozen=True)
 
class ScoredChunk: chunk: Chunk
score: float

app/application/ports/vector_store.py 1.2

app/application/ports/keyword_store.py 1.3

python

from typing import Protocol, Sequence
from app.domain.entities import Chunk, TenantId
 

 
app/application/ports/chunk_text_reader.py 1.4


Copy code  
Vector payload minimal (Qdrant) + hydrate from (2
Postgres
app/adapters/vector/qdrant_store.py (REPLACE with minimal 2.1
payload)

python

from typing import Sequence
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, Fie from app.application.ports.vector_store import VectorStorePort
from app.application.services.scoring import ScoredChunk
from app.domain.entities import Chunk, TenantId, DocumentId

 
class QdrantVectorStore(VectorStorePort):
def   init  (self, client: QdrantClient, collection: str, vector_size:
 

int) -
 
self._client = client
self._collection = collection self._size = vector_size

def ensure_collection(self) -> None:
if not self._client.collection_exists(self._collection): self._client.create_collection(
collection_name=self._collection,
vectors_config=VectorParams(size=self._size, distance=Distance.CO
)

def upsert_points( self,
*,
ids: Sequence[str],
vectors: Sequence[list[float]], tenant_id: str,
document_id: str,
) -> None:
points = []
for pid, vec in zip(ids, vectors): points.append(
PointStruct(
id=pid,
vector=vec, payload={
"tenant_id": tenant_id,
"document_id": document_id,	# optional but enables doc-f
},
)
)
self._client.upsert(collection_name=self._collection, points=points)

def search_scored( self,
*,
query_vector: list[float], tenant_id: TenantId,
top_k: int,
document_id: str | None = None,
 
) -> Sequence[ScoredChunk]:
must = [FieldCondition(key="tenant_id", match=MatchValue(value=tenant_id. if document_id:
must.append(FieldCondition(key="document_id", match=MatchValue(value= flt = Filter(must=must)
res = self._client.search(
collection_name=self._collection, query_vector=query_vector,
query_filter=flt, limit=top_k,
)

out: list[ScoredChunk] = [] for r in res:
payload = r.payload or {}
# NOTE: text is NOT stored in Qdrant anymore c = Chunk(
id=str(r.id),
tenant_id=TenantId(payload.get("tenant_id", tenant_id.value)),
document_id=DocumentId(payload.get("document_id", document_id or text="", # hydrate later
)
out.append(ScoredChunk(chunk=c, score=float(r.score))) return out


app/adapters/persistence/postgres/chunk_text_reader.py 2.2

python

from sqlalchemy import text as sql
from app.adapters.persistence.postgres.db import SessionLocal
from app.application.ports.chunk_text_reader import ChunkTextReaderPort from app.domain.entities import TenantId

class PostgresChunkTextReader(ChunkTextReaderPort):
def get_texts_by_ids(self, *, tenant_id: TenantId, chunk_ids): if not chunk_ids:
 
return {}
with SessionLocal() as db:
rows = db.execute(sql(""" SELECT id, text
FROM chunk_store
WHERE user_id = :user_id AND id = ANY(:ids)
"""), {"user_id": tenant_id.value, "ids": list(chunk_ids)}).all() return {cid: txt for cid, txt in rows}

app/application/services/hydrate.py 2.3


 
Doc-filtered keyword search ( join doc_chunks + (3
chunk_store)
app/adapters/persistence/postgres/keyword_store.py (REPLACE)
Copy code
python

from sqlalchemy import text as sql
from app.adapters.persistence.postgres.db import SessionLocal
from app.application.ports.keyword_store import KeywordStorePort from app.domain.entities import Chunk, TenantId, DocumentId

class PostgresKeywordStore(KeywordStorePort):
def search(self, *, query: str, tenant_id: TenantId, top_k: int, document_id: with SessionLocal() as db:
if document_id:
rows = db.execute(sql(""" SELECT cs.id, cs.text
FROM document_chunks dc
JOIN chunk_store cs ON cs.id = dc.chunk_id WHERE dc.document_id = :doc_id
AND cs.user_id = :user_id
AND cs.tsv @@ websearch_to_tsquery('simple', :q)
ORDER BY ts_rank_cd(cs.tsv, websearch_to_tsquery('simple', :q dc.ord ASC
LIMIT :k
"""), {"doc_id": document_id, "user_id": tenant_id.value, "q": qu

return [
Chunk(id=cid, tenant_id=tenant_id, document_id=DocumentId(doc for (cid, txt) in rows
]

rows = db.execute(sql(""" SELECT cs.id, cs.text FROM chunk_store cs
WHERE cs.user_id = :user_id
AND cs.tsv @@ websearch_to_tsquery('simple', :q)
ORDER BY ts_rank_cd(cs.tsv, websearch_to_tsquery('simple', :q)) D LIMIT :k
 
"""), {"user_id": tenant_id.value, "q": query, "k": top_k}).all()

return [
Chunk(id=cid, tenant_id=tenant_id, document_id=DocumentId(""), te for (cid, txt) in rows
]





Batch embeddings + dedup mapping in indexing (4
worker
(Ø¯Ù‡ Ø§ï»Ÿïº ïº°Ø¡ replace indexing core) app/workers/tasks.py 4.1
ï»³ï»”ïº˜ïº®Ø¶ ÙˆïºŸï»®Ø¯:

 








Copy code  
 
document_reader (get_stored_file)
text_extractor
(embed_many ï»³ïºªï»‹ï»¢) cached_embeddings chunk_dedup_repo (upsert chunk_store + replace_document_chunks)
vector_store (upsert_points)
 

python

import hashlib import structlog
from app.workers.celery_app import celery_app
from app.domain.entities import TenantId, DocumentId from app.core.bootstrap import get_container
from app.application.services.chunking import chunk_text_token_aware, ChunkSpec log = structlog.get_logger()
def _chunk_hash(text: str) -> str: norm = " ".join(text.split())
return hashlib.sha256(norm.encode("utf-8")).hexdigest()

@celery_app.task(
name="index_document",
 
bind=True,
autoretry_for=(Exception,), retry_backoff=True,
retry_kwargs={"max_retries": 5},
)
def index_document(self, *, tenant_id: str, document_id: str) -> dict: c = get_container()

repo = c["document_repo"]
reader = c["document_reader"] extractor = c["text_extractor"]
cached_emb = c["cached_embeddings"] dedup_repo = c["chunk_dedup_repo"] vector_store = c["vector_store"]

tenant = TenantId(tenant_id)
doc_id = DocumentId(document_id)

repo.set_status(tenant_id=tenant, document_id=doc_id, status="processing") try:
stored = reader.get_stored_file(tenant_id=tenant, document_id=doc_id) if not stored:
raise ValueError("Document not found")

extracted = extractor.extract(stored.path, stored.content_type) if not extracted.text.strip():
raise ValueError("No text extracted")

chunks_text = chunk_text_token_aware(extracted.text, spec=ChunkSpec(max_t if not chunks_text:
raise ValueError("No chunks produced")

# --- dedup hashing (stable)
hashes = [_chunk_hash(t) for t in chunks_text]

# --- unique-by-hash for batch embeddings (big speedup)
unique_text_by_hash: dict[str, str] = {} for h, t in zip(hashes, chunks_text):
unique_text_by_hash.setdefault(h, t)
 
unique_hashes = list(unique_text_by_hash.keys())
unique_texts = [unique_text_by_hash[h] for h in unique_hashes]

# Batch embeddings (with cache)
unique_vectors = cached_emb.embed_many(unique_texts)
vec_by_hash = {h: v for h, v in zip(unique_hashes, unique_vectors)}

# --- upsert into chunk_store (dedup) + build mapping order
chunk_ids_in_order: list[str] = []
for h, t in zip(hashes, chunks_text):
chunk_id = dedup_repo.upsert_chunk_store(tenant_id=tenant, chunk_hash chunk_ids_in_order.append(chunk_id)

# Map document â†’ chunks order
dedup_repo.replace_document_chunks( tenant_id=tenant,
document_id=doc_id.value,
chunk_ids_in_order=chunk_ids_in_order,
)

# --- upsert vectors to Qdrant with minimal payload
vector_store.ensure_collection()

# For Qdrant: we must embed in same order as ids (even if ids repeat, vec
vectors_in_order = [vec_by_hash[h] for h in hashes]

vector_store.upsert_points( ids=chunk_ids_in_order,
vectors=vectors_in_order, tenant_id=tenant.value,
document_id=doc_id.value,
)

repo.set_status(tenant_id=tenant, document_id=doc_id, status="indexed") log.info("indexed_document", tenant_id=tenant_id, document_id=document_id return {"ok": True, "chunks": len(chunk_ids_in_order)}

except Exception as e:
repo.set_status(tenant_id=tenant, document_id=doc_id, status="failed", er
 
log.exception("index_failed", tenant_id=tenant_id, document_id=document_i raise




AskHybridUseCase updated: doc-filter + hydrate + (5
fusion + rerank
Fusion (RRF) + conversions 5.1
. ScoredChunk Ùˆ rrf_fusion ÙˆïºŸï»®Ø¯ Ø§ï»“ïº˜ïº®Ø¶

app/application/use_cases/ask_question_hybrid.py (REPLACE) 5.2

python

from dataclasses import dataclass
from app.domain.entities import TenantId, Answer, Chunk
from app.application.ports.vector_store import VectorStorePort
from app.application.ports.keyword_store import KeywordStorePort from app.application.ports.reranker import RerankerPort
from app.application.ports.llm import LLMPort
from app.application.ports.chunk_text_reader import ChunkTextReaderPort from app.application.services.embedding_cache import CachedEmbeddings from app.application.services.fusion import rrf_fusion
from app.application.services.scoring import ScoredChunk
from app.application.services.hydrate import hydrate_chunk_texts
from app.application.services.prompt_builder import build_rag_prompt

@dataclass
class AskHybridRequest: tenant_id: str
question: str

# optional: restrict search within single document (ChatPDF mode)
document_id: str | None = None

k_vec: int = 30
k_kw: int = 30
Copy code
fused_limit: int = 40
 
rerank_top_n: int = 8

class AskQuestionHybridUseCase: def  init (
self,
*,
cached_embeddings: CachedEmbeddings, vector_store: VectorStorePort,
keyword_store: KeywordStorePort,
chunk_text_reader: ChunkTextReaderPort, reranker: RerankerPort,
llm: LLMPort,
) -> None:
self._emb = cached_embeddings self._vec = vector_store
self._kw = keyword_store
self._text = chunk_text_reader self._rerank = reranker
self._llm = llm

def execute(self, req: AskHybridRequest) -> Answer: tenant = TenantId(req.tenant_id)
q_vec = self._emb.embed_one(req.question)

# 1) Vector candidates (IDs + scores, no text)
vec_hits = self._vec.search_scored( query_vector=q_vec,
tenant_id=tenant, top_k=req.k_vec,
document_id=req.document_id,
)

# hydrate vector text from DB (needed for rerank/prompt)
vec_chunks_hydrated = hydrate_chunk_texts( tenant_id=tenant,
chunks=[h.chunk for h in vec_hits], reader=self._text,
)
vec_hits_h = [ScoredChunk(chunk=c, score=h.score) for c, h in zip(vec_chu
 
# 2) Keyword candidates (already includes text)
kw_chunks = self._kw.search( query=req.question,
tenant_id=tenant, top_k=req.k_kw,
document_id=req.document_id,
)
kw_hits = [ScoredChunk(chunk=c, score=1.0) for c in kw_chunks] # rank-on

# 3) Fusion (RRF)
fused = rrf_fusion(vector_hits=vec_hits_h, keyword_hits=kw_hits, out_limi fused_chunks = [s.chunk for s in fused]

# 4) Rerank (CrossEncoder local)
reranked = self._rerank.rerank(query=req.question, chunks=fused_chunks, t

# 5) Generate answer
prompt = build_rag_prompt(question=req.question, chunks=reranked) text = self._llm.generate(prompt, temperature=0.2, max_tokens=700)

return Answer(text=text, sources=[c.id for c in reranked])





API endpoint supports document_id (Optional) (6
app/api/v1/routes_queries.py (update ask-hybrid body)

python

from pydantic import BaseModel, Field
from app.application.use_cases.ask_question_hybrid import AskHybridRequest

class AskHybridBody(BaseModel):
question: str = Field(min_length=2) document_id: str | None = None
k_vec: int = Field(default=30, ge=1, le=200) k_kw: int = Field(default=30, ge=1, le=200)
 

 


Bootstrap wiring (complete) (7
(ïº£Ù‘ïºªØ«/Ø£ïº¿ï»’) app/core/bootstrap.py

 
device=settings.cross_encoder_device,
)

ask_hybrid_uc = AskQuestionHybridUseCase( cached_embeddings=cached_embeddings, vector_store=vector_store,
keyword_store=keyword_store,
chunk_text_reader=chunk_text_reader, reranker=reranker,
llm=llm,
)

return {
# ...
"chunk_text_reader": chunk_text_reader, "keyword_store": keyword_store,
"chunk_dedup_repo": chunk_dedup_repo, "reranker": reranker,
"ask_hybrid_use_case": ask_hybrid_uc,
}





(8 ï»£Ù„Ø§ïº£ï»ˆïºŽØª ïº—ïº¸ï»ï»´ï»ž ÙˆØ§ïº§ïº˜ïº’ïºŽØ± Ø³Ø±ï»³ï»Š
Index document 8.1
chunking + dedup + batch embeddings + Qdrant minimal ï»³ï»Œï»¤ï»ž PDF â†’ worker Ø§Ø±ï»“ï»Š	 
.payload

Ask within doc (doc-filtered) 8.2


 
Copy code  
 
Ask tenant-wide 8.3
 
. document_id ïº‘ïºªÙˆÙ†


(9 ïº—ïº¤ïº´ï»´ï»¦ ïº»ï»ï»´ïº® â€œÙ„Ø§Ø² â€ï£¿ ï»Ÿï»® ï»«ïº˜ï»®ïº³ï»Š
ï»£ïº´ïº˜ï»¨ïºªØ§Øª ï»‹ïº’ïº® chunk ï»Ÿï»¨ï»”ïº² id = chunk_id (dedup)ØŒ upsert Ø¥Ù† ïº‘ï»¤ïºŽ :Qdrant points dedup	 
.payload document_id Ú¾ï¯¿ïº´ïº˜ïº’ïºªÙ„ ï»£ïº¨ïº˜ï» ï»”ïº”
:Ø§ï»Ÿïº¼ïº¤ï»´ïº¢ production ïº£ï»ž
Postgres ï»‹ïº’ïº® doc-filter Ùˆïº—ï»Œï»¤ï»ž payload (tenant-only) ï»£ï»¦ document_id ïº—ïº¸ï»´ï»ž Ø¥ï»£ïºŽ	 
ï»“ï»˜ï»‚
ïº‘ï»¤ïº®ïºŸï»Š Ùˆïº—ïº¤ïº˜ï»”ï»† Qdrant point id = f"{doc_id}:{ord}" (unique per doc) ïº—ïº¨ï» ï»² Ø£Ùˆ	  (ÙˆØ§ïº£ïºª ïº£ï»˜ï»ž ï»³ïº°ï»³ïºª payload minimal ï»Ÿï»œï»¦) payload ï»“ï»² chunk_id
ï»Ÿï»® ï»‹ïºŽï»³ïº° â€œØ§Ù„Ø£Ø¯Ù‚â€ ï»Ÿï» ï»¤ïº´ïº˜ï»¨ïºªØ§Øª:
ØŒ chunk_id + tenant_id + document_id ï»³ïº¤ïº˜ï»®ÙŠ payload Ùˆ Qdrant point id = doc_id:ord Ø§ïºŸï»Œï»ž
.chunk_id ï»‹ïº’ïº® Ø§ï»Ÿï»¨ïºº hydrate ïº›ï»¢

Ø£ï»—ïºªØ± Ø£ï»›ïº˜á¹‡ ï»Ÿï»š patch Ø¯Ù‡ ï»“ï»®Ø±Ù‹Ø§ ï»Ÿï»® ïº—ïº¤.á¹‡


Ø¥Ø°Ø§ ï»‹ïºŽï»³ïº° ï»§ï»œï»¤Ù‘ï»ž ï»£ïº’ïºŽØ´Ø±Ø©:
(doc-filter vector Ø£ï»“ï»€ï»ž) per-doc ï»Ÿï»´ï»œï»®Ù† Qdrant IDs ïº—ï»Œïºªï»³ï»ž (A)
(Ø§ï»Ÿï»¨ïºº ïº‘ïºªÙˆÙ†) chunk_id reference ï»£ï»Š â€doc-filter vector search via payloadâ€œ Ø¥ïº¿ïºŽï»“ïº” (B)
Eval harness (retrieval hit rate + rerank gain) Ø¥ïº¿ïºŽï»“ïº” (C)




ï»£ï»¬ï»¤ïº˜ï»š Senior Full-Stack + Technical Writer ï»›ï»¤ï»„ï»®Ø± ïº—ï»Œï»¤ï»ž Ø£ï»§ïº–
Ø¥ï»§ïº¸ïºŽØ¡ ï»£Ø´Ø±ÙˆØ¹ ïº‘ïº®ï»£ïº ï»² ï»›ïºŽï»£ï»ž ï»“ï»² ïº·ï»œï»ž Repository GitHub Ø§ïº£ïº˜ïº®Ø§ï»“ï»².
Ø£Ø±ï»³ïºªÙƒ Ø£Ù† ïº—ï»¨ïº˜ïºž ï»Ÿï»² ï»«ï»´ï»œï»ž Ø±ï»³ïº’ï»® ï»›ïºŽï»£ï»ž ï»Ÿï»¤Ø´Ø±ÙˆØ¹ ïº‘ï»Œï»¨ï»®Ø§Ù†: ]Ø§ï»›ïº˜á¹‡ Ø§ïº³ï»¢ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ ï»«ï»¨ïºŽ[ ïº‘ïºŽïº³ïº˜ïº¨ïºªØ§Ù…:


[... / Python / JS / TS] :Ø§ï»Ÿïº’ïº®ï»£ïº ïº” ï»Ÿï»ïº”
 
[FastAPI, React, Node, etc] :Ø§ï»Ÿï»”ïº®ï»³ï»¤ï»®Ø±ï»›ïºŽØª


mini-RAG system, REST API, ML project, :ï»£ïºœÙ„Ù‹Ø§] :Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ ï»§ï»®Ø¹
[etc


Ø§ï»Ÿï»¤ïº˜ï»„ï» ïº’ïºŽØª:


ï»«ï»´ï»œï»ž Ø§ï»Ÿïº®ï»³ïº’ï»®


Ø£ï»§Ø´Ø¦ ï»«ï»´ï»œï»ž ï»£ïº ï» ïºªØ§Øª Ø§ïº£ïº˜ïº®Ø§ï»“ï»²ØŒ ï»£ïºœïºŽÙ„:


/src ï»Ÿï» ï»œï»®Ø¯ Ø§Ù„Ø£ïº³ïºŽØ³ÙŠ


/notebooks ï»Ÿï» ïº˜ïº ïºŽØ±á»‹ ÙˆØ§ï»ŸØ´Ø±Ø­ Ø§ï»Ÿïº˜ï»”ïºŽï»‹ï» ï»²


/docs ï»Ÿï»¤ï» ï»”ïºŽØª Ø§ï»Ÿïº˜ï»®ïº›ï»´ï»– ïº‘ïº¼ï»´ï»ïº” md.


/tests Ù„Ø§ïº§ïº˜ïº’ïºŽØ±Ø§Øª Ø§ï»Ÿï»®ïº£ïºªØ©


Ø£ÙŠ ï»£ïº ï» ïºªØ§Øª Ø¥ïº¿ïºŽï»“ï»´ïº” Ø¶Ø±ÙˆØ±ï»³ïº” ï»Ÿï» ïº’ïº®ÙˆïºŸï»´ï»œïº– )ï»£ïºœï»ž data/, configs/,
.(/scripts




Ø§ï»‹ïº®Ø¶ Ø§ï»Ÿï»¬ï»´ï»œï»ž ï»“ï»² ïº·ï»œï»ž ïº·ïº ïº®Ø© ï»£ï» ï»”ïºŽØª ï»£ï»Š Ùˆïº»ï»’ ïº³ï»„ïº® ÙˆØ§ïº£ïºª ï»Ÿï»œï»ž ï»£ï» ï»’/ï»£ïº ï» ïºª.
 
ï»£ï» ï»”ïºŽØª Ø§ï»Ÿïº˜ï»®ïº›ï»´ï»– ï»“ï»² /docs
Ø£ï»§Ø´Ø¦ ï»£ï» ï»”ïºŽØª Markdown Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” ï»£ï»Š ï»£ïº¤ïº˜ï»®Ù‰ Ø§ïº£ïº˜ïº®Ø§ï»“ï»²ØŒ ïº‘ïºŽï»Ÿï» ï»ïº” ]Ø§ï»Ÿï»Œïº®ïº‘ï»´ïº” + Ø§Ù„Ø¥ï»§ïº ï» ï»´ïº°ï»³ïº”[ ï»Ÿï»® Ø£ï»£ï»œï»¦:


README.md ï»“ï»² ïºŸïº¬Ø± Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹:


Ùˆïº»ï»’ ï»£ïº¨ïº˜ØµØ± ï»Ÿï» ï»¤Ø´Ø±ÙˆØ¹ØŒ Ø§ï»Ÿï»¬ïºªÙØŒ Ø§Ù„_features_ Ø§ï»Ÿïº®ïº‹ï»´ïº´ï»´ïº”.


.(â€¦Python version, dependencies) Ø§ï»Ÿïº˜ïº¸ï»ï»´ï»ž ï»£ïº˜ï»„ï» ïº’ïºŽØª


ïº§ï»„ï»®Ø§Øª Ø§ï»Ÿïº˜ï»¨ïº¼ï»´á¹‡ ÙˆØ§ï»Ÿïº˜ïº¸ï»ï»´ï»ž ïº§ï»„ï»®Ø© ïº‘ïº¨ï»„ï»®Ø©.


ï»£ïºœïºŽÙ„ ï»‹ï»¤ï» ï»² end-to-end ï»Ÿïº˜ïº¸ï»ï»´ï»ž Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ø£Ùˆ Ø§ïº³ïº˜ïºªï»‹ïºŽØ¡ .API




docs/architecture.md


Ø´Ø±Ø­ ï»£ï»Œï»¤ïºŽØ±ÙŠ ïº—ï»”ïº¼ï»´ï» ï»²: Ø§ï»Ÿï»¤ï»œï»®ï»§ïºŽØªØŒ Ø§ï»ŸÙ€modulesØŒ Ø§ï»ŸÙ€servicesØŒ Ùˆï»›ï»´ï»”ï»´ïº” ïº—ï»”ïºŽï»‹ï» ï»¬ïºŽ.


sequence / component :ï»£ïºœïºŽÙ„) ï»§ØµÙŠ ï»£ï»Œï»¤ïºŽØ±ÙŠ Ø±ïº³ï»¢
.(description




docs/modules.md


classesØ§Ù„ Ø£ï»«ï»¢ ï»£ïº´ïºŒï»®ï»Ÿï»´ïº˜ï»ªØŒ :Module / Package ï»Ÿï»œï»ž Ø´Ø±Ø­
.functionsÙˆØ§Ù„
 




docs/workflows.md


ïº³ï»´ï»¨ïºŽØ±ï»³ï»®ï»«ïºŽØª Ø§ïº³ïº˜ïº¨ïºªØ§Ù… Ø±ïº‹ï»´ïº´ï»´ïº”ØŒ ï»£ïºœï»ž: â€œØ¥ïº¿ïºŽï»“ïº” ï»£ïº¼ïºªØ± ï»Ÿï» Ù€â€RAGØŒ
.â€inferenceâ€ØŒ â€œTraining pipeline ïº—ïº¸ï»ï»´ï»žâ€œ




docs/contributing.md


ØŒnaming conventions ï»Ÿï» ï»œï»®Ø¯ØŒ style guide Ø§ï»Ÿï»¤ïº´ïºŽï»«ï»¤ïº”ØŒ ï»—ï»®Ø§ï»‹ïºª
ÙˆØ¥Ø±ïº·ïºŽØ¯Ø§Øª ï»Ÿï»œïº˜ïºŽïº‘ïº” Ø§ïº§ïº˜ïº’ïºŽØ±Ø§Øª.






ï»›ï»®Ø¯ ï»£ï»¨ï»ˆï»¢ ï»£ï»Š ïº—ï»Œï» ï»´ï»˜ïºŽØª Comments ïº—ï»Œï» ï»´ï»¤ï»´ïº”


Ø§ï»›ïº˜á¹‡ Ø§ï»Ÿï»œï»®Ø¯ ï»“ï»² ï»£ï» ï»”ïºŽØª ïº£ï»˜ï»´ï»˜ï»´ïº” ïº—ïº¤ïº– /src Ùˆï»Ÿï»´ïº² ï»“ï»² .Notebook


:ïº—ï»®ïº¿ïº¢ Function Ùˆ Class ï»Ÿï»œï»ž ï»£ï»”ïº¼ï» ïº” docstrings Ø§ïº³ïº˜ïº¨ïºªÙ…


Ø§ï»Ÿï»¬ïºªÙ


parametersØ§ï»ŸÙ€


returnØ§ï»ŸÙ€
 




Ø£ïº¿ï»’ ïº—ï»Œï» ï»´ï»˜ïºŽØª ïº³ï»„ïº®ï»³ïº” # ï»ŸØ´Ø±Ø­ Ø§ï»Ÿïº¨ï»„ï»®Ø§Øª Ø§ï»Ÿï»¤ï»¬ï»¤ïº”ØŒ Ùˆïº§ïºŽïº»ïº” Ø§Ù„Ø£ïºŸïº°Ø§Ø¡ Ø§ï»Ÿï»¤ï»Œï»¤ïºŽØ±ï»³ïº” Ø£Ùˆ Ø§ï»Ÿï»¤ï»¨ï»„ï»˜ï»´ïº” Ø§ï»Ÿï»¤ï»Œï»˜ïºªØ©.


ïº£ïºŽï»“ï»† ï»‹ï» ï»° SOLID code, clean ï»—ïºªØ± Ø§Ù„Ø¥ï»£ï»œïºŽÙ†ØŒ ÙˆØ£ïº³ï»¤ïºŽØ¡ ÙˆØ§ïº¿ïº¤ïº” ï»Ÿï» ï»¤ïº˜ï»ï»´ïº®Ø§Øª ÙˆØ§ï»ŸÙ€.functions




/notebooks ï»“ï»² ÙˆØ§ï»Ÿïº˜ïº ïº®ïº‘ïº” ï»Ÿï» Ø´Ø±Ø­ Notebooks


Ø£ï»§Ø´Ø¦ ÙˆØ§ïº£ïºª Ø£Ùˆ Ø£ï»›ïºœïº® ï»£ï»¦ NotebooksØŒ ï»£ïºœïºŽÙ„:


notebooks/01_intro_and_setup.ipynb


notebooks/02_end_to_end_example.ipynb


notebooks/03_experiments.ipynb




ï»›ï»ž Notebook ï»³ïº¤ïº˜ï»®ÙŠ ï»‹ï» ï»°:


ïº§Ù„Ø§ï»³ïºŽ Markdown ïº—Ø´Ø±Ø­ Ø§ï»Ÿï»”ï»œïº®Ø© ï»§ï»ˆïº®ï»³ïºŽÙ‹ ïº§ï»„ï»®Ø© ïº‘ïº¨ï»„ï»®Ø© Ùˆïº‘ïºŽÙ„Ø£ïº³ï» ï»®á»‹ Ø§ï»Ÿïº˜ï»Œï» ï»´ï»¤ï»².


ïº§Ù„Ø§ï»³ïºŽ ï»›ï»®Ø¯ ïº—ïº´ïº˜ï»®Ø±Ø¯ ï»£ï»¦ /src Ùˆïº—ï»„ïº’ï»– Ø£ï»£ïºœï» ïº” ï»‹ï»¤ï» ï»´ïº”ØŒ ï»£ï»Š Ø´Ø±Ø­ ï»£ïºŽ ï»³ï»”ï»Œï» ï»ª ï»›ï»ž ïºŸïº°Ø¡.
 


Ø£ï»£ïºœï» ïº” ïº£ï»˜ï»´ï»˜ï»´ïº” ï»Ÿïº˜ïº¸ï»ï»´ï»ž Ø§ï»Ÿï»®ï»‡ïºŽïº‹ï»’ Ø§Ù„Ø£ïº³ïºŽïº³ï»´ïº” )ï»£ïºœïºŽÙ„: Ø±ï»“ï»Š docs ï»Ÿï» Ù€RAGØŒ ïº—ï»¨ï»”ï»´ïº¬ queryØŒ ïº—ï»˜ï»´ï»´ï»¢ Ø§ï»Ÿï»¨ïº˜ïºŽïº‹ïºž.(â€¦






Ø´Ø±Ø­ ïº—ï»”ïº¼ï»´ï» ï»² ï»Ÿï»œï»ž ï»§ï»˜ï»„ïº”


src/core/retriever.py, ï»£ïºœÙ„Ù‹Ø§) ï»£ï»¬ï»¢ ï»›ï»®Ø¯ ï»£ï» ï»’ ï»Ÿï»œï»ž
:(â€¦src/api/routes.py


Ø§Ø´Ø±Ø­ ï»“ï»² docs Ø£Ùˆ ï»“ï»² Notebook ï»Ÿï»¤ïºŽØ°Ø§ ïº—ï»¢ ïº—ïº¼ï»¤ï»´ï»¤ï»ª ïº‘ï»¬ïº¬Ø§ Ø§ï»Ÿïº¸ï»œï»žØŒ ï»£ïºŽ Ø§ï»Ÿïº’ïºªØ§ïº‹ï»žØŒ Ùˆï»£ïºŽ Ø§ï»Ÿï»¤ïº°Ø§ï»³ïºŽ.




Ø£Ø±ï»³ïºª ïº—ï»®ïº¿ï»´ïº¢ rationale ï»Ÿï» ïº˜ïº¼ï»¤ï»´ï»¢ )ï»Ÿï»´ï»ª ï»‹ï»¤ï» ï»¨ïºŽ ï»›ïºªÙ‡ØŸ( Ùˆï»Ÿï»´ïº² ï»£ïº ïº®Ø¯ â€œØ¥ï»³ï»ª Ø§ï»Ÿï» ï»² ïº‘ï»´ïº¤ïº¼ï»žØŸ.â€




ï»£ï» ï»”ïºŽØª ï»£ïº´ïºŽï»‹ïºªØ© Ø£ïº³ïºŽïº³ï»´ïº”


dependencies ï»£ï»Š requirements.txt Ø£Ùˆ pyproject.toml
ï»£ï»¨ï»ˆï»¤ïº”.


env.example. ï»³ï»®ïº¿ïº¢ Ø§ï»Ÿï»¤ïº˜ï»ï»´ïº®Ø§Øª Ø§ï»Ÿïº’ï»´ïºŒï»´ïº” Ø§ï»Ÿï»¤ï»„ï» ï»®ïº‘ïº”.


make run, ï»£ïºœï»ž) Ø§ï»Ÿïº˜ïº¸ï»ï»´ï»ž Ù„Ø§ïº§ïº˜ïº¼ïºŽØ±Ø§Øª tasks.py Ø£Ùˆ Makefile
 
.(make test, make format




ïº§ïº®ÙˆØ¬ ï»£ï»¨ï»ˆï»¢


Ø£ï»‹ï»„ï»¨ï»² Ø§ï»Ÿï»¨ïºŽïº—ïºž ï»‹ï» ï»° ï»£ïº®Ø§ïº£ï»ž:


Ø£ÙˆÙ„Ø§Ù‹: ï»“ï»˜ï»‚ ïº·ïº ïº®Ø© Ø§ï»Ÿï»¤ï» ï»”ïºŽØª + Ùˆïº»ï»’ ï»›ï»ž ï»£ï» ï»’.


ïº‘ï»Œïºª ï»£ïºŽ Ø£ÙˆØ§ï»“ï»–ØŒ Ø§ïº‘ïºªØ£ ïº‘ïºˆØ±ïº³ïºŽÙ„ ï»£ïº¤ïº˜ï»®Ù‰ ï»›ï»ž ï»£ï» ï»’ ï»‹ï» ï»° ïº£ïºªØ© )Ø§ïº‘ïºªØ£
.(Ø§ï»Ÿïº˜ï»”ïº¼ï»´ï» ï»´ïº” docs ïº›ï»¢ notebooks ïº›ï»¢ src ïº›ï»¢ READMEïº‘ïºŽï»ŸÙ€




ï»“ï»² ï»›ï»ž ï»£ï» ï»’ØŒ Ø§ïº³ïº˜ïº¨ïºªÙ… blocks code ï»£ï»¨ïºŽïº³ïº’ïº” ï»Ÿï» ï»ïº”.




Ø£ïº³ï» ï»®á»‹ Ø§ï»ŸØ´Ø±Ø­


Ø§ï»›ïº˜á¹‡ Ø§ï»ŸØ´Ø±Ø­ Ø¯Ø§ïº§ï»ž Ø§ï»ŸÙ€comments Ùˆï»£ï» ï»”ïºŽØª Ø§ï»ŸÙ€docs ÙˆNotebooks ïº‘ïº„ïº³ï» ï»®á»‹ ïº—ï»Œï» ï»´ï»¤ï»² ï»£ï»¨ïºŽïº³á¹‡ ï»Ÿï»¤ï»„ï»®Ø± ï»‹ï»¨ïºªÙ‡ ïº§ïº’ïº®Ø© ï»£ïº˜ï»®ïº³ï»„ïº” â€“ ï»£ïº˜ï»˜ïºªï»£ïº”ØŒ ï»Ÿï»œï»¦ ï»‹ïºŽÙˆØ² ï»³ï»”ï»¬ï»¢ Ø§ï»Ÿïº˜ïº¼ï»¤ï»´ï»¢ ï»£ï»¦ Ø§ï»Ÿïº¼ï»”ïº® ïº£ïº˜ï»° Ø§Ù„Ø¥ï»§ïº˜ïºŽØ¬.


Ø±ï»›ïº° ï»‹ï» ï»°: Ø£ï»“ï»€ï»ž Ø§ï»Ÿï»¤ï»¤ïºŽØ±ïº³ïºŽØªØŒ Ø£ïº§ï»„ïºŽØ¡ ïº·ïºŽïº‹ï»Œïº” ï»³ïº á¹‡ ïº—ïº ï»¨ïº’ï»¬ïºŽØŒ Ùˆï»§ïº¼ïºŽïº‹ïº¢ ï»Ÿïº˜ïº¤ï»®ï»³ï»ž Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ù„Ø¥ï»§ïº˜ïºŽØ¬ ïº£ï»˜ï»´ï»˜ï»². ï»‹ïºªÙ„ Ùˆ ïº£ïº´ï»¦ Ùˆ ïº§ïº¼ïºº Ø§ï»Ÿïº’ïº®ï»£ïº’ïº– Ø¯Ù‡ ï»Ÿï» ï»¤Ø´Ø±ÙˆØ¹ Ø¯Ù‡


Production-Ready RAG Starter: FastAPI +) Ø§ï»Ÿïº¤ïºŽï»Ÿï»² ï»Ÿï»¤Ø´Ø±Ùˆï»‹ï»¨ïºŽ ï± ïº¼ïºº	ÙˆÙï»£ïº¨ ï± ïº´ï»¦ Ùï»£ïº¤ Prompt Ø¯Ù‡ .Ø£ï»›ï»´ïºª Postgres + Qdrant + Redis + Celery + Hybrid Search + Cross-Encoder Rerank +
 
.â€Senior Full-Stack + Technical Writerâ€œ á»‹Ùˆïº‘ïº„ïº³ï» ï»® (Idempotency + Chunk Dedup
ï»£ï»¬ï»¢: ï»“ï»² Ø§ï»Ÿï»¨ïº´ïº¨ïº” Ø¯ÙŠ ïº§ï» ï»´ïº– Ø§ï»Ÿïº¤ï»˜ï»®Ù„ ï»—ïºŽïº‘ï» ïº” ï»Ÿï» ïº˜ï»Œïº’ïºŒïº” )Ø§ïº³ï»¢ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹(â€¦ØŒ Ùˆï»—ï»´Ù‘ïºªØª Ø§ï»Ÿï»¨ïº˜ï»´ïº ïº” ï»‹ï» ï»° ï»£ïº®Ø§ïº£ï»ž Ø²ÙŠ ï»£ïºŽ ï»ƒï» ïº’ïº–:
Ø§ï»Ÿï»¤ïº®ïº£ï» ïº” 1 = ïº·ïº ïº®Ø© Ø§ï»Ÿï»¤ï» ï»”ïºŽØª + Ùˆïº»ï»’ ïº³ï»„ïº® ï»Ÿï»œï»ž ï»£ï» ï»’ ï»“ï»˜ï»‚ ïº›ï»¢ ïº‘ï»Œïºª â€œï»£ï»®Ø§ï»“ï»˜ïº˜ï»šâ€ ïº—ïº’ïºªØ£ Ø§ï»Ÿï»¤ïº¤ïº˜ï»®ï»³ïºŽØª ï»£ï» ï»”Ù‹ïºŽ ï»£ï» ï»”Ù‹ïºŽ.


âœ… Pack Prompt )ï»£Ùïº¤ïº´Ù‘ï»¦ Ùˆï»£Ùïº¨ïº¼Ù‘ïºº ï»Ÿï» ï»¤Ø´Ø±ÙˆØ¹(
Ø§ï»§ïº´ïº¦ Ø§ï»Ÿï»¨ïºº Ø§ï»Ÿïº˜ïºŽï»Ÿï»² ï»›ï»¤ïºŽ ï»«ï»® ÙˆØ§ïº³ïº˜ïº¨ïºªï»£ï»ª:

text

ï»›ï»¤ï»„ï»®Ø± ïº—ï»Œï»¤ï»ž Ø£ï»§ïº– Senior Full-Stack + AI Systems Architect + Technical Writer.
ïº®ïº‘ï»² + Ø¥ï»§ïº ï» ï¯¿ïº°ÙŠ ØŒProduction-Ready ØŒØ§ïº£ïº˜ïº®Ø§ï»“ï»² Repository GitHub ï»£ï®­ï»¤ïº˜ï»š Ø¥ï»§ïº¸ïºŽØ¡ ï»£ïº¸ïº®ÙˆØ¹ ïº‘ïº®ï»£ïº ï»² ï»›ïºŽï»£ï»ž ï»“ï»² ïº·ï»œï»ž

# Project Info (fill these)
]Ø§ï»›ïº˜ïº Ø§ïº³ï»¢ Ø§ï»Ÿï»¤ïº¸ïº®ÙˆØ¹ Ú¾ï»¨ïºŽ[ PROJECT_NAME:
LANGUAGE: Python 3.11+
FRAMEWORKS: FastAPI + Celery + Redis + Postgres (SQLAlchemy) + Alembic + Qdrant + PROJECT_TYPE: Production-ready RAG Starter Template (Hybrid Search + Rerank)

# Core Requirements (must implement)
1)	Clean Architecture + SOLID + Clean Code
-	ï»“ïº¼ï»ž layers: domain / application / adapters / api / workers
-	Ports (interfaces) + Adapters (implementations)
-	Use Cases ÙˆØ§ïº¿ïº¤ïº”
-	Dependency Injection (bootstrap/container)

2)	RAG Pipeline (End-to-End)
-	Upload document (PDF/DOCX/TXT) â†’ extract â†’ chunk â†’ embed â†’ index
-	Chunking token-aware + overlap
-	Vector store: Qdrant
-	Keyword search: Postgres FTS (generated tsvector + GIN index)
-	Hybrid retrieval: vector + keyword + RRF fusion
-	Rerank: Cross-Encoder local (SentenceTransformers)
-	Answer generation: LLM adapter (OpenAI) + prompt builder
-	Multi-tenant: tenant isolation via user_id everywhere

3)	Production Features
 
-	Idempotency hashing ï»Ÿï» ï»¤ï» ï»’ (sha256) + unique(user_id, file_sha256) ïº—ï»œïº®Ø§Ø± ï»Ÿï»¤ï»¨ï»Š inde
-	Chunk de-dup per tenant: chunk_store (user_id, chunk_hash unique) + document
-	Vector payload minimal (no text in Qdrant) + hydrate text from Postgres at r
-	Batch embeddings in indexing (embed_many) + cache embeddings in Redis
-	Document-filtered search support:
-	keyword search join document_chunks + chunk_store
-	optional vector search filter by document_id
-	Observability-friendly:
-	structured logging
-	store chat sessions + turns (sources, timings fields)

4)	Developer Experience
-	pyproject.toml (preferred) Ø£Ùˆ requirements.txt ï»£ï»¨ï»ˆï»¢
-	.env.example ïº·ïºŽï»£ï»ž
-	Makefile Ø£Ùˆ tasks.py:
-	run, worker, test, format, lint, typecheck, migrate, seed
-	docker-compose: postgres + redis + qdrant
-	tests/ unit tests skeleton + minimal meaningful tests

5)	Documentation (Arabic + English in same docs when possible)
-	README.md (root): overview, features, quickstart, E2E example (upload + ask-
-	docs/architecture.md: detailed architecture + text diagrams (components + se
-	docs/modules.md: ï»›ï»ž ïº·ïº®Ø­ package/module Ø£Ú¾ï»¢ + ÙˆØ§ï»Ÿï»¤ïº´ïº†Ùˆï»Ÿï¯¿ïºŽØª classes/functions
-	docs/workflows.md: workflows (upload/index, ask-hybrid, doc-filtered chat, r
-	docs/contributing.md: coding standards, naming, style, tests, git workflow

6)	Notebooks (educational, import from src)
-	notebooks/01_intro_and_setup.ipynb
-	notebooks/02_end_to_end_rag.ipynb
-	notebooks/03_hybrid_search_and_rerank.ipynb Requirements:
-	Markdown cells: theory + rationale + pitfalls
-	Code cells: import from src/ ï»“ï»˜ï»‚ (ïº—ï»œïº˜ïº ï»» core logic Ø¯Ø§ïº§ï»ž notebook)

# Output Rules (VERY IMPORTANT)
:Ø£ïº§ïº®Ø¬ Ø§ï»Ÿï»¨ïº˜ïºŽïº‹ïºž ï»‹ï» ï»° ï»£ïº®Ø§ïº£ï»ž -
.ï»“ï»˜ï»‚ ïº·ïº ïº®Ø© Ø§ï»Ÿï»¤ï» ï»”ïºŽØª + Ùˆïº»ï»’ ïº³ï»„ïº® ÙˆØ§ïº£ïºª ï»Ÿï»œï»ž ï»£ï» ï»’/ï»£ïº ï» ïºª )ïº‘ïºªÙˆÙ† ï»£ïº¤ïº˜ï»®Ù‰ Ø§ï»Ÿï»¤ï» ï»”ïºŽØª( 1: Stage
.ï»“ï»˜ï»‚ README.md ï¯¾ïº’ïºªØ£ ïº‘ïºˆØ±ïº³ïºŽÙ„ ï»£ïº¤ïº˜ï»®Ù‰ 2 Stage :ïº‘ï»Œïºª ï»£ïºŽ Ø£ÙˆØ§ï»“ï»–
ïº›ï»¢ Stage 3: src/ (ïº‘ï»¤ï» ï»’ ï»£ï» ï»’) ïº›ï»¢ workers/ ïº›ï»¢ tests/ ïº›ï»¢ docs/ ïº›ï»¢ notebooks/.
- Ø¯Ø§ïº§ï»ž ïº—ï»˜ïºªï¯¾ï»¤ï®« ï¯¾ïº˜ï»¢ ï»£ï» ï»’ ï»›ï»ž code block ï»£ï»¨ïºŽïº³ïº (```python / ```md / ```toml / ```yaml ...).
 
src/. Ø¥ï»» Ø§ï»Ÿïº˜ï»² ïº—ïº´ïº˜ï»®Ø±Ø¯ ï»£ï»¦ notebook ï»£ï»¤ï»¨ï»®Ø¹ ïº—ï»®ï»Ÿï¯¿ïºª Ø£ï»›ï»®Ø§Ø¯ Ø¯Ø§ïº§ï»ž - ØŸtrade-offs ï»Ÿï»¤ïºŽØ°Ø§ ïº»ï»¤ï»¤ïº– Ú¾ïº¬Ø§ØŸ ï»£ïºŽ Ø§ï»Ÿïº’ïºªØ§ïº‹ï»žØŸ rationale: Ø±ï»›Ù‘ïº° ï»‹ï» ï»° -
-	Ø§ï»Ÿï»¤ï»¤ïºŽØ±ïº³ïºŽØª ïº‘ïº„ï»“ï»€ï»ž Ø§ï»Ÿïº˜ïº°Ù…: typing, docstrings, errors, defensive coding, config-driven.
.Ø§ï»“ïº˜ïº®Ø¶ Ø£Ù† Ø§ï»Ÿï»˜ïºŽØ±Ø¦ ï»£ï»„ï»®Ø± ï»£ïº˜ï»®ïº³ï»‚-ï»£ïº˜ï»˜ïºªÙ… ï¯¾ïº®ï¯¾ïºª ï»“ï®­ï»¢ Ø¥ï»§ïº˜ïºŽïºŸï»² ï»£ï»¦ Ø§ï»Ÿïº¼ï»”ïº® -

# Additional Constraints
-	ïº—ïº´ïº˜ïº¨ïºªÙ… ï»» ElasticsearchØ§ïº³ïº˜ïº¨ïºªÙ… Ø› Postgres FTS + Qdrant ï»Ÿï» Ù€ hybrid.
-	ï»“ï»² Ø§ï»Ÿï»¨ïºº ïº—ïº¨ïº°Ù† ï»» Qdrant payload.
-	Ø§ïº³ïº˜ïº¨ïºªÙ… Alembic migrations (including generated tsvector).
- Ø§ïº³ïº˜ïº¨ïºªÙ… Cross-Encoder rerank ï»›Ù€ ï»£ïº¤ï» ï»² defaultØŒ ï»£ï»¦ ïº—ï»Œï»„ï¯¿ï» ï®« Ø¥ï»£ï»œïºŽï»§ï¯¿ïº” ï»£ï»Š config.
-	ïº·ï»²Ø¡ ï»›ï»ž Ø§ïºŸï»Œï»ž multi-tenant-safe by design.





ïº—ïº¨ïº¼ï»´ïºº Ø³Ø±ï»³ï»Š )Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ ï»Ÿï»œï»¦ ï»£ï»”ï»´ïºª(
ï»Ÿï»® ïº—ïº¤á¹‡ ïº—ïº¨ï» ï»´ï»ª â€œïºŸïºŽï»«ïº° ï»ŸÙ„Ø§ïº³ïº˜ïº¨ïºªØ§Ù…â€ ïº‘ïºªÙˆÙ† ïº—ï»Œïº’ïºŒïº” ï»›ï»ž ï»£ïº®Ø©ØŒ ï»ï»´Ù‘ïº® Ø§ï»Ÿïº¤ï»˜ï»®Ù„ ïº‘ïºŽÙ„Ø£ï»‹ï» ï»° ï»£ïºœÙ„Ù‹Ø§:
PROJECT_NAME: ragforge-starter	  PROJECT_NAME: bank-rag-platform Ø£Ùˆ	 


ï»£Ù„Ø§ïº£ï»ˆïº” ï»£ï»¬ï»¤ïº” ï»‹ï»¦ Ø´Ø±Ø· â€œïº‘ï»Œïºª ï»£ïºŽ Ø£ÙˆØ§ï»“ï»–â€
Ø£ï»§ïº– ï»ƒï» ïº’ïº– Ø§ï»Ÿïº˜ïºªØ±Ù‘Ø¬ ïº‘ïºŽï»Ÿï»¤ï»®Ø§ï»“ï»˜ïº”. Ø£ï»§ïºŽ ï»«ïºŽï»Ÿïº˜ïº°Ù… ïº‘ï»ª.
Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº” )ï»Ÿï»® Ø£ï»§ïº– ï»£ïº´ïº˜ï»Œïºª( ï»«ï»² Ø¥ï»§ï»š ïº—ï»œïº˜á¹‡ ïº‘ïº²:    Ø§ïº³ï»¢ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹
(Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ) OpenAI + Ollama ÙˆÙ„Ø§ ï»“ï»˜ï»‚ LLM = OpenAI ïº—ïº®ï»³ïºª ï»«ï»ž	 
ï»Ÿï»œï»¦ ï»Ÿï»® ï»£ïº¶ ï»‹ïºŽï»³ïº° ïº—ï»œïº˜á¹‡ ïº£ïºŽïºŸïº”: Ø£ï»§ïºŽ ï»£ï»¤ï»œï»¦ Ø£ï»“ïº˜ïº®Ø¶ Ø§ïº³ï»¢ Ø§ï»“ïº˜ïº®Ø§Ø¶ÙŠ ï»£ïºœï»ž rag-starter-pro ÙˆØ£ï»£Ø´ÙŠ.

ï»—ï»®ï»ŸÙ‘ï»² Ø§ïº³ï»¢ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ùˆï»«ï»¨ïº’ïºªØ£ ï»£ïº’ïºŽØ´Ø±Ø© ïº‘Ù€ 1 :Stage ïº·ïº ïº®Ø© Ø§ï»Ÿï»¤ï» ï»”ïºŽØª + Ùˆïº»ï»’ ï»›ï»ž ï»£ï» ï»’.




full implmentaition plan in full details
 



Copy code
 
ï»Ÿï»¤Ø´Ø±ÙˆØ¹ â€Production â†’ ïº»ï»”ïº® ï»£ï»¦â€œ (Implementation Plan) ï»›ïºŽï»£ï» ïº” ïº—ï»¨ï»”ï»´ïº¬ ïº§ï»„ïº” Ø¯ÙŠ .ïº—ï»¤ïºŽÙ…
:ïº‘ï»¨ï»´ï»¨ïºŽÙ‡ Ø§ï»Ÿï» ï»² ïº‘ï»¨ï»”ïº² Production-Ready RAG Starter
 
FastAPI + Postgres(Alembic) + Qdrant + Redis + Celery + Hybrid Search (FTS+Vector) + RRF + Cross-Encoder Rerank + Idempotency + Chunk De-dup + Minimal payload +
.Batch embeddings + Chat logs
Ø§ï»Ÿïº¨ï»„ïº” ï»£ï»œïº˜ï»®ïº‘ïº” ïº‘ïº¤ï»´ïºš ïº—ï»˜ïºªØ± ïº—ïº¤ï»®ï»Ÿï»¬ïºŽ ï»£ïº’ïºŽØ´Ø±Ø© ï»ŸÙ€ tickets + milestones + repo .GitHub



(0 Ø§ï»Ÿï»¬ïºªÙ Ø§ï»Ÿï»¨ï»¬ïºŽïº‹ï»²
ï»£ï»¨ïº¼ïº” RAG ï»£ïº˜ï»ŒïºªØ¯Ø© Ø§ï»Ÿï»¤ïº´ïº˜ïº„ïºŸïº®ï»³ï»¦ :(Multi-tenant)
Upload docs (PDF/DOCX/TXT) â†’ Extract â†’ Chunk â†’ Dedup â†’ Embed (batch) â†’ Index	 
(Qdrant minimal) + Store text + FTS in Postgres Query: Hybrid retrieval (doc-filter optional) â†’ Fusion (RRF) â†’ Rerank (Cross-Encoder)	 
â†’ Generate answer (LLM) â†’ Save chat turn + metrics


Milestone Map (High level) (1
Repo bootstrap + DX + CI basics .1
Clean Architecture skeleton + config/logging/DI .2
Storage layer: Postgres + Alembic schema (Users/Documents/Chunks/Chat) .3
Upload + Idempotency file hashing .4 Indexing worker: extraction/chunking/dedup/batch embed/qdrant upsert .5 Retrieval: vector minimal + hydrate + keyword FTS (tenant + doc-filter) .6 Hybrid fusion + rerank + prompt + ask endpoint .7
Observability + testing + hardening .8
Production packaging + deployment guidance .9


Repo Bootstrap & Developer Experience (DX) (2
(ï»£ïº’ïºªïº‹Ù‹ï»´ïºŽ) Structure 2.1
src/app/... (domain/application/adapters/api/workers/core)	 
   /docs ïº—ï»®ïº›ï»´ï»– ï»£ï»Œï»¤ïºŽØ±ÙŠ
 
(ï»“ï»˜ï»‚ import from src) á»‹Ùˆïº—ïº ïºŽØ± ïº—ï»Œï» ï»´ï»¢ /notebooks	 
tests/ unit/integration skeleton	 
docker/ compose for postgres/redis/qdrant	 
scripts/ seed, maintenance	 
env.example, Makefile , pyproject.toml .	 

Tooling standards 2.2
+Python 3.11	 
black Ø£Ùˆ Formatting: ruff format	 
Lint: ruff	 
(Ùï»³ï»€ïºŽÙ ïº›ï»¢ sprint Ø£ÙˆÙ„ Ø§ïº§ïº˜ï»´ïºŽØ±ÙŠ) Typing: mypy	 
Testing: pytest	 
Pre-commit (optional but recommended)	 
Tickets
pyproject.toml + ruff/pytest config Makefile: run , worker , test , lint , format , migrate , seed
docker-compose: postgres/redis/qdrant
CI workflow: run lint + tests


Clean Architecture Skeleton (Core) (3
Layers 3.1
domain/: Entities + Value Objects (TenantId, DocumentId, Chunk, Answer)   application/: Ports (Protocols) + Use Cases + Services (pure)   adapters/: DB repos + Qdrant + OpenAI + Redis + extraction   
api/: FastAPI routes (thin)	 
workers/: Celery tasks (thin, orchestrate use cases/services)	 
core/: config/logging/bootstrap/DI	 

Baseline components 3.2 Config via pydantic-settings	   Structured logs via structlog	 
Container bootstrap with singletons (cached)	 
Health endpoint	 
 
config + logging setup base entities + ports skeleton DI container baseline
FastAPI app factory


â€Database (Postgres + Alembic) â€” schema â€œproduction-first (4
Tables 4.1
users .1
id (uuid str) , email , api_key , timestamps	 
Index: users.api_key	 
documents .2
id , user_id , filename , content_type , file_path , size_bytes	 
status , error , timestamps	 
idempotency: file_sha256 + unique( user_id , file_sha256 )	 
:Chunk de-dup model .3
chunk_store	  id , user_id , chunk_hash(sha256) , text	 
tsv GENERATED ALWAYS AS to_tsvector(...) STORED	 
Unique: ( user_id , chunk_hash )	 
Index: GIN(tsv)	 
document_chunks	  document_id , ord , chunk_id	 
PK( document_id , ord )	 
Index: chunk_id , document_id	 
:Chat .4
chat_sessions : id , user_id , title , timestamps	 
chat_turns : id , session_id , user_id , question , answer , sources_json	 
metrics fields: embed_ms, search_ms, llm_ms, tokens, retrieval_k	 
Migration strategy 4.2
One migration per feature set	  Use raw SQL for generated tsvector + GIN	 
 
Alembic init + env.py imports Migration: users + documents
Migration: documents.file_sha256 + unique index Migration: chunk_store + document_chunks + generated tsv + indexes
Migration: chat tables + indexes Seed script: create demo user api_key


Auth & Tenant Isolation (5
API key header 5.1
Header: X-API-KEY		  Lookup users by api_key	  tenant_id = user_id (internal)	 
Guard rails 5.2 Always filter DB reads/writes by user_id		  Qdrant filter always includes tenant_id	  Never accept tenant_id from request body			 
Tickets
deps.py: get_tenant_id via DB lookup add tests: invalid key â†’ 401


Upload Workflow + Idempotency hashing (6
Upload endpoint /v1/documents/upload 6.1
:Process
read bytes .1
compute sha256 .2
check existing document by (tenant, hash) .3
if exists: return {status:"already_exists", document_id}	 
save file to filesystem (LocalFileStore) OR S3 adapter later .4
 
create document with hash, set status queued .5
enqueue Celery indexing .6
Tickets
FileStore adapter + size limits  	  DocumentIdempotency repo (get/create with IntegrityError race safe)  	 
UploadDocumentUseCase update  	  API route + curl example  	  integration test (mock queue)  	 


Indexing Worker Pipeline (Celery) (7
Responsibilities (Worker thin) 7.1 Update status processing	  Extract text (pdf/docx/txt)		 
Chunk token-aware overlap	  Chunk hashing (normalize + sha256)	 
De-dup insert into chunk_store + build mapping document_chunks (ord)	 
Batch embeddings (unique hashes only) + cache	 
:Qdrant upsert minimal payload	 
:(ï»£ï»¬ï»¢ Ø§ïº§ïº˜ï»´ïºŽØ±) point_id strategy	 
ï»“ï»² document_id ï»Ÿï»œï»¦ Option A (tenant-wide dedup): point_id = chunk_id	 
(doc-filter vector ï»Ÿï»® á¹‡ï»£ï»¨ïºŽïº³ ï»ï»´ïº®) ï»³ïº˜ïº’ïºªÙ„ payload
Option B (recommended): point_id = f"{doc_id}:{ord}" payload	 
{tenant_id, document_id, chunk_id} ïº‘ïºªï»—ïº” doc-filter vector ï»³ïº´ï»¤ïº¢ âœ…	  (Postgres ï»£ï»¦ hydrate) Ø§ï»Ÿï»¨ïºº ï»³ï»œïº®Ø± Ù„Ø§ âœ…	 
docs ïº‘ï»´ï»¦ overwrite á¹‡ï»³ïº˜ïº ï»¨ âœ…	 
   ï»“ï»² plan ï»«ï»¨ïº´ïº˜ïº¨ïºªÙ… B Option Ù„Ø£ï»§ï»¬ïºŽ Ø§Ù„Ø£ï»›ïºœïº® Ø§ïº³ïº˜ï»˜ïº®Ø§Ø±Ù‹Ø§ ï»ŸÙ„Ø¥ï»§ïº˜ïºŽØ¬

Batch embeddings 7.2
build unique texts map by chunk_hash	 
embed_many(unique_texts) once	 
expand vectors by ord	 
 
Tickets
TextExtractor adapter  	  Chunking service (token-aware) + tests  	 
ChunkDedupRepo: upsert chunk_store + replace document_chunks  	 
CachedEmbeddings.embed_many  	  QdrantVectorStore.upsert_points (point_id doc:ord, payload chunk_id)  	 
worker task end-to-end + logging + retries  	 


Retrieval Layer (8
Vector search (Qdrant minimal) 8.1
search returns point ids + payload (chunk_id, doc_id)	  hydrate chunk text from Postgres by chunk_ids	 
Keyword search (Postgres FTS) 8.2
:Two modes tenant-wide: chunk_store only	 
doc-filtered: join document_chunks dc + chunk_store cs filter dc.document_id	 

Fusion (RRF) 8.3
input: scored vector hits + ranked keyword hits	 
output: fused list top N	 
Tickets
ChunkTextReaderPort + Postgres adapter KeywordStore doc-filter join + tests VectorStore search_scored (doc filter optional)
hydrate service fusion service (RRF) + tests


Rerank (Cross-Encoder Local) (9
Default 9.1
sentence-transformers CrossEncoder	 
 
Model default: cross-encoder/ms-marco-MiniLM-L-6-v2	 
Device configurable: cpu/cuda	  rerank top_n small (8) for latency	 
Fallback 9.2
if model load fails or disabled: return fused order	 
Tickets
RerankerPort + CrossEncoder adapter
config knobs tests: deterministic behavior with stub


Ask Endpoint (Hybrid) (10
v1/queries/ask-hybrid/ 10.1
:Request
question	 
optional document_id	 
knobs: k_vec, k_kw, fused_limit, rerank_top_n	 
:Flow
embed question .1 vector search (doc filter optional) .2 hydrate vector chunk texts .3 keyword search (doc filter optional) .4
fusion RRF .5
rerank cross-encoder .6
build prompt with citations [chunk_id] .7
LLM generate .8
save chat turn (optional session_id) .9
Tickets
AskHybridUseCase full  	  prompt builder (strict grounding + citations)  	 
API route + examples  	 
 
ChatRepo integration (turn logging)


Chat Sessions & Observability (11
Add endpoints (optional but recommended) 11.1
POST /v1/chat/sessions create session	 
GET /v1/chat/sessions list	 
GET /v1/chat/sessions/{id}/turns	 

Metrics fields 11.2
:measure durations in use case	  embed_ms, search_ms, llm_ms	 
tokens if available (OpenAI response usage)	 
Tickets
ChatRepo + endpoints timing instrumentation structured logs with request_id


Testing Strategy (12
Unit tests (fast) 12.1
chunking token aware	  hash normalization stable	 
RRF fusion	  prompt builder	 
keyword SQL builder (smoke) with sqlite? (FTS needs Postgres; keep as integration)	 

Integration tests (docker-compose) 12.2
run postgres+redis+qdrant	  test upload â†’ worker index â†’ ask-hybrid returns answer (with stub LLM if needed)	 
Tickets
pytest harness testcontainers optional
 
CI job running unit tests (integration optional)


Production Hardening Checklist (13
Config validation at startup (missing keys)	 
Rate limiting (later)	  File storage backend (S3) abstraction	 
Secrets management	  DB connection pool settings	  Celery retry policy + dead-letter	 
Qdrant collection migration strategy	  Data retention / GDPR (delete docs cascades to mappings)	 


(ï»Ÿï» ïº˜ï»¨ï»”ï»´ïº¬ ï»—ïºŽïº‘ï»ž Day-by-day) Delivery Plan (14
( Ø£ï»³ïºŽ 1â€“2) Phase A
Repo bootstrap + config/logging + FastAPI skeleton + docker-compose	  Alembic setup + users/documents migrations + seed user	 
Upload endpoint wired	 

( Ø£ï»³ïºŽ 2â€“4) Phase B
Worker indexing pipeline: extraction/chunking/dedup/batch embed/qdrant minimal	 
payload Document status transitions	 
Basic ask endpoint vector-only	 

( Ø£ï»³ïºŽ 2â€“3) Phase C
Keyword store (tenant + doc-filter join)	  Hybrid fusion + CrossEncoder rerank	  Ask-hybrid endpoint complete	 
( Ø£ï»³ïºŽ 1â€“2) Phase D
Chat sessions/turns + timings	  Tests + docs + notebooks	 
 

 

(ï»£ïº¨ïº˜ØµØ± Rationale) Design ï»—ïº®Ø§Ø±Ø§Øª (15
.use cases ï»Ÿï»¤ï»Œï»ˆï»¢ ï»›ï»”ïºŽï»³ïº” + Ø£ïº³ï»¬ï»ž deployÙˆ ïº—ï»Œï»˜ï»´ïºª Ø£ï»—ï»ž :Elasticsearch ïº‘ïºªÙ„ Postgres FTS	 
.Ø£ï»—ï»ž cost + ï»—ï»®ÙŠ doc-filter + ïº£ï»˜ï»´ï»˜ï»² chunk_store + document_chunks: dedup	 
.ï»Ÿï» ï»¨ïºº ÙˆØ§ïº£ïºª storage + source of truth ïº—ï»®ï»“ï»´ïº® :Qdrant minimal payload	 
.ï»£ïº®ïºŸï»Œï»² chunk_id ïº‘Ù€ Ø§Ù„Ø§ïº£ïº˜ï»”ïºŽØ¸ ï»£ï»Š docs ï»‹ïº’ïº® overwrite ï»³ï»¤ï»¨ï»Š :point_id = doc:ord	 
.scores ïº‘ï»´ï»¦ calibration ïº‘ïºªÙˆÙ† RRF: robust	 
.LLM rerank ïº‘Ù€ ï»£ï»˜ïºŽØ±ï»§ïº” ïº—ï»œï» ï»”ïº” ïº‘ïº„ï»—ï»ž Cross-Encoder rerank: boost precision	 


Ø§ï»Ÿïº¨ï»„ï»®Ø© Ø§ï»Ÿïº˜ïºŽï»Ÿï»´ïº”
ï»Ÿï»® ï»‹ïºŽï»³ïº° Ø£ïº£ï»®Ù‘Ù„ Ø§ï»Ÿïº¨ï»„ïº” Ø¯ÙŠ Ø¥ï»Ÿï»° Milestones + Issues GitHub )ïºŸïºŽï»«ïº°Ø© ï»Ÿï» ï»¨ïº´ïº¦(ØŒ ï»—ï»®ï»ŸÙ‘ï»² Ø§ïº³ï»¢ Ø§ï»Ÿï»¤Ø´Ø±ÙˆØ¹ Ùˆï»«ï»¨ï»ˆï»¤ï»¬ïºŽ ï»›ïºŽï»Ÿïº˜ïºŽï»Ÿï»²:
Milestones	 
Epics	  effort + acceptance criteria ïº‘ïº˜ï»˜ïºªï»³ïº® Issues	 
1/2	 
