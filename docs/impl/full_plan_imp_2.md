# Ø§Ø³ØªÙƒÙ…Ø§Ù„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: AI Engineer Toolkit 2025

Ø³Ø£ÙƒÙ…Ù„ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆÙÙ‚Ø§Ù‹ Ù„ÙÙ„Ø³ÙØ© "Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø¨ÙŠØ¶" (White-box) Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ÙÙ†ÙŠØ© ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ.

## ðŸ“ notebooks/04_deep_learning/01_neural_networks_from_scratch.ipynb

```python
# =====================
# Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙØ±: Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØŒ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ ÙˆØ§Ù„Ø®Ù„ÙÙŠØŒ ÙˆØ±Ø¨Ø· Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø¨Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
# =====================

"""
## 1. Ø§Ù„Ø¥Ø·Ø§Ø± Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ù„Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©: Ù„Ù…Ø§Ø°Ø§ ØªØ¹Ù…Ù„ØŸ
Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù„ÙŠØ³Øª Ø³Ø­Ø±Ù‹Ø§ØŒ Ø¨Ù„ Ù‡ÙŠ ØªØ­ÙˆÙŠÙ„Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ù…ØªØ±Ø§ÙƒÙ…Ø© ØªØ³Ù…Ø­ Ù„Ù†Ø§ Ø¨ØªÙ…Ø«ÙŠÙ„ Ø¯ÙˆØ§Ù„ Ù…Ø¹Ù‚Ø¯Ø©. Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„Ù‡Ø§ ÙŠØ³Ù…Ø­ Ù„Ù†Ø§ Ø¨ØªØ´Ø®ÙŠØµ Ø§Ù„ÙØ´Ù„ØŒ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ù‡ÙŠØ§ÙƒÙ„ØŒ ÙˆØ§Ø¨ØªÙƒØ§Ø± Ø­Ù„ÙˆÙ„ Ø¬Ø¯ÙŠØ¯Ø©.

### 1.1 Ù†Ø¸Ø±ÙŠØ© Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠ (Universal Approximation Theorem)
ØªØ´ÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø¥Ù„Ù‰ Ø£Ù† Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ø°Ø§Øª Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© ÙˆØ§Ø­Ø¯Ø© (Ø­ØªÙ‰ Ù„Ùˆ ÙƒØ§Ù†Øª Ø¶Ø®Ù…Ø©) Ù‚Ø§Ø¯Ø±Ø© Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ¨ Ø£ÙŠ Ø¯Ø§Ù„Ø© Ù…Ø³ØªÙ…Ø±Ø© Ø¨Ø¯Ù‚Ø© Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ø¨Ø´Ø±Ø· ÙˆØ¬ÙˆØ¯ Ø¹Ø¯Ø¯ ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©. Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ Ù„Ù‚ÙˆØ© Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©.

### 1.2 Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø­Ø³Ø§Ø¨ÙŠØ©: Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ· ÙˆØ§Ù„ÙˆØ²Ù†
Ù†Ù‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¨Ø§Ø¯Ø¦ Ø£ÙˆÙ„ÙŠØ©:
- ÙƒÙ„ Ø·Ø¨Ù‚Ø© Ø¹ØµØ¨ÙŠØ©: `z = Wx + b`
- Ø«Ù… Ù†Ø·Ø¨Ù‚ Ø¯Ø§Ù„Ø© ØºÙŠØ± Ø®Ø·ÙŠØ©: `a = Ïƒ(z)`
- ÙŠØªÙ… ØªØ±Ø§ÙƒÙ… Ù‡Ø°Ù‡ Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ø¯Ø± Ø¹Ù„Ù‰ ØªÙ…Ø«ÙŠÙ„ Ø¹Ù„Ø§Ù‚Ø§Øª Ù…Ø¹Ù‚Ø¯Ø©

### 1.3 Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ (Backpropagation)
Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ Ù„ÙŠØ³ Ø³ÙˆÙ‰ ØªØ·Ø¨ÙŠÙ‚ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø³Ù„Ø³Ù„Ø© (Chain Rule) ÙÙŠ Ø§Ù„ØªÙØ§Ø¶Ù„ Ù„Ø­Ø³Ø§Ø¨ Ù…Ø´ØªÙ‚Ø§Øª Ø¯Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ø£ÙˆØ²Ø§Ù†. Ù„Ù†ÙØªØ±Ø¶ Ø´Ø¨ÙƒØ© Ù…Ø¹ Ø·Ø¨Ù‚Ø© Ø¥Ø¯Ø®Ø§Ù„ØŒ Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© ÙˆØ§Ø­Ø¯Ø©ØŒ ÙˆØ·Ø¨Ù‚Ø© Ù…Ø®Ø±Ø¬Ø©:

- Ø§Ù„Ø®Ø·Ø£: `E = Â½(y - Å·)Â²`
- Ø§Ù„Ù…Ø®Ø±Ø¬: `Å· = Ïƒ(Wâ‚‚Â·aâ‚ + bâ‚‚)`
- Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©: `aâ‚ = Ïƒ(Wâ‚Â·x + bâ‚)`

Ù„ØªØ­Ø¯ÙŠØ« `Wâ‚`ØŒ Ù†Ø­Ø³Ø¨:
`âˆ‚E/âˆ‚Wâ‚ = âˆ‚E/âˆ‚Å· Â· âˆ‚Å·/âˆ‚aâ‚ Â· âˆ‚aâ‚/âˆ‚Wâ‚`

Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠ ÙŠÙˆØ¶Ø­ Ø³Ø¨Ø¨ "ØªÙ„Ø§Ø´ÙŠ Ø§Ù„ØªØ¯Ø±Ø¬" (Vanishing Gradient) ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø© Ø¹Ù†Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙˆØ§Ù„ ØªÙ†Ø´ÙŠØ· Ù…Ø«Ù„ sigmoid - Ø­ÙŠØ« Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª ØµØºÙŠØ±Ø© (Ø£Ù‚Ù„ Ù…Ù† 0.25)ØŒ ÙˆØ¹Ù†Ø¯ Ø§Ù„Ø¶Ø±Ø¨ Ø§Ù„Ù…ØªÙƒØ±Ø± ØªÙ‚ØªØ±Ø¨ Ù…Ù† Ø§Ù„ØµÙØ±.

### 1.4 Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ø£Ù†ÙˆØ§Ø¹ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ·
- Sigmoid: Ø¬ÙŠØ¯Ø© Ù„Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø© ÙÙŠ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠØŒ Ù„ÙƒÙ†Ù‡Ø§ ØªØ¹Ø§Ù†ÙŠ Ù…Ù† ØªÙ„Ø§Ø´ÙŠ Ø§Ù„ØªØ¯Ø±Ø¬.
- Tanh: Ù…Ù…Ø§Ø«Ù„Ø© Ù„Ù€ sigmoid Ù„ÙƒÙ†Ù‡Ø§ Ù…Ø±ÙƒØ²ÙŠØ© Ø­ÙˆÙ„ Ø§Ù„ØµÙØ±ØŒ Ù…Ù…Ø§ ÙŠØ­Ø³Ù† Ø§Ù„ØªØ¹Ù„Ù….
- ReLU: `max(0,x)` - ØªØ¬Ù†Ø¨ ØªÙ„Ø§Ø´ÙŠ Ø§Ù„ØªØ¯Ø±Ø¬ ÙÙŠ Ø§Ù„Ù†ØµÙ Ø§Ù„Ù…ÙˆØ¬Ø¨ØŒ Ù„ÙƒÙ†Ù‡Ø§ ØªØ¹Ø§Ù†ÙŠ Ù…Ù† "Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù…ÙˆØª" (Dead Neurons) Ø¹Ù†Ø¯ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ù„Ø¨ÙŠØ©.
- Leaky ReLU/PReLU: Ø­Ù„ Ù„Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù…ÙˆØª Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù†Ø­Ø¯Ø§Ø± Ø·ÙÙŠÙ ÙÙŠ Ø§Ù„Ù†ØµÙ Ø§Ù„Ø³Ù„Ø¨ÙŠ.
"""

import numpy as np
import matplotlib.pyplot as plt
from typing import List, Dict, Any, Callable, Tuple
import math

"""
## 2. Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ: ØªÙ†ÙÙŠØ° Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙØ± (Ø¨Ø¯ÙˆÙ† PyTorch/TensorFlow)
Ø³Ù†Ù‚ÙˆÙ… Ø¨Ø¨Ù†Ø§Ø¡ Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ®ØµÙŠØµ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ·ØŒ ÙˆØ®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ­Ø³ÙŠÙ†ØŒ ÙˆØ¢Ù„ÙŠØ§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ….
"""

class NeuralNetworkFromScratch:
    """Ø´Ø¨ÙƒØ© Ø¹ØµØ¨ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù…Ø¨Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy ÙÙ‚Ø·"""
    
    def __init__(self, layer_sizes: List[int], activation: str = 'relu', 
                 output_activation: str = 'sigmoid', seed: int = 42):
        """
        Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        
        Args:
            layer_sizes: Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ø·Ø¨Ù‚Ø§Øª (Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø©)
            activation: Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· Ù„Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ© ('relu', 'sigmoid', 'tanh')
            output_activation: Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· Ù„Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
            seed: Ø¨Ø°Ø±Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±
        """
        self.layer_sizes = layer_sizes
        self.n_layers = len(layer_sizes) - 1  # Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ© + Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        
        # ØªØ­Ø¯ÙŠØ¯ Ø¯ÙˆØ§Ù„ Ø§Ù„ØªÙ†Ø´ÙŠØ·
        self.activation_fn, self.activation_derivative = self._get_activation(activation)
        self.output_activation_fn, self.output_activation_derivative = self._get_activation(output_activation)
        
        # ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¨Ø°Ø±Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
        np.random.seed(seed)
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„ØªØ­ÙŠØ²Ø§Øª
        self.weights = []
        self.biases = []
        
        # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… "He initialization" Ù„Ù„Ù€ ReLU Ø£Ùˆ "Xavier" Ù„Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø®Ø±Ù‰
        for i in range(self.n_layers):
            prev_size = layer_sizes[i]
            curr_size = layer_sizes[i+1]
            
            if activation == 'relu':
                # He initialization
                weight = np.random.randn(curr_size, prev_size) * np.sqrt(2 / prev_size)
            else:
                # Xavier initialization
                weight = np.random.randn(curr_size, prev_size) * np.sqrt(1 / prev_size)
            
            bias = np.zeros((curr_size, 1))
            
            self.weights.append(weight)
            self.biases.append(bias)
    
    def _get_activation(self, name: str) -> Tuple[Callable, Callable]:
        """Ø¥Ø±Ø¬Ø§Ø¹ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ†Ø´ÙŠØ· ÙˆÙ…Ø´ØªÙ‚ØªÙ‡Ø§"""
        if name == 'relu':
            def relu(x):
                return np.maximum(0, x)
            
            def relu_derivative(x):
                return (x > 0).astype(float)
            
            return relu, relu_derivative
        
        elif name == 'sigmoid':
            def sigmoid(x):
                return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
            
            def sigmoid_derivative(x):
                s = sigmoid(x)
                return s * (1 - s)
            
            return sigmoid, sigmoid_derivative
        
        elif name == 'tanh':
            def tanh(x):
                return np.tanh(x)
            
            def tanh_derivative(x):
                return 1 - np.tanh(x)**2
            
            return tanh, tanh_derivative
        
        elif name == 'linear':
            def linear(x):
                return x
            
            def linear_derivative(x):
                return np.ones_like(x)
            
            return linear, linear_derivative
        
        else:
            raise ValueError(f"Unsupported activation function: {name}")
    
    def forward(self, X: np.ndarray, training: bool = True) -> np.ndarray:
        """
        Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ Ù„Ù„Ø´Ø¨ÙƒØ©
        
        Args:
            X: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ (m Ã— n_features)
            training: ÙˆØ¶Ø¹ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ Dropout)
        
        Returns:
            Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        """
        # ØªØ­ÙˆÙŠÙ„ X Ù„ÙŠÙƒÙˆÙ† (n_features Ã— m) Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ØµÙÙˆÙÙŠØ©
        X = X.T
        self.activations = [X]  # Ù†Ø®Ø²Ù† Ø¬Ù…ÙŠØ¹ ØªÙ†Ø´ÙŠØ·Ø§Øª Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        self.z_values = []  # Ù†Ø®Ø²Ù† Ù‚ÙŠÙ… z Ù‚Ø¨Ù„ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø´ÙŠØ·
        
        # Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø¹Ø¨Ø± Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ù…Ø®ÙÙŠØ©
        for i in range(self.n_layers - 1):
            z = np.dot(self.weights[i], self.activations[-1]) + self.biases[i]
            self.z_values.append(z)
            a = self.activation_fn(z)
            self.activations.append(a)
        
        # Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        z = np.dot(self.weights[-1], self.activations[-1]) + self.biases[-1]
        self.z_values.append(z)
        a = self.output_activation_fn(z)
        self.activations.append(a)
        
        return a.T  # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø´ÙƒÙ„ (m Ã— n_outputs)
    
    def backward(self, X: np.ndarray, y: np.ndarray, learning_rate: float = 0.01):
        """
        Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
        
        Args:
            X: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            y: Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ØµØ­ÙŠØ­Ø©
            learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
        """
        m = X.shape[0]
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª
        y_pred = self.activations[-1]
        y = y.T  # ØªØ­ÙˆÙŠÙ„ y Ù„ÙŠÙƒÙˆÙ† (n_outputs Ã— m)
        
        # Ø­Ø³Ø§Ø¨ Ø®Ø·Ø£ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
        if self.output_activation_fn.__name__ == 'sigmoid':
            # Ø®Ø·Ø£ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ (Cross-entropy)
            delta = y_pred - y
        else:
            # Ø®Ø·Ø£ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± (MSE)
            delta = (y_pred - y) * self.output_activation_derivative(self.z_values[-1])
        
        # Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ø¹Ø¨Ø± Ø§Ù„Ø·Ø¨Ù‚Ø§Øª
        for l in reversed(range(self.n_layers)):
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆØ§Ù„ØªØ­ÙŠØ²Ø§Øª
            dw = np.dot(delta, self.activations[l].T) / m
            db = np.sum(delta, axis=1, keepdims=True) / m
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ù…Ø¹ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø§Ù†Ø²ÙŠØ§Ø­ (Momentum) - Ù†Ø³ØªØ®Ø¯Ù… Ù‚ÙŠÙ…Ø© Ø¨Ø³ÙŠØ·Ø© Ù‡Ù†Ø§
            if not hasattr(self, 'velocity_weights'):
                self.velocity_weights = [np.zeros_like(w) for w in self.weights]
                self.velocity_biases = [np.zeros_like(b) for b in self.biases]
            
            # Momentum
            self.velocity_weights[l] = 0.9 * self.velocity_weights[l] - learning_rate * dw
            self.velocity_biases[l] = 0.9 * self.velocity_biases[l] - learning_rate * db
            
            self.weights[l] += self.velocity_weights[l]
            self.biases[l] += self.velocity_biases[l]
            
            if l > 0:  # Ù„Ø§ Ù†Ø­Ø³Ø¨ Ø¯Ù„ØªØ§ Ù„Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ÙŠØ©
                delta = np.dot(self.weights[l].T, delta) * self.activation_derivative(self.z_values[l-1])
    
    def train(self, X: np.ndarray, y: np.ndarray, epochs: int = 1000, 
              learning_rate: float = 0.01, batch_size: int = 32,
              validation_data: Tuple[np.ndarray, np.ndarray] = None,
              verbose: bool = True):
        """
        ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        
        Args:
            X, y: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            epochs: Ø¹Ø¯Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
            learning_rate: Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            batch_size: Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©
            validation_data: Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ (X_val, y_val)
            verbose: Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
        """
        m = X.shape[0]
        history = {'train_loss': [], 'val_loss': []}
        
        for epoch in range(epochs):
            # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
            indices = np.random.permutation(m)
            X_shuffled = X[indices]
            y_shuffled = y[indices]
            
            epoch_loss = 0
            
            for i in range(0, m, batch_size):
                X_batch = X_shuffled[i:i+batch_size]
                y_batch = y_shuffled[i:i+batch_size]
                
                # Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø£Ù…Ø§Ù…ÙŠ
                y_pred = self.forward(X_batch)
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø®Ø·Ø£
                if self.output_activation_fn.__name__ == 'sigmoid':
                    # Cross-entropy loss
                    epsilon = 1e-15  # Ù„ØªØ¬Ù†Ø¨ log(0)
                    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
                    loss = -np.mean(y_batch * np.log(y_pred) + (1 - y_batch) * np.log(1 - y_pred))
                else:
                    # MSE loss
                    loss = np.mean((y_batch - y_pred) ** 2) / 2
                
                epoch_loss += loss * len(X_batch)
                
                # Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ø§Ù„Ø®Ù„ÙÙŠ ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                self.backward(X_batch, y_batch, learning_rate)
            
            epoch_loss /= m
            history['train_loss'].append(epoch_loss)
            
            # ØªØ­Ù‚Ù‚ Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ù† ÙˆØ¬Ø¯Øª
            if validation_data is not None:
                X_val, y_val = validation_data
                y_val_pred = self.forward(X_val, training=False)
                
                if self.output_activation_fn.__name__ == 'sigmoid':
                    epsilon = 1e-15
                    y_val_pred = np.clip(y_val_pred, epsilon, 1 - epsilon)
                    val_loss = -np.mean(y_val * np.log(y_val_pred) + (1 - y_val) * np.log(1 - y_val_pred))
                else:
                    val_loss = np.mean((y_val - y_val_pred.T) ** 2) / 2
                
                history['val_loss'].append(val_loss)
            
            # Ø·Ø¨Ø§Ø¹Ø© ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨
            if verbose and (epoch % 100 == 0 or epoch == epochs - 1):
                if validation_data is not None:
                    print(f"Epoch {epoch}/{epochs} - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}")
                else:
                    print(f"Epoch {epoch}/{epochs} - loss: {epoch_loss:.4f}")
        
        return history
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
        return self.forward(X, training=False)

"""
## 3. Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠ: Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© ØªØµÙ†ÙŠÙ XOR
Ù…Ø´ÙƒÙ„Ø© XOR Ù‡ÙŠ Ù…Ø´ÙƒÙ„Ø© ÙƒÙ„Ø§Ø³ÙŠÙƒÙŠØ© Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø­Ù„Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØµÙ†Ù Ø®Ø·ÙŠ Ø¨Ø³ÙŠØ·ØŒ Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡Ø§ Ø§Ø®ØªØ¨Ø§Ø±Ù‹Ø§ Ù…Ø«Ø§Ù„ÙŠÙ‹Ø§ Ù„Ù‚Ø¯Ø±Ø© Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ØºÙŠØ± Ø§Ù„Ø®Ø·ÙŠØ©.
"""

def xor_classification_example():
    """Ù…Ø«Ø§Ù„ ØªØ·Ø¨ÙŠÙ‚ÙŠ Ø¹Ù„Ù‰ ØªØµÙ†ÙŠÙ XOR"""
    # Ø¨ÙŠØ§Ù†Ø§Øª XOR
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])  # XOR: 1 Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Ù† Ù…Ø®ØªÙ„ÙÙŠÙ†
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© Ø¨Ø­Ø¬Ù… 4 ÙˆØ­Ø¯Ø§Øª)
    nn = NeuralNetworkFromScratch(
        layer_sizes=[2, 4, 1],  # 2 Ù…ÙŠØ²Ø© Ø¥Ø¯Ø®Ø§Ù„ØŒ 4 ÙˆØ­Ø¯Ø§Øª Ù…Ø®ÙÙŠØ©ØŒ 1 Ù…Ø®Ø±Ø¬
        activation='relu',
        output_activation='sigmoid'
    )
    
    # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    history = nn.train(
        X, y,
        epochs=5000,
        learning_rate=0.1,
        batch_size=4,  # Ù†Ø³ØªØ®Ø¯Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ ÙƒÙ„ ØªØ­Ø¯ÙŠØ« (Batch Gradient Descent)
        verbose=False
    )
    
    # Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
    predictions = nn.predict(X)
    print("ØªÙˆÙ‚Ø¹Ø§Øª XOR:")
    for i in range(len(X)):
        print(f"Ø§Ù„Ù…Ø¯Ø®Ù„: {X[i]}, Ø§Ù„ØªÙˆÙ‚Ø¹: {predictions[i][0]:.4f}, Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØµØ­ÙŠØ­Ø©: {y[i][0]}")
    
    # Ø±Ø³Ù… Ù…Ù†Ø­Ù†Ù‰ Ø§Ù„ØªØ¹Ù„Ù…
    plt.figure(figsize=(10, 5))
    plt.plot(history['train_loss'], label='Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('XOR Classification - Learning Curve')
    plt.legend()
    plt.grid(True)
    plt.savefig('xor_learning_curve.png')
    plt.show()

# xor_classification_example()

"""
## 4. Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©: Ù…Ù‚Ø§Ø±Ù†Ø© ØªØ£Ø«ÙŠØ± Ø§Ù„Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
Ù„Ù†Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø´Ø¨ÙƒØ§Øª Ø¹ØµØ¨ÙŠØ© Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ù…Ø³Ø£Ù„Ø© XOR:
1. Ø´Ø¨ÙƒØ© Ø¨Ø¯ÙˆÙ† Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© (Ù…ØµÙ†Ù Ø®Ø·ÙŠ)
2. Ø´Ø¨ÙƒØ© Ø¨Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© ÙˆØ§Ø­Ø¯Ø©
3. Ø´Ø¨ÙƒØ© Ø¨Ø·Ø¨Ù‚ØªÙŠÙ† Ù…Ø®ÙÙŠØªÙŠÙ†
"""

def compare_network_architectures():
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    # Ù†ÙØ³ Ø¨ÙŠØ§Ù†Ø§Øª XOR
    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
    y = np.array([[0], [1], [1], [0]])
    
    # 1. Ø´Ø¨ÙƒØ© Ø¨Ø¯ÙˆÙ† Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© (Ù…ØµÙ†Ù Ø®Ø·ÙŠ)
    linear_nn = NeuralNetworkFromScratch(
        layer_sizes=[2, 1],
        activation='linear',
        output_activation='sigmoid'
    )
    
    # 2. Ø´Ø¨ÙƒØ© Ø¨Ø·Ø¨Ù‚Ø© Ù…Ø®ÙÙŠØ© ÙˆØ§Ø­Ø¯Ø© (4 ÙˆØ­Ø¯Ø§Øª)
    single_hidden_nn = NeuralNetworkFromScratch(
        layer_sizes=[2, 4, 1],
        activation='relu',
        output_activation='sigmoid'
    )
    
    # 3. Ø´Ø¨ÙƒØ© Ø¨Ø·Ø¨Ù‚ØªÙŠÙ† Ù…Ø®ÙÙŠØªÙŠÙ† (4 ÙˆØ­Ø¯Ø§Øª Ø«Ù… 2 ÙˆØ­Ø¯Ø§Øª)
    double_hidden_nn = NeuralNetworkFromScratch(
        layer_sizes=[2, 4, 2, 1],
        activation='relu',
        output_activation='sigmoid'
    )
    
    architectures = [
        ("Linear (No Hidden Layers)", linear_nn),
        ("Single Hidden Layer (4 units)", single_hidden_nn),
        ("Double Hidden Layers (4,2 units)", double_hidden_nn)
    ]
    
    results = {}
    
    for name, nn in architectures:
        history = nn.train(
            X, y,
            epochs=3000,
            learning_rate=0.1,
            batch_size=4,
            verbose=False
        )
        
        predictions = nn.predict(X)
        loss = history['train_loss'][-1]
        
        results[name] = {
            'predictions': predictions,
            'loss': loss,
            'history': history
        }
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    plt.figure(figsize=(15, 10))
    
    # Ø±Ø³Ù… Ù…Ù†Ø­Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù…
    plt.subplot(2, 1, 1)
    for name, result in results.items():
        plt.plot(result['history']['train_loss'], label=name)
    
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Learning Curves for Different Network Architectures')
    plt.legend()
    plt.grid(True)
    
    # Ø±Ø³Ù… Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª
    plt.subplot(2, 1, 2)
    x = np.arange(len(X))
    width = 0.25
    
    for i, (name, result) in enumerate(results.items()):
        plt.bar(x + i*width, result['predictions'].flatten(), width, label=name)
    
    plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)
    plt.xticks(x + width, [str(x) for x in X])
    plt.xlabel('Input')
    plt.ylabel('Prediction')
    plt.title('Predictions for XOR Problem')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('network_architecture_comparison.png')
    plt.show()
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
    print("\nÙ†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©:")
    print("=" * 60)
    for name, result in results.items():
        print(f"\nØ§Ù„Ù‡ÙŠÙƒÙ„: {name}")
        print(f"Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {result['loss']:.6f}")
        print("Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª:")
        for i, (inp, pred, true) in enumerate(zip(X, result['predictions'], y)):
            print(f"  {inp} â†’ ØªÙˆÙ‚Ø¹: {pred[0]:.4f}, Ø­Ù‚ÙŠÙ‚ÙŠ: {true[0]}")

# compare_network_architectures()

"""
## 5. Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ© Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø©
Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙÙŠ Ø¯ÙØ§ØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…ÙÙŠØ¯Ø© Ù„Ù„Ø£Ø¨Ø­Ø§Ø«ØŒ Ù„ÙƒÙ† ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ù†Ø­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹. Ù„Ù†Ø­ÙˆÙ„ Ø´Ø¨ÙƒØªÙ†Ø§ Ø¥Ù„Ù‰ Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© FastAPI.
"""

def production_ready_neural_network():
    """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø© Ø¥Ù†ØªØ§Ø¬ÙŠØ©"""
    # Ù„Ù†ÙØªØ±Ø¶ Ø£Ù†Ù†Ø§ Ø¯Ø±Ø¨Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§
    # Ù‡Ø°Ø§ Ø§Ù„Ù…Ø«Ø§Ù„ ÙŠÙˆØ¶Ø­ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ø®Ø¯Ù…Ø© Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
    
    production_code = '''
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import numpy as np
import joblib
import logging
from typing import List, Dict, Any
import time

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("neural_network_api")

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨
try:
    model = joblib.load("models/neural_network_model.pkl")
    logger.info("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­")
except Exception as e:
    logger.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
    raise

app = FastAPI(title="Neural Network API", version="1.0.0")

class PredictionRequest(BaseModel):
    features: List[float]
    request_id: str = None

class PredictionResponse(BaseModel):
    prediction: float
    confidence: float = None
    processing_time_ms: float

@app.post("/predict", response_model=PredictionResponse)
async def predict(request: PredictionRequest):
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©"""
    start_time = time.time()
    
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª
        if len(request.features) != model.layer_sizes[0]:
            raise HTTPException(
                status_code=400, 
                detail=f"Invalid number of features. Expected {model.layer_sizes[0]}, got {len(request.features)}"
            )
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        X = np.array([request.features])
        prediction = model.predict(X)[0][0]
        
        # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø© Ø§Ù„ÙˆÙ‚Øª
        processing_time = (time.time() - start_time) * 1000  # Ù…Ù„ÙŠ Ø«Ø§Ù†ÙŠØ©
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© (Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ)
        confidence = prediction if prediction > 0.5 else 1 - prediction
        
        logger.info(f"Request {request.request_id or 'N/A'} processed in {processing_time:.2f}ms")
        
        return PredictionResponse(
            prediction=float(prediction),
            confidence=float(confidence),
            processing_time_ms=processing_time
        )
    
    except Exception as e:
        logger.error(f"Prediction error: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/model_info")
async def model_info():
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨"""
    return {
        "architecture": model.layer_sizes,
        "activation_functions": {
            "hidden_layers": "relu",
            "output_layer": "sigmoid"
        },
        "training_samples": 1000,
        "last_updated": "2025-12-30"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
'''
    
    print("Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©:")
    print("=" * 60)
    print(production_code)
    
    # Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¥Ù†ØªØ§Ø¬ÙŠØ©
    print("\nÙ…Ù„Ø§Ø­Ø¸Ø§Øª Ù‡Ù†Ø¯Ø³ÙŠØ© Ù„Ù„Ø¥Ø·Ù„Ø§Ù‚ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬:")
    print("-" * 40)
    print("1. Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (Caching): Ø§Ø³ØªØ®Ø¯Ù… Redis Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©")
    print("2. Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø­Ù…Ù„ (Load Balancing): Ù†Ø´Ø± Ø¹Ø¯Ø© Ù†Ø³Ø® Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø©")
    print("3. Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©: ØªØªØ¨Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ø«Ù„ latencyØŒ Ùˆ throughputØŒ Ùˆ Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡")
    print("4. Ø§Ù„ØªØ¯Ø±ÙŠØ¬ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Auto-scaling): Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø©Ù‹ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ù…Ù„")
    print("5. Ø§Ù„Ø­Ù…Ø§ÙŠØ©: ØªÙ†ÙÙŠØ° Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„ (rate limiting) ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©")

# production_ready_neural_network()

"""
## 6. ØªØ­Ø¯ÙŠØ§Øª Ù…ØªÙ‚Ø¯Ù…Ø©: ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
### 6.1 Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±/Ø§Ù„ØªÙ„Ø§Ø´ÙŠ ÙÙŠ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª
ÙÙŠ Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹Ù…ÙŠÙ‚Ø©ØŒ ÙŠÙ…ÙƒÙ† Ø£Ù† ØªÙƒØ¨Ø± Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª (Ø§Ù†ÙØ¬Ø§Ø± Ø§Ù„ØªØ¯Ø±Ø¬) Ø£Ùˆ ØªØªÙ‚Ù„Øµ (ØªÙ„Ø§Ø´ÙŠ Ø§Ù„ØªØ¯Ø±Ø¬) Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±ØŒ Ù…Ù…Ø§ ÙŠØ¹Ø·Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¹Ù„Ù….

### 6.2 Ø­Ù„ÙˆÙ„ Ø¹Ù…Ù„ÙŠØ©:
1. **ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©**: 
   - He initialization Ù„Ù„Ù€ ReLU
   - Xavier initialization Ù„Ø¯ÙˆØ§Ù„ Ø£Ø®Ø±Ù‰

2. **Normalization Layers**:
   - Batch Normalization
   - Layer Normalization
   - Group Normalization

3. **Skip Connections** (Ù…Ø«Ù„ ResNet):
   - Ø§Ù„Ø³Ù…Ø§Ø­ Ø¨ØªØ¯ÙÙ‚ Ø§Ù„ØªØ¯Ø±Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ©

4. **Gradient Clipping**:
   - Ù‚Øµ Ø§Ù„ØªØ¯Ø±Ø¬Ø§Øª Ø§Ù„ØªÙŠ ØªØªØ¬Ø§ÙˆØ² Ø­Ø¯Ø§Ù‹ Ù…Ø¹ÙŠÙ†Ø§Ù‹ Ù„Ù…Ù†Ø¹ Ø§Ù„Ø§Ù†ÙØ¬Ø§Ø±

### 6.3 ØªØ­Ø¯ÙŠ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©:
Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† Ø´Ø¨ÙƒØ© XOR Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù†Ø§ Ù„Ø¥Ø¶Ø§ÙØ©:
- Batch Normalization Ù„Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù…Ø®ÙÙŠØ©
- Gradient clipping
- Early stopping ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… ØªØ­Ø³Ù† Ø§Ù„Ø®Ø³Ø§Ø±Ø©

### 6.4 Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©:
- **Mixed Precision Training**: Ø§Ø³ØªØ®Ø¯Ø§Ù… 16-bit Ù„Ù„Ø­Ø³Ø§Ø¨ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
- **Distributed Training**: ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ø¨Ø± Ø¹Ø¯Ø© Ø¨Ø·Ø§Ù‚Ø§Øª Ø±Ø³ÙˆÙ…ÙŠØ©
- **Model Quantization**: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ 8-bit Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù…Ù‡ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ø±Ø¹Ø©
"""

"""
## 7. Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆØ§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
1. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.
2. He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. ICCV.
3. Glorot, X., & Bengio, Y. (2010). Understanding the difficulty of training deep feedforward neural networks. AISTATS.
4. Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. NeurIPS.

---

## Ø®Ø§ØªÙ…Ø©
Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù…Ù† Ø§Ù„ØµÙØ± Ù„ÙŠØ³Øª Ù…Ø¬Ø±Ø¯ ØªÙ…Ø§Ø±ÙŠÙ† Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠØ©ØŒ Ø¨Ù„ Ù‡ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ Ù„ÙÙ‡Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙŠ Ù†Ø¨Ù†ÙŠÙ‡. Ø¹Ù†Ø¯Ù…Ø§ Ù†ÙÙ‡Ù… Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙˆØ§Ù„ØªÙ†ÙÙŠØ° Ù…Ù† Ø§Ù„Ø¯Ø§Ø®Ù„ØŒ Ù†ØµØ¨Ø­ Ù…Ù‡Ù†Ø¯Ø³ÙŠÙ† ÙˆÙ„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ù…Ø³ØªØ®Ø¯Ù…ÙŠ Ø£Ø¯ÙˆØ§Øª. Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø±ÙˆØ­ Ø§Ù„ØªÙŠ ÙŠØªØ¨Ù†Ø§Ù‡Ø§ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹: ÙÙ‡Ù… Ø¹Ù…ÙŠÙ‚ØŒ ØªÙ†ÙÙŠØ° Ø¯Ù‚ÙŠÙ‚ØŒ ÙˆÙ‡Ù†Ø¯Ø³Ø© Ø¥Ù†ØªØ§Ø¬ÙŠØ© ØµÙ„Ø¨Ø©.
"""
```

## ðŸ“ src/core/math_operations.py (Complete Implementation)

```python
"""
This module provides core mathematical operations used throughout the AI engineering toolkit.
All implementations are from first principles using only NumPy and Python standard library.
"""

import numpy as np
from typing import Tuple, List, Dict, Any, Optional
import math
import logging

logger = logging.getLogger(__name__)

def dot_product(v1: np.ndarray, v2: np.ndarray) -> float:
    """
    Compute dot product of two vectors manually.
    
    Args:
        v1: First vector
        v2: Second vector
    
    Returns:
        Dot product result
    
    Raises:
        ValueError: If vectors have different dimensions
    """
    if v1.shape != v2.shape:
        raise ValueError(f"Vectors must have the same shape. Got {v1.shape} and {v2.shape}")
    
    return np.sum(v1 * v2)

def matrix_multiply(A: np.ndarray, B: np.ndarray) -> np.ndarray:
    """
    Multiply two matrices manually with dimension checking.
    
    Args:
        A: First matrix of shape (m, n)
        B: Second matrix of shape (n, p)
    
    Returns:
        Resulting matrix of shape (m, p)
    
    Raises:
        ValueError: If matrices cannot be multiplied
    """
    if A.shape[1] != B.shape[0]:
        raise ValueError(f"Matrix dimensions don't match for multiplication. "
                        f"A shape: {A.shape}, B shape: {B.shape}")
    
    m, n = A.shape
    _, p = B.shape
    
    # Pre-allocate result matrix
    C = np.zeros((m, p))
    
    # Efficient matrix multiplication
    for i in range(m):
        for k in range(n):
            # Skip near-zero values for efficiency
            if abs(A[i, k]) < 1e-12:
                continue
            for j in range(p):
                C[i, j] += A[i, k] * B[k, j]
    
    return C

def matrix_inverse(A: np.ndarray) -> np.ndarray:
    """
    Compute matrix inverse using Gaussian elimination with partial pivoting.
    
    Args:
        A: Square matrix to invert
    
    Returns:
        Inverse matrix
    
    Raises:
        ValueError: If matrix is not square or singular
    """
    if A.shape[0] != A.shape[1]:
        raise ValueError(f"Matrix must be square. Got shape {A.shape}")
    
    n = A.shape[0]
    # Create augmented matrix [A | I]
    augmented = np.hstack((A.copy(), np.eye(n)))
    
    # Gaussian elimination with partial pivoting
    for i in range(n):
        # Partial pivoting: find row with maximum element in column i
        max_row = i + np.argmax(np.abs(augmented[i:, i]))
        if abs(augmented[max_row, i]) < 1e-12:
            raise ValueError("Matrix is singular or nearly singular")
        
        # Swap rows if needed
        if max_row != i:
            augmented[[i, max_row]] = augmented[[max_row, i]]
        
        # Normalize pivot row
        pivot = augmented[i, i]
        augmented[i] = augmented[i] / pivot
        
        # Eliminate other rows
        for j in range(n):
            if j != i:
                factor = augmented[j, i]
                augmented[j] = augmented[j] - factor * augmented[i]
    
    # Extract inverse matrix
    inverse = augmented[:, n:]
    return inverse

def svd(A: np.ndarray, k: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute Singular Value Decomposition (SVD) manually.
    
    Args:
        A: Input matrix of shape (m, n)
        k: Number of singular values to keep (None for all)
    
    Returns:
        U, S, V such that A = U @ S @ V.T
    
    Note:
        This is a simplified implementation for educational purposes.
        For large matrices, use scipy.linalg.svd or numpy.linalg.svd.
    """
    m, n = A.shape
    
    # Compute A.T @ A for eigen decomposition
    ATA = A.T @ A
    
    # Compute eigenvalues and eigenvectors of A.T @ A
    eigenvalues, V = np.linalg.eigh(ATA)
    
    # Sort eigenvalues in descending order
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    V = V[:, idx]
    
    # Compute singular values
    singular_values = np.sqrt(np.maximum(eigenvalues, 0))
    
    # Determine number of components to keep
    if k is None:
        k = min(m, n)
    k = min(k, len(singular_values))
    
    # Keep only top k components
    singular_values = singular_values[:k]
    V = V[:, :k]
    
    # Compute U matrix
    U = A @ V
    U = U / np.linalg.norm(U, axis=0)
    
    # Construct S matrix
    S = np.diag(singular_values)
    
    return U[:, :k], S[:k, :k], V.T

def pca(X: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Principal Component Analysis (PCA) implementation from scratch.
    
    Args:
        X: Data matrix of shape (n_samples, n_features)
        n_components: Number of principal components to keep
    
    Returns:
        Transformed data, components, explained variance ratio
    """
    # Center the data
    X_centered = X - np.mean(X, axis=0)
    
    # Compute covariance matrix
    cov_matrix = np.cov(X_centered, rowvar=False)
    
    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    
    # Sort eigenvalues in descending order
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # Select top n_components
    components = eigenvectors[:, :n_components]
    
    # Transform data
    X_transformed = X_centered @ components
    
    # Calculate explained variance ratio
    explained_variance = eigenvalues[:n_components] / np.sum(eigenvalues)
    
    return X_transformed, components, explained_variance

def softmax(x: np.ndarray) -> np.ndarray:
    """
    Compute softmax function for input array.
    
    Args:
        x: Input array of shape (n_samples, n_classes) or (n_classes,)
    
    Returns:
        Softmax probabilities
    """
    # Handle both 1D and 2D arrays
    if x.ndim == 1:
        x = x.reshape(1, -1)
    
    # Subtract max for numerical stability
    x_max = np.max(x, axis=1, keepdims=True)
    exp_x = np.exp(x - x_max)
    return exp_x / np.sum(exp_x, axis=1, keepdims=True)

def cross_entropy_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Compute cross-entropy loss between true labels and predictions.
    
    Args:
        y_true: True labels (one-hot encoded) of shape (n_samples, n_classes)
        y_pred: Predicted probabilities of shape (n_samples, n_classes)
    
    Returns:
        Cross-entropy loss
    """
    # Clip predictions to avoid log(0)
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    
    # Compute cross-entropy
    loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]
    return loss

def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:
    """
    Compute Kullback-Leibler divergence between two distributions.
    
    Args:
        p: First probability distribution
        q: Second probability distribution
    
    Returns:
        KL divergence D_KL(p || q)
    """
    epsilon = 1e-15
    p = np.clip(p, epsilon, 1)
    q = np.clip(q, epsilon, 1)
    
    return np.sum(p * np.log(p / q))

def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
    """
    Compute cosine similarity between two vectors.
    
    Args:
        v1: First vector
        v2: Second vector
    
    Returns:
        Cosine similarity value between -1 and 1
    """
    dot = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot / (norm1 * norm2)

def jacobian(f: callable, x: np.ndarray, h: float = 1e-5) -> np.ndarray:
    """
    Compute Jacobian matrix of function f at point x using finite differences.
    
    Args:
        f: Function that takes a vector and returns a vector
        x: Point at which to evaluate the Jacobian
        h: Step size for finite differences
    
    Returns:
        Jacobian matrix
    """
    n = len(x)
    m = len(f(x))
    J = np.zeros((m, n))
    
    for i in range(n):
        x_plus = x.copy()
        x_plus[i] += h
        J[:, i] = (f(x_plus) - f(x)) / h
    
    return J

def hessian(f: callable, x: np.ndarray, h: float = 1e-5) -> np.ndarray:
    """
    Compute Hessian matrix of function f at point x using finite differences.
    
    Args:
        f: Function that takes a vector and returns a scalar
        x: Point at which to evaluate the Hessian
        h: Step size for finite differences
    
    Returns:
        Hessian matrix
    """
    n = len(x)
    H = np.zeros((n, n))
    
    for i in range(n):
        for j in range(i, n):
            # Central difference approximation
            x_ij = x.copy()
            x_i = x.copy()
            x_j = x.copy()
            x_0 = x.copy()
            
            x_ij[i] += h
            x_ij[j] += h
            
            x_i[i] += h
            x_j[j] += h
            
            H[i, j] = (f(x_ij) - f(x_i) - f(x_j) + f(x_0)) / (h * h)
            H[j, i] = H[i, j]  # Hessian is symmetric
    
    return H

def newton_raphson(f: callable, df: callable, x0: float, 
                  tol: float = 1e-6, max_iter: int = 100) -> Tuple[float, int]:
    """
    Newton-Raphson method for finding roots of a function.
    
    Args:
        f: Function to find root of
        df: Derivative of function
        x0: Initial guess
        tol: Tolerance for convergence
        max_iter: Maximum number of iterations
    
    Returns:
        Root value and number of iterations
    """
    x = x0
    for i in range(max_iter):
        fx = f(x)
        dfx = df(x)
        
        if abs(dfx) < 1e-12:
            raise ValueError("Derivative is zero. No solution found.")
        
        x_new = x - fx / dfx
        
        if abs(x_new - x) < tol:
            return x_new, i + 1
        
        x = x_new
    
    raise ValueError(f"Newton-Raphson failed to converge after {max_iter} iterations")

def gradient_descent(f: callable, df: callable, x0: np.ndarray, 
                    learning_rate: float = 0.01, 
                    max_iter: int = 1000,
                    tol: float = 1e-6) -> Tuple[np.ndarray, List[float]]:
    """
    Gradient descent optimization algorithm.
    
    Args:
        f: Objective function to minimize
        df: Gradient of objective function
        x0: Initial point
        learning_rate: Learning rate
        max_iter: Maximum number of iterations
        tol: Tolerance for convergence
    
    Returns:
        Optimal point and history of function values
    """
    x = x0.copy()
    history = [f(x)]
    
    for i in range(max_iter):
        grad = df(x)
        x_new = x - learning_rate * grad
        
        if np.linalg.norm(x_new - x) < tol:
            return x_new, history
        
        x = x_new
        history.append(f(x))
    
    return x, history

def conjugate_gradient(A: np.ndarray, b: np.ndarray, x0: Optional[np.ndarray] = None,
                      tol: float = 1e-6, max_iter: int = 1000) -> Tuple[np.ndarray, List[float]]:
    """
    Conjugate Gradient method for solving Ax = b.
    
    Args:
        A: Symmetric positive definite matrix
        b: Right-hand side vector
        x0: Initial guess (None for zero vector)
        tol: Tolerance for convergence
        max_iter: Maximum number of iterations
    
    Returns:
        Solution vector and residual history
    """
    n = b.shape[0]
    
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    
    r = b - A @ x
    p = r.copy()
    residual_history = [np.linalg.norm(r)]
    
    for i in range(max_iter):
        r_norm_sq = np.dot(r, r)
        if r_norm_sq < tol:
            break
        
        Ap = A @ p
        alpha = r_norm_sq / np.dot(p, Ap)
        
        x = x + alpha * p
        r_new = r - alpha * Ap
        
        beta = np.dot(r_new, r_new) / r_norm_sq
        
        p = r_new + beta * p
        r = r_new
        
        residual_history.append(np.linalg.norm(r))
    
    return x, residual_history

def kmeans(X: np.ndarray, k: int, max_iter: int = 100, 
           tol: float = 1e-4, random_state: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
    """
    K-means clustering algorithm from scratch.
    
    Args:
        X: Data matrix of shape (n_samples, n_features)
        k: Number of clusters
        max_iter: Maximum number of iterations
        tol: Tolerance for convergence
        random_state: Random seed for reproducibility
    
    Returns:
        Cluster centers and cluster assignments
    """
    if random_state is not None:
        np.random.seed(random_state)
    
    n_samples, n_features = X.shape
    
    # Initialize cluster centers randomly
    idx = np.random.choice(n_samples, k, replace=False)
    centers = X[idx].copy()
    
    # Initialize cluster assignments
    labels = np.zeros(n_samples, dtype=int)
    
    for iteration in range(max_iter):
        # Save old centers for convergence check
        old_centers = centers.copy()
        
        # Assign each point to the nearest center
        for i in range(n_samples):
            distances = np.linalg.norm(X[i] - centers, axis=1)
            labels[i] = np.argmin(distances)
        
        # Update centers
        for i in range(k):
            cluster_points = X[labels == i]
            if len(cluster_points) > 0:
                centers[i] = np.mean(cluster_points, axis=0)
        
        # Check for convergence
        center_shift = np.linalg.norm(centers - old_centers)
        if center_shift < tol:
            break
    
    return centers, labels

def linear_regression(X: np.ndarray, y: np.ndarray, 
                     regularization: str = None, alpha: float = 1.0) -> np.ndarray:
    """
    Linear regression with optional regularization.
    
    Args:
        X: Feature matrix of shape (n_samples, n_features)
        y: Target vector of shape (n_samples,)
        regularization: None, 'l1' (Lasso), or 'l2' (Ridge)
        alpha: Regularization strength
    
    Returns:
        Coefficients vector
    """
    n_samples, n_features = X.shape
    
    if regularization is None:
        # Normal equation solution
        return np.linalg.inv(X.T @ X) @ X.T @ y
    
    elif regularization == 'l2':
        # Ridge regression
        I = np.eye(n_features)
        I[0, 0] = 0  # Don't regularize the intercept term
        return np.linalg.inv(X.T @ X + alpha * I) @ X.T @ y
    
    elif regularization == 'l1':
        # Lasso regression using coordinate descent
        # Simple implementation for demonstration
        beta = np.zeros(n_features)
        max_iter = 1000
        tol = 1e-4
        
        for _ in range(max_iter):
            beta_old = beta.copy()
            
            for j in range(n_features):
                X_j = X[:, j]
                y_pred = X @ beta
                rho = np.dot(X_j, y - y_pred + beta[j] * X_j)
                
                if j == 0:  # Intercept term
                    beta[j] = rho / n_samples
                else:  # Regularized coefficients
                    beta[j] = soft_threshold(rho, alpha * n_samples / 2) / (np.dot(X_j, X_j) + 1e-10)
            
            if np.linalg.norm(beta - beta_old) < tol:
                break
        
        return beta
    
    else:
        raise ValueError(f"Unsupported regularization type: {regularization}")

def soft_threshold(rho: float, alpha: float) -> float:
    """
    Soft thresholding operator used in Lasso regression.
    
    Args:
        rho: Input value
        alpha: Threshold parameter
    
    Returns:
        Soft thresholded value
    """
    if rho < -alpha:
        return rho + alpha
    elif rho > alpha:
        return rho - alpha
    else:
        return 0.0

def polynomial_features(X: np.ndarray, degree: int) -> np.ndarray:
    """
    Generate polynomial features up to specified degree.
    
    Args:
        X: Input features of shape (n_samples, n_features)
        degree: Maximum polynomial degree
    
    Returns:
        Polynomial feature matrix
    """
    n_samples, n_features = X.shape
    combinations = []
    
    # Generate all combinations of features up to the specified degree
    def _gen_combinations(current_deg, start_idx, current_comb):
        if current_deg == 0:
            combinations.append(tuple(current_comb))
            return
        
        for i in range(start_idx, n_features):
            current_comb.append(i)
            _gen_combinations(current_deg - 1, i, current_comb)
            current_comb.pop()
    
    # Start with bias term
    poly_X = np.ones((n_samples, 1))
    
    # Generate features for each degree
    for d in range(1, degree + 1):
        _gen_combinations(d, 0, [])
        
        for comb in combinations:
            feature = np.ones(n_samples)
            for idx in comb:
                feature *= X[:, idx]
            poly_X = np.column_stack((poly_X, feature))
        
        combinations = []
    
    return poly_X

def rbf_kernel(X: np.ndarray, Y: np.ndarray = None, gamma: float = 1.0) -> np.ndarray:
    """
    Radial Basis Function (RBF) kernel.
    
    Args:
        X: First set of samples of shape (n_samples_X, n_features)
        Y: Second set of samples of shape (n_samples_Y, n_features). If None, use X.
        gamma: Kernel coefficient
    
    Returns:
        Kernel matrix of shape (n_samples_X, n_samples_Y)
    """
    if Y is None:
        Y = X
    
    n_samples_X = X.shape[0]
    n_samples_Y = Y.shape[0]
    
    # Compute squared Euclidean distances
    XX = np.sum(X ** 2, axis=1).reshape(-1, 1)
    YY = np.sum(Y ** 2, axis=1).reshape(1, -1)
    distances = XX + YY - 2 * X @ Y.T
    
    # Apply RBF kernel
    K = np.exp(-gamma * distances)
    return K

def sigmoid(x: np.ndarray) -> np.ndarray:
    """
    Sigmoid function with numerical stability.
    
    Args:
        x: Input array
    
    Returns:
        Sigmoid output
    """
    # Clip input to avoid overflow
    x = np.clip(x, -500, 500)
    return 1 / (1 + np.exp(-x))

def relu(x: np.ndarray) -> np.ndarray:
    """
    Rectified Linear Unit (ReLU) activation function.
    
    Args:
        x: Input array
    
    Returns:
        ReLU output
    """
    return np.maximum(0, x)

def relu_derivative(x: np.ndarray) -> np.ndarray:
    """
    Derivative of ReLU function.
    
    Args:
        x: Input array
    
    Returns:
        ReLU derivative
    """
    return (x > 0).astype(float)

def log_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Binary cross-entropy (log loss).
    
    Args:
        y_true: True binary labels (0 or 1)
        y_pred: Predicted probabilities
    
    Returns:
        Log loss value
    """
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

def accuracy(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Compute accuracy for binary or multiclass classification.
    
    Args:
        y_true: True labels
        y_pred: Predicted labels (or probabilities for binary classification)
    
    Returns:
        Accuracy value
    """
    if y_pred.ndim > 1 and y_pred.shape[1] > 1:
        # Multiclass case - y_pred is probabilities
        y_pred = np.argmax(y_pred, axis=1)
    elif y_pred.ndim == 1 or y_pred.shape[1] == 1:
        # Binary case - y_pred is probabilities
        y_pred = (y_pred > 0.5).astype(int)
    
    return np.mean(y_true == y_pred)

def precision(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Compute precision for binary classification.
    
    Args:
        y_true: True binary labels
        y_pred: Predicted binary labels
    
    Returns:
        Precision value
    """
    true_positives = np.sum((y_true == 1) & (y_pred == 1))
    false_positives = np.sum((y_true == 0) & (y_pred == 1))
    
    if true_positives + false_positives == 0:
        return 0.0
    
    return true_positives / (true_positives + false_positives)

def recall(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Compute recall for binary classification.
    
    Args:
        y_true: True binary labels
        y_pred: Predicted binary labels
    
    Returns:
        Recall value
    """
    true_positives = np.sum((y_true == 1) & (y_pred == 1))
    false_negatives = np.sum((y_true == 1) & (y_pred == 0))
    
    if true_positives + false_negatives == 0:
        return 0.0
    
    return true_positives / (true_positives + false_negatives)

def f1_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """
    Compute F1 score for binary classification.
    
    Args:
        y_true: True binary labels
        y_pred: Predicted binary labels
    
    Returns:
        F1 score value
    """
    p = precision(y_true, y_pred)
    r = recall(y_true, y_pred)
    
    if p + r == 0:
        return 0.0
    
    return 2 * (p * r) / (p + r)

def confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, 
                    labels: Optional[List] = None) -> np.ndarray:
    """
    Compute confusion matrix for classification.
    
    Args:
        y_true: True labels
        y_pred: Predicted labels
        labels: List of label indices to include in the confusion matrix
    
    Returns:
        Confusion matrix array
    """
    if labels is None:
        labels = np.unique(np.concatenate((y_true, y_pred)))
    
    n_classes = len(labels)
    matrix = np.zeros((n_classes, n_classes), dtype=int)
    
    for true_label, pred_label in zip(y_true, y_pred):
        if true_label in labels and pred_label in labels:
            i = np.where(labels == true_label)[0][0]
            j = np.where(labels == pred_label)[0][0]
            matrix[i, j] += 1
    
    return matrix

def roc_curve(y_true: np.ndarray, y_score: np.ndarray, 
             pos_label: int = 1) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Compute Receiver Operating Characteristic (ROC) curve.
    
    Args:
        y_true: True binary labels
        y_score: Target scores (probability estimates)
        pos_label: Label of the positive class
    
    Returns:
        False positive rates, true positive rates, thresholds
    """
    # Sort scores in descending order
    desc_score_indices = np.argsort(y_score)[::-1]
    y_score = y_score[desc_score_indices]
    y_true = y_true[desc_score_indices]
    
    # Count total positives and negatives
    n_pos = np.sum(y_true == pos_label)
    n_neg = len(y_true) - n_pos
    
    # Initialize arrays
    tps = np.cumsum(y_true == pos_label)
    fps = np.cumsum(y_true != pos_label)
    
    # Add initial point
    tps = np.r_[0, tps]
    fps = np.r_[0, fps]
    thresholds = np.r_[y_score[0] + 1, y_score]
    
    # Compute TPR and FPR
    tpr = tps / n_pos
    fpr = fps / n_neg
    
    return fpr, tpr, thresholds

def auc(fpr: np.ndarray, tpr: np.ndarray) -> float:
    """
    Compute Area Under the ROC Curve (AUC).
    
    Args:
        fpr: False positive rates
        tpr: True positive rates
    
    Returns:
        AUC value
    """
    return np.trapz(tpr, fpr)

def entropy(p: np.ndarray) -> float:
    """
    Compute entropy of a probability distribution.
    
    Args:
        p: Probability distribution
    
    Returns:
        Entropy value
    """
    epsilon = 1e-15
    p = np.clip(p, epsilon, 1)
    return -np.sum(p * np.log2(p))

def gini_impurity(p: np.ndarray) -> float:
    """
    Compute Gini impurity of a probability distribution.
    
    Args:
        p: Probability distribution
    
    Returns:
        Gini impurity value
    """
    return 1 - np.sum(p ** 2)

def mutual_information(X: np.ndarray, y: np.ndarray, 
                       n_bins: int = 10) -> np.ndarray:
    """
    Compute mutual information between features and target.
    
    Args:
        X: Feature matrix
        y: Target vector
        n_bins: Number of bins for discretization
    
    Returns:
        Mutual information for each feature
    """
    n_samples, n_features = X.shape
    mi = np.zeros(n_features)
    
    # Discretize target
    y_discrete = np.digitize(y, bins=np.linspace(np.min(y), np.max(y), n_bins))
    
    for i in range(n_features):
        # Discretize feature
        x_discrete = np.digitize(X[:, i], bins=np.linspace(np.min(X[:, i]), np.max(X[:, i]), n_bins))
        
        # Compute mutual information
        mi[i] = _compute_mutual_information(x_discrete, y_discrete)
    
    return mi

def _compute_mutual_information(x: np.ndarray, y: np.ndarray) -> float:
    """Helper function to compute mutual information between two discrete variables."""
    # Get unique values
    x_vals = np.unique(x)
    y_vals = np.unique(y)
    
    # Compute marginal and joint probabilities
    p_x = np.array([np.mean(x == xv) for xv in x_vals])
    p_y = np.array([np.mean(y == yv) for yv in y_vals])
    
    p_xy = np.zeros((len(x_vals), len(y_vals)))
    for i, xv in enumerate(x_vals):
        for j, yv in enumerate(y_vals):
            p_xy[i, j] = np.mean((x == xv) & (y == yv))
    
    # Compute mutual information
    mi = 0.0
    for i in range(len(x_vals)):
        for j in range(len(y_vals)):
            if p_xy[i, j] > 0 and p_x[i] > 0 and p_y[j] > 0:
                mi += p_xy[i, j] * np.log2(p_xy[i, j] / (p_x[i] * p_y[j]))
    
    return mi

def covariance_matrix(X: np.ndarray) -> np.ndarray:
    """
    Compute covariance matrix of data matrix.
    
    Args:
        X: Data matrix of shape (n_samples, n_features)
    
    Returns:
        Covariance matrix of shape (n_features, n_features)
    """
    # Center the data
    X_centered = X - np.mean(X, axis=0)
    
    # Compute covariance matrix
    n_samples = X.shape[0]
    cov = X_centered.T @ X_centered / (n_samples - 1)
    
    return cov

def correlation_matrix(X: np.ndarray) -> np.ndarray:
    """
    Compute correlation matrix of data matrix.
    
    Args:
        X: Data matrix of shape (n_samples, n_features)
    
    Returns:
        Correlation matrix of shape (n_features, n_features)
    """
    # Compute covariance matrix
    cov = covariance_matrix(X)
    
    # Compute standard deviations
    stds = np.sqrt(np.diag(cov))
    
    # Compute correlation matrix
    corr = cov / np.outer(stds, stds)
    
    # Set diagonal to 1 (fix numerical errors)
    np.fill_diagonal(corr, 1.0)
    
    return corr

def eigen_decomposition(A: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    Compute eigenvalue decomposition of a symmetric matrix.
    
    Args:
        A: Symmetric matrix
    
    Returns:
        Eigenvalues and eigenvectors
    """
    # Check if matrix is symmetric
    if not np.allclose(A, A.T, atol=1e-8):
        logger.warning("Matrix is not symmetric. Results may be inaccurate.")
    
    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = np.linalg.eigh(A)
    
    # Sort eigenvalues in descending order
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    return eigenvalues, eigenvectors

def pca_transform(X: np.ndarray, n_components: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    Transform data using PCA.
    
    Args:
        X: Data matrix of shape (n_samples, n_features)
        n_components: Number of components to keep
    
    Returns:
        Transformed data and explained variance ratio
    """
    # Center the data
    X_centered = X - np.mean(X, axis=0)
    
    # Compute covariance matrix
    cov = covariance_matrix(X_centered)
    
    # Compute eigenvalues and eigenvectors
    eigenvalues, eigenvectors = eigen_decomposition(cov)
    
    # Select top n_components
    components = eigenvectors[:, :n_components]
    
    # Transform data
    X_transformed = X_centered @ components
    
    # Compute explained variance ratio
    explained_variance = eigenvalues[:n_components] / np.sum(eigenvalues)
    
    return X_transformed, explained_variance

def mahalanobis_distance(x: np.ndarray, mean: np.ndarray, cov: np.ndarray) -> float:
    """
    Compute Mahalanobis distance between a point and a distribution.
    
    Args:
        x: Point vector
        mean: Mean vector of distribution
        cov: Covariance matrix of distribution
    
    Returns:
        Mahalanobis distance
    """
    # Compute inverse covariance matrix
    cov_inv = matrix_inverse(cov)
    
    # Compute difference vector
    diff = x - mean
    
    # Compute Mahalanobis distance
    return np.sqrt(diff.T @ cov_inv @ diff)

def manhattan_distance(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Manhattan distance between two vectors.
    
    Args:
        x: First vector
        y: Second vector
    
    Returns:
        Manhattan distance
    """
    return np.sum(np.abs(x - y))

def chebyshev_distance(x: np.ndarray, y: np.ndarray) -> float:
    """
    Compute Chebyshev distance between two vectors.
    
    Args:
        x: First vector
        y: Second vector
    
    Returns:
        Chebyshev distance
    """
    return np.max(np.abs(x - y))

def minkowski_distance(x: np.ndarray, y: np.ndarray, p: float = 2.0) -> float:
    """
    Compute Minkowski distance between two vectors.
    
    Args:
        x: First vector
        y: Second vector
        p: Order of the norm
    
    Returns:
        Minkowski distance
    """
    return np.sum(np.abs(x - y) ** p) ** (1 / p)
```

## ðŸ“ notebooks/05_production_engineering/01_fastapi_model_deployment.ipynb (Complete)

```python
# =====================
# Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FastAPI: Ù…Ù† Ø§Ù„Ø¯ÙØªØ± Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
# Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© -> Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ -> Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©
# =====================

"""
## 1. ÙÙ„Ø³ÙØ© Ø§Ù„Ù†Ø´Ø± ÙÙŠ Ø¹ØµØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
ÙÙŠ Ø¹ØµØ± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«ØŒ Ù„Ù… ÙŠØ¹Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯ÙØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª ÙƒØ§ÙÙŠØ§Ù‹. Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ØªÙƒÙ…Ù† ÙÙŠ Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙÙŠ Ø®Ø·ÙˆØ· Ø§Ù„Ø¥Ù†ØªØ§Ø¬ ÙˆØªÙ‚Ø¯ÙŠÙ…Ù‡Ø§ ÙƒØ®Ø¯Ù…Ø§Øª ÙŠÙ…ÙƒÙ† Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§. FastAPI Ø¸Ù‡Ø± ÙƒØ¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ù…Ø«Ø§Ù„ÙŠ Ù„Ù‡Ø°Ø§ Ø§Ù„ØºØ±Ø¶ Ø¨Ø³Ø¨Ø¨:

1. **Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¹Ø§Ù„ÙŠ**: ÙŠØ³ØªØ®Ø¯Ù… ASGI (Asynchronous Server Gateway Interface) Ù…Ø¹ uvicornØŒ Ù…Ù…Ø§ ÙŠÙˆÙØ± Ø£Ø¯Ø§Ø¡Ù‹ Ø£ÙØ¶Ù„ Ø¨Ù†Ø³Ø¨Ø© 3-5 Ù…Ø±Ø§Øª Ù…Ù† Flask ÙÙŠ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†.
2. **Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ**: ÙŠÙˆÙ„Ø¯ ÙˆØ§Ø¬Ù‡Ø© ØªÙˆØ«ÙŠÙ‚ ØªÙØ§Ø¹Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAPI Ùˆ Swagger UIØŒ Ù…Ù…Ø§ ÙŠÙ‚Ù„Ù„ Ù…Ù† ÙˆÙ‚Øª Ø§Ù„ØªØ·ÙˆÙŠØ± ÙˆØ²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©.
3. **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ**: ÙŠØ³ØªØ®Ø¯Ù… Pydantic Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¯Ø®Ù„Ø© ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø©ØŒ Ù…Ù…Ø§ ÙŠÙ…Ù†Ø¹ 40% Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª.
4. **Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„ÙˆØ¸Ø§Ø¦Ù ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©**: Ø¶Ø±ÙˆØ±ÙŠ Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªÙŠ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªØªØ¶Ù…Ù† Ø¹Ù…Ù„ÙŠØ§Øª Ø¥Ø¯Ø®Ø§Ù„/Ø¥Ø®Ø±Ø§Ø¬ (I/O) Ù…ÙƒØ«ÙØ©.

Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ù‡Ùˆ Ù…Ø§ ÙŠÙØµÙ„ Ø¨ÙŠÙ† "ÙƒÙˆØ¯ ÙŠØ¹Ù…Ù„" Ùˆ"Ø®Ø¯Ù…Ø© ØªØ¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ù„Ù Ù…Ø·Ù„ÙˆØ¨".
"""

"""
## 2. ØªØµÙ…ÙŠÙ… Ø§Ù„Ø¨Ù†ÙŠØ©: Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø©
Ø¹Ù†Ø¯ ØªØµÙ…ÙŠÙ… Ø®Ø¯Ù…Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ÙŠØ¬Ø¨ Ø£Ù† Ù†Ø£Ø®Ø° ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø± Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©:
1. **ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…**: ÙƒÙŠÙ ÙŠØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø¹ Ø§Ù„Ø®Ø¯Ù…Ø©ØŸ
2. **ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª**: Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©ØŒ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚Ø§ØªØŒ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©.
3. **Ù…Ù†Ø·Ù‚ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬**: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©ØŒ Ø§Ù„ØªÙ†Ø¨Ø¤ØŒ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø©.
4. **Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©**: Ø§Ù„Ù†Ø´Ø±ØŒ Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø­Ù…Ù„ØŒ Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ.
5. **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©**: ØªØªØ¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ØŒ Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©.

Ø³Ù†Ø±ÙƒØ² ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ± Ø¹Ù„Ù‰ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª 2 Ùˆ 3ØŒ Ù…Ø¹ Ù…Ù†Ø§Ù‚Ø´Ø© ÙƒÙŠÙÙŠØ© Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰.
"""

import time
import json
import logging
from typing import Dict, Any, List, Union, Optional
from dataclasses import dataclass
from enum import Enum

import numpy as np
import pandas as pd
from fastapi import FastAPI, HTTPException, Request, BackgroundTasks, Depends
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
import uvicorn
import joblib
import redis

# ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("api.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("model_api")

"""
## 3. Ø§Ù„ØªÙ†ÙÙŠØ°: Ø¨Ù†Ø§Ø¡ Ø®Ø¯Ù…Ø© Ù†Ù…ÙˆØ°Ø¬ ÙƒØ§Ù…Ù„Ø©
ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ ØªÙ†ÙÙŠØ° ÙƒØ§Ù…Ù„ Ù„Ø®Ø¯Ù…Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FastAPIØŒ Ù…Ø¹ Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©.
"""

@dataclass
class ModelConfig:
    """ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    model_path: str = "models/model.pkl"
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0
    cache_ttl: int = 3600  # Ø«Ø§Ù†ÙŠØ©
    max_batch_size: int = 32

class HealthStatus(str, Enum):
    """Ø­Ø§Ù„Ø© Ø§Ù„ØµØ­Ø© Ù„Ù„Ø®Ø¯Ù…Ø©"""
    HEALTHY = "healthy"
    DEGRADED = "degraded"
    UNHEALTHY = "unhealthy"

class PredictionRequest(BaseModel):
    """Ù†Ù…ÙˆØ°Ø¬ Ø·Ù„Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤"""
    features: Dict[str, Union[float, int, str, bool]]
    request_id: Optional[str] = None
    
    @validator('features')
    def validate_features(cls, v):
        if not v:
            raise ValueError("Features dictionary cannot be empty")
        return v

class BatchPredictionRequest(BaseModel):
    """Ù†Ù…ÙˆØ°Ø¬ Ø·Ù„Ø¨ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¯ÙØ¹ÙŠ"""
    requests: List[PredictionRequest]
    max_concurrency: int = Field(default=4, ge=1, le=16)

class PredictionResponse(BaseModel):
    """Ù†Ù…ÙˆØ°Ø¬ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„ØªÙ†Ø¨Ø¤"""
    prediction: Union[float, int, str, list]
    confidence: Optional[float] = None
    probabilities: Optional[Dict[str, float]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)

class ModelService:
    """Ø®Ø¯Ù…Ø© Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…Ø¹ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¥Ù†ØªØ§Ø¬"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.model = None
        self.redis_client = None
        self.load_model()
        self.setup_cache()
    
    def load_model(self):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
        try:
            start_time = time.time()
            self.model = joblib.load(self.config.model_path)
            load_time = time.time() - start_time
            logger.info(f"Model loaded successfully in {load_time:.2f} seconds")
        except Exception as e:
            logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def setup_cache(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Redis Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        try:
            self.redis_client = redis.Redis(
                host=self.config.redis_host,
                port=self.config.redis_port,
                db=self.config.redis_db,
                decode_responses=True
            )
            self.redis_client.ping()
            logger.info("Redis cache initialized successfully")
        except Exception as e:
            logger.warning(f"Redis cache initialization failed: {str(e)}")
            self.redis_client = None
    
    def generate_cache_key(self, features: Dict[str, Any]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª"""
        sorted_features = dict(sorted(features.items()))
        return json.dumps(sorted_features, sort_keys=True)
    
    async def get_cached_prediction(self, cache_key: str) -> Optional[PredictionResponse]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¥Ù† ÙˆØ¬Ø¯"""
        if not self.redis_client:
            return None
        
        try:
            cached = self.redis_client.get(cache_key)
            if cached:
                data = json.loads(cached)
                return PredictionResponse(**data)
        except Exception as e:
            logger.warning(f"Cache retrieval failed: {str(e)}")
        
        return None
    
    async def cache_prediction(self, cache_key: str, prediction: PredictionResponse):
        """ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ†Ø¨Ø¤ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª"""
        if not self.redis_client:
            return
        
        try:
            data = prediction.dict()
            self.redis_client.setex(cache_key, self.config.cache_ttl, json.dumps(data))
        except Exception as e:
            logger.warning(f"Cache storage failed: {str(e)}")
    
    def preprocess_features(self, features: Dict[str, Any]) -> np.ndarray:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù„Ù„Ù…ÙŠØ²Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤"""
        # ÙÙŠ ØªØ·Ø¨ÙŠÙ‚ Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù‚Ø¯ ØªØ´Ù…Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø®Ø·ÙˆØ©:
        # - Ø§Ù„ØªØ±Ù…ÙŠØ² (encoding) Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ¦ÙˆÙŠØ©
        # - Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ (scaling) Ù„Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ©
        # - Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©
        
        # Ù„Ù„ØªØ¨Ø³ÙŠØ·ØŒ Ù†ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªÙ†Ø¨Ø¤
        feature_values = np.array(list(features.values())).reshape(1, -1)
        return feature_values
    
    def postprocess_prediction(self, raw_prediction: np.ndarray, 
                             probabilities: Optional[np.ndarray] = None) -> PredictionResponse:
        """Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ù„ØªØ­ÙˆÙŠÙ„Ù‡ Ø¥Ù„Ù‰ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…ÙÙŠØ¯Ø©"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø®Ø§Ù… Ø¥Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        prediction_value = raw_prediction[0]
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        prob_dict = None
        confidence = None
        
        if probabilities is not None:
            if len(probabilities.shape) > 1 and probabilities.shape[1] > 1:
                # ØªØµÙ†ÙŠÙ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª
                confidence = np.max(probabilities[0])
                if hasattr(self.model, 'classes_'):
                    prob_dict = {
                        str(cls): float(prob) 
                        for cls, prob in zip(self.model.classes_, probabilities[0])
                    }
            else:
                # ØªØµÙ†ÙŠÙ Ø«Ù†Ø§Ø¦ÙŠ
                confidence = float(probabilities[0][0])
                prob_dict = {"positive": confidence, "negative": 1 - confidence}
        
        return PredictionResponse(
            prediction=float(prediction_value) if isinstance(prediction_value, (int, float, np.number)) else str(prediction_value),
            confidence=confidence,
            probabilities=prob_dict,
            metadata={
                "model_version": getattr(self.model, "version", "1.0.0"),
                "feature_names": list(features.keys()) if 'features' in locals() else [],
                "timestamp": time.time()
            }
        )
    
    async def predict(self, features: Dict[str, Any]) -> PredictionResponse:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        if self.model is None:
            raise HTTPException(status_code=500, detail="Model not loaded")
        
        try:
            # ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cache_key = self.generate_cache_key(features)
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤ Ù…Ù† Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            cached_prediction = await self.get_cached_prediction(cache_key)
            if cached_prediction:
                logger.info("Cache hit for prediction")
                return cached_prediction
            
            # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
            processed_features = self.preprocess_features(features)
            
            # Ø§Ù„ØªÙ†Ø¨Ø¤
            start_time = time.time()
            raw_prediction = self.model.predict(processed_features)
            prediction_time = time.time() - start_time
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø¥Ù† Ø£Ù…ÙƒÙ†
            probabilities = None
            if hasattr(self.model, "predict_proba"):
                try:
                    probabilities = self.model.predict_proba(processed_features)
                except Exception as e:
                    logger.warning(f"Probability prediction failed: {str(e)}")
            
            # Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø©
            prediction_response = self.postprocess_prediction(raw_prediction, probabilities)
            
            # ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙ†Ø¨Ø¤ ÙÙŠ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
            await self.cache_prediction(cache_key, prediction_response)
            
            logger.info(f"Prediction completed in {prediction_time:.4f} seconds")
            return prediction_response
            
        except Exception as e:
            logger.error(f"Prediction failed: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")
    
    async def batch_predict(self, requests: List[Dict[str, Any]], 
                           max_concurrency: int = 4) -> List[PredictionResponse]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¯ÙØ¹ÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…ØªØ²Ø§Ù…Ù†Ø©"""
        if len(requests) > self.config.max_batch_size:
            raise HTTPException(
                status_code=400,
                detail=f"Batch size exceeds maximum of {self.config.max_batch_size}"
            )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
        predictions = []
        start_time = time.time()
        
        for request in requests:
            try:
                prediction = await self.predict(request)
                predictions.append(prediction)
            except Exception as e:
                predictions.append(PredictionResponse(
                    prediction="error",
                    metadata={"error": str(e)}
                ))
        
        total_time = time.time() - start_time
        logger.info(f"Batch prediction completed for {len(requests)} items in {total_time:.4f} seconds")
        
        return predictions
    
    def get_model_metadata(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        metadata = {
            "model_type": type(self.model).__name__,
            "features": getattr(self.model, "feature_names_in_", []) if hasattr(self.model, "feature_names_in_") else [],
            "classes": getattr(self.model, "classes_", []).tolist() if hasattr(self.model, "classes_") else [],
            "n_features": getattr(self.model, "n_features_in_", None),
            "n_classes": getattr(self.model, "n_classes_", None),
            "training_samples": getattr(self.model, "n_samples_", None),
            "version": getattr(self.model, "version", "1.0.0"),
            "training_date": getattr(self.model, "training_date", time.strftime("%Y-%m-%d"))
        }
        return metadata
    
    def health_check(self) -> Dict[str, Any]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø©"""
        status = HealthStatus.HEALTHY
        issues = []
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        if self.model is None:
            status = HealthStatus.UNHEALTHY
            issues.append("Model not loaded")
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
        cache_status = "connected" if self.redis_client else "not configured"
        if self.redis_client:
            try:
                self.redis_client.ping()
            except Exception as e:
                cache_status = f"disconnected: {str(e)}"
                status = HealthStatus.DEGRADED if status == HealthStatus.HEALTHY else status
                issues.append(f"Redis connection failed: {str(e)}")
        
        return {
            "status": status.value,
            "timestamp": time.time(),
            "uptime": time.time() - getattr(self, "start_time", time.time()),
            "components": {
                "model": {
                    "status": "loaded" if self.model else "not loaded",
                    "path": self.config.model_path
                },
                "cache": {
                    "status": cache_status,
                    "host": self.config.redis_host if self.redis_client else "not configured"
                }
            },
            "issues": issues
        }

# ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
app = FastAPI(
    title="AI Model API",
    description="Production-ready API for AI models with caching, monitoring, and reliability features",
    version="1.0.0"
)

# Ø¥Ø¶Ø§ÙØ© middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ØªÙ‡ÙŠØ¦Ø© Ø®Ø¯Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
config = ModelConfig()
model_service = ModelService(config)
model_service.start_time = time.time()  # ØªØªØ¨Ø¹ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """ØªØ³Ø¬ÙŠÙ„ Ø·Ù„Ø¨Ø§Øª HTTP"""
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    
    logger.info(f"{request.method} {request.url.path} {response.status_code} - {process_time:.4f}s")
    
    # Ø¥Ø¶Ø§ÙØ© Ø±Ø£Ø³ ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
    response.headers["X-Process-Time"] = f"{process_time:.4f}"
    
    return response

@app.get("/health")
async def health_check():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø®Ø¯Ù…Ø©"""
    return model_service.health_check()

@app.get("/metadata")
async def get_model_metadata():
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
    try:
        return model_service.get_model_metadata()
    except Exception as e:
        logger.error(f"Metadata retrieval failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to retrieve model metadata")

@app.post("/predict", response_model=PredictionResponse)
async def predict_endpoint(request: PredictionRequest, background_tasks: BackgroundTasks):
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ÙØ±Ø¯ÙŠ"""
    try:
        return await model_service.predict(request.features)
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Prediction endpoint failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/predict/batch", response_model=List[PredictionResponse])
async def batch_predict_endpoint(request: BatchPredictionRequest):
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¯ÙØ¹ÙŠ"""
    try:
        return await model_service.batch_predict(
            [req.features for req in request.requests],
            max_concurrency=request.max_concurrency
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Batch prediction endpoint failed: {str(e)}")
        raise HTTPException(status_code=500, detail="Internal server error")

@app.post("/predict/stream")
async def stream_predict_endpoint(request: PredictionRequest):
    """Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„ (streaming)"""
    
    async def generate_predictions():
        """Ù…ÙˆÙ„Ø¯ Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ØªØ³Ù„Ø³Ù„Ø©"""
        try:
            yield json.dumps({"status": "processing", "stage": "preparing"}) + "\n"
            
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©
            yield json.dumps({"status": "processing", "stage": "preprocessing"}) + "\n"
            processed_features = model_service.preprocess_features(request.features)
            
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªÙ†Ø¨Ø¤
            yield json.dumps({"status": "processing", "stage": "predicting"}) + "\n"
            raw_prediction = model_service.model.predict(processed_features)
            
            # Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„Ø§Ø­Ù‚Ø©
            yield json.dumps({"status": "processing", "stage": "postprocessing"}) + "\n"
            prediction = model_service.postprocess_prediction(raw_prediction)
            
            # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
            yield json.dumps({
                "status": "complete",
                "result": prediction.dict()
            }) + "\n"
            
        except Exception as e:
            logger.error(f"Streaming prediction failed: {str(e)}")
            yield json.dumps({
                "status": "error",
                "message": str(e)
            }) + "\n"
    
    return StreamingResponse(generate_predictions(), media_type="application/json")

if __name__ == "__main__":
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚"""
    import argparse
    
    parser = argparse.ArgumentParser(description='AI Model API Server')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='Host to bind to')
    parser.add_argument('--port', type=int, default=8000, help='Port to listen on')
    parser.add_argument('--model-path', type=str, help='Path to the model file')
    args = parser.parse_args()
    
    if args.model_path:
        config.model_path = args.model_path
    
    logger.info(f"Starting API server on {args.host}:{args.port}")
    logger.info(f"Model path: {config.model_path}")
    
    uvicorn.run(
        "fastapi_model_deployment:app",
        host=args.host,
        port=args.port,
        reload=False,
        workers=4,
        log_level="info"
    )

"""
## 4. Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©: Ù…Ø§ ÙˆØ±Ø§Ø¡ Ø§Ù„ÙƒÙˆØ¯
Ø§Ù„ÙƒÙˆØ¯ Ø£Ø¹Ù„Ø§Ù‡ ÙŠÙˆÙØ± Ø£Ø³Ø§Ø³Ù‹Ø§ Ù…ØªÙŠÙ†Ù‹Ø§ØŒ Ù„ÙƒÙ† ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ù‡Ù†Ø§Ùƒ Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© ÙŠØ¬Ø¨ Ø£Ø®Ø°Ù‡Ø§ ÙÙŠ Ø§Ù„Ø§Ø¹ØªØ¨Ø§Ø±:

### 4.1 Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ§Ù„ØªÙˆØ³Ø¹
- **Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª (Caching)**: Ø§Ø³ØªØ®Ø¯Ù… Redis Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©ØŒ ÙƒÙ…Ø§ ÙØ¹Ù„Ù†Ø§ ÙÙŠ Ø§Ù„Ù…Ø«Ø§Ù„.
- **Ø§Ù„ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† Ø§Ù„Ø­Ù…Ù„ (Load Balancing)**: Ù†Ø´Ø± Ø¹Ø¯Ø© Ù†Ø³Ø® Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… nginx Ø£Ùˆ Kubernetes.
- **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ (Auto-scaling)**: Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ø³ØªØ¬Ø§Ø¨Ø©Ù‹ Ù„Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø­Ù…Ù„.
- **Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø£ÙÙ‚ÙŠ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø±Ø£Ø³ÙŠ**: Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„ØªØ£Ø®ÙŠØ± Ø§Ù„Ù…Ù†Ø®ÙØ¶ØŒ Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø±Ø£Ø³ÙŠ (Ù…Ø¹Ø§Ù„Ø¬Ø§Øª Ø£Ù‚ÙˆÙ‰) Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø£ÙØ¶Ù„. Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø°Ø§Øª Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø¹Ø§Ù„ÙŠØ©ØŒ Ø§Ù„ØªÙˆØ³Ø¹ Ø§Ù„Ø£ÙÙ‚ÙŠ (Ù†Ø³Ø® Ø£ÙƒØ«Ø±) Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø£ÙØ¶Ù„.

### 4.2 Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
- **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©**: ØªØªØ¨Ø¹ Ù…Ù‚Ø§ÙŠÙŠØ³ Ù…Ø«Ù„:
  - Ø²Ù…Ù† Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© (latency)
  - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ (error rate)
  - Ù…Ø¹Ø¯Ù„ Ø§Ù„Ø·Ù„Ø¨Ø§Øª ÙÙŠ Ø§Ù„Ø«Ø§Ù†ÙŠØ© (RPS)
  - Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬
- **ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø¢Ù„ÙŠØ©**: Ø¥Ø±Ø³Ø§Ù„ ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø¹Ù†Ø¯ ØªØ¬Ø§ÙˆØ² Ø¹ØªØ¨Ø§Øª Ù…Ø­Ø¯Ø¯Ø© (Ù…Ø«Ù„Ø§Ù‹ØŒ Ø®Ø·Ø£ 500 Ù„Ø£ÙƒØ«Ø± Ù…Ù† 1% Ù…Ù† Ø§Ù„Ø·Ù„Ø¨Ø§Øª).
- **Ø§Ù„ØªØ¹Ø§ÙÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ**: Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø© Ø¹Ù†Ø¯ ØªØ¹Ø·Ù„Ù‡Ø§.

### 4.3 Ø§Ù„Ø£Ù…Ø§Ù†
- **Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‡ÙˆÙŠØ© (Authentication)**: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙØ§ØªÙŠØ­ API Ø£Ùˆ JWT Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù‡ÙˆÙŠØ© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†.
- **Ø§Ù„ØªÙÙˆÙŠØ¶ (Authorization)**: Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ÙˆØµÙˆÙ„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯ÙˆØ§Ø±.
- **Ø§Ù„Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª**: Ø§Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ø¯Ù„ (rate limiting) Ù„Ù…Ù†Ø¹ Ù‡Ø¬Ù…Ø§Øª Ø§Ù„Ø­Ø±Ù…Ø§Ù† Ù…Ù† Ø§Ù„Ø®Ø¯Ù…Ø© (DoS).
- **ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª**: Ø§Ø³ØªØ®Ø¯Ø§Ù… HTTPS ÙˆØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©.

### 4.4 ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙƒÙ„ÙØ©
- **Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†**: Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª Ù…Ø«Ù„ Ø§Ù„ØªÙƒÙ…ÙŠÙ… (quantization) Ø£Ùˆ Ø§Ù„ØªÙ‚Ù„ÙŠÙ… (pruning) Ù„Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯.
- **Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠ**: Ø­ÙØ¸ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø¹Ø¯Ø¯ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.
- **Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª ØºÙŠØ± Ø§Ù„Ù…ØªØ²Ø§Ù…Ù†Ø©**: Ù„Ø·Ù„Ø¨Ø§Øª ØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§ Ø·ÙˆÙŠÙ„Ù‹Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø·ÙˆØ§Ø¨ÙŠØ± (queues) Ù„ØªØ¬Ù†Ø¨ Ø­Ø¸Ø± Ø§Ù„Ø®Ø§Ø¯Ù….

### 4.5 Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø³ØªÙ…Ø±
- **Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù…Ø³ØªÙ…Ø± (CI/CD)**: Ø£ØªÙ…ØªØ© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙˆØ§Ù„Ù†Ø´Ø±.
- **Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø­Ù…Ù„ (Load Testing)**: Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø¯Ù…Ø© ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø­Ù…Ù„ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹.
- **Ø§Ù„ØªØ±Ø¬ÙŠØ¹ Ø§Ù„Ø³Ø±ÙŠØ¹ (Rollback)**: Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ Ø¥ØµØ¯Ø§Ø± Ø³Ø§Ø¨Ù‚ Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ù…Ø´ÙƒÙ„Ø©.
- **Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ø´Ø±**: Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø®Ø¯Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„ÙƒØ´Ù Ø§Ù„Ù…Ø¨ÙƒØ± Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„.

### 4.6 ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ
- **ØªÙƒÙ„ÙØ© ÙƒÙ„ Ø·Ù„Ø¨**: ØªØªØ¨Ø¹ ØªÙƒÙ„ÙØ© ÙƒÙ„ Ø·Ù„Ø¨ (Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ ÙˆØ­Ø¯Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±ÙƒØ²ÙŠØ©/Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©).
- **Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ¹Ø§Ø¯Ù„ (Break-even Point)**: Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„ØªØºØ·ÙŠØ© ØªÙƒÙ„ÙØ© Ø§Ù„Ø®Ø¯Ù…Ø©.
- **Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø³ØªÙ…Ø±**: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ÙØ±Øµ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙƒÙØ§Ø¡Ø© ÙˆØ®ÙØ¶ Ø§Ù„ØªÙƒØ§Ù„ÙŠÙ.

## 5. Ø§Ù„ØªØ­Ø¯ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: ØªÙ…Ø§Ø±ÙŠÙ† Ù„Ù„ØªØ·Ø¨ÙŠÙ‚
### Ù…Ø³ØªÙˆÙ‰ Ù…Ø¨ØªØ¯Ø¦
1. Ø£Ø¶Ù Ù†Ù‚Ø·Ø© Ù†Ù‡Ø§ÙŠØ© GET `/` ØªØ¹Ø±Ø¶ Ø±Ø³Ø§Ù„Ø© ØªØ±Ø­ÙŠØ¨ ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø©.
2. Ù†ÙØ° Ø¯Ø§Ù„Ø© ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ø·Ù„Ø¨.
3. Ø£Ø¶Ù Ø¯Ø¹Ù…Ù‹Ø§ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ± Ø¹Ø§Ù„Ù…ÙŠ (ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©) ÙƒØ­Ù„ Ù…Ø¤Ù‚Øª Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ Redis.

### Ù…Ø³ØªÙˆÙ‰ Ù…ØªÙˆØ³Ø·
1. Ù†ÙØ° Ù†Ø¸Ø§Ù… Ø­Ù…Ø§ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª (rate limiting) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© slowapi.
2. Ø£Ø¶Ù Ù…Ø±Ø§Ù‚Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Prometheus Ùˆ Grafana Ù„Ø¹Ø±Ø¶ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø®Ø¯Ù…Ø©.
3. Ù†ÙØ° Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù„ØªØ®Ø²ÙŠÙ† Ø·Ù„Ø¨Ø§Øª Ø§Ù„ØªÙ†Ø¨Ø¤ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙÙŠ Ø¥Ø¹Ø§Ø¯Ø© ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬.

### Ù…Ø³ØªÙˆÙ‰ Ù…ØªÙ‚Ø¯Ù…
1. Ù†ÙØ° Ø®Ø¯Ù…Ø© Ø®Ù„ÙÙŠØ© (background worker) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Celery Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø©.
2. Ø£Ø¶Ù Ø¯Ø¹Ù…Ù‹Ø§ Ù„Ù€ gRPC Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ HTTP/REST Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡.
3. Ù†ÙØ° Ù†Ø¸Ø§Ù…Ù‹Ø§ Ù…ØªÙ‚Ø¯Ù…Ù‹Ø§ Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª ÙŠØ³ØªØ®Ø¯Ù… Ø¹Ø¯Ø© Ù…Ø³ØªÙˆÙŠØ§Øª (L1: Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ L2: RedisØŒ L3: Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª).

### ØªØ­Ø¯ÙŠ Ø¥Ù†ØªØ§Ø¬ÙŠ
Ø£Ù†Ø´Ø¦ Ù†Ø¸Ø§Ù…Ù‹Ø§ ÙƒØ§Ù…Ù„Ù‹Ø§ Ù„Ø®Ø¯Ù…Ø© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙŠØªØ¶Ù…Ù†:
- Ø®Ø¯Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ù…Ø«Ù„ Ù…Ø§ Ø£Ù†Ø´Ø£Ù†Ø§Ù‡)
- Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
- Ù†Ø¸Ø§Ù… ØªÙ†Ø¨ÙŠÙ‡Ø§Øª Ø¹Ù†Ø¯ ÙˆØ¬ÙˆØ¯ Ù…Ø´Ø§ÙƒÙ„
- ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù„Ù„Ø¥Ø¯Ø§Ø±Ø© (Ù„ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ØŒ Ù…Ø³Ø­ Ø§Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚ØªØŒ Ø¥Ù„Ø®)
- Ù…Ø³ØªÙ†Ø¯Ø§Øª API ÙƒØ§Ù…Ù„Ø© ÙˆÙ…Ø­Ø¯Ø«Ø©

## 6. Ø§Ù„Ø®Ù„Ø§ØµØ©
Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… FastAPI Ù‡Ùˆ Ø¹Ù…Ù„ÙŠØ© Ù…Ø¹Ù‚Ø¯Ø© ØªØªØ·Ù„Ø¨ ÙÙ‡Ù…Ù‹Ø§ Ø¹Ù…ÙŠÙ‚Ù‹Ø§ Ù„ÙƒÙ„ Ù…Ù† Ø¹Ù„Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©. Ø§Ù„Ù†Ø¬Ø§Ø­ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ Ù„Ø§ ÙŠØªØ¹Ù„Ù‚ ÙÙ‚Ø· Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¨Ù„ Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø¬Ø¹Ù„Ù‡Ø§ Ù…ØªØ§Ø­Ø©ØŒ Ù…ÙˆØ«ÙˆÙ‚Ø©ØŒ ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªÙˆØ³Ø¹. Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ù‚Ø¯Ù…Ù†Ø§Ù‡ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ø¯ÙØªØ± Ù‡Ùˆ Ù†Ù‚Ø·Ø© Ø¨Ø¯Ø§ÙŠØ© Ù‚ÙˆÙŠØ©ØŒ ÙˆÙ„ÙƒÙ†Ù‡ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ¹Ø¯ÙŠÙ„Ø§Øª ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª ÙƒØ¨ÙŠØ±Ø© Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©.

ØªØ°ÙƒØ± Ø¯Ø§Ø¦Ù…Ù‹Ø§: "Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø¯ÙØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù„ÙŠØ³ Ù…Ù†ØªØ¬Ù‹Ø§". Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ØªØ¨Ø¯Ø£ Ø¹Ù†Ø¯Ù…Ø§ ÙŠÙ…ÙƒÙ† Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø­Ù‚ÙŠÙ‚ÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ù…Ù† Ø¹Ù…Ù„Ùƒ.
"""
```

## ðŸ“ case_studies/medical_diagnosis_agent/implementation/diagnostic_engine.py (Complete)

```python
"""
Ù†Ø¸Ø§Ù… ØªØ´Ø®ÙŠØµ Ø·Ø¨ÙŠ Ù…ØªØ·ÙˆØ± Ù…Ø¹ Ø¶Ù…Ø§Ù† Ø§Ù„Ø®ØµÙˆØµÙŠØ© ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©
Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠÙ…Ø«Ù„ Ù†ÙˆØ§Ø© Ù†Ø¸Ø§Ù… ØªØ´Ø®ÙŠØµ Ø·Ø¨ÙŠ Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù…ØµÙ…Ù… Ù„Ù„Ø¥Ø·Ù„Ø§Ù‚ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø¥Ù†ØªØ§Ø¬ Ø­Ù‚ÙŠÙ‚ÙŠØ©.
Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø¯Ù‚Ø© Ø§Ù„ØªØ´Ø®ÙŠØµØŒ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØ§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ© ÙÙŠ Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø­Ø±Ø¬Ø©.
"""

import os
import json
import logging
import time
from typing import Dict, Any, List, Tuple, Optional, Union
from dataclasses import dataclass
from enum import Enum
import numpy as np
import pandas as pd
from datetime import datetime
import uuid
import hashlib
import re

# ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø³Ø¬Ù„
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("diagnostic_engine.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("medical_diagnosis_agent")

class DiagnosticConfidenceLevel(str, Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø«Ù‚Ø© Ø§Ù„ØªØ´Ø®ÙŠØµ"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    UNSURE = "unsure"

class DiagnosticRiskLevel(str, Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø®Ø·ÙˆØ±Ø© Ø§Ù„ØªØ´Ø®ÙŠØµ"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"

class MedicalSpecialty(str, Enum):
    """Ø§Ù„ØªØ®ØµØµØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ©"""
    CARDIOLOGY = "cardiology"
    NEUROLOGY = "neurology"
    ONCOLOGY = "oncology"
    ENDOCRINOLOGY = "endocrinology"
    GASTROENTEROLOGY = "gastroenterology"
    PULMONOLOGY = "pulmonology"
    DERMATOLOGY = "dermatology"
    GENERAL = "general"

@dataclass
class PatientSymptom:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ø·Ø¨ÙŠØ©"""
    symptom_name: str
    severity: int = Field(ge=1, le=10, description="Ø´Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¶ Ù…Ù† 1 Ø¥Ù„Ù‰ 10")
    duration_hours: float = Field(ge=0, description="Ø§Ù„Ù…Ø¯Ø© Ø¨Ø§Ù„Ø³Ø§Ø¹Ø§Øª")
    metadata: Dict[str, Any] = Field(default_factory=dict)

@dataclass
class MedicalCondition:
    """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ©"""
    condition_name: str
    icd_code: Optional[str] = None
    medical_specialty: MedicalSpecialty
    severity_level: int = Field(ge=1, le=10)
    urgency_level: int = Field(ge=1, le=10)
    treatment_options: List[str]
    diagnostic_criteria: List[str]
    risk_factors: List[str]

@dataclass
class DiagnosticResult:
    """Ù†ØªÙŠØ¬Ø© Ø§Ù„ØªØ´Ø®ÙŠØµ"""
    condition: MedicalCondition
    confidence_level: DiagnosticConfidenceLevel
    risk_level: DiagnosticRiskLevel
    supporting_evidence: List[str]
    contradicting_evidence: List[str]
    recommended_actions: List[str]
    required_tests: List[str]
    explanation: str
    ai_model_version: str
    timestamp: datetime = field(default_factory=datetime.utcnow)

class MedicalKnowledgeBase:
    """Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø·Ø¨ÙŠØ©"""
    
    def __init__(self, knowledge_base_path: str):
        self.knowledge_base_path = knowledge_base_path
        self.conditions = self._load_knowledge_base()
        self.symptom_mappings = self._build_symptom_mappings()
    
    def _load_knowledge_base(self) -> Dict[str, MedicalCondition]:
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…Ù„Ù JSON"""
        try:
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            conditions = {}
            for condition_data in data['conditions']:
                condition = MedicalCondition(
                    condition_name=condition_data['name'],
                    icd_code=condition_data.get('icd_code'),
                    medical_specialty=MedicalSpecialty(condition_data['specialty']),
                    severity_level=condition_data['severity_level'],
                    urgency_level=condition_data['urgency_level'],
                    treatment_options=condition_data['treatment_options'],
                    diagnostic_criteria=condition_data['diagnostic_criteria'],
                    risk_factors=condition_data.get('risk_factors', [])
                )
                conditions[condition.condition_name] = condition
            
            logger.info(f"Loaded {len(conditions)} medical conditions from knowledge base")
            return conditions
        except Exception as e:
            logger.error(f"Failed to load knowledge base: {str(e)}")
            raise
    
    def _build_symptom_mappings(self) -> Dict[str, List[str]]:
        """Ø¨Ù†Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙˆØ§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©"""
        symptom_to_conditions = {}
        
        for condition_name, condition in self.conditions.items():
            for criterion in condition.diagnostic_criteria:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ù…Ù† Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ´Ø®ÙŠØµ
                symptoms = self._extract_symptoms_from_criterion(criterion)
                for symptom in symptoms:
                    if symptom not in symptom_to_conditions:
                        symptom_to_conditions[symptom] = []
                    symptom_to_conditions[symptom].append(condition_name)
        
        logger.info(f"Built symptom mappings for {len(symptom_to_conditions)} symptoms")
        return symptom_to_conditions
    
    def _extract_symptoms_from_criterion(self, criterion: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ù…Ù† Ù†Øµ Ù…Ø¹ÙŠØ§Ø± Ø§Ù„ØªØ´Ø®ÙŠØµ"""
        # Ù‡Ø°Ù‡ Ø¯Ø§Ù„Ø© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ØŒ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ Ø³ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹
        symptoms = []
        
        # Ø£Ù†Ù…Ø§Ø· Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø£Ø¹Ø±Ø§Ø¶
        common_symptoms = [
            'Ø£Ù„Ù…', 'Ø­Ø±Ø§Ø±Ø©', 'Ø³Ø¹Ø§Ù„', 'ØºØ«ÙŠØ§Ù†', 'Ø¯ÙˆØ®Ø©', 'ØµØ¯Ø§Ø¹', 'ØªØ¹Ø¨',
            'Ø¶Ø¹Ù', 'ØªÙˆØ±Ù…', 'Ø§Ø­Ù…Ø±Ø§Ø±', 'Ø·ÙØ­', 'Ø¶ÙŠÙ‚ ØªÙ†ÙØ³', 'Ø®ÙÙ‚Ø§Ù†'
        ]
        
        for symptom in common_symptoms:
            if symptom in criterion.lower():
                symptoms.append(symptom)
        
        return symptoms

class PrivacyEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„Ø®ØµÙˆØµÙŠØ© ÙˆØ§Ù„Ø£Ù…Ø§Ù† Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ©"""
    
    def __init__(self, pii_patterns_path: Optional[str] = None):
        self.pii_patterns = self._load_pii_patterns(pii_patterns_path)
        self.hash_salt = os.getenv('PRIVACY_HASH_SALT', 'medical_diagnosis_salt_2025')
    
    def _load_pii_patterns(self, pii_patterns_path: Optional[str]) -> List[re.Pattern]:
        """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø© (PII)"""
        default_patterns = [
            # Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆØ§ØªÙ
            re.compile(r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'),
            # Ø¹Ù†Ø§ÙˆÙŠÙ† Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ
            re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'),
            # Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù‡ÙˆÙŠØ© Ø§Ù„ÙˆØ·Ù†ÙŠØ©
            re.compile(r'\b\d{10}\b'),
            # ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯
            re.compile(r'\b\d{1,2}/\d{1,2}/\d{4}\b'),
            # Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø´Ø®Ø§Øµ (Ù†Ù…Ø· Ø¨Ø³ÙŠØ·)
            re.compile(r'\b(Ø§Ù„Ø£Ø³ØªØ§Ø°|Ø§Ù„Ø¯ÙƒØªÙˆØ±|Ø§Ù„Ø³ÙŠØ¯Ø©|Ø§Ù„Ø³ÙŠØ¯)\s+[Ø£-ÙŠ][\w\s]+'),
            # Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
            re.compile(r'\b(Ø´Ø§Ø±Ø¹|Ø·Ø±ÙŠÙ‚|Ù…Ø¨Ù†Ù‰|Ø¹Ù…Ø§Ø±Ø©)\s+[\w\s,]+')
        ]
        
        if pii_patterns_path and os.path.exists(pii_patterns_path):
            try:
                with open(pii_patterns_path, 'r', encoding='utf-8') as f:
                    custom_patterns = json.load(f)
                for pattern in custom_patterns:
                    default_patterns.append(re.compile(pattern))
            except Exception as e:
                logger.warning(f"Failed to load custom PII patterns: {str(e)}")
        
        return default_patterns
    
    def anonymize_text(self, text: str) -> str:
        """Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© ÙÙŠ Ø§Ù„Ù†Øµ"""
        anonymized_text = text
        
        for pattern in self.pii_patterns:
            matches = pattern.findall(anonymized_text)
            for match in matches:
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø© Ø¨Ù€ [REDACTED]
                anonymized_text = anonymized_text.replace(match, '[REDACTED]')
        
        return anonymized_text
    
    def hash_patient_id(self, patient_id: str) -> str:
        """ØªØ´ÙÙŠØ± Ù‡ÙˆÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¶"""
        return hashlib.sha256((patient_id + self.hash_salt).encode()).hexdigest()
    
    def validate_consent(self, consent_data: Dict[str, Any]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙˆØ§ÙÙ‚Ø© Ø§Ù„Ù…Ø±ÙŠØ¶"""
        required_fields = ['patient_id', 'consent_granted', 'consent_date', 'consent_version']
        
        for field in required_fields:
            if field not in consent_data:
                logger.warning(f"Missing required consent field: {field}")
                return False
        
        if not consent_data['consent_granted']:
            logger.warning("Patient has not granted consent")
            return False
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø© (ÙŠØ¬Ø¨ Ø£Ù„Ø§ ÙŠÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„)
        consent_date = datetime.fromisoformat(consent_data['consent_date'])
        if consent_date > datetime.utcnow():
            logger.warning("Consent date is in the future")
            return False
        
        return True

class DiagnosticEngine:
    """Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    def __init__(self, knowledge_base_path: str, pii_patterns_path: Optional[str] = None):
        self.knowledge_base = MedicalKnowledgeBase(knowledge_base_path)
        self.privacy_engine = PrivacyEngine(pii_patterns_path)
        self.model_version = "1.2.0"
        self.confidence_thresholds = {
            DiagnosticConfidenceLevel.HIGH: 0.85,
            DiagnosticConfidenceLevel.MEDIUM: 0.65,
            DiagnosticConfidenceLevel.LOW: 0.40
        }
    
    def _validate_symptoms(self, symptoms: List[PatientSymptom]) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©"""
        if not symptoms:
            logger.warning("No symptoms provided")
            return False
        
        for symptom in symptoms:
            if not symptom.symptom_name:
                logger.warning("Empty symptom name")
                return False
            if not (1 <= symptom.severity <= 10):
                logger.warning(f"Invalid severity level: {symptom.severity}")
                return False
            if symptom.duration_hours < 0:
                logger.warning(f"Negative duration: {symptom.duration_hours}")
                return False
        
        return True
    
    def _calculate_symptom_weight(self, symptom: PatientSymptom, condition: MedicalCondition) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆØ²Ù† Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø­Ø§Ù„Ø© Ø·Ø¨ÙŠØ© Ù…Ø¹ÙŠÙ†Ø©"""
        weight = 0.0
        
        # 1. Ù…Ø¯Ù‰ ØªØ·Ø§Ø¨Ù‚ Ø§Ù„Ø¹Ø±Ø¶ Ù…Ø¹ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ´Ø®ÙŠØµ
        for criterion in condition.diagnostic_criteria:
            if symptom.symptom_name.lower() in criterion.lower():
                weight += 0.3
        
        # 2. Ø´Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¶ (Ø§Ù„Ø´Ø¯Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© ØªØ²ÙŠØ¯ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ ØªØ´Ø®ÙŠØµ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø±Ø¬Ø©)
        severity_factor = symptom.severity / 10.0
        if condition.urgency_level > 7 and symptom.severity > 7:
            weight += severity_factor * 0.4
        
        # 3. Ù…Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¶ (Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ø§Ù„Ø§Øª ØªØªØ·Ù„Ø¨ Ø£Ø¹Ø±Ø§Ø¶Ù‹Ø§ Ø°Ø§Øª Ù…Ø¯Ø© Ù…Ø­Ø¯Ø¯Ø©)
        duration_factor = min(symptom.duration_hours / 24.0, 1.0)  # Ø­ØªÙ‰ 24 Ø³Ø§Ø¹Ø©
        if "Ø­Ø§Ø¯Ø©" in condition.condition_name.lower() and symptom.duration_hours < 48:
            weight += duration_factor * 0.3
        
        return min(weight, 1.0)
    
    def _calculate_condition_match_score(self, symptoms: List[PatientSymptom], 
                                        condition: MedicalCondition) -> Tuple[float, List[str], List[str]]:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ ÙˆØ§Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ø·Ø¨ÙŠØ©"""
        total_weight = 0.0
        max_possible_weight = 0.0
        supporting_evidence = []
        contradicting_evidence = []
        
        for symptom in symptoms:
            weight = self._calculate_symptom_weight(symptom, condition)
            total_weight += weight
            max_possible_weight += 1.0
            
            if weight > 0.2:
                supporting_evidence.append(f"Ø§Ù„Ø¹ÙŽØ±ÙŽØ¶ '{symptom.symptom_name}' (Ø´Ø¯Ø©: {symptom.severity}/10) ÙŠØ¯Ø¹Ù… ØªØ´Ø®ÙŠØµ {condition.condition_name}")
            elif weight == 0 and symptom.severity > 7:
                contradicting_evidence.append(f"Ø§Ù„Ø¹ÙŽØ±ÙŽØ¶ '{symptom.symptom_name}' (Ø´Ø¯Ø© Ø¹Ø§Ù„ÙŠØ©) Ù„Ø§ ÙŠØªÙˆØ§ÙÙ‚ Ù…Ø¹ {condition.condition_name}")
        
        match_score = total_weight / max_possible_weight if max_possible_weight > 0 else 0.0
        return match_score, supporting_evidence, contradicting_evidence
    
    def _determine_confidence_level(self, match_score: float, 
                                   symptoms_count: int) -> DiagnosticConfidenceLevel:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ø§Ù„ØªØ´Ø®ÙŠØµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ·Ø§Ø¨Ù‚"""
        if symptoms_count < 2:
            return DiagnosticConfidenceLevel.LOW
        
        if match_score >= self.confidence_thresholds[DiagnosticConfidenceLevel.HIGH]:
            return DiagnosticConfidenceLevel.HIGH
        elif match_score >= self.confidence_thresholds[DiagnosticConfidenceLevel.MEDIUM]:
            return DiagnosticConfidenceLevel.MEDIUM
        elif match_score >= self.confidence_thresholds[DiagnosticConfidenceLevel.LOW]:
            return DiagnosticConfidenceLevel.LOW
        else:
            return DiagnosticConfidenceLevel.UNSURE
    
    def _determine_risk_level(self, condition: MedicalCondition, 
                            confidence_level: DiagnosticConfidenceLevel) -> DiagnosticRiskLevel:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø®Ø·ÙˆØ±Ø© Ø§Ù„ØªØ´Ø®ÙŠØµ"""
        urgency_score = condition.urgency_level
        
        if confidence_level == DiagnosticConfidenceLevel.HIGH:
            if urgency_score >= 8:
                return DiagnosticRiskLevel.CRITICAL
            elif urgency_score >= 6:
                return DiagnosticRiskLevel.HIGH
            elif urgency_score >= 4:
                return DiagnosticRiskLevel.MEDIUM
            else:
                return DiagnosticRiskLevel.LOW
        else:  # Ù…Ø³ØªÙˆÙ‰ Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶
            if urgency_score >= 7:
                return DiagnosticRiskLevel.HIGH
            elif urgency_score >= 5:
                return DiagnosticRiskLevel.MEDIUM
            else:
                return DiagnosticRiskLevel.LOW
    
    def _generate_explanation(self, condition: MedicalCondition, 
                            confidence_level: DiagnosticConfidenceLevel,
                            supporting_evidence: List[str],
                            contradicting_evidence: List[str]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø´Ø±Ø­ Ù…ÙØµÙ„ Ù„Ù„ØªØ´Ø®ÙŠØµ"""
        explanation = f"ØªÙ… ØªØ´Ø®ÙŠØµ Ø­Ø§Ù„Ø© {condition.condition_name} "
        
        # Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©
        confidence_text = {
            DiagnosticConfidenceLevel.HIGH: "Ø¨Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©",
            DiagnosticConfidenceLevel.MEDIUM: "Ø¨Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©",
            DiagnosticConfidenceLevel.LOW: "Ø¨Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©",
            DiagnosticConfidenceLevel.UNSURE: "Ø¨Ø¯ÙˆÙ† Ø«Ù‚Ø© ÙƒØ§ÙÙŠØ©"
        }
        explanation += f"{confidence_text[confidence_level]} Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ Ø§Ù„Ù…Ø¯Ø®Ù„Ø©."
        
        # Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¯Ø§Ø¹Ù…Ø©
        if supporting_evidence:
            explanation += "\n\nØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ø¯Ø§Ø¹Ù…Ø©:"
            for evidence in supporting_evidence[:3]:  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ 3 Ø£Ø¯Ù„Ø©
                explanation += f"\n- {evidence}"
        
        # Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„Ù…Ù†Ø§Ù‚Ø¶Ø©
        if contradicting_evidence:
            explanation += "\n\nØ§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„ØªÙŠ Ù‚Ø¯ ØªÙ†Ø§Ù‚Ø¶ Ù‡Ø°Ø§ Ø§Ù„ØªØ´Ø®ÙŠØµ:"
            for evidence in contradicting_evidence[:2]:  # Ø¹Ø±Ø¶ Ø£ÙˆÙ„ Ø¯Ù„ÙŠÙ„ÙŠÙ†
                explanation += f"\n- {evidence}"
        
        # ØªÙˆØµÙŠØ§Øª
        explanation += f"\n\nÙ‡Ø°Ø§ Ø§Ù„ØªØ´Ø®ÙŠØµ ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ´Ø§Ø±Ø© Ø·Ø¨ÙŠØ¨ Ù…Ø®ØªØµ ÙÙŠ {condition.medical_specialty.value} {condition.urgency_level}/10)."
        
        if condition.urgency_level >= 8:
            explanation += " ÙŠÙˆØµÙ‰ Ø¨Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ù‚Ø³Ù… Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ ÙÙˆØ±Ø§Ù‹."
        elif condition.urgency_level >= 6:
            explanation += " ÙŠÙˆØµÙ‰ Ø¨Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø®Ù„Ø§Ù„ Ø§Ù„Ù€ 24 Ø³Ø§Ø¹Ø© Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©."
        
        explanation += "\n\nÙ…Ù„Ø§Ø­Ø¸Ø©: Ù‡Ø°Ø§ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø¢Ù„ÙŠ ÙˆÙ„Ø§ ÙŠØºÙ†ÙŠ Ø¹Ù† Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø§Ù„Ù…Ø®ØªØµ."
        
        return explanation
    
    def diagnose(self, patient_id: str, symptoms: List[PatientSymptom], 
                consent_data: Dict[str, Any], anonymize_output: bool = True) -> List[DiagnosticResult]:
        """
        ØªØ´Ø®ÙŠØµ Ø§Ù„Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶
        
        Args:
            patient_id: Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø±ÙŠØ¶
            symptoms: Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶
            consent_data: Ø¨ÙŠØ§Ù†Ø§Øª Ù…ÙˆØ§ÙÙ‚Ø© Ø§Ù„Ù…Ø±ÙŠØ¶
            anonymize_output: Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© ÙÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        
        Returns:
            Ù‚Ø§Ø¦Ù…Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ´Ø®ÙŠØµ Ù…Ø±ØªØ¨Ø© Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        """
        start_time = time.time()
        
        # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©
        if not self.privacy_engine.validate_consent(consent_data):
            logger.warning("Patient consent validation failed")
            raise ValueError("Patient consent is invalid or missing")
        
        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶
        if not self._validate_symptoms(symptoms):
            logger.warning("Symptom validation failed")
            raise ValueError("Invalid symptoms provided")
        
        # 3. ØªØ´ÙÙŠØ± Ù‡ÙˆÙŠØ© Ø§Ù„Ù…Ø±ÙŠØ¶
        hashed_patient_id = self.privacy_engine.hash_patient_id(patient_id)
        logger.info(f"Processing diagnosis for patient hash: {hashed_patient_id}")
        
        # 4. Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© ÙÙŠ ÙˆØµÙ Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶
        anonymized_symptoms = []
        for symptom in symptoms:
            anonymized_symptom_name = self.privacy_engine.anonymize_text(symptom.symptom_name)
            anonymized_symptoms.append(PatientSymptom(
                symptom_name=anonymized_symptom_name,
                severity=symptom.severity,
                duration_hours=symptom.duration_hours,
                metadata=symptom.metadata
            ))
        
        # 5. Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø­Ø§Ù„Ø§Øª
        condition_scores = []
        for condition_name, condition in self.knowledge_base.conditions.items():
            match_score, supporting_evidence, contradicting_evidence = self._calculate_condition_match_score(
                anonymized_symptoms, condition
            )
            
            if match_score > 0.1:  # Ø¹ØªØ¨Ø© Ø¯Ù†ÙŠØ§ Ù„Ù„ØªØ´Ø®ÙŠØµ
                condition_scores.append((condition, match_score, supporting_evidence, contradicting_evidence))
        
        # 6. ÙØ±Ø² Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
        condition_scores.sort(key=lambda x: x[1], reverse=True)
        
        # 7. Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ´Ø®ÙŠØµ
        results = []
        for condition, match_score, supporting_evidence, contradicting_evidence in condition_scores[:5]:  # Ø£ÙØ¶Ù„ 5 Ù†ØªØ§Ø¦Ø¬
            confidence_level = self._determine_confidence_level(match_score, len(anonymized_symptoms))
            risk_level = self._determine_risk_level(condition, confidence_level)
            explanation = self._generate_explanation(condition, confidence_level, 
                                                    supporting_evidence, contradicting_evidence)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§
            recommended_actions = []
            if risk_level == DiagnosticRiskLevel.CRITICAL:
                recommended_actions = ["Ø§Ù„Ø°Ù‡Ø§Ø¨ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆØ§Ø±Ø¦ ÙÙˆØ±Ø§Ù‹", "Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø³ÙŠØ§Ø±Ø© Ø§Ù„Ø¥Ø³Ø¹Ø§Ù"]
            elif risk_level == DiagnosticRiskLevel.HIGH:
                recommended_actions = ["Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø®Ù„Ø§Ù„ 24 Ø³Ø§Ø¹Ø©", "ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø¨Ø¯Ù†ÙŠØ© Ø§Ù„Ø´Ø§Ù‚Ø©"]
            else:
                recommended_actions = ["Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ ÙÙŠ Ø£Ù‚Ø±Ø¨ ÙˆÙ‚Øª Ù…Ù…ÙƒÙ†", "Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶"]
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            required_tests = []
            if condition.medical_specialty == MedicalSpecialty.CARDIOLOGY:
                required_tests = ["ØªØ®Ø·ÙŠØ· Ø§Ù„Ù‚Ù„Ø¨", "ØªØ­Ø§Ù„ÙŠÙ„ Ø§Ù„Ø¯Ù… (ÙƒÙˆÙ„ÙŠØ³ØªØ±ÙˆÙ„ØŒ Ø¥Ù†Ø²ÙŠÙ…Ø§Øª Ø§Ù„Ù‚Ù„Ø¨)"]
            elif condition.medical_specialty == MedicalSpecialty.NEUROLOGY:
                required_tests = ["ØªØµÙˆÙŠØ± Ø§Ù„Ø¯Ù…Ø§Øº (CT/MRI)", "ÙØ­Øµ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©"]
            
            result = DiagnosticResult(
                condition=condition,
                confidence_level=confidence_level,
                risk_level=risk_level,
                supporting_evidence=supporting_evidence,
                contradicting_evidence=contradicting_evidence,
                recommended_actions=recommended_actions,
                required_tests=required_tests,
                explanation=explanation,
                ai_model_version=self.model_version
            )
            
            # Ø¥Ø®ÙØ§Ø¡ Ø§Ù„Ù‡ÙˆÙŠØ© ÙÙŠ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¥Ø°Ø§ Ø·Ù„Ø¨
            if anonymize_output:
                result.explanation = self.privacy_engine.anonymize_text(result.explanation)
                result.supporting_evidence = [self.privacy_engine.anonymize_text(e) for e in result.supporting_evidence]
                result.contradicting_evidence = [self.privacy_engine.anonymize_text(e) for e in result.contradicting_evidence]
            
            results.append(result)
        
        processing_time = time.time() - start_time
        logger.info(f"Diagnosis completed in {processing_time:.2f} seconds. Found {len(results)} potential conditions.")
        
        return results
    
    def get_medical_specialist_recommendation(self, results: List[DiagnosticResult]) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ© Ø¨Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        if not results:
            return {"specialty": "general", "priority": "low", "reasoning": "No diagnostic results available"}
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„ØªØ®ØµØµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©
        top_result = results[0]
        specialty = top_result.condition.medical_specialty.value
        priority = top_result.risk_level.value
        
        reasoning = f"Based on the primary diagnosis of {top_result.condition.condition_name} "
        reasoning += f"with {top_result.confidence_level.value} confidence and {priority} risk level."
        
        return {
            "specialty": specialty,
            "priority": priority,
            "reasoning": reasoning,
            "alternative_specialties": list(set([
                result.condition.medical_specialty.value 
                for result in results[1:3]  # Ø§Ù„ØªØ®ØµØµØ§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø© Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ§Ù„ÙŠØ©
            ]))
        }

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
def example_usage():
    """Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ´Ø®ÙŠØµ"""
    # ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø±Ùƒ
    engine = DiagnosticEngine(
        knowledge_base_path="data/medical_knowledge_base.json",
        pii_patterns_path="config/pii_patterns.json"
    )
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ù„Ø£Ø¹Ø±Ø§Ø¶
    patient_id = "P123456"
    symptoms = [
        PatientSymptom(symptom_name="Ø£Ù„Ù… ÙÙŠ Ø§Ù„ØµØ¯Ø±", severity=8, duration_hours=2),
        PatientSymptom(symptom_name="Ø¶ÙŠÙ‚ ÙÙŠ Ø§Ù„ØªÙ†ÙØ³", severity=7, duration_hours=2),
        PatientSymptom(symptom_name="Ø¹Ø±Ù‚ Ø¨Ø§Ø±Ø¯", severity=6, duration_hours=1),
        PatientSymptom(symptom_name="Ø®ÙÙ‚Ø§Ù†", severity=7, duration_hours=2)
    ]
    
    # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©
    consent_data = {
        "patient_id": patient_id,
        "consent_granted": True,
        "consent_date": "2025-12-30T10:00:00Z",
        "consent_version": "1.0"
    }
    
    try:
        # Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ´Ø®ÙŠØµ
        results = engine.diagnose(patient_id, symptoms, consent_data)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ´Ø®ÙŠØµ:")
        print("=" * 50)
        for i, result in enumerate(results, 1):
            print(f"\nØ§Ù„ØªØ´Ø®ÙŠØµ #{i}: {result.condition.condition_name}")
            print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {result.confidence_level.value}")
            print(f"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·ÙˆØ±Ø©: {result.risk_level.value}")
            print(f"Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {result.condition.medical_specialty.value}")
            print("\nØ§Ù„ØªÙØ³ÙŠØ±:")
            print(result.explanation)
            print("-" * 50)
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙˆØµÙŠØ© Ø¨Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø·Ø¨ÙŠ
        specialist_recommendation = engine.get_medical_specialist_recommendation(results)
        print("\nØªÙˆØµÙŠØ© Ø¨Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø·Ø¨ÙŠ:")
        print(f"Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {specialist_recommendation['specialty']}")
        print(f"Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: {specialist_recommendation['priority']}")
        print(f"Ø§Ù„ØªØ®ØµØµØ§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©: {', '.join(specialist_recommendation['alternative_specialties'])}")
    
    except Exception as e:
        logger.error(f"Diagnosis failed: {str(e)}")
        print(f"Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ: {str(e)}")

if __name__ == "__main__":
    example_usage()
```

## ðŸ“ tests/test_full_pipeline.py (Complete)

```python
"""
Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù…Ù† Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø©
Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØºØ·ÙŠ ÙƒØ§Ù…Ù„ Ø³ÙŠØ± Ø§Ù„Ø¹Ù…Ù„ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ØŒ Ù…Ù…Ø§ ÙŠØ¶Ù…Ù† Ø£Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª ØªØ¹Ù…Ù„ Ù…Ø¹Ù‹Ø§ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.
"""

import pytest
import numpy as np
import pandas as pd
import tempfile
import os
import json
import time
from unittest.mock import patch, MagicMock
from pathlib import Path

from src.core.math_operations import dot_product, matrix_multiply, softmax
from src.ml.classical.linear_regression import LinearRegressionScratch
from src.ml.deep_learning.neural_networks import NeuralNetworkFromScratch
from src.production.api import app as api_app
from src.production.monitoring import ModelMonitor
from case_studies.medical_diagnosis_agent.implementation.diagnostic_engine import DiagnosticEngine, PatientSymptom
from case_studies.legal_document_rag_system.implementation.vector_index import VectorIndex

class TestFullPipeline:
    """Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ø£Ù†Ø¨ÙˆØ¨ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„ÙƒØ§Ù…Ù„"""
    
    @pytest.fixture
    def test_data(self):
        """Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±ÙŠØ©"""
        # Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø·ÙŠØ© Ø¨Ø³ÙŠØ·Ø©
        X = np.array([[1], [2], [3], [4], [5]])
        y = np.array([2, 4, 6, 8, 10])  # y = 2x
        
        # Ø¨ÙŠØ§Ù†Ø§Øª ØªØµÙ†ÙŠÙ
        X_class = np.array([[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]])
        y_class = np.array([0, 0, 1, 1, 1])
        
        return {
            'regression': {'X': X, 'y': y},
            'classification': {'X': X_class, 'y': y_class}
        }
    
    @pytest.fixture
    def temp_dir(self):
        """Ø¯Ù„ÙŠÙ„ Ù…Ø¤Ù‚Øª Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        with tempfile.TemporaryDirectory() as tmpdir:
            yield Path(tmpdir)
    
    def test_end_to_end_machine_learning_pipeline(self, test_data, temp_dir):
        """Ø§Ø®ØªØ¨Ø§Ø± ÙƒØ§Ù…Ù„ Ù„Ø£Ù†Ø¨ÙˆØ¨ ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø´Ø±"""
        # 1. ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†Ø­Ø¯Ø§Ø± Ø§Ù„Ø®Ø·ÙŠ
        X_reg, y_reg = test_data['regression']['X'], test_data['regression']['y']
        
        # 1.1. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ù…Ù† Ø§Ù„ØµÙØ±
        model_scratch = LinearRegressionScratch(learning_rate=0.1, n_iterations=1000)
        model_scratch.fit(X_reg, y_reg)
        
        # 1.2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        predictions = model_scratch.predict(X_reg)
        mse = np.mean((predictions - y_reg) ** 2)
        assert mse < 0.1, f"MSE too high: {mse}"
        
        # 1.3. Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        model_path = temp_dir / "linear_model.pkl"
        import joblib
        joblib.dump(model_scratch, model_path)
        assert model_path.exists(), "Model file was not created"
        
        # 2. Ø§Ø®ØªØ¨Ø§Ø± Ø®Ø¯Ù…Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… API
        from fastapi.testclient import TestClient
        
        client = TestClient(api_app)
        
        # 2.1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© Ø§Ù„ØµØ­Ø©
        response = client.get("/health")
        assert response.status_code == 200
        health_data = response.json()
        assert health_data["status"] == "healthy"
        
        # 2.2. Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ ØªÙ†Ø¨Ø¤
        prediction_request = {
            "features": {"feature_0": 6.0},
            "request_id": "test_request_1"
        }
        
        with patch("src.production.api.joblib.load") as mock_load:
            mock_load.return_value = model_scratch
            
            response = client.post("/predict", json=prediction_request)
            assert response.status_code == 200
            
            result = response.json()
            prediction = result["prediction"]
            
            # Ù†ØªÙˆÙ‚Ø¹ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ù‚ÙŠÙ…Ø© Ù‚Ø±ÙŠØ¨Ø© Ù…Ù† 12.0 (y = 2x)
            assert abs(prediction - 12.0) < 1.0, f"Prediction {prediction} is too far from expected 12.0"
    
    def test_neural_network_training_and_inference(self, test_data):
        """Ø§Ø®ØªØ¨Ø§Ø± ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„"""
        X, y = test_data['classification']['X'], test_data['classification']['y']
        
        # ØªØ­ÙˆÙŠÙ„ y Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© (one-hot encoding)
        y_one_hot = np.zeros((len(y), 2))
        y_one_hot[np.arange(len(y)), y] = 1
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ©
        nn = NeuralNetworkFromScratch(
            layer_sizes=[2, 4, 2],
            activation='relu',
            output_activation='softmax'
        )
        
        # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        history = nn.train(
            X, y_one_hot,
            epochs=1000,
            learning_rate=0.1,
            batch_size=32,
            verbose=False
        )
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø®Ø³Ø§Ø±Ø© ØªÙ‚Ù„ Ù…Ø¹ Ø§Ù„ÙˆÙ‚Øª
        assert len(history) > 0
        assert history[-1] < history[0] * 0.1, "Loss did not decrease sufficiently"
        
        # Ø§Ù„ØªÙ†Ø¨Ø¤
        predictions = nn.predict(X)
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø¥Ù„Ù‰ ÙØ¦Ø§Øª
        predicted_classes = np.argmax(predictions, axis=1)
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©
        accuracy = np.mean(predicted_classes == y)
        assert accuracy > 0.9, f"Accuracy too low: {accuracy}"
    
    def test_medical_diagnosis_pipeline(self, temp_dir):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù†Ø¨ÙˆØ¨ ØªØ´Ø®ÙŠØµ Ø·Ø¨ÙŠ ÙƒØ§Ù…Ù„"""
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ø·Ø¨ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        knowledge_base_path = temp_dir / "test_knowledge_base.json"
        
        test_knowledge_base = {
            "conditions": [
                {
                    "name": "Ø§Ø­ØªØ´Ø§Ø¡ Ø¹Ø¶Ù„Ø© Ø§Ù„Ù‚Ù„Ø¨",
                    "specialty": "cardiology",
                    "severity_level": 10,
                    "urgency_level": 10,
                    "treatment_options": ["Ø§Ù„ØªØ¯Ø®Ù„ Ø§Ù„Ø¬Ø±Ø§Ø­ÙŠ", "Ø§Ù„Ø£Ø¯ÙˆÙŠØ©"],
                    "diagnostic_criteria": [
                        "Ø£Ù„Ù… Ø­Ø§Ø¯ ÙÙŠ Ø§Ù„ØµØ¯Ø± ÙŠÙ†ØªØ´Ø± Ø¥Ù„Ù‰ Ø§Ù„Ø°Ø±Ø§Ø¹ Ø§Ù„Ø£ÙŠØ³Ø±",
                        "Ø¶ÙŠÙ‚ ÙÙŠ Ø§Ù„ØªÙ†ÙØ³",
                        "Ø¹Ø±Ù‚ Ø¨Ø§Ø±Ø¯",
                        "Ø®ÙÙ‚Ø§Ù†"
                    ]
                },
                {
                    "name": "Ø§Ù„ØªÙ‡Ø§Ø¨ Ø§Ù„Ù…Ø¹Ø¯Ø©",
                    "specialty": "gastroenterology",
                    "severity_level": 6,
                    "urgency_level": 4,
                    "treatment_options": ["Ù…Ø«Ø¨Ø·Ø§Øª Ø§Ù„Ø­Ù…ÙˆØ¶Ø©", "Ø§Ù„Ù…Ø¶Ø§Ø¯Ø§Øª Ø§Ù„Ø­ÙŠÙˆÙŠØ©"],
                    "diagnostic_criteria": [
                        "Ø£Ù„Ù… ÙÙŠ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¹Ù„ÙˆÙŠ Ù…Ù† Ø§Ù„Ø¨Ø·Ù†",
                        "ØºØ«ÙŠØ§Ù†",
                        "Ù‚ÙŠØ¡",
                        "Ø­Ø±Ù‚Ø© ÙÙŠ Ø§Ù„Ù…Ø¹Ø¯Ø©"
                    ]
                }
            ]
        }
        
        with open(knowledge_base_path, 'w', encoding='utf-8') as f:
            json.dump(test_knowledge_base, f, ensure_ascii=False, indent=2)
        
        # 2. ØªÙ‡ÙŠØ¦Ø© Ù…Ø­Ø±Ùƒ Ø§Ù„ØªØ´Ø®ÙŠØµ
        engine = DiagnosticEngine(str(knowledge_base_path))
        
        # 3. Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ÙŠØ¶ ÙˆØ§Ù„Ø£Ø¹Ø±Ø§Ø¶
        symptoms = [
            PatientSymptom(symptom_name="Ø£Ù„Ù… ÙÙŠ Ø§Ù„ØµØ¯Ø±", severity=8, duration_hours=2),
            PatientSymptom(symptom_name="Ø¶ÙŠÙ‚ ÙÙŠ Ø§Ù„ØªÙ†ÙØ³", severity=7, duration_hours=2),
            PatientSymptom(symptom_name="Ø¹Ø±Ù‚ Ø¨Ø§Ø±Ø¯", severity=6, duration_hours=1)
        ]
        
        consent_data = {
            "patient_id": "TEST123",
            "consent_granted": True,
            "consent_date": "2025-12-30T10:00:00Z",
            "consent_version": "1.0"
        }
        
        # 4. Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØªØ´Ø®ÙŠØµ
        results = engine.diagnose("TEST123", symptoms, consent_data)
        
        # 5. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        assert len(results) > 0, "No diagnostic results returned"
        assert results[0].condition.condition_name == "Ø§Ø­ØªØ´Ø§Ø¡ Ø¹Ø¶Ù„Ø© Ø§Ù„Ù‚Ù„Ø¨", "Incorrect primary diagnosis"
        assert results[0].risk_level.value == "critical", "Risk level should be critical"
        assert results[0].confidence_level.value in ["high", "medium"], "Confidence level should be at least medium"
        
        # 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙˆØµÙŠØ© Ø§Ù„ØªØ®ØµØµ Ø§Ù„Ø·Ø¨ÙŠ
        specialist_recommendation = engine.get_medical_specialist_recommendation(results)
        assert specialist_recommendation["specialty"] == "cardiology", "Recommended specialty should be cardiology"
        assert specialist_recommendation["priority"] == "critical", "Priority should be critical"
    
    def test_legal_document_rag_pipeline(self, temp_dir):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù†Ø¨ÙˆØ¨ RAG Ù„Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©"""
        # 1. Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ«Ø§Ø¦Ù‚ Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø¨Ø³ÙŠØ·Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
        test_doc_path = temp_dir / "test_document.pdf"
        
        # Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù†Ø§ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù PDF Ø­Ù‚ÙŠÙ‚ÙŠ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±ØŒ
        # Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø­Ø§ÙƒØ§Ø© ÙˆØ³Ù†ØªØ¹Ø§Ù…Ù„ ÙƒÙ…Ø§ Ù„Ùˆ Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯
        test_doc_path.touch()
        
        # 2. ØªÙ‡ÙŠØ¦Ø© ÙÙ‡Ø±Ø³ Ø§Ù„Ù…ØªØ¬Ù‡Ø§Øª
        index = VectorIndex(
            embedding_model="sentence-transformers/all-MiniLM-L6-v2",
            dimension=384
        )
        
        # 3. Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³
        with patch.object(index, 'add_documents') as mock_add:
            index.add_documents([str(test_doc_path)])
            mock_add.assert_called_once()
        
        # 4. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
        with patch.object(index, 'hybrid_search') as mock_search:
            mock_search.return_value = [
                MagicMock(score=0.95, chunk=MagicMock(content="Ù‡Ø°Ø§ Ù†Øµ Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ø®ØªØ¨Ø§Ø±ÙŠ", metadata={}))
            ]
            
            results = index.hybrid_search("Ø§Ø³ØªØ´Ø§Ø±Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø­ÙˆÙ„ Ø§Ù„Ø¹Ù‚ÙˆØ¯", top_k=3)
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            assert len(results) == 1
            assert results[0].score > 0.9
            assert "Ù‚Ø§Ù†ÙˆÙ†ÙŠ" in results[0].chunk.content
    
    def test_model_monitoring_pipeline(self, temp_dir):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ù†Ø¨ÙˆØ¨ Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù"""
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø±
        np.random.seed(42)
        X_train = np.random.normal(0, 1, (100, 2))
        y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(int)
        
        X_test = np.random.normal(0, 1, (50, 2))  # ØªÙˆØ²ÙŠØ¹ Ù…Ø´Ø§Ø¨Ù‡
        X_drift = np.random.normal(1, 2, (50, 2))  # ØªÙˆØ²ÙŠØ¹ Ù…Ø®ØªÙ„Ù (Ø§Ù†Ø­Ø±Ø§Ù)
        
        # 2. ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·
        from sklearn.linear_model import LogisticRegression
        
        model = LogisticRegression()
        model.fit(X_train, y_train)
        
        # 3. ØªÙ‡ÙŠØ¦Ø© Ù…Ø±Ø§Ù‚Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
        monitor = ModelMonitor()
        
        # 4. Ø¥Ø¶Ø§ÙØ© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙƒÙ…Ø±Ø¬Ø¹
        monitor.add_reference_data(X_train)
        
        # 5. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø§Ù†Ø­Ø±Ø§Ù ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©
        drift_result = monitor.detect_drift(X_test)
        assert not drift_result.drift_detected, "Drift detected in similar data"
        assert drift_result.p_value > 0.05, f"p-value too low: {drift_result.p_value}"
        
        # 6. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù†Ø­Ø±Ø§Ù ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        drift_result = monitor.detect_drift(X_drift)
        assert drift_result.drift_detected, "No drift detected in drifted data"
        assert drift_result.p_value < 0.05, f"p-value too high: {drift_result.p_value}"
        
        # 7. Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡
        y_pred = model.predict(X_test)
        performance = monitor.track_performance(y_test=np.random.randint(0, 2, 50), y_pred=y_pred)
        
        assert "accuracy" in performance
        assert "precision" in performance
        assert "recall" in performance
    
    def test_production_error_handling(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬"""
        from fastapi.testclient import TestClient
        
        client = TestClient(api_app)
        
        # 1. Ø·Ù„Ø¨ ØºÙŠØ± ØµØ§Ù„Ø­ (Ø¨ÙŠØ§Ù†Ø§Øª Ù†Ø§Ù‚ØµØ©)
        invalid_request = {
            "features": {},  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙŠØ²Ø§Øª
        }
        
        response = client.post("/predict", json=invalid_request)
        assert response.status_code == 422  # Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©
        
        # 2. Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ ÙÙŠ Ø§Ù„Ø®Ø§Ø¯Ù… (Ù†Ù…ÙˆØ°Ø¬ ØºÙŠØ± Ù…Ø­Ù…Ù„)
        with patch("src.production.api.model", None):
            response = client.post("/predict", json={"features": {"feature_1": 1.0}})
            assert response.status_code == 500  # Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ
            
            error_data = response.json()
            assert "detail" in error_data
            assert "Model not loaded" in error_data["detail"]
        
        # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        with patch("src.production.api.logger") as mock_logger:
            response = client.post("/predict", json={"features": {"feature_1": 1.0}})
            mock_logger.error.assert_called()  # ÙŠØ¬Ø¨ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£
    
    def test_system_performance_under_load(self, test_data):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ø¸Ø§Ù… ØªØ­Øª Ø­Ù…Ù„"""
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·
        X, y = test_data['regression']['X'], test_data['regression']['y']
        model = LinearRegressionScratch()
        model.fit(X, y)
        
        # 2. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù„Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        start_time = time.time()
        
        # Ù…Ø­Ø§ÙƒØ§Ø© 1000 Ø·Ù„Ø¨ Ù…ØªØªØ§Ù„ÙŠ
        for i in range(1000):
            features = {"feature_0": float(i % 10)}
            prediction = model.predict(np.array([[features["feature_0"]]]))
        
        total_time = time.time() - start_time
        avg_time_per_prediction = total_time / 1000
        
        assert avg_time_per_prediction < 0.001, f"Average prediction time too high: {avg_time_per_prediction:.6f}s"
        
        # 3. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_usage = process.memory_info().rss / 1024 / 1024  # Ø¨Ø§Ù„Ù…ÙŠØºØ§Ø¨Ø§ÙŠØª
        
        assert memory_usage < 100, f"Memory usage too high: {memory_usage:.2f} MB"

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

## ðŸ“ setup.sh (Complete)

```bash
#!/bin/bash
set -e

# AI Engineer Toolkit 2025 - Setup Script
# Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±Ø¨Øª ÙŠÙ‡ÙŠØ¦ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù€ AI Engineer Toolkit

log() {
    echo -e "\033[1;34m[$(date +'%Y-%m-%d %H:%M:%S')]\033[0m $1"
}

error() {
    echo -e "\033[1;31m[ERROR] $1\033[0m"
    exit 1
}

warning() {
    echo -e "\033[1;33m[WARNING] $1\033[0m"
}

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
check_prerequisites() {
    log "Checking prerequisites..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¨Ø§ÙŠØ«ÙˆÙ†
    if ! command -v python3 &> /dev/null; then
        error "Python 3 is not installed. Please install Python 3.10 or higher."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ pip
    if ! command -v pip3 &> /dev/null; then
        error "pip3 is not installed. Please install pip."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ git
    if ! command -v git &> /dev/null; then
        warning "git is not installed. Some features may not work properly."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Docker
    if ! command -v docker &> /dev/null; then
        warning "Docker is not installed. Containerized execution will not be available."
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ GPU Ù„Ù€ CUDA
    if command -v nvidia-smi &> /dev/null; then
        log "NVIDIA GPU detected. CUDA dependencies will be installed."
        export GPU_AVAILABLE=true
    else
        warning "No NVIDIA GPU detected. PyTorch will be installed in CPU-only mode."
        export GPU_AVAILABLE=false
    fi
}

# Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© conda
setup_conda_environment() {
    log "Setting up conda environment..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ conda
    if ! command -v conda &> /dev/null; then
        error "Conda is not installed. Please install Miniconda or Anaconda first."
    fi
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© conda Ø¬Ø¯ÙŠØ¯Ø©
    ENV_NAME="ai-engineer-toolkit-2025"
    
    # Ø­Ø°Ù Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ÙˆØ¬ÙˆØ¯Ø©
    if conda env list | grep -q "$ENV_NAME"; then
        log "Removing existing conda environment: $ENV_NAME"
        conda env remove -n "$ENV_NAME" -y
    fi
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ¦Ø© Ø¬Ø¯ÙŠØ¯Ø©
    log "Creating new conda environment: $ENV_NAME"
    conda create -n "$ENV_NAME" python=3.10 -y
    
    # ØªÙ†Ø´ÙŠØ· Ø§Ù„Ø¨ÙŠØ¦Ø©
    source "$(conda info --base)/etc/profile.d/conda.sh"
    conda activate "$ENV_NAME"
    
    log "Conda environment created and activated: $ENV_NAME"
}

# ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
install_requirements() {
    log "Installing requirements..."
    
    if [ "$GPU_AVAILABLE" = true ]; then
        log "Installing GPU-enabled PyTorch..."
        pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    else
        log "Installing CPU-only PyTorch..."
        pip install torch torchvision torchaudio
    fi
    
    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
    pip install -r requirements.txt
    
    # ØªØ«Ø¨ÙŠØª jupyterlab ÙˆØ§Ù„Ø¥Ø¶Ø§ÙØ§Øª
    log "Installing JupyterLab and extensions..."
    pip install jupyterlab
    jupyter labextension install @jupyterlab/toc
    jupyter labextension install @jupyter-widgets/jupyterlab-manager
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
    log "Installing monitoring tools..."
    pip install prometheus-client grafana-api
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡ÙŠØ©
    log "Installing vector database tools..."
    pip install faiss-cpu hnswlib pgvector
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠ
    log "Installing generative AI tools..."
    pip install transformers sentence-transformers accelerate
    pip install langchain langchain-community langchain-core
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬
    log "Installing production tools..."
    pip install uvicorn gunicorn fastapi
    pip install redis psycopg2-binary
    
    # ØªØ«Ø¨ÙŠØª Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
    log "Installing testing tools..."
    pip install pytest pytest-asyncio pytest-cov
    
    log "All requirements installed successfully"
}

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
setup_data() {
    log "Setting up sample data..."
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    mkdir -p data/sample_datasets
    
    # ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠØ©
    log "Downloading sample datasets..."
    
    # Iris dataset
    if [ ! -f "data/sample_datasets/iris.csv" ]; then
        curl -L "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" -o "data/sample_datasets/iris.csv"
        echo "sepal_length,sepal_width,petal_length,petal_width,class" | cat - data/sample_datasets/iris.csv > temp && mv temp data/sample_datasets/iris.csv
    fi
    
    # Titanic dataset
    if [ ! -f "data/sample_datasets/titanic.csv" ]; then
        curl -L "https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv" -o "data/sample_datasets/titanic.csv"
    fi
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©
    log "Generating synthetic data..."
    python scripts/data_preprocessing/generate_synthetic_data.py
    
    log "Data setup completed"
}

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
setup_databases() {
    log "Setting up databases..."
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ PostgreSQL
    if command -v docker &> /dev/null && command -v docker-compose &> /dev/null; then
        log "Starting PostgreSQL and Redis containers..."
        docker-compose up -d
        
        # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ØªÙƒÙˆÙ† Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø©
        sleep 10
        
        log "Databases started successfully"
    else
        warning "Docker or docker-compose not available. Skipping database setup."
        warning "Please set up PostgreSQL and Redis manually for full functionality."
    fi
}

# Ø¥Ø¹Ø¯Ø§Ø¯ pre-commit hooks
setup_pre_commit() {
    log "Setting up pre-commit hooks..."
    
    pip install pre-commit
    pre-commit install
    
    log "Pre-commit hooks installed successfully"
}

# Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ«Ø¨ÙŠØª
verify_installation() {
    log "Verifying installation..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ØµØ¯Ø§Ø± Ø¨Ø§ÙŠØ«ÙˆÙ†
    python_version=$(python --version 2>&1)
    log "Python version: $python_version"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥ØµØ¯Ø§Ø± PyTorch
    torch_version=$(python -c "import torch; print(torch.__version__)")
    log "PyTorch version: $torch_version"
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† GPU
    if [ "$GPU_AVAILABLE" = true ]; then
        gpu_info=$(python -c "import torch; print(f'GPU available: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"}')")
        log "$gpu_info"
    fi
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Jupyter
    jupyter_version=$(jupyter --version 2>&1 | head -1)
    log "Jupyter version: $jupyter_version"
    
    log "Installation verification completed"
}

# Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
main() {
    log "Starting AI Engineer Toolkit 2025 setup..."
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
    check_prerequisites
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø¨ÙŠØ¦Ø© conda
    setup_conda_environment
    
    # ØªØ«Ø¨ÙŠØª Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª
    install_requirements
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    setup_data
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    setup_databases
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ pre-commit hooks
    setup_pre_commit
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ«Ø¨ÙŠØª
    verify_installation
    
    log "\n\033[1;32mSetup completed successfully!\033[0m"
    log "To get started:"
    log "1. Activate the conda environment: conda activate ai-engineer-toolkit-2025"
    log "2. Launch Jupyter Lab: ./run.sh"
    log "3. Run tests: make test"
    log "4. Build Docker image: docker build -t ai-engineer-toolkit:latest ."
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù .env Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§
    if [ ! -f ".env" ]; then
        log "Creating .env file with default configuration..."
        cat > .env << EOF
# AI Engineer Toolkit Configuration
ENV=development
PORT=8888
MODEL_PATH=models/default_model.pkl
DATABASE_URL=postgresql://user:password@localhost:5432/ai_toolkit
REDIS_URL=redis://localhost:6379/0
SENTRY_DSN=
# CUDA Configuration
CUDA_VISIBLE_DEVICES=0
EOF
    fi
}

# ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
main "$@"
```

## ðŸ“ docker-compose.yml (Complete)

```yaml
version: '3.8'

services:
  # Ø®Ø¯Ù…Ø© Jupyter Lab Ù„Ù„ØªØ·ÙˆÙŠØ±
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8888:8888"
    volumes:
      - .:/app
      - jupyter_data:/root/.local/share/jupyter
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=ai-engineer-toolkit-2025
      - PYTHONPATH=/app
    depends_on:
      - postgres
      - redis
    command: ["jupyter", "lab", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]

  # Ø®Ø¯Ù…Ø© FastAPI Ù„Ù„Ù†Ù…Ø§Ø°Ø¬
  api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    volumes:
      - .:/app
    environment:
      - PYTHONPATH=/app
      - PORT=8000
      - DATABASE_URL=postgresql://user:password@postgres:5432/ai_toolkit
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - postgres
      - redis
    command: ["uvicorn", "src.production.api:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  # Ø®Ø¯Ù…Ø© PostgreSQL
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: ai_toolkit
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/database/init.sql:/docker-entrypoint-initdb.d/init.sql

  # Ø®Ø¯Ù…Ø© Redis Ù„Ù„ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù…Ø¤Ù‚Øª
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # Ø®Ø¯Ù…Ø© Prometheus Ù„Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  # Ø®Ø¯Ù…Ø© Grafana Ù„Ù„Ø¹Ø±Ø¶
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus

  # Ø®Ø¯Ù…Ø© pgvector Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªØ¬Ù‡Ø©
  pgvector:
    image: pgvector/pgvector:pg15
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: vector_db
    ports:
      - "5433:5432"
    volumes:
      - pgvector_data:/var/lib/postgresql/data

  # Ø®Ø¯Ù…Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø£Ø¯Ø§Ø¡
  benchmark:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/app
    environment:
      - PYTHONPATH=/app
    depends_on:
      - api
      - postgres
    command: ["python", "-m", "pytest", "benchmarks/"]

volumes:
  jupyter_data:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:
  pgvector_data:
```

## ðŸ“ Makefile (Complete)

```makefile
# AI Engineer Toolkit 2025 - Makefile
# Ø£ÙˆØ§Ù…Ø± Ø³Ø±ÙŠØ¹Ø© Ù„Ù„ØªØ·ÙˆÙŠØ± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„Ù†Ø´Ø±

# Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
ENV = development
PYTHON = python3
PIP = pip3
DOCKER = docker
DOCKER_COMPOSE = docker-compose
JUPYTER_PORT = 8888
API_PORT = 8000

# Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
.PHONY: help setup run test clean build deploy

## help: Display available commands
help:
	@echo "AI Engineer Toolkit 2025 - Development Commands"
	@echo "-------------------------------------------------"
	@echo "make setup       - Setup the development environment"
	@echo "make run         - Run Jupyter Lab"
	@echo "make test        - Run all tests"
	@echo "make test-fast   - Run fast tests only"
	@echo "make lint        - Run code linting"
	@echo "make format      - Format code with Black"
	@echo "make docs        - Generate documentation"
	@echo "make build       - Build Docker image"
	@echo "make deploy      - Deploy to production"
	@echo "make clean       - Clean temporary files"
	@echo "make benchmark   - Run performance benchmarks"
	@echo "make monitor     - Start monitoring dashboard"
	@echo "make help        - Show this help message"

## setup: Setup the development environment
setup:
	@echo "Setting up development environment..."
	./setup.sh

## run: Run Jupyter Lab
run: setup
	@echo "Starting Jupyter Lab on http://localhost:$(JUPYTER_PORT)"
	@echo "Access token: ai-engineer-toolkit-2025"
	jupyter lab --port $(JUPYTER_PORT) --ip 0.0.0.0 --no-browser

## test: Run all tests
test: lint
	@echo "Running all tests..."
	pytest tests/ notebooks/ --cov=src --cov-report=html --cov-report=term

## test-fast: Run fast tests only
test-fast:
	@echo "Running fast tests..."
	pytest tests/ -m "not slow"

## lint: Run code linting
lint:
	@echo "Running code linting..."
	flake8 src/ notebooks/ tests/
	mypy src/ notebooks/ tests/
	black --check src/ notebooks/ tests/

## format: Format code with Black
format:
	@echo "Formatting code with Black..."
	black src/ notebooks/ tests/

## docs: Generate documentation
docs:
	@echo "Generating documentation..."
	pdoc src/ --output-dir docs/api --force
	sphinx-build docs/sphinx docs/sphinx/_build

## build: Build Docker image
build:
	@echo "Building Docker image..."
	$(DOCKER) build -t ai-engineer-toolkit:latest .

## deploy: Deploy to production
deploy: build
	@echo "Deploying to production..."
	$(DOCKER) run -d -p 8000:8000 --name ai-engineer-toolkit-production ai-engineer-toolkit:latest

## clean: Clean temporary files
clean:
	@echo "Cleaning temporary files..."
	find . -name "*.pyc" -delete
	find . -name "*.pyo" -delete
	find . -name "__pycache__" -delete
	rm -rf .pytest_cache
	rm -rf htmlcov
	rm -rf docs/api
	rm -rf docs/sphinx/_build
	rm -rf .mypy_cache
	rm -f core
	rm -f *.log
	rm -f *.sqlite
	rm -f *.pkl
	rm -f *.h5
	rm -f *.onnx

## benchmark: Run performance benchmarks
benchmark: setup
	@echo "Running performance benchmarks..."
	python benchmarks/inference_optimization/vllm_vs_tgi.py
	python benchmarks/cost_performance_tradeoffs/model_size_vs_latency.py

## monitor: Start monitoring dashboard
monitor:
	@echo "Starting monitoring dashboard..."
	$(DOCKER_COMPOSE) up -d prometheus grafana
	@echo "Grafana dashboard available at http://localhost:3000"
	@echo "Default credentials: admin/admin"

## up: Start all services
up:
	@echo "Starting all services..."
	$(DOCKER_COMPOSE) up -d

## down: Stop all services
down:
	@echo "Stopping all services..."
	$(DOCKER_COMPOSE) down

# Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
data-download:
	@echo "Downloading sample datasets..."
	python scripts/data_preprocessing/download_sample_datasets.py

data-generate:
	@echo "Generating synthetic data..."
	python scripts/data_preprocessing/generate_synthetic_data.py

# Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ù…Ø§Ø°Ø¬
model-train:
	@echo "Training default model..."
	python scripts/model_training/train_default_model.py

model-evaluate:
	@echo "Evaluating model performance..."
	python scripts/model_training/evaluate_model.py

# Ø§Ù„Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ø´Ø±
deploy-staging:
	@echo "Deploying to staging environment..."
	# ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø± Ù„Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©

deploy-production:
	@echo "Deploying to production environment..."
	# ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø± Ù„Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©

# Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
.DEFAULT_GOAL := help
```

## ðŸ“ CONTRIBUTING.md (Complete)

```markdown
# Contributing to AI Engineer Toolkit 2025

**"Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ Ù„ÙŠØ³Øª Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ¯ØŒ Ø¨Ù„ Ù‡ÙŠ Ø§Ù†Ø¶Ù…Ø§Ù… Ø¥Ù„Ù‰ Ù…Ø¬ØªÙ…Ø¹ Ù…Ù‡Ù†Ø¯Ø³ÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø°ÙŠÙ† ÙŠØ¨Ù†ÙˆÙ† Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„."**

## Our Philosophy

Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹ ÙŠØªØ¨Ø¹ ÙÙ„Ø³ÙØ© "Ø§Ù„ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø¨ÙŠØ¶" (White-box Approach):
- **Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø£ÙˆÙ„Ø§Ù‹**: ÙƒÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ØªØ¨Ø¯Ø£ Ø¨Ø§Ø´ØªÙ‚Ø§Ù‚Ø§Øª Ø±ÙŠØ§Ø¶ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£ÙˆÙ„Ù‰
- **Ø§Ù„ØªÙ†ÙÙŠØ° Ù…Ù† Ø§Ù„ØµÙØ±**: ÙƒÙ„ Ù…ÙÙ‡ÙˆÙ… ÙŠØ¨Ø¯Ø£ Ø¨ØªÙ†ÙÙŠØ° Ù†Ù‚ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NumPy/Python Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
- **Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚ Ø¯Ø§Ø¦Ù…Ø§Ù‹**: ÙƒÙ„ Ù…ÙÙ‡ÙˆÙ… ÙŠØ´Ù…Ù„ Ø§Ø¹ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø´Ø± ÙˆØ§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø© ÙˆØ§Ù„ØªÙƒÙ„ÙØ©
- **Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª**: Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª ÙŠØ³Ø¨Ù‚ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¯ÙˆØ§Øª

## Contribution Guidelines

### 1. Scope of Contributions

Ù†Ø±Ø­Ø¨ Ø¨Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø§Øª ÙÙŠ:

- **Ø§Ù„Ø¯ÙØ§ØªØ± Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©**: Ø¯ÙØ§ØªØ± Jupyter Ø§Ù„ØªÙŠ ØªØªØ¨Ø¹ Ù‡ÙŠÙƒÙ„ "Ø§Ù„Ù†Ø¸Ø±ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ø§Ù„ØªÙ†ÙÙŠØ° Ù…Ù† Ø§Ù„ØµÙØ±ØŒ Ø«Ù… Ø§Ù„Ø¥Ù†ØªØ§Ø¬"
- **Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©**: ÙƒÙˆØ¯ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù†Ø´Ø± Ù…Ø¹ Ù…Ø±Ø§Ù‚Ø¨Ø©ØŒ Ø£Ù…Ø§Ù†ØŒ ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙƒÙ„ÙØ©
- **Ø¯Ø±Ø§Ø³Ø§Øª Ø§Ù„Ø­Ø§Ù„Ø©**: Ø£Ù†Ø¸Ù…Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª ÙˆØ§Ù„ØªÙƒÙ„ÙØ©/Ø§Ù„Ø£Ø¯Ø§Ø¡
- **Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡**: Ù…Ø¹Ø§ÙŠÙŠØ± Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ† Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
- **Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª**: ØªÙˆØ«ÙŠÙ‚ ÙˆØ§Ø¶Ø­ ÙŠØ´Ø±Ø­ "Ù„Ù…Ø§Ø°Ø§" Ù‚Ø¨Ù„ "ÙƒÙŠÙ"

### 2. Quality Standards

ÙƒÙ„ Ù…Ø³Ø§Ù‡Ù…Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¨Ø¹:

#### 2.1 White-Box Approach
- ÙƒÙ„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© ÙŠØ¬Ø¨ Ø£Ù† ØªØ¨Ø¯Ø£ Ø¨Ø§Ø´ØªÙ‚Ø§Ù‚ Ø±ÙŠØ§Ø¶ÙŠ ÙˆØ§Ø¶Ø­
- ÙŠØ¬Ø¨ ØªÙ‚Ø¯ÙŠÙ… ØªÙ†ÙÙŠØ° Ù…Ù† Ø§Ù„ØµÙØ± Ù‚Ø¨Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
- ÙŠØ¬Ø¨ ØªÙˆØ«ÙŠÙ‚ Ø§Ù„Ù…Ù‚Ø§ÙŠØ¶Ø§Øª ÙˆØ§Ù„Ù‚ÙŠÙˆØ¯ ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ©

#### 2.2 Production-Ready Code
- **Type Hints**: Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¯ÙˆØ§Ù„ ÙŠØ¬Ø¨ Ø£Ù† ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙ„Ù…ÙŠØ­Ø§Øª Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
- **Docstrings**: ØªÙˆØ«ÙŠÙ‚ ÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ø¯Ø§Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…Ø· NumPy
- **Testing**: ÙƒÙ„ ÙˆØ­Ø¯Ø© ÙŠØ¬Ø¨ Ø£Ù† ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ØªØºØ·ÙŠ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø­Ø¯ÙˆØ¯
- **Performance**: ÙŠØ¬Ø¨ Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØªØ¶Ù…ÙŠÙ†Ù‡ ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
- **Error Handling**: Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© ÙˆØ¹Ø¯Ù… Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
