<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# ุดุฑุญ ุชูุตููู ูุณูุณูุฉ mini-RAG - ูู ุงูู Notebooks ุฅูู ุงูู Production

## ูุธุฑุฉ ุนุงูุฉ ุนูู ุงููุดุฑูุน

ูุดุฑูุน **mini-RAG** ูู ูุดุฑูุน ุชุนูููู ุดุงูู ููุฏู ูุจูุงุก ุชุทุจูู RAG (Retrieval-Augmented Generation) ุงุญุชุฑุงูู ุฌุงูุฒ ููุฅูุชุงุฌ ูู ุงูุตูุฑ ุญุชู ุงููุดุฑ. ุงููุดุฑูุน ูุชููุฑ ุนูู GitHub ููุญุชูู ุนูู 25 ููุฏูู ุชุนูููู ุจุงููุบุฉ ุงูุนุฑุจูุฉ.[^1_1][^1_2][^1_3]

## ุงูููุฏูููุงุช ุจุงูุชูุตูู:

### **1๏ธโฃ ุงูููุฏูู ุงูุฃูู: About the Course - ูุงุฐุง ูููุงุฐุง** (5 ุฏูุงุฆู)

**ุงููุฏู:** ููุฏูุฉ ุนู ุงูุณูุณูุฉ ูุฃูููุชูุง

**ุงููุญุชูู:**

- **ุงููุดููุฉ ุงูุฃุณุงุณูุฉ:** ุงููุฌูุฉ ุจูู Data Science ู Software Engineering - ูุซูุฑ ูู ุงููุงุณ ูุนุฑููู Machine Learning ููู ูุนูุดูู ูู "ููุงุนุฉ Notebooks" ููุง ูุนุฑููู ููู ูุญูููุง ูุดุงุฑูุนูู ูุชุทุจููุงุช ุญููููุฉ[^1_3]
- **ุงูุญู:** ุจูุงุก ุชุทุจูู RAG ุฎุทูุฉ ุจุฎุทูุฉ ุจุงุณุชุฎุฏุงู Web Framework (FastAPI)
- **ุงูููุฑุฉ:** ุชุญููู ุงูุชุฌุงุฑุจ ูู Jupyter Notebooks ุฅูู Production Application
- **GitHub Structure:** ูู tutorial ูู branch ุฎุงุตุ ูุงูู main branch ููู ุงููุณุฎุฉ ุงูููุงุฆูุฉ[^1_1]

**ูุง ููุฌุฏ ุฃููุงุฏ ูู ูุฐุง ุงูููุฏูู**

***

### **2๏ธโฃ ุงูููุฏูู ุงูุซุงูู: What will we build - ูุงุฐุง ุณูุจูู** (3 ุฏูุงุฆู)

**ุงููุฏู:** ุชูุถูุญ ุงููุดุฑูุน ุงูููุงุฆู

**ุงููุญุชูู:**

- ุดุฑุญ ูููุฐุฌ RAG: ูุธุงู ูุณูุญ ุจุทุฑุญ ุฃุณุฆูุฉ ุนูู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช ูุงูุญุตูู ุนูู ุฅุฌุงุจุงุช ูุฏุนููุฉ ุจุงูุณูุงู
- ุงุณุชุฎุฏุงูุงุช RAG ูู ุงููุคุณุณุงุช ูุงูุดุฑูุงุช
- ุงูุจููุฉ ุงููุนูุงุฑูุฉ ููุชุทุจูู ุงููุณุชูุฏู

**ูุง ููุฌุฏ ุฃููุงุฏ ูู ูุฐุง ุงูููุฏูู**

***

### **3๏ธโฃ ุงูููุฏูู ุงูุซุงูุซ: Setup your tools - ุงูุฃุฏูุงุช ุงูุฃุณุงุณูุฉ** (18 ุฏูููุฉ)

**ุงููุฏู:** ุฅุนุฏุงุฏ ุจูุฆุฉ ุงูุชุทููุฑ

**ุงูุฃุฏูุงุช ุงููุทููุจุฉ:**

- Python 3.10
- MiniConda ูุฅุฏุงุฑุฉ ุงูุจูุฆุงุช
- Visual Studio Code
- Git \& GitHub

**ุงูุฃููุงุฏ:**

```bash
# ุฅูุดุงุก ุจูุฆุฉ conda ุฌุฏูุฏุฉ
conda create -n mini-rag python=3.10

# ุชูุนูู ุงูุจูุฆุฉ
conda activate mini-rag

# ุชุญุณูู command line interface
export PS1="\\[\\033[01;32m\\]\\u@\\h:\\w\\n\\[\\033[00m\\]\\$ "
```

**ูุง ููุฌุฏ branch ุฎุงุต ุจูุฐุง ุงูุฏุฑุณ**

***

### **4๏ธโฃ ุงูููุฏูู ุงูุฑุงุจุน: Project Architecture** (26 ุฏูููุฉ)

**Branch:** `tut-001`

**ุงููุฏู:** ููู ุงูุจููุฉ ุงููุนูุงุฑูุฉ ูููุดุฑูุน

**ุงููุญุชูู:**

- ุดุฑุญ ูุนูุงุฑูุฉ ุงููุดุฑูุน ุงููุงููุฉ
- ุชูุณูู ุงููุดุฑูุน ุฅูู Layers
- ููู Data Flow ุจูู ุงูููููุงุช
- ุดุฑุญ ุฏูุฑ ูู component ูู ุงููุธุงู

**ุงูุจููุฉ ุงูุฃุณุงุณูุฉ:**

```
mini-rag/
โโโ src/           # ุงูููุฏ ุงูุฑุฆูุณู
โโโ docker/        # Docker configurations
โโโ .env.example   # ููู ุงูุจูุฆุฉ ุงูุชุฌุฑูุจู
โโโ requirements.txt
```


***

### **5๏ธโฃ ุงูููุฏูู ุงูุฎุงูุณ: Welcome to FastAPI** (15 ุฏูููุฉ)

**Branch:** `tut-002`

**ุงููุฏู:** ุงูุจุฏุงูุฉ ูุน FastAPI Framework

**ุงูููุงููู:**

- ููุฏูุฉ ุนู FastAPI ูููุณูุชู
- ุฅูุดุงุก ุฃูู API endpoint
- Auto-generated documentation (Swagger UI)
- Type hints ู Pydantic models

**ุงูุฃููุงุฏ ุงูุฃุณุงุณูุฉ:**

```python
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
async def root():
    return {"message": "Welcome to mini-RAG"}

@app.get("/health")
async def health():
    return {"status": "healthy"}
```

**ุชุดุบูู ุงูุชุทุจูู:**

```bash
uvicorn main:app --reload --host 0.0.0.0 --port 5000
```


***

### **6๏ธโฃ ุงูููุฏูู ุงูุณุงุฏุณ: Nested Routes + Env Values** (15 ุฏูููุฉ)

**Branch:** `tut-003`

**ุงููุฏู:** ุชูุธูู ุงูู Routes ูุงุณุชุฎุฏุงู Environment Variables

**ุงูููุงููู:**

- FastAPI Router ูุชูุธูู ุงูู endpoints
- ุฅุฏุงุฑุฉ Environment Variables ุจุงุณุชุฎุฏุงู Pydantic Settings
- ูุตู ุงูู configurations ุนู ุงูููุฏ

**ุงูุฃููุงุฏ:**

```python
# config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    app_name: str = "mini-RAG"
    openai_api_key: str
    
    class Config:
        env_file = ".env"

settings = Settings()

# routes/documents.py
from fastapi import APIRouter

router = APIRouter(prefix="/documents", tags=["documents"])

@router.get("/")
async def list_documents():
    return {"documents": []}

# main.py
from fastapi import FastAPI
from routes import documents

app = FastAPI()
app.include_router(documents.router)
```

**ููู .env:**

```bash
APP_NAME=mini-RAG
OPENAI_API_KEY=your-api-key-here
```


***

### **7๏ธโฃ ุงูููุฏูู ุงูุณุงุจุน: Uploading a File** (ุณุงุนุฉ ู23 ุฏูููุฉ)

**Branch:** `tut-004`

**ุงููุฏู:** ุฑูุน ุงููููุงุช ููุนุงูุฌุชูุง

**ุงูููุงููู:**

- File upload ูู FastAPI
- Validation ูููููุงุช (type, size)
- ุญูุธ ุงููููุงุช ุจุดูู ุขูู
- ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก

**ุงูุฃููุงุฏ:**

```python
from fastapi import UploadFile, File, HTTPException
import aiofiles
import os

UPLOAD_DIR = "uploads"

@router.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    # ุงูุชุญูู ูู ููุน ุงูููู
    allowed_types = ["application/pdf", "text/plain"]
    if file.content_type not in allowed_types:
        raise HTTPException(400, "File type not allowed")
    
    # ุงูุชุญูู ูู ุญุฌู ุงูููู
    file.file.seek(0, 2)
    file_size = file.file.tell()
    if file_size > 10 * 1024 * 1024:  # 10MB
        raise HTTPException(400, "File too large")
    
    file.file.seek(0)
    
    # ุญูุธ ุงูููู
    file_path = os.path.join(UPLOAD_DIR, file.filename)
    async with aiofiles.open(file_path, 'wb') as f:
        content = await file.read()
        await f.write(content)
    
    return {
        "filename": file.filename,
        "size": file_size,
        "content_type": file.content_type
    }
```


***

### **8๏ธโฃ ุงูููุฏูู ุงูุซุงูู: File Processing** (42 ุฏูููุฉ)

**Branch:** `tut-005`

**ุงููุฏู:** ูุนุงูุฌุฉ ุงููููุงุช ูุงุณุชุฎุฑุงุฌ ุงููุตูุต

**ุงูููุงููู:**

- ุงุณุชุฎุฏุงู libraries ูุงุณุชุฎุฑุงุฌ ุงููุตูุต (PyPDF2, python-docx)
- Text chunking strategies
- Metadata extraction

**ุงูุฃููุงุฏ:**

```python
from PyPDF2 import PdfReader
import docx

class FileProcessor:
    def process_pdf(self, file_path: str) -> str:
        reader = PdfReader(file_path)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text
    
    def process_docx(self, file_path: str) -> str:
        doc = docx.Document(file_path)
        text = "\n".join([para.text for para in doc.paragraphs])
        return text
    
    def chunk_text(self, text: str, chunk_size: int = 1000, overlap: int = 200):
        chunks = []
        start = 0
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            chunks.append(chunk)
            start = end - overlap
        return chunks
```


***

### **9๏ธโฃ ุงูููุฏูู ุงูุชุงุณุน: Docker - MongoDB - Motor** (38 ุฏูููุฉ)

**Branch:** `tut-006`

**ุงููุฏู:** ุฅุนุฏุงุฏ ูุงุนุฏุฉ ุจูุงูุงุช MongoDB ุจุงุณุชุฎุฏุงู Docker

**ุงูููุงููู:**

- Docker Compose ููุฎุฏูุงุช
- MongoDB ููุงุนุฏุฉ ุจูุงูุงุช NoSQL
- Motor: async MongoDB driver for Python
- Connection pooling

**ุงูุฃููุงุฏ:**

```yaml
# docker-compose.yml
version: '3.8'
services:
  mongodb:
    image: mongo:latest
    container_name: minirag-mongodb
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    volumes:
      - mongodb_data:/data/db

volumes:
  mongodb_data:
```

```python
# database.py
from motor.motor_asyncio import AsyncIOMotorClient
from config import settings

class Database:
    client: AsyncIOMotorClient = None
    
db = Database()

async def connect_to_mongo():
    db.client = AsyncIOMotorClient(settings.mongodb_url)
    
async def close_mongo_connection():
    db.client.close()

def get_database():
    return db.client[settings.database_name]
```

**ุชุดุบูู Docker:**

```bash
cd docker
cp .env.example .env
sudo docker compose up -d
```


***

### **๐ ุงูููุฏูู ุงูุนุงุดุฑ: Mongo Schemes and Models** (53 ุฏูููุฉ)

**Branch:** `tut-007`

**ุงููุฏู:** ุชุตููู ููุงุฐุฌ ุงูุจูุงูุงุช

**ุงูููุงููู:**

- Pydantic models ููุชุญูู ูู ุงูุจูุงูุงุช
- MongoDB collections design
- CRUD operations
- Data validation

**ุงูุฃููุงุฏ:**

```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime
from bson import ObjectId

class PyObjectId(ObjectId):
    @classmethod
    def __get_validators__(cls):
        yield cls.validate
    
    @classmethod
    def validate(cls, v):
        if not ObjectId.is_valid(v):
            raise ValueError("Invalid objectid")
        return ObjectId(v)

class DocumentModel(BaseModel):
    id: Optional[PyObjectId] = Field(alias="_id")
    filename: str
    content: str
    chunks: list[str]
    metadata: dict
    created_at: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        populate_by_name = True
        json_encoders = {ObjectId: str}

# CRUD operations
class DocumentRepository:
    def __init__(self, database):
        self.collection = database["documents"]
    
    async def create(self, document: DocumentModel):
        result = await self.collection.insert_one(
            document.dict(by_alias=True, exclude={"id"})
        )
        return str(result.inserted_id)
    
    async def get(self, doc_id: str):
        doc = await self.collection.find_one({"_id": ObjectId(doc_id)})
        return DocumentModel(**doc) if doc else None
    
    async def list(self, skip: int = 0, limit: int = 10):
        cursor = self.collection.find().skip(skip).limit(limit)
        return [DocumentModel(**doc) async for doc in cursor]
```


***

### **1๏ธโฃ1๏ธโฃ ุงูููุฏูู ุงูุญุงุฏู ุนุดุฑ: Mongo Indexing** (ุณุงุนุฉ ู13 ุฏูููุฉ)

**Branch:** `tut-008`

**ุงููุฏู:** ุชุญุณูู ุงูุฃุฏุงุก ุจุงุณุชุฎุฏุงู Indexes

**ุงูููุงููู:**

- MongoDB indexing strategies
- Single field indexes
- Compound indexes
- Text indexes ููุจุญุซ
- Performance optimization

**ุงูุฃููุงุฏ:**

```python
async def create_indexes():
    database = get_database()
    
    # Index ุนูู filename
    await database.documents.create_index("filename", unique=True)
    
    # Compound index
    await database.documents.create_index([
        ("created_at", -1),
        ("filename", 1)
    ])
    
    # Text index ููุจุญุซ ูู ุงููุญุชูู
    await database.documents.create_index([
        ("content", "text"),
        ("metadata.title", "text")
    ])
    
    # Index ุนูู metadata fields
    await database.documents.create_index("metadata.category")

# ุงูุจุญุซ ุจุงุณุชุฎุฏุงู text index
async def search_documents(query: str):
    database = get_database()
    cursor = database.documents.find(
        {"$text": {"$search": query}}
    ).limit(10)
    return [doc async for doc in cursor]
```


***

### **1๏ธโฃ2๏ธโฃ ุงูููุฏูู ุงูุซุงูู ุนุดุฑ: Data Pipeline Enhancements** (43 ุฏูููุฉ)

**Branch:** `tut-008`

**ุงููุฏู:** ุชุญุณูู pipeline ูุนุงูุฌุฉ ุงูุจูุงูุงุช

**ุงูููุงููู:**

- Background tasks
- Pipeline stages
- Error handling
- Progress tracking

**ุงูุฃููุงุฏ:**

```python
from fastapi import BackgroundTasks

class DataPipeline:
    def __init__(self, repository: DocumentRepository):
        self.repository = repository
    
    async def process_document(self, file_path:


<div align="center">โ</div>

[^1_1]: https://github.com/bakrianoo/mini-rag
[^1_2]: https://github.com/bakrianoo/mini-rag
[^1_3]: https://www.youtube.com/watch?v=Vv6e2Rb1Q6w&list=PLvLvlVqNQGHCUR2p0b8a0QpVjDUg50wQj

---

# ุดุฑุญ ุชูุตููู ูุฏููู ูุฃูู 10 ููุฏูููุงุช ูู ุณูุณูุฉ mini-RAG

## ๐ ูุธุฑุฉ ุนุงูุฉ ุนูู ุงููุดุฑูุน
**mini-RAG** ูู ูุดุฑูุน ุชุนูููู ุดุงูู ูุจูุงุก ุชุทุจูู RAG (Retrieval-Augmented Generation) ุฌุงูุฒ ููุฅูุชุงุฌุ ููุฏู ูุณุฏ ุงููุฌูุฉ ุจูู Data Science ู Software Engineering.[^2_1][^2_2]

***

## ๐ฌ ุงูููุฏูู ุงูุฃูู: About the Course - ูุงุฐุง ูููุงุฐุง (5:52 ุฏูููุฉ)

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:
- **ุงููุดููุฉ ุงูุฑุฆูุณูุฉ**: ูุซูุฑ ูู ูููุฏุณู ุงูู Data Science ูุนูุดูู ูู "ููุงุนุฉ Jupyter Notebooks" ููุง ูุนุฑููู ููููุฉ ุชุญููู ุชุฌุงุฑุจูู ุฅูู ุชุทุจููุงุช ุญููููุฉ ูุงุจูุฉ ููุงุณุชุฎุฏุงู
- **ุงูุญู**: ุจูุงุก ุชุทุจูู RAG ูุงูู ูู ุงูุจุฏุงูุฉ ููููุงูุฉ ุจุงุณุชุฎุฏุงู Web Framework (FastAPI) ูู Python
- **ุงูููุณูุฉ**: ุฃูุช ูู ุงูููุงูุฉ ูููุฏุณ ุจุฑูุฌูุงุชุ ููุณ ููุท Data Scientist

### ุงููุญุชูู ุงูุฃุณุงุณู:
1. **ููุงุฐุง ูุฐู ุงูุณูุณูุฉุ**
   - ุดุฑูุงุช ูุซูุฑุฉ ุชุฑูุถ ูููุฏุณู ML ุจุณุจุจ ุถุนู ููุงุฑุงุช Software Engineering
   - ูุดุงุฑูุน ูุซูุฑุฉ ุชูุดู ุจุณุจุจ ุถุนู ุงูู Software Engineering ูููุณ ุงูู Machine Learning
   - ุงูุญุงุฌุฉ ูุชุญููู Notebooks ุฅูู Production Applications

2. **ุนู ุงููุดุฑูุน:**
   - ุงุณู ุงููุดุฑูุน: mini-RAG
   - ูุดุฑูุน ููุชูุญ ุงููุตุฏุฑ ุนูู GitHub
   - ูู tutorial ูู branch ุฎุงุต ุจุงูุฃููุงุฏ
   - Main branch ูุญุชูู ุนูู ุงููุณุฎุฉ ุงูููุงุฆูุฉ[^2_1]

3. **ูุง ูู RAGุ**
   - ุชุทุจูู ูุณูุญ ุจุทุฑุญ ุฃุณุฆูุฉ ุนูู ูุฌููุนุฉ ูู ุงููุณุชูุฏุงุช
   - ูุณุชุฎุฑุฌ ุงูุฅุฌุงุจุงุช ูู ุงููุณุชูุฏุงุช ุจุงุณุชุฎุฏุงู LLMs
   - ุฃุตุจุญ ููุน ุชุทุจูู ููุชุดุฑ ุฌุฏูุง ูู ุงููุคุณุณุงุช

### ุงูุจููุฉ ุงูุชูุธูููุฉ ุนูู GitHub:
```

mini-rag/
โโโ main branch (ุงููุณุฎุฉ ุงูููุงุฆูุฉ)
โโโ tut-001 (Tutorial 4: Project Architecture)
โโโ tut-002 (Tutorial 5: Welcome to FastAPI)
โโโ tut-003 (Tutorial 6: Nested Routes)
โโโ ... (ุจุงูู ุงูู tutorials)

```

**ูุง ุชูุฌุฏ ุฃููุงุฏ ูู ูุฐุง ุงูููุฏูู - ููุฏูุฉ ูุธุฑูุฉ**

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู: What will we build - ูุงุฐุง ุณูุจูู (3:37 ุฏูููุฉ)

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:
- ููู ุงููุดุฑูุน ุงูููุงุฆู ุงูุฐู ุณูุจููู
- ุงุณุชูุนุงุจ ููููู RAG applications
- ูุนุฑูุฉ ุงูููููุงุช ุงูุฃุณุงุณูุฉ ููุชุทุจูู

### ูุง ุณูุจููู:
1. **Backend API** ุจุงุณุชุฎุฏุงู FastAPI
2. **Document Upload System** - ุฑูุน ููุนุงูุฌุฉ ุงููุณุชูุฏุงุช
3. **Vector Database** - ูุชุฎุฒูู embeddings
4. **LLM Integration** - ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ
5. **Semantic Search** - ููุจุญุซ ูู ุงููุณุชูุฏุงุช
6. **Production-ready features** - Docker, Database, APIs

### ูุนูุงุฑูุฉ RAG:
```

User Question โ Semantic Search โ Retrieve Context โ LLM โ Augmented Answer
โ
Vector DB
โ
Documents

```

**ูุง ุชูุฌุฏ ุฃููุงุฏ ูู ูุฐุง ุงูููุฏูู - ุดุฑุญ ูุนูุงุฑู**

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูุซ: Setup your tools - ุงูุฃุฏูุงุช ุงูุฃุณุงุณูุฉ (18 ุฏูููุฉ)

### ุงูุฃุฏูุงุช ุงููุทููุจุฉ:
1. **Python 3.10** (ุฃู ุฃุญุฏุซ)
2. **MiniConda** - ูุฅุฏุงุฑุฉ ุงูุจูุฆุงุช
3. **VS Code** - ูุญุฑุฑ ุงูุฃููุงุฏ
4. **Git & GitHub** - ููุชุญูู ุจุงููุณุฎ

### ุงูุฃููุงุฏ ุงูุฃุณุงุณูุฉ:

#### 1. ุชุซุจูุช Python ุจุงุณุชุฎุฏุงู MiniConda:
```bash
# ุชุญููู MiniConda
# ูู ุจุชุญูููู ูู: https://docs.anaconda.com/free/miniconda/

# ุฅูุดุงุก ุจูุฆุฉ ุฌุฏูุฏุฉ
$ conda create -n mini-rag python=3.10

# ุชูุนูู ุงูุจูุฆุฉ
$ conda activate mini-rag

# ุงูุชุญูู ูู ุฅุตุฏุงุฑ Python
$ python --version
```


#### 2. ุชุญุณูู Command Line Interface:

```bash
# ุฅุถุงูุฉ ูุฐุง ุงูุณุทุฑ ูู ~/.bashrc ุฃู ~/.zshrc
export PS1="\[\033[01;32m\]\u@\h:\w\n\[\033[00m\]\$ "

# ุชุทุจูู ุงูุชุบููุฑุงุช
$ source ~/.bashrc
```

ูุฐุง ุงูุฃูุฑ ูุฌุนู ุงูู terminal ุฃูุซุฑ ูุถูุญูุง ุจุฅุธูุงุฑ:

- `\u` = username
- `\h` = hostname
- `\w` = working directory
- `\n` = ุณุทุฑ ุฌุฏูุฏ


#### 3. ุฅุนุฏุงุฏ Git:

```bash
# ุฅุนุฏุงุฏ ุงููุนูููุงุช ุงูุดุฎุตูุฉ
$ git config --global user.name "Your Name"
$ git config --global user.email "your.email@example.com"

# ุงุณุชูุณุงุฎ ุงููุดุฑูุน
$ git clone https://github.com/bakrianoo/mini-rag.git
$ cd mini-rag
```

**ูุง ููุฌุฏ branch ุฎุงุต ุจูุฐุง ุงูุฏุฑุณ**

***

## ๐ฌ ุงูููุฏูู ุงูุฑุงุจุน: Project Architecture (26 ุฏูููุฉ)

**Branch:** `tut-001`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููู ุงูุจููุฉ ุงููุนูุงุฑูุฉ ุงููุงููุฉ ูููุดุฑูุน
- ุชุตููู ุงูู Layers ูุงูููููุงุช
- ููู Data Flow


### ุงูุจููุฉ ุงููุนูุงุฑูุฉ:

```
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ          API Layer (FastAPI)            โ
โ  โโโโโโโโโโโโ  โโโโโโโโโโโโ            โ
โ  โ Upload   โ  โ  Query   โ            โ
โ  โ Endpoint โ  โ Endpoint โ            โ
โ  โโโโโโโโโโโโ  โโโโโโโโโโโโ            โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
                    โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ        Business Logic Layer             โ
โ  โโโโโโโโโโโโโโ  โโโโโโโโโโโโโโโ       โ
โ  โ Document   โ  โ   Search    โ       โ
โ  โ Processor  โ  โ   Engine    โ       โ
โ  โโโโโโโโโโโโโโ  โโโโโโโโโโโโโโโ       โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
                    โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
โ         Data Access Layer               โ
โ  โโโโโโโโโโโโ  โโโโโโโโโโโโโโโโ        โ
โ  โ MongoDB  โ  โ Vector DB    โ        โ
โ  โ (Docs)   โ  โ (Embeddings) โ        โ
โ  โโโโโโโโโโโโ  โโโโโโโโโโโโโโโโ        โ
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ
```


### ูููู ุงููุฌูุฏุงุช ุงูุฃุณุงุณู:

```
mini-rag/
โโโ assets/              # ูููุงุช ุงููุณุงุฆุท ูุงูุตูุฑ
โโโ .env.example         # ูุซุงู ูููู ุงูุจูุฆุฉ
โโโ .gitignore          # ูููุงุช ูุณุชุจุนุฏุฉ ูู Git
โโโ LICENSE             # ุฑุฎุตุฉ ุงููุดุฑูุน (Apache-2.0)
โโโ README.md           # ุชูุซูู ุงููุดุฑูุน
โโโ requirements.txt    # ุงูููุชุจุงุช ุงููุทููุจุฉ
```


### ููู .env.example:

```bash
# Application Settings
APP_NAME=mini-RAG
APP_VERSION=1.0.0
DEBUG=True

# OpenAI API
OPENAI_API_KEY=your-openai-api-key-here

# Database
DATABASE_URL=mongodb://localhost:27017
DATABASE_NAME=minirag

# Server
HOST=0.0.0.0
PORT=5000
```


### ููู .gitignore:

```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/

# Environment
.env

# IDE
.vscode/
.idea/

# OS
.DS_Store
Thumbs.db
```

**ุงูุฃููุงุฏ ูู ูุฐุง Branch:**

- ุฅุนุฏุงุฏ ุงูุจููุฉ ุงูุฃุณุงุณูุฉ ูููุดุฑูุน
- ูููุงุช ุงูุฅุนุฏุงุฏ ุงูุฃูููุฉ

***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ: Welcome to FastAPI (14:55 ุฏูููุฉ)

**Branch:** `tut-002`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููุฏูุฉ ุนู FastAPI ูููุฒุงุชู
- ุฅูุดุงุก ุฃูู API endpoint
- ููู Auto-generated Documentation
- ุงุณุชุฎุฏุงู Type Hints ู Pydantic


### ููุงุฐุง FastAPIุ

1. **ุณุฑูุน ุฌุฏูุง** - ูู ุฃุณุฑุน Python frameworks
2. **Type Safety** - ุงุณุชุฎุฏุงู Type Hints
3. **Auto Documentation** - Swagger UI ุชููุงุฆู
4. **Async Support** - ุฏุนู ุงูุจุฑูุฌุฉ ุบูุฑ ุงููุชุฒุงููุฉ
5. **Modern Python** - ูุจูู ุนูู Python 3.6+

### ุงูุฃููุงุฏ ุงูุฃุณุงุณูุฉ:

#### 1. ููู requirements.txt:

```txt
fastapi==0.95.0
uvicorn[standard]==0.21.1
python-dotenv==1.0.0
```


#### 2. ููู main.py (ุงูุฅุตุฏุงุฑ ุงูุฃูู):

```python
from fastapi import FastAPI

# ุฅูุดุงุก ุชุทุจูู FastAPI
app = FastAPI()

# ุฃูู endpoint - ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ
@app.get("/")
def welcome():
    return {
        "message": "Hello World!"
    }
```


#### 3. ุชุดุบูู ุงูุชุทุจูู:

```bash
# ุชุซุจูุช ุงูููุชุจุงุช
$ pip install -r requirements.txt

# ุชุดุบูู ุงูุฎุงุฏู ูุน auto-reload
$ uvicorn main:app --reload --host 0.0.0.0 --port 5000
```

**Output:**

```
INFO:     Uvicorn running on http://0.0.0.0:5000
INFO:     Application startup complete.
```


#### 4. ุงุฎุชุจุงุฑ ุงูู API:

```bash
# ุจุงุณุชุฎุฏุงู curl
$ curl http://localhost:5000/
{"message":"Hello World!"}

# ุฃู ุงูุชุญ ุงููุชุตูุญ:
http://localhost:5000/
```


#### 5. ุงููุตูู ููู Documentation ุงูุชููุงุฆูุฉ:

```
# Swagger UI
http://localhost:5000/docs

# ReDoc
http://localhost:5000/redoc
```


### ุฅุถุงูุฉ Health Check Endpoint:

```python
from fastapi import FastAPI

app = FastAPI(
    title="mini-RAG API",
    description="A minimal RAG implementation",
    version="1.0.0"
)

@app.get("/")
def welcome():
    return {
        "message": "Welcome to mini-RAG API"
    }

@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "service": "mini-RAG"
    }
```

**ุงูููุฏ ุงูููุงุฆู ูู tut-002:**[^2_3]

```python
def welcome():
    return {
        "message": "Hello World!"
    }
```


***

## ๐ฌ ุงูููุฏูู ุงูุณุงุฏุณ: Nested Routes + Env Values (15 ุฏูููุฉ)

**Branch:** `tut-003`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชูุธูู ุงูู Routes ุจุงุณุชุฎุฏุงู APIRouter
- ุฅุฏุงุฑุฉ Environment Variables
- ุงุณุชุฎุฏุงู Pydantic Settings
- ูุตู ุงูุฅุนุฏุงุฏุงุช ุนู ุงูููุฏ


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง ูุญุชุงุฌ ูู APIRouterุ

- **ุชูุธูู ุงูููุฏ**: ูุตู endpoints ุญุณุจ ุงููุธููุฉ
- **Scalability**: ุณูููุฉ ุฅุถุงูุฉ features ุฌุฏูุฏุฉ
- **Maintainability**: ููุฏ ุฃุณูู ููุตูุงูุฉ
- **Modularity**: ูู module ูุณุชูู


### ุงูุจููุฉ ุงูุฌุฏูุฏุฉ:

```
mini-rag/
โโโ main.py              # ููุทุฉ ุงูุฏุฎูู ุงูุฑุฆูุณูุฉ
โโโ config.py            # ุฅุนุฏุงุฏุงุช ุงูุชุทุจูู
โโโ routes/              # ูุฌูุฏ ุงูู routes
โ   โโโ __init__.py
โ   โโโ documents.py     # routes ุงูุฎุงุตุฉ ุจุงููุณุชูุฏุงุช
โ   โโโ queries.py       # routes ุงูุฎุงุตุฉ ุจุงูุงุณุชุนูุงูุงุช
โโโ .env                 # ุงููุชุบูุฑุงุช ุงูุจูุฆูุฉ
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู config.py - ุฅุฏุงุฑุฉ ุงูุฅุนุฏุงุฏุงุช:

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # ูุนูููุงุช ุงูุชุทุจูู
    app_name: str = "mini-RAG"
    app_version: str = "1.0.0"
    debug: bool = False
    
    # OpenAI API
    openai_api_key: str
    
    # Database
    mongodb_url: str = "mongodb://localhost:27017"
    database_name: str = "minirag"
    
    # Server
    host: str = "0.0.0.0"
    port: int = 5000
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# ุฅ


<div align="center">โ</div>

[^2_1]: https://github.com/bakrianoo/mini-rag
[^2_2]: https://github.com/bakrianoo/mini-rag
[^2_3]: https://github.com/bakrianoo/mini-rag/commit/308e00f0715481c03f9f59a5f0662eea1406e204

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ุงูุฏููู:

***

## ๐ฌ ุงูููุฏูู ุงูุณุงุฏุณ: Nested Routes + Env Values (ุชูููุฉ)

### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู config.py - ุฅุฏุงุฑุฉ ุงูุฅุนุฏุงุฏุงุช (ุชูููุฉ):
```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # ูุนูููุงุช ุงูุชุทุจูู
    app_name: str = "mini-RAG"
    app_version: str = "1.0.0"
    debug: bool = False
    
    # OpenAI API
    openai_api_key: str
    
    # Database
    mongodb_url: str = "mongodb://localhost:27017"
    database_name: str = "minirag"
    
    # Server
    host: str = "0.0.0.0"
    port: int = 5000
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"
        case_sensitive = False

# ุฅูุดุงุก instance ูู Settings
settings = Settings()
```


#### 2. ููู .env:

```bash
APP_NAME=mini-RAG
APP_VERSION=1.0.0
DEBUG=true

OPENAI_API_KEY=sk-your-openai-api-key-here

MONGODB_URL=mongodb://localhost:27017
DATABASE_NAME=minirag

HOST=0.0.0.0
PORT=5000
```


#### 3. ููู routes/documents.py:

```python
from fastapi import APIRouter, HTTPException
from typing import List

# ุฅูุดุงุก router ูููุณุชูุฏุงุช
router = APIRouter(
    prefix="/documents",
    tags=["documents"],
    responses={404: {"description": "Not found"}}
)

@router.get("/")
async def list_documents():
    """
    ุงูุญุตูู ุนูู ูุงุฆูุฉ ุจุฌููุน ุงููุณุชูุฏุงุช
    """
    return {
        "documents": [],
        "total": 0
    }

@router.get("/{document_id}")
async def get_document(document_id: str):
    """
    ุงูุญุตูู ุนูู ูุณุชูุฏ ูุญุฏุฏ ุจูุงุณุทุฉ ID
    """
    return {
        "id": document_id,
        "filename": "example.pdf",
        "status": "processed"
    }

@router.post("/")
async def create_document():
    """
    ุฑูุน ูุณุชูุฏ ุฌุฏูุฏ
    """
    return {
        "message": "Document created",
        "id": "doc_123"
    }
```


#### 4. ููู routes/queries.py:

```python
from fastapi import APIRouter
from pydantic import BaseModel

router = APIRouter(
    prefix="/queries",
    tags=["queries"]
)

class QueryRequest(BaseModel):
    question: str
    top_k: int = 5

@router.post("/search")
async def search(query: QueryRequest):
    """
    ุงูุจุญุซ ูู ุงููุณุชูุฏุงุช
    """
    return {
        "question": query.question,
        "results": [],
        "total": 0
    }

@router.post("/answer")
async def answer(query: QueryRequest):
    """
    ุงูุญุตูู ุนูู ุฅุฌุงุจุฉ ูุงููุฉ
    """
    return {
        "question": query.question,
        "answer": "This is a sample answer",
        "sources": []
    }
```


#### 5. ููู main.py ุงููุญุฏุซ:

```python
from fastapi import FastAPI
from config import settings
from routes import documents, queries

# ุฅูุดุงุก ุงูุชุทุจูู
app = FastAPI(
    title=settings.app_name,
    description="A minimal RAG implementation for production",
    version=settings.app_version,
    debug=settings.debug
)

# ุชุถููู ุงูู routers
app.include_router(documents.router)
app.include_router(queries.router)

@app.get("/")
async def root():
    return {
        "message": f"Welcome to {settings.app_name}",
        "version": settings.app_version,
        "docs": "/docs"
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "service": settings.app_name
    }

# Event handlers
@app.on_event("startup")
async def startup_event():
    print(f"Starting {settings.app_name}...")
    print(f"Debug mode: {settings.debug}")

@app.on_event("shutdown")
async def shutdown_event():
    print("Shutting down...")
```


#### 6. ุชุดุบูู ุงูุชุทุจูู:

```bash
# ุชุดุบูู ุงูุฎุงุฏู
$ uvicorn main:app --reload --host 0.0.0.0 --port 5000

# ุงุฎุชุจุงุฑ ุงูู endpoints
$ curl http://localhost:5000/documents/
$ curl http://localhost:5000/queries/search -X POST \
  -H "Content-Type: application/json" \
  -d '{"question": "What is RAG?"}'
```


***

## ๐ฌ ุงูููุฏูู ุงูุณุงุจุน: Uploading a File (1:23 ุณุงุนุฉ)

**Branch:** `tut-004`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุฑูุน ุงููููุงุช ูู FastAPI
- Validation ูููููุงุช
- ุญูุธ ุงููููุงุช ุจุดูู ุขูู
- ูุนุงูุฌุฉ ุงูุฃุฎุทุงุก ูุงูู exceptions


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง ูุญุชุงุฌ File Uploadุ

- RAG ูุญุชุงุฌ ููุณุชูุฏุงุช ูุชุญููููุง
- ุงููุณุชุฎุฏู ูุฑูุน PDF, Word, Text files
- ูุญุชุงุฌ ูุญูุธ ูุชุชุจุน ุงููููุงุช


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ requirements.txt:

```txt
fastapi==0.95.0
uvicorn[standard]==0.21.1
python-dotenv==1.0.0
python-multipart==0.0.6  # ูุทููุจ ูู file upload
aiofiles==23.1.0          # ูููุชุงุจุฉ async
```


#### 2. ููู models/document.py:

```python
from pydantic import BaseModel, Field
from typing import Optional
from datetime import datetime

class DocumentBase(BaseModel):
    filename: str
    file_size: int
    content_type: str

class DocumentCreate(DocumentBase):
    pass

class DocumentResponse(DocumentBase):
    id: str
    upload_date: datetime
    status: str = "uploaded"
    
    class Config:
        from_attributes = True
```


#### 3. ููู helpers/file_handler.py:

```python
import os
import aiofiles
from typing import BinaryIO
from fastapi import UploadFile, HTTPException
import hashlib
from datetime import datetime

class FileHandler:
    def __init__(self, upload_dir: str = "uploads"):
        self.upload_dir = upload_dir
        self.allowed_extensions = {".pdf", ".txt", ".docx", ".doc"}
        self.max_file_size = 10 * 1024 * 1024  # 10 MB
        
        # ุฅูุดุงุก ุงููุฌูุฏ ุฅุฐุง ูู ููู ููุฌูุฏูุง
        os.makedirs(upload_dir, exist_ok=True)
    
    def validate_file_extension(self, filename: str) -> bool:
        """
        ุงูุชุญูู ูู ุงูุชุฏุงุฏ ุงูููู
        """
        ext = os.path.splitext(filename)[1].lower()
        return ext in self.allowed_extensions
    
    async def validate_file_size(self, file: UploadFile) -> int:
        """
        ุงูุชุญูู ูู ุญุฌู ุงูููู
        """
        # ูุฑุงุกุฉ ุงูููู ููุญุตูู ุนูู ุงูุญุฌู
        file.file.seek(0, 2)  # ุงูุงูุชูุงู ูููุงูุฉ ุงูููู
        file_size = file.file.tell()  # ุงูุญุตูู ุนูู ุงููููุน = ุงูุญุฌู
        file.file.seek(0)  # ุงูุนูุฏุฉ ููุจุฏุงูุฉ
        
        if file_size > self.max_file_size:
            raise HTTPException(
                status_code=413,
                detail=f"File too large. Max size: {self.max_file_size / 1024 / 1024} MB"
            )
        
        return file_size
    
    def generate_unique_filename(self, original_filename: str) -> str:
        """
        ุฅูุดุงุก ุงุณู ููู ูุฑูุฏ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name, ext = os.path.splitext(original_filename)
        # ุฅูุดุงุก hash ูู ุงูุงุณู ุงูุฃุตูู
        hash_obj = hashlib.md5(name.encode())
        hash_str = hash_obj.hexdigest()[:8]
        
        return f"{timestamp}_{hash_str}{ext}"
    
    async def save_file(self, file: UploadFile) -> tuple[str, int]:
        """
        ุญูุธ ุงูููู ูุฅุฑุฌุงุน ุงููุณุงุฑ ูุงูุญุฌู
        """
        # ุงูุชุญูู ูู ุงูุงูุชุฏุงุฏ
        if not self.validate_file_extension(file.filename):
            raise HTTPException(
                status_code=400,
                detail=f"File extension not allowed. Allowed: {self.allowed_extensions}"
            )
        
        # ุงูุชุญูู ูู ุงูุญุฌู
        file_size = await self.validate_file_size(file)
        
        # ุฅูุดุงุก ุงุณู ูุฑูุฏ
        unique_filename = self.generate_unique_filename(file.filename)
        file_path = os.path.join(self.upload_dir, unique_filename)
        
        # ุญูุธ ุงูููู
        try:
            async with aiofiles.open(file_path, 'wb') as f:
                content = await file.read()
                await f.write(content)
        except Exception as e:
            raise HTTPException(
                status_code=500,
                detail=f"Error saving file: {str(e)}"
            )
        
        return file_path, file_size
    
    def delete_file(self, file_path: str) -> bool:
        """
        ุญุฐู ููู
        """
        try:
            if os.path.exists(file_path):
                os.remove(file_path)
                return True
            return False
        except Exception:
            return False
```


#### 4. ุชุญุฏูุซ routes/documents.py:

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, status
from helpers.file_handler import FileHandler
from models.document import DocumentResponse
from typing import List
import uuid
from datetime import datetime

router = APIRouter(
    prefix="/documents",
    tags=["documents"]
)

# ุฅูุดุงุก instance ูู FileHandler
file_handler = FileHandler(upload_dir="uploads")

# ูุงุนุฏุฉ ุจูุงูุงุช ูุคูุชุฉ ูู ุงูุฐุงูุฑุฉ
documents_db = {}

@router.post("/upload", response_model=DocumentResponse, status_code=status.HTTP_201_CREATED)
async def upload_document(file: UploadFile = File(...)):
    """
    ุฑูุน ูุณุชูุฏ ุฌุฏูุฏ
    
    Args:
        file: ุงูููู ุงููุฑุงุฏ ุฑูุนู
    
    Returns:
        ูุนูููุงุช ุงููุณุชูุฏ ุงููุฑููุน
    """
    try:
        # ุญูุธ ุงูููู
        file_path, file_size = await file_handler.save_file(file)
        
        # ุฅูุดุงุก ID ูููุณุชูุฏ
        doc_id = str(uuid.uuid4())
        
        # ุญูุธ ูุนูููุงุช ุงููุณุชูุฏ
        document = {
            "id": doc_id,
            "filename": file.filename,
            "file_size": file_size,
            "content_type": file.content_type,
            "file_path": file_path,
            "upload_date": datetime.now(),
            "status": "uploaded"
        }
        
        documents_db[doc_id] = document
        
        return DocumentResponse(**document)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error uploading file: {str(e)}"
        )

@router.get("/", response_model=List[DocumentResponse])
async def list_documents():
    """
    ุงูุญุตูู ุนูู ูุงุฆูุฉ ุจุฌููุน ุงููุณุชูุฏุงุช
    """
    return [DocumentResponse(**doc) for doc in documents_db.values()]

@router.get("/{document_id}", response_model=DocumentResponse)
async def get_document(document_id: str):
    """
    ุงูุญุตูู ุนูู ูุณุชูุฏ ูุญุฏุฏ
    """
    if document_id not in documents_db:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    return DocumentResponse(**documents_db[document_id])

@router.delete("/{document_id}")
async def delete_document(document_id: str):
    """
    ุญุฐู ูุณุชูุฏ
    """
    if document_id not in documents_db:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    document = documents_db[document_id]
    
    # ุญุฐู ุงูููู ูู ุงููุธุงู
    file_handler.delete_file(document["file_path"])
    
    # ุญุฐู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
    del documents_db[document_id]
    
    return {"message": "Document deleted successfully"}
```


#### 5. ุงุฎุชุจุงุฑ ุงูู Upload:

```bash
# ุฑูุน ููู
$ curl -X POST "http://localhost:5000/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@/path/to/your/file.pdf"

# ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููุณุชูุฏุงุช
$ curl http://localhost:5000/documents/

# ุญุฐู ูุณุชูุฏ
$ curl -X DELETE http://localhost:5000/documents/{document_id}
```


#### 6. ุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู Python:

```python
import requests

# ุฑูุน ููู
url = "http://localhost:5000/documents/upload"
files = {"file": open("example.pdf", "rb")}
response = requests.post(url, files=files)
print(response.json())

# ุงูุญุตูู ุนูู ุงููุงุฆูุฉ
response = requests.get("http://localhost:5000/documents/")
print(response.json())
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู: File Processing (42 ุฏูููุฉ)

**Branch:** `tut-005`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุงุณุชุฎุฑุงุฌ ุงููุตูุต ูู ุงููููุงุช
- Text chunking strategies
- ูุนุงูุฌุฉ ุฃููุงุน ูููุงุช ูุฎุชููุฉ
- Metadata extraction


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง ูุญุชุงุฌ File Processingุ

- ุงุณุชุฎุฑุงุฌ ุงููุต ูู PDF, Word
- ุชูุณูู ุงููุต ูู chunks ุตุบูุฑุฉ
- ุชุญุถูุฑ ุงูุจูุงูุงุช ููู embedding


### ุงูุฃููุงุฏ


---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุจุงูุชูุตูู ุงูุฏููู:

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู: File Processing (ุชูููุฉ)

### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ requirements.txt:

```txt
fastapi==0.95.0
uvicorn[standard]==0.21.1
python-dotenv==1.0.0
python-multipart==0.0.6
aiofiles==23.1.0
PyPDF2==3.0.1          # ููุฑุงุกุฉ PDF
python-docx==0.8.11    # ููุฑุงุกุฉ Word documents
```


#### 2. ููู helpers/text_processor.py:

```python
from PyPDF2 import PdfReader
import docx
from typing import List, Dict
import re

class TextProcessor:
    """
    ูุนุงูุฌ ุงููุตูุต - ูุณุชุฎุฑุฌ ุงููุตูุต ูููุณููุง
    """
    
    def __init__(self):
        self.supported_formats = {'.pdf', '.txt', '.docx', '.doc'}
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """
        ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู PDF
        
        Args:
            file_path: ูุณุงุฑ ููู PDF
            
        Returns:
            ุงููุต ุงููุณุชุฎุฑุฌ
        """
        try:
            reader = PdfReader(file_path)
            text = ""
            
            # ูุฑุงุกุฉ ูู ุตูุญุฉ
            for page_num, page in enumerate(reader.pages, 1):
                page_text = page.extract_text()
                if page_text:
                    text += f"\n--- Page {page_num} ---\n"
                    text += page_text
            
            return text.strip()
        except Exception as e:
            raise Exception(f"Error extracting PDF: {str(e)}")
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """
        ุงุณุชุฎุฑุงุฌ ุงููุต ูู ููู Word
        
        Args:
            file_path: ูุณุงุฑ ููู Word
            
        Returns:
            ุงููุต ุงููุณุชุฎุฑุฌ
        """
        try:
            doc = docx.Document(file_path)
            
            # ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุงูููุฑุงุช
            paragraphs = [para.text for para in doc.paragraphs if para.text.strip()]
            
            # ุงุณุชุฎุฑุงุฌ ุงููุต ูู ุงูุฌุฏุงูู
            table_texts = []
            for table in doc.tables:
                for row in table.rows:
                    row_text = [cell.text for cell in row.cells]
                    table_texts.append(" | ".join(row_text))
            
            # ุฏูุฌ ูู ุงููุตูุต
            all_text = "\n".join(paragraphs)
            if table_texts:
                all_text += "\n\n--- Tables ---\n"
                all_text += "\n".join(table_texts)
            
            return all_text.strip()
        except Exception as e:
            raise Exception(f"Error extracting DOCX: {str(e)}")
    
    def extract_text_from_txt(self, file_path: str) -> str:
        """
        ูุฑุงุกุฉ ููู ูุตู
        
        Args:
            file_path: ูุณุงุฑ ุงูููู ุงููุตู
            
        Returns:
            ูุญุชูู ุงูููู
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except UnicodeDecodeError:
            # ูุญุงููุฉ ูุฑุงุกุฉ ุจุชุฑููุฒ ูุฎุชูู
            with open(file_path, 'r', encoding='latin-1') as f:
                return f.read().strip()
        except Exception as e:
            raise Exception(f"Error reading TXT: {str(e)}")
    
    def extract_text(self, file_path: str, file_extension: str) -> str:
        """
        ุงุณุชุฎุฑุงุฌ ุงููุต ุญุณุจ ููุน ุงูููู
        
        Args:
            file_path: ูุณุงุฑ ุงูููู
            file_extension: ุงูุชุฏุงุฏ ุงูููู
            
        Returns:
            ุงููุต ุงููุณุชุฎุฑุฌ
        """
        ext = file_extension.lower()
        
        if ext == '.pdf':
            return self.extract_text_from_pdf(file_path)
        elif ext in ['.docx', '.doc']:
            return self.extract_text_from_docx(file_path)
        elif ext == '.txt':
            return self.extract_text_from_txt(file_path)
        else:
            raise ValueError(f"Unsupported file format: {ext}")
    
    def clean_text(self, text: str) -> str:
        """
        ุชูุธูู ุงููุต ูู ุงูุฃุญุฑู ุบูุฑ ุงููุฑุบูุจุฉ
        
        Args:
            text: ุงููุต ุงูุฃุตูู
            
        Returns:
            ุงููุต ุงูููุธู
        """
        # ุฅุฒุงูุฉ ุงูุฃุณุทุฑ ุงููุงุฑุบุฉ ุงูุฒุงุฆุฏุฉ
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
        text = re.sub(r' +', ' ', text)
        
        # ุฅุฒุงูุฉ ุงูุฃุญุฑู ุงูุฎุงุตุฉ ุบูุฑ ุงููุฑุบูุจุฉ
        text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        return text.strip()
    
    def chunk_text(
        self, 
        text: str, 
        chunk_size: int = 1000, 
        overlap: int = 200
    ) -> List[Dict[str, any]]:
        """
        ุชูุณูู ุงููุต ุฅูู chunks
        
        Args:
            text: ุงููุต ุงููุฑุงุฏ ุชูุณููู
            chunk_size: ุญุฌู ูู chunk ุจุงูุฃุญุฑู
            overlap: ุงูุชุฏุงุฎู ุจูู chunks
            
        Returns:
            ูุงุฆูุฉ ูู chunks ูุน metadata
        """
        chunks = []
        start = 0
        chunk_id = 0
        
        while start < len(text):
            # ุญุณุงุจ ููุงูุฉ ุงูู chunk
            end = start + chunk_size
            
            # ูุญุงููุฉ ุฅููุงุก ุงูู chunk ุนูุฏ ููุงูุฉ ุฌููุฉ
            if end < len(text):
                # ุงูุจุญุซ ุนู ููุทุฉ ุฃู ุณุทุฑ ุฌุฏูุฏ
                last_period = text.rfind('.', start, end)
                last_newline = text.rfind('\n', start, end)
                
                # ุงุฎุชูุงุฑ ุงูุฃูุฑุจ
                best_end = max(last_period, last_newline)
                if best_end > start:
                    end = best_end + 1
            
            # ุงุณุชุฎุฑุงุฌ ุงูู chunk
            chunk_text = text[start:end].strip()
            
            if chunk_text:
                chunks.append({
                    'chunk_id': chunk_id,
                    'text': chunk_text,
                    'start_char': start,
                    'end_char': end,
                    'length': len(chunk_text)
                })
                chunk_id += 1
            
            # ุงูุงูุชูุงู ููู chunk ุงูุชุงูู ูุน overlap
            start = end - overlap
            
            # ุงูุชุฃูุฏ ูู ุนุฏู ุงูุชูุฑุงุฑ ุงููุงููุงุฆู
            if start >= len(text):
                break
        
        return chunks
    
    def chunk_by_sentences(
        self, 
        text: str, 
        sentences_per_chunk: int = 5
    ) -> List[Dict[str, any]]:
        """
        ุชูุณูู ุงููุต ุญุณุจ ุนุฏุฏ ุงูุฌูู
        
        Args:
            text: ุงููุต ุงููุฑุงุฏ ุชูุณููู
            sentences_per_chunk: ุนุฏุฏ ุงูุฌูู ูู ูู chunk
            
        Returns:
            ูุงุฆูุฉ ูู chunks
        """
        # ุชูุณูู ุงููุต ุฅูู ุฌูู
        sentences = re.split(r'(?<=[.!?])\s+', text)
        
        chunks = []
        chunk_id = 0
        
        for i in range(0, len(sentences), sentences_per_chunk):
            chunk_sentences = sentences[i:i + sentences_per_chunk]
            chunk_text = ' '.join(chunk_sentences).strip()
            
            if chunk_text:
                chunks.append({
                    'chunk_id': chunk_id,
                    'text': chunk_text,
                    'sentence_count': len(chunk_sentences),
                    'length': len(chunk_text)
                })
                chunk_id += 1
        
        return chunks
```


#### 3. ุชุญุฏูุซ routes/documents.py ูุฅุถุงูุฉ Processing:

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, status, BackgroundTasks
from helpers.file_handler import FileHandler
from helpers.text_processor import TextProcessor
from models.document import DocumentResponse
from typing import List
import uuid
from datetime import datetime
import os

router = APIRouter(
    prefix="/documents",
    tags=["documents"]
)

# ุฅูุดุงุก instances
file_handler = FileHandler(upload_dir="uploads")
text_processor = TextProcessor()

# ูุงุนุฏุฉ ุจูุงูุงุช ูุคูุชุฉ
documents_db = {}

async def process_document_background(doc_id: str):
    """
    ูุนุงูุฌุฉ ุงููุณุชูุฏ ูู ุงูุฎูููุฉ
    
    Args:
        doc_id: ูุนุฑู ุงููุณุชูุฏ
    """
    try:
        document = documents_db[doc_id]
        
        # ุชุญุฏูุซ ุงูุญุงูุฉ
        document['status'] = 'processing'
        
        # ุงุณุชุฎุฑุงุฌ ุงููุต
        file_path = document['file_path']
        file_ext = os.path.splitext(document['filename'])[1]
        
        raw_text = text_processor.extract_text(file_path, file_ext)
        
        # ุชูุธูู ุงููุต
        clean_text = text_processor.clean_text(raw_text)
        
        # ุชูุณูู ุงููุต
        chunks = text_processor.chunk_text(
            clean_text,
            chunk_size=1000,
            overlap=200
        )
        
        # ุญูุธ ุงููุชุงุฆุฌ
        document['raw_text'] = raw_text
        document['clean_text'] = clean_text
        document['chunks'] = chunks
        document['chunk_count'] = len(chunks)
        document['status'] = 'processed'
        document['processed_at'] = datetime.now()
        
    except Exception as e:
        document['status'] = 'failed'
        document['error'] = str(e)

@router.post("/upload", response_model=DocumentResponse, status_code=status.HTTP_201_CREATED)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """
    ุฑูุน ููุนุงูุฌุฉ ูุณุชูุฏ
    
    Args:
        background_tasks: ูุชุดุบูู ุงููุนุงูุฌุฉ ูู ุงูุฎูููุฉ
        file: ุงูููู ุงููุฑุงุฏ ุฑูุนู
    
    Returns:
        ูุนูููุงุช ุงููุณุชูุฏ
    """
    try:
        # ุญูุธ ุงูููู
        file_path, file_size = await file_handler.save_file(file)
        
        # ุฅูุดุงุก ุงููุณุชูุฏ
        doc_id = str(uuid.uuid4())
        
        document = {
            "id": doc_id,
            "filename": file.filename,
            "file_size": file_size,
            "content_type": file.content_type,
            "file_path": file_path,
            "upload_date": datetime.now(),
            "status": "uploaded"
        }
        
        documents_db[doc_id] = document
        
        # ุฅุถุงูุฉ ุงููุนุงูุฌุฉ ููุฎูููุฉ
        background_tasks.add_task(process_document_background, doc_id)
        
        return DocumentResponse(**document)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error uploading file: {str(e)}"
        )

@router.post("/{document_id}/process")
async def process_document(document_id: str):
    """
    ูุนุงูุฌุฉ ูุณุชูุฏ ูุฏูููุง
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
    
    Returns:
        ูุชูุฌุฉ ุงููุนุงูุฌุฉ
    """
    if document_id not in documents_db:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    document = documents_db[document_id]
    
    if document['status'] == 'processing':
        return {"message": "Document is already being processed"}
    
    try:
        # ูุนุงูุฌุฉ ูุจุงุดุฑุฉ
        await process_document_background(document_id)
        
        return {
            "message": "Document processed successfully",
            "chunks": len(document.get('chunks', []))
        }
    
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error processing document: {str(e)}"
        )

@router.get("/{document_id}/chunks")
async def get_document_chunks(document_id: str):
    """
    ุงูุญุตูู ุนูู chunks ุงููุณุชูุฏ
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
    
    Returns:
        ูุงุฆูุฉ chunks
    """
    if document_id not in documents_db:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    document = documents_db[document_id]
    
    if document['status'] != 'processed':
        raise HTTPException(
            status_code=400,
            detail=f"Document not processed yet. Status: {document['status']}"
        )
    
    return {
        "document_id": document_id,
        "filename": document['filename'],
        "chunk_count": document['chunk_count'],
        "chunks": document['chunks']
    }

@router.get("/{document_id}/text")
async def get_document_text(document_id: str, clean: bool = True):
    """
    ุงูุญุตูู ุนูู ูุต ุงููุณุชูุฏ
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
        clean: ุฅุฑุฌุงุน ุงููุต ุงูููุธู ุฃู ุงูุฎุงู
    
    Returns:
        ูุต ุงููุณุชูุฏ
    """
    if document_id not in documents_db:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    document = documents_db[document_id]
    
    if document['status'] != 'processed':
        raise HTTPException(
            status_code=400,
            detail="Document not processed yet"
        )
    
    text_key = 'clean_text' if clean else 'raw_text'
    
    return {
        "document_id": document_id,
        "filename": document['filename'],
        "text": document[text_key],
        "length": len(document[text_key])
    }
```


#### 4. ุงุฎุชุจุงุฑ File Processing:

```bash
# ุฑูุน ููู (ุณูุชู

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ุงูุฏููู:

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู: File Processing (ุชูููุฉ)

#### 4. ุงุฎุชุจุงุฑ File Processing:
```bash
# ุฑูุน ููู (ุณูุชู ูุนุงูุฌุชู ุชููุงุฆููุง ูู ุงูุฎูููุฉ)
$ curl -X POST "http://localhost:5000/documents/upload" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@document.pdf"

# Response:
{
  "id": "abc-123",
  "filename": "document.pdf",
  "status": "uploaded",
  ...
}

# ุงูุญุตูู ุนูู chunks
$ curl http://localhost:5000/documents/abc-123/chunks

# ุงูุญุตูู ุนูู ุงููุต
$ curl "http://localhost:5000/documents/abc-123/text?clean=true"
```


#### 5. ูุซุงู ุนูู Chunking Strategy:

```python
# ูุซุงู ุนูู ุงููุต ุงูุฃุตูู
text = """
Machine learning is a subset of artificial intelligence. 
It focuses on developing systems that can learn from data. 
These systems improve their performance over time without being explicitly programmed.
"""

# Chunking ุจุญุฌู 100 ุญุฑู ูุน overlap 20
processor = TextProcessor()
chunks = processor.chunk_text(text, chunk_size=100, overlap=20)

# ุงููุชูุฌุฉ:
# Chunk 0: "Machine learning is a subset of artificial intelligence. It focuses on developing systems"
# Chunk 1: "developing systems that can learn from data. These systems improve their performance"
# Chunk 2: "their performance over time without being explicitly programmed."
```


***

## ๐ฌ ุงูููุฏูู ุงูุชุงุณุน: Docker - MongoDB - Motor (38 ุฏูููุฉ)

**Branch:** `tut-006`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุฅุนุฏุงุฏ MongoDB ุจุงุณุชุฎุฏุงู Docker
- ููู Docker Compose
- ุงุณุชุฎุฏุงู Motor (MongoDB async driver)
- ุฅุฏุงุฑุฉ ุงูุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง MongoDBุ

- **NoSQL Database** - ูุฑููุฉ ูู ุงูุจูุงูุงุช
- **Document-based** - ุชุฎุฒูู JSON-like documents
- **Scalable** - ูุงุจู ููุชูุณุน
- **Perfect for RAG** - ููุงุณุจ ูุชุฎุฒูู ุงููุณุชูุฏุงุช ูุงูู metadata


#### 2. ููุงุฐุง Dockerุ

- **Consistency** - ููุณ ุงูุจูุฆุฉ ููุฌููุน
- **Easy Setup** - ุชุซุจูุช ุณุฑูุน
- **Isolation** - ุนุฒู ุงูุฎุฏูุงุช
- **Production Ready** - ุฌุงูุฒ ููุฅูุชุงุฌ


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู docker/docker-compose.yml:

```yaml
version: '3.8'

services:
  # MongoDB Service
  mongodb:
    image: mongo:6.0
    container_name: minirag-mongodb
    restart: unless-stopped
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_ROOT_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_ROOT_PASSWORD}
      MONGO_INITDB_DATABASE: ${MONGO_DATABASE}
    volumes:
      - mongodb_data:/data/db
      - mongodb_config:/data/configdb
    networks:
      - minirag-network
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5

  # Mongo Express (Web UI ููุชุทููุฑ)
  mongo-express:
    image: mongo-express:1.0.0
    container_name: minirag-mongo-express
    restart: unless-stopped
    ports:
      - "8081:8081"
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: ${MONGO_ROOT_USER}
      ME_CONFIG_MONGODB_ADMINPASSWORD: ${MONGO_ROOT_PASSWORD}
      ME_CONFIG_MONGODB_URL: mongodb://${MONGO_ROOT_USER}:${MONGO_ROOT_PASSWORD}@mongodb:27017/
      ME_CONFIG_BASICAUTH_USERNAME: ${MONGO_EXPRESS_USER}
      ME_CONFIG_BASICAUTH_PASSWORD: ${MONGO_EXPRESS_PASSWORD}
    depends_on:
      - mongodb
    networks:
      - minirag-network

volumes:
  mongodb_data:
    driver: local
  mongodb_config:
    driver: local

networks:
  minirag-network:
    driver: bridge
```


#### 2. ููู docker/.env:

```bash
# MongoDB Configuration
MONGO_ROOT_USER=admin
MONGO_ROOT_PASSWORD=secure_password_123
MONGO_DATABASE=minirag

# Mongo Express (Web UI)
MONGO_EXPRESS_USER=admin
MONGO_EXPRESS_PASSWORD=express_pass_123

# Application Database User (ููุงุณุชุฎุฏุงู ูู ุงูุชุทุจูู)
MONGO_APP_USER=minirag_user
MONGO_APP_PASSWORD=minirag_pass_123
```


#### 3. ุชุดุบูู Docker Compose:

```bash
# ุงูุงูุชูุงู ููุฌูุฏ docker
$ cd docker

# ูุณุฎ ููู ุงูุจูุฆุฉ
$ cp .env.example .env

# ุชุนุฏูู ุงููุชุบูุฑุงุช ูู .env
$ nano .env

# ุจุฏุก ุงูุฎุฏูุงุช
$ docker compose up -d

# ุงูุชุญูู ูู ุงูุญุงูุฉ
$ docker compose ps

# ุนุฑุถ Logs
$ docker compose logs -f mongodb

# ุฅููุงู ุงูุฎุฏูุงุช
$ docker compose down

# ุฅููุงู ูุญุฐู ุงูุจูุงูุงุช
$ docker compose down -v
```


#### 4. ุงููุตูู ูู MongoDB:

```bash
# ุงูุงุชุตุงู ุนุจุฑ MongoDB Shell
$ docker exec -it minirag-mongodb mongosh -u admin -p secure_password_123

# ุงููุตูู ูู Mongo Express
# ุงูุชุญ ุงููุชุตูุญ: http://localhost:8081
# Username: admin
# Password: express_pass_123
```


#### 5. ุชุญุฏูุซ requirements.txt:

```txt
fastapi==0.95.0
uvicorn[standard]==0.21.1
python-dotenv==1.0.0
python-multipart==0.0.6
aiofiles==23.1.0
PyPDF2==3.0.1
python-docx==0.8.11
motor==3.1.1           # MongoDB async driver
pymongo==4.3.3         # MongoDB sync driver (dependency)
```


#### 6. ููู database/connection.py:

```python
from motor.motor_asyncio import AsyncIOMotorClient
from pymongo.errors import ConnectionFailure
from config import settings
import logging

logger = logging.getLogger(__name__)

class Database:
    """
    ูุฏูุฑ ุงุชุตุงู MongoDB
    """
    client: AsyncIOMotorClient = None
    
    @classmethod
    async def connect_to_database(cls):
        """
        ุฅูุดุงุก ุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช
        """
        try:
            logger.info("Connecting to MongoDB...")
            
            # ุฅูุดุงุก ุงูุงุชุตุงู
            cls.client = AsyncIOMotorClient(
                settings.mongodb_url,
                maxPoolSize=10,
                minPoolSize=1,
                serverSelectionTimeoutMS=5000
            )
            
            # ุงูุชุญูู ูู ุงูุงุชุตุงู
            await cls.client.admin.command('ping')
            
            logger.info("Successfully connected to MongoDB")
            
        except ConnectionFailure as e:
            logger.error(f"Failed to connect to MongoDB: {str(e)}")
            raise
    
    @classmethod
    async def close_database_connection(cls):
        """
        ุฅุบูุงู ุงูุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช
        """
        if cls.client:
            logger.info("Closing MongoDB connection...")
            cls.client.close()
            logger.info("MongoDB connection closed")
    
    @classmethod
    def get_database(cls):
        """
        ุงูุญุตูู ุนูู database instance
        
        Returns:
            AsyncIOMotorDatabase
        """
        if not cls.client:
            raise Exception("Database not connected")
        
        return cls.client[settings.database_name]
    
    @classmethod
    def get_collection(cls, collection_name: str):
        """
        ุงูุญุตูู ุนูู collection
        
        Args:
            collection_name: ุงุณู ุงูู collection
            
        Returns:
            AsyncIOMotorCollection
        """
        db = cls.get_database()
        return db[collection_name]

# Instance ุนุงู
db = Database()
```


#### 7. ุชุญุฏูุซ config.py:

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # Application
    app_name: str = "mini-RAG"
    app_version: str = "1.0.0"
    debug: bool = False
    
    # MongoDB
    mongodb_url: str = "mongodb://admin:secure_password_123@localhost:27017"
    database_name: str = "minirag"
    
    # Collections
    documents_collection: str = "documents"
    chunks_collection: str = "chunks"
    
    # Server
    host: str = "0.0.0.0"
    port: int = 5000
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```


#### 8. ุชุญุฏูุซ main.py ูุน Database Events:

```python
from fastapi import FastAPI
from config import settings
from database.connection import db
from routes import documents, queries
import logging

# ุฅุนุฏุงุฏ Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ุฅูุดุงุก ุงูุชุทุจูู
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    debug=settings.debug
)

# ุชุถููู ุงูู routers
app.include_router(documents.router)
app.include_router(queries.router)

@app.on_event("startup")
async def startup_event():
    """
    ุชูููุฐ ุนูุฏ ุจุฏุก ุงูุชุทุจูู
    """
    logger.info(f"Starting {settings.app_name} v{settings.app_version}")
    
    # ุงูุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช
    try:
        await db.connect_to_database()
        logger.info("Database connection established")
    except Exception as e:
        logger.error(f"Failed to connect to database: {str(e)}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """
    ุชูููุฐ ุนูุฏ ุฅููุงู ุงูุชุทุจูู
    """
    logger.info("Shutting down application...")
    
    # ุฅุบูุงู ุงุชุตุงู ูุงุนุฏุฉ ุงูุจูุงูุงุช
    await db.close_database_connection()
    logger.info("Application shutdown complete")

@app.get("/")
async def root():
    return {
        "message": f"Welcome to {settings.app_name}",
        "version": settings.app_version,
        "status": "running"
    }

@app.get("/health")
async def health():
    """
    ูุญุต ุตุญุฉ ุงูุชุทุจูู
    """
    try:
        # ูุญุต ุงุชุตุงู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        await db.client.admin.command('ping')
        db_status = "connected"
    except:
        db_status = "disconnected"
    
    return {
        "status": "healthy",
        "database": db_status,
        "service": settings.app_name
    }
```


#### 9. ุงุฎุชุจุงุฑ ุงูุงุชุตุงู:

```bash
# ุจุฏุก ุงูู Docker services
$ cd docker
$ docker compose up -d

# ุงูุนูุฏุฉ ูููุฌูุฏ ุงูุฑุฆูุณู
$ cd ..

# ุชุดุบูู ุงูุชุทุจูู
$ uvicorn main:app --reload

# ุงุฎุชุจุงุฑ health endpoint
$ curl http://localhost:5000/health

# Response:
{
  "status": "healthy",
  "database": "connected",
  "service": "mini-RAG"
}
```


***

## ๐ฌ ุงูููุฏูู ุงูุนุงุดุฑ: Mongo Schemes and Models (53 ุฏูููุฉ)

**Branch:** `tut-007`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชุตููู MongoDB schemas
- ุงุณุชุฎุฏุงู Pydantic models ููุชุญูู
- ุฅูุดุงุก CRUD operations
- ููู Document-based structure


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. MongoDB Collections ูู ูุดุฑูุนูุง:

- **documents**: ุชุฎุฒูู ูุนูููุงุช ุงููุณุชูุฏุงุช
- **chunks**: ุชุฎุฒูู ูุทุน ุงููุตูุต
- **queries**: ุชุฎุฒูู ุงูุงุณุชุนูุงูุงุช (ุงุฎุชูุงุฑู)


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู models/document.py (ูุญุฏุซ):

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict, Any
from datetime import datetime
from bson import ObjectId

class PyObjectId(ObjectId):
    """
    Custom ObjectId type ูู Pydantic
    """
    @classmethod
    def __get_validators__(cls):
        yield cls.validate
    
    @classmethod
    def validate(cls, v):
        if not ObjectId.is_valid(v):
            raise ValueError("Invalid ObjectId")
        return ObjectId(v)
    
    @classmethod
    def __modify_schema__(cls, field_schema):
        field_schema.update(type="string")

class DocumentBase(BaseModel):
    """
    ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ ูููุณุชูุฏ
    """
    filename: str = Field(..., description="ุงุณู ุงูููู")
    file_size: int = Field(..., description="ุญุฌู ุงูููู ุจุงูุจุงูุช")
    content_type: str = Field(..., description="ููุน ุงูููู")
    file_path: str = Field(..., description="ูุณุงุฑ ุงูููู ุนูู ุงูุณูุฑูุฑ")

class DocumentCreate(DocumentBase):
    """
    ุจูุงูุงุช ุฅูุดุงุก ูุณุชูุฏ ุฌุฏูุฏ
    """
    pass

class DocumentInDB(DocumentBase):
    """
    ุงููุณุชูุฏ ููุง ูู ูุญููุธ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
    """
    id: PyObjectId = Field(default_factory=PyObjectId, alias="_id")
    upload_date: datetime = Field(default_factory=datetime.now)
    status: str = Field(default="uploaded", description="ุญุงูุฉ ุงููุนุงูุฌุฉ")
    raw_text: Optional[str] = None
    clean_text: Optional[str] = None
    chunk_count: Optional

---

# ุฃููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ุงูุฏููู:

***

## ๐ฌ ุงูููุฏูู ุงูุนุงุดุฑ: Mongo Schemes and Models (ุชูููุฉ)

#### 1. ููู models/document.py (ุชูููุฉ):
```python
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, List, Dict, Any
from datetime import datetime
from bson import ObjectId

class PyObjectId(ObjectId):
    """
    Custom ObjectId type ูู Pydantic
    """
    @classmethod
    def __get_validators__(cls):
        yield cls.validate
    
    @classmethod
    def validate(cls, v):
        if not ObjectId.is_valid(v):
            raise ValueError("Invalid ObjectId")
        return ObjectId(v)
    
    @classmethod
    def __get_pydantic_json_schema__(cls, field_schema):
        field_schema.update(type="string")

class DocumentBase(BaseModel):
    """
    ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ ูููุณุชูุฏ
    """
    filename: str = Field(..., description="ุงุณู ุงูููู")
    file_size: int = Field(..., description="ุญุฌู ุงูููู ุจุงูุจุงูุช")
    content_type: str = Field(..., description="ููุน ุงูููู")
    file_path: str = Field(..., description="ูุณุงุฑ ุงูููู")

class DocumentCreate(DocumentBase):
    """
    ุจูุงูุงุช ุฅูุดุงุก ูุณุชูุฏ
    """
    pass

class DocumentInDB(DocumentBase):
    """
    ุงููุณุชูุฏ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
    """
    id: PyObjectId = Field(default_factory=PyObjectId, alias="_id")
    upload_date: datetime = Field(default_factory=datetime.now)
    status: str = Field(default="uploaded")
    raw_text: Optional[str] = None
    clean_text: Optional[str] = None
    chunk_count: Optional[int] = 0
    processed_at: Optional[datetime] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    model_config = ConfigDict(
        populate_by_name=True,
        arbitrary_types_allowed=True,
        json_encoders={ObjectId: str}
    )

class DocumentResponse(BaseModel):
    """
    ุงุณุชุฌุงุจุฉ API ูููุณุชูุฏ
    """
    id: str = Field(..., description="ูุนุฑู ุงููุณุชูุฏ")
    filename: str
    file_size: int
    content_type: str
    upload_date: datetime
    status: str
    chunk_count: Optional[int] = 0
    
    model_config = ConfigDict(
        populate_by_name=True
    )

class DocumentUpdate(BaseModel):
    """
    ุชุญุฏูุซ ุจูุงูุงุช ุงููุณุชูุฏ
    """
    status: Optional[str] = None
    raw_text: Optional[str] = None
    clean_text: Optional[str] = None
    chunk_count: Optional[int] = None
    processed_at: Optional[datetime] = None
    error: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None
```


#### 2. ููู models/chunk.py:

```python
from pydantic import BaseModel, Field, ConfigDict
from typing import Optional, Dict, Any
from datetime import datetime
from bson import ObjectId
from models.document import PyObjectId

class ChunkBase(BaseModel):
    """
    ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ ููู chunk
    """
    document_id: str = Field(..., description="ูุนุฑู ุงููุณุชูุฏ ุงูุฃุตูู")
    chunk_id: int = Field(..., description="ุฑูู ุงูู chunk")
    text: str = Field(..., description="ูุต ุงูู chunk")
    start_char: int = Field(..., description="ูููุน ุงูุจุฏุงูุฉ ูู ุงููุต")
    end_char: int = Field(..., description="ูููุน ุงูููุงูุฉ ูู ุงููุต")
    length: int = Field(..., description="ุทูู ุงููุต")

class ChunkCreate(ChunkBase):
    """
    ุฅูุดุงุก chunk ุฌุฏูุฏ
    """
    pass

class ChunkInDB(ChunkBase):
    """
    ุงูู chunk ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
    """
    id: PyObjectId = Field(default_factory=PyObjectId, alias="_id")
    created_at: datetime = Field(default_factory=datetime.now)
    embedding: Optional[List[float]] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    model_config = ConfigDict(
        populate_by_name=True,
        arbitrary_types_allowed=True,
        json_encoders={ObjectId: str}
    )

class ChunkResponse(BaseModel):
    """
    ุงุณุชุฌุงุจุฉ API ููู chunk
    """
    id: str
    document_id: str
    chunk_id: int
    text: str
    length: int
    has_embedding: bool = False
    
    model_config = ConfigDict(
        populate_by_name=True
    )
```


#### 3. ููู database/repositories/document_repository.py:

```python
from typing import List, Optional, Dict, Any
from bson import ObjectId
from database.connection import db
from models.document import DocumentInDB, DocumentCreate, DocumentUpdate
from config import settings

class DocumentRepository:
    """
    ูุณุชูุฏุน ุงูุนูููุงุช ุนูู ุงููุณุชูุฏุงุช
    """
    
    def __init__(self):
        self.collection_name = settings.documents_collection
    
    @property
    def collection(self):
        """
        ุงูุญุตูู ุนูู ุงูู collection
        """
        return db.get_collection(self.collection_name)
    
    async def create(self, document: DocumentCreate) -> str:
        """
        ุฅูุดุงุก ูุณุชูุฏ ุฌุฏูุฏ
        
        Args:
            document: ุจูุงูุงุช ุงููุณุชูุฏ
            
        Returns:
            ูุนุฑู ุงููุณุชูุฏ ุงููููุดุฃ
        """
        # ุชุญููู Pydantic model ุฅูู dict
        doc_dict = document.model_dump(by_alias=True)
        
        # ุฅุถุงูุฉ ุชุงุฑูุฎ ุงูุฑูุน
        from datetime import datetime
        doc_dict['upload_date'] = datetime.now()
        doc_dict['status'] = 'uploaded'
        doc_dict['metadata'] = {}
        
        # ุงูุฅุฏุฑุงุฌ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        result = await self.collection.insert_one(doc_dict)
        
        return str(result.inserted_id)
    
    async def get_by_id(self, document_id: str) -> Optional[DocumentInDB]:
        """
        ุงูุญุตูู ุนูู ูุณุชูุฏ ุจูุงุณุทุฉ ID
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            ุงููุณุชูุฏ ุฃู None
        """
        if not ObjectId.is_valid(document_id):
            return None
        
        doc = await self.collection.find_one({"_id": ObjectId(document_id)})
        
        if doc:
            return DocumentInDB(**doc)
        
        return None
    
    async def get_all(
        self, 
        skip: int = 0, 
        limit: int = 100,
        status: Optional[str] = None
    ) -> List[DocumentInDB]:
        """
        ุงูุญุตูู ุนูู ุฌููุน ุงููุณุชูุฏุงุช
        
        Args:
            skip: ุนุฏุฏ ุงููุณุชูุฏุงุช ููุชุฎุทู
            limit: ุงูุญุฏ ุงูุฃูุตู ูููุชุงุฆุฌ
            status: ุชุตููุฉ ุญุณุจ ุงูุญุงูุฉ
            
        Returns:
            ูุงุฆูุฉ ุงููุณุชูุฏุงุช
        """
        # ุจูุงุก query
        query = {}
        if status:
            query['status'] = status
        
        # ุงูุจุญุซ
        cursor = self.collection.find(query).skip(skip).limit(limit)
        
        # ุชุญููู ุฅูู list
        documents = []
        async for doc in cursor:
            documents.append(DocumentInDB(**doc))
        
        return documents
    
    async def update(
        self, 
        document_id: str, 
        update_data: DocumentUpdate
    ) -> bool:
        """
        ุชุญุฏูุซ ูุณุชูุฏ
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            update_data: ุงูุจูุงูุงุช ุงููุฑุงุฏ ุชุญุฏูุซูุง
            
        Returns:
            True ุฅุฐุง ุชู ุงูุชุญุฏูุซ
        """
        if not ObjectId.is_valid(document_id):
            return False
        
        # ุฅุฒุงูุฉ ุงูููู None
        update_dict = update_data.model_dump(exclude_unset=True)
        
        if not update_dict:
            return False
        
        # ุงูุชุญุฏูุซ
        result = await self.collection.update_one(
            {"_id": ObjectId(document_id)},
            {"$set": update_dict}
        )
        
        return result.modified_count > 0
    
    async def delete(self, document_id: str) -> bool:
        """
        ุญุฐู ูุณุชูุฏ
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            True ุฅุฐุง ุชู ุงูุญุฐู
        """
        if not ObjectId.is_valid(document_id):
            return False
        
        result = await self.collection.delete_one(
            {"_id": ObjectId(document_id)}
        )
        
        return result.deleted_count > 0
    
    async def count(self, status: Optional[str] = None) -> int:
        """
        ุนุฏ ุงููุณุชูุฏุงุช
        
        Args:
            status: ุชุตููุฉ ุญุณุจ ุงูุญุงูุฉ
            
        Returns:
            ุนุฏุฏ ุงููุณุชูุฏุงุช
        """
        query = {}
        if status:
            query['status'] = status
        
        return await self.collection.count_documents(query)
    
    async def update_status(
        self, 
        document_id: str, 
        status: str,
        error: Optional[str] = None
    ) -> bool:
        """
        ุชุญุฏูุซ ุญุงูุฉ ุงููุณุชูุฏ
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            status: ุงูุญุงูุฉ ุงูุฌุฏูุฏุฉ
            error: ุฑุณุงูุฉ ุงูุฎุทุฃ (ุงุฎุชูุงุฑู)
            
        Returns:
            True ุฅุฐุง ุชู ุงูุชุญุฏูุซ
        """
        if not ObjectId.is_valid(document_id):
            return False
        
        update_dict = {"status": status}
        
        if status == "processed":
            from datetime import datetime
            update_dict["processed_at"] = datetime.now()
        
        if error:
            update_dict["error"] = error
        
        result = await self.collection.update_one(
            {"_id": ObjectId(document_id)},
            {"$set": update_dict}
        )
        
        return result.modified_count > 0
```


#### 4. ููู database/repositories/chunk_repository.py:

```python
from typing import List, Optional
from bson import ObjectId
from database.connection import db
from models.chunk import ChunkInDB, ChunkCreate
from config import settings

class ChunkRepository:
    """
    ูุณุชูุฏุน ุงูุนูููุงุช ุนูู ุงูู chunks
    """
    
    def __init__(self):
        self.collection_name = settings.chunks_collection
    
    @property
    def collection(self):
        return db.get_collection(self.collection_name)
    
    async def create_many(self, chunks: List[ChunkCreate]) -> List[str]:
        """
        ุฅูุดุงุก ุนุฏุฉ chunks ุฏูุนุฉ ูุงุญุฏุฉ
        
        Args:
            chunks: ูุงุฆูุฉ ุงูู chunks
            
        Returns:
            ูุงุฆูุฉ ูุนุฑูุงุช ุงูู chunks
        """
        # ุชุญููู ุฅูู dicts
        chunks_dict = [chunk.model_dump(by_alias=True) for chunk in chunks]
        
        # ุฅุถุงูุฉ timestamp
        from datetime import datetime
        for chunk in chunks_dict:
            chunk['created_at'] = datetime.now()
            chunk['metadata'] = {}
        
        # ุงูุฅุฏุฑุงุฌ
        result = await self.collection.insert_many(chunks_dict)
        
        return [str(id) for id in result.inserted_ids]
    
    async def get_by_document_id(
        self, 
        document_id: str
    ) -> List[ChunkInDB]:
        """
        ุงูุญุตูู ุนูู chunks ูุณุชูุฏ ูุนูู
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            ูุงุฆูุฉ ุงูู chunks
        """
        cursor = self.collection.find(
            {"document_id": document_id}
        ).sort("chunk_id", 1)
        
        chunks = []
        async for chunk in cursor:
            chunks.append(ChunkInDB(**chunk))
        
        return chunks
    
    async def get_by_id(self, chunk_id: str) -> Optional[ChunkInDB]:
        """
        ุงูุญุตูู ุนูู chunk ุจูุงุณุทุฉ ID
        """
        if not ObjectId.is_valid(chunk_id):
            return None
        
        chunk = await self.collection.find_one({"_id": ObjectId(chunk_id)})
        
        if chunk:
            return ChunkInDB(**chunk)
        
        return None
    
    async def update_embedding(
        self, 
        chunk_id: str, 
        embedding: List[float]
    ) -> bool:
        """
        ุชุญุฏูุซ embedding ููู chunk
        
        Args:
            chunk_id: ูุนุฑู ุงูู chunk
            embedding: vector ุงูู embedding
            
        Returns:
            True ุฅุฐุง ุชู ุงูุชุญุฏูุซ
        """
        if not ObjectId.is_valid(chunk_id):
            return False
        
        result = await self.collection.update_one(
            {"_id": ObjectId(chunk_id)},
            {"$set": {"embedding": embedding}}
        )
        
        return result.modified_count > 0
    
    async def delete_by_document_id(self, document_id: str) -> int:
        """
        ุญุฐู ุฌููุน chunks ูุณุชูุฏ ูุนูู
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            ุนุฏุฏ ุงูู chunks ุงููุญุฐููุฉ
        """
        result = await self.collection.delete_many(
            {"document_id": document_id}
        )
        
        return result.deleted_count
    
    async def count_by_document(self, document_id: str) -> int:
        """
        ุนุฏ chunks ูุณุชูุฏ ูุนูู
        """
        return await self.collection.count_documents(
            {"document_id": document_id}
        )
```


#### 5. ุชุญุฏูุซ routes/documents.py ูุน MongoDB:

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, status, BackgroundTasks
from typing import List, Optional
from helpers.file_handler import FileHandler
from helpers.text_processor import TextProcessor
from models.document import DocumentCreate, DocumentResponse, DocumentInDB
from models.chunk import ChunkCreate
from database.repositories.document_repository import DocumentRepository
from database.repositories.chunk_repository import ChunkRepository
import os

router = APIRouter(

---

# ุฃููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ุงูุฏููู:

***

## ๐ฌ ุงูููุฏูู ุงูุนุงุดุฑ: Mongo Schemes and Models (ุชูููุฉ)

#### 5. ุชุญุฏูุซ routes/documents.py ูุน MongoDB:
```python
from fastapi import APIRouter, UploadFile, File, HTTPException, status, BackgroundTasks, Query
from typing import List, Optional
from helpers.file_handler import FileHandler
from helpers.text_processor import TextProcessor
from models.document import DocumentCreate, DocumentResponse, DocumentInDB, DocumentUpdate
from models.chunk import ChunkCreate
from database.repositories.document_repository import DocumentRepository
from database.repositories.chunk_repository import ChunkRepository
import os

router = APIRouter(
    prefix="/documents",
    tags=["documents"]
)

# ุฅูุดุงุก instances
file_handler = FileHandler(upload_dir="uploads")
text_processor = TextProcessor()
doc_repo = DocumentRepository()
chunk_repo = ChunkRepository()

async def process_document_background(document_id: str):
    """
    ูุนุงูุฌุฉ ุงููุณุชูุฏ ูู ุงูุฎูููุฉ
    """
    try:
        # ุงูุญุตูู ุนูู ุงููุณุชูุฏ
        document = await doc_repo.get_by_id(document_id)
        if not document:
            return
        
        # ุชุญุฏูุซ ุงูุญุงูุฉ ุฅูู processing
        await doc_repo.update_status(document_id, "processing")
        
        # ุงุณุชุฎุฑุงุฌ ุงููุต
        file_ext = os.path.splitext(document.filename)[1]
        raw_text = text_processor.extract_text(document.file_path, file_ext)
        
        # ุชูุธูู ุงููุต
        clean_text = text_processor.clean_text(raw_text)
        
        # ุชูุณูู ุงููุต
        chunks_data = text_processor.chunk_text(
            clean_text,
            chunk_size=1000,
            overlap=200
        )
        
        # ุญูุธ ุงูู chunks ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        chunks_to_create = []
        for chunk_data in chunks_data:
            chunk = ChunkCreate(
                document_id=document_id,
                chunk_id=chunk_data['chunk_id'],
                text=chunk_data['text'],
                start_char=chunk_data['start_char'],
                end_char=chunk_data['end_char'],
                length=chunk_data['length']
            )
            chunks_to_create.append(chunk)
        
        # ุฅุฏุฑุงุฌ ุงูู chunks
        if chunks_to_create:
            await chunk_repo.create_many(chunks_to_create)
        
        # ุชุญุฏูุซ ุงููุณุชูุฏ
        update_data = DocumentUpdate(
            status="processed",
            raw_text=raw_text,
            clean_text=clean_text,
            chunk_count=len(chunks_data)
        )
        await doc_repo.update(document_id, update_data)
        
    except Exception as e:
        # ูู ุญุงูุฉ ุงูุฎุทุฃ
        await doc_repo.update_status(document_id, "failed", error=str(e))

@router.post("/upload", response_model=DocumentResponse, status_code=status.HTTP_201_CREATED)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...)
):
    """
    ุฑูุน ูุณุชูุฏ ุฌุฏูุฏ
    
    Args:
        background_tasks: ูุชุดุบูู ุงููุนุงูุฌุฉ ูู ุงูุฎูููุฉ
        file: ุงูููู ุงููุฑุงุฏ ุฑูุนู
    
    Returns:
        ูุนูููุงุช ุงููุณุชูุฏ ุงููุฑููุน
    """
    try:
        # ุญูุธ ุงูููู
        file_path, file_size = await file_handler.save_file(file)
        
        # ุฅูุดุงุก document model
        document_create = DocumentCreate(
            filename=file.filename,
            file_size=file_size,
            content_type=file.content_type or "application/octet-stream",
            file_path=file_path
        )
        
        # ุญูุธ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        document_id = await doc_repo.create(document_create)
        
        # ุฅุถุงูุฉ ุงููุนุงูุฌุฉ ููุฎูููุฉ
        background_tasks.add_task(process_document_background, document_id)
        
        # ุงูุญุตูู ุนูู ุงููุณุชูุฏ ูุฅุฑุฌุงุนู
        document = await doc_repo.get_by_id(document_id)
        
        return DocumentResponse(
            id=str(document.id),
            filename=document.filename,
            file_size=document.file_size,
            content_type=document.content_type,
            upload_date=document.upload_date,
            status=document.status,
            chunk_count=document.chunk_count or 0
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error uploading file: {str(e)}"
        )

@router.get("/", response_model=List[DocumentResponse])
async def list_documents(
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100),
    status: Optional[str] = None
):
    """
    ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููุณุชูุฏุงุช
    
    Args:
        skip: ุนุฏุฏ ุงูุณุฌูุงุช ููุชุฎุทู
        limit: ุงูุญุฏ ุงูุฃูุตู ูููุชุงุฆุฌ
        status: ุชุตููุฉ ุญุณุจ ุงูุญุงูุฉ
    
    Returns:
        ูุงุฆูุฉ ุงููุณุชูุฏุงุช
    """
    documents = await doc_repo.get_all(skip=skip, limit=limit, status=status)
    
    return [
        DocumentResponse(
            id=str(doc.id),
            filename=doc.filename,
            file_size=doc.file_size,
            content_type=doc.content_type,
            upload_date=doc.upload_date,
            status=doc.status,
            chunk_count=doc.chunk_count or 0
        )
        for doc in documents
    ]

@router.get("/{document_id}", response_model=DocumentResponse)
async def get_document(document_id: str):
    """
    ุงูุญุตูู ุนูู ูุณุชูุฏ ูุญุฏุฏ
    """
    document = await doc_repo.get_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    return DocumentResponse(
        id=str(document.id),
        filename=document.filename,
        file_size=document.file_size,
        content_type=document.content_type,
        upload_date=document.upload_date,
        status=document.status,
        chunk_count=document.chunk_count or 0
    )

@router.delete("/{document_id}")
async def delete_document(document_id: str):
    """
    ุญุฐู ูุณุชูุฏ
    """
    # ุงูุญุตูู ุนูู ุงููุณุชูุฏ
    document = await doc_repo.get_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    # ุญุฐู ุงูููู ูู ุงููุธุงู
    file_handler.delete_file(document.file_path)
    
    # ุญุฐู ุงูู chunks
    await chunk_repo.delete_by_document_id(document_id)
    
    # ุญุฐู ุงููุณุชูุฏ
    deleted = await doc_repo.delete(document_id)
    
    if not deleted:
        raise HTTPException(
            status_code=500,
            detail="Failed to delete document"
        )
    
    return {"message": "Document deleted successfully"}

@router.get("/{document_id}/chunks")
async def get_document_chunks(document_id: str):
    """
    ุงูุญุตูู ุนูู chunks ุงููุณุชูุฏ
    """
    # ุงูุชุญูู ูู ูุฌูุฏ ุงููุณุชูุฏ
    document = await doc_repo.get_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    if document.status != "processed":
        raise HTTPException(
            status_code=400,
            detail=f"Document not processed yet. Status: {document.status}"
        )
    
    # ุงูุญุตูู ุนูู ุงูู chunks
    chunks = await chunk_repo.get_by_document_id(document_id)
    
    return {
        "document_id": document_id,
        "filename": document.filename,
        "chunk_count": len(chunks),
        "chunks": [
            {
                "id": str(chunk.id),
                "chunk_id": chunk.chunk_id,
                "text": chunk.text,
                "length": chunk.length,
                "has_embedding": chunk.embedding is not None
            }
            for chunk in chunks
        ]
    }

@router.get("/{document_id}/text")
async def get_document_text(document_id: str, clean: bool = True):
    """
    ุงูุญุตูู ุนูู ูุต ุงููุณุชูุฏ
    """
    document = await doc_repo.get_by_id(document_id)
    
    if not document:
        raise HTTPException(
            status_code=404,
            detail="Document not found"
        )
    
    if document.status != "processed":
        raise HTTPException(
            status_code=400,
            detail="Document not processed yet"
        )
    
    text = document.clean_text if clean else document.raw_text
    
    if not text:
        raise HTTPException(
            status_code=404,
            detail="Text not found"
        )
    
    return {
        "document_id": document_id,
        "filename": document.filename,
        "text": text,
        "length": len(text),
        "type": "clean" if clean else "raw"
    }

@router.get("/stats/summary")
async def get_statistics():
    """
    ุงูุญุตูู ุนูู ุฅุญุตุงุฆูุงุช ุงููุณุชูุฏุงุช
    """
    total = await doc_repo.count()
    uploaded = await doc_repo.count(status="uploaded")
    processing = await doc_repo.count(status="processing")
    processed = await doc_repo.count(status="processed")
    failed = await doc_repo.count(status="failed")
    
    return {
        "total_documents": total,
        "uploaded": uploaded,
        "processing": processing,
        "processed": processed,
        "failed": failed
    }
```


#### 6. ุฅูุดุงุก Indexes ููุฃุฏุงุก:

```python
# ููู database/indexes.py
from database.connection import db
from config import settings

async def create_indexes():
    """
    ุฅูุดุงุก indexes ูุชุญุณูู ุงูุฃุฏุงุก
    """
    # Documents collection indexes
    documents_collection = db.get_collection(settings.documents_collection)
    
    # Index ุนูู filename
    await documents_collection.create_index("filename")
    
    # Index ุนูู status
    await documents_collection.create_index("status")
    
    # Index ุนูู upload_date (ุชูุงุฒูู)
    await documents_collection.create_index([("upload_date", -1)])
    
    # Compound index
    await documents_collection.create_index([
        ("status", 1),
        ("upload_date", -1)
    ])
    
    # Chunks collection indexes
    chunks_collection = db.get_collection(settings.chunks_collection)
    
    # Index ุนูู document_id
    await chunks_collection.create_index("document_id")
    
    # Compound index ุนูู document_id ู chunk_id
    await chunks_collection.create_index([
        ("document_id", 1),
        ("chunk_id", 1)
    ], unique=True)
    
    print("โ Database indexes created successfully")
```


#### 7. ุชุญุฏูุซ main.py ูุฅูุดุงุก Indexes:

```python
from fastapi import FastAPI
from config import settings
from database.connection import db
from database.indexes import create_indexes
from routes import documents, queries
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version
)

app.include_router(documents.router)
app.include_router(queries.router)

@app.on_event("startup")
async def startup_event():
    """
    ุชูููุฐ ุนูุฏ ุจุฏุก ุงูุชุทุจูู
    """
    logger.info(f"๐ Starting {settings.app_name} v{settings.app_version}")
    
    try:
        # ุงูุงุชุตุงู ุจูุงุนุฏุฉ ุงูุจูุงูุงุช
        await db.connect_to_database()
        logger.info("โ Database connected")
        
        # ุฅูุดุงุก indexes
        await create_indexes()
        
    except Exception as e:
        logger.error(f"โ Startup failed: {str(e)}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """
    ุชูููุฐ ุนูุฏ ุฅููุงู ุงูุชุทุจูู
    """
    logger.info("๐ Shutting down application...")
    await db.close_database_connection()
    logger.info("โ Application shutdown complete")

@app.get("/")
async def root():
    return {
        "message": f"Welcome to {settings.app_name}",
        "version": settings.app_version,
        "docs": "/docs"
    }
```


#### 8. ุงุฎุชุจุงุฑ ูุงูู:

```bash
# 1. ุชุดุบูู Docker
$ cd docker && docker compose up -d

# 2. ุชุดุบูู ุงูุชุทุจูู
$ cd .. && uvicorn main:app --reload

# 3. ุฑูุน ูุณุชูุฏ
$ curl -X POST "http://localhost:5000/documents/upload" \
  -F "file=@test.pdf"

# 4. ูุงุฆูุฉ ุงููุณุชูุฏุงุช
$ curl "http://localhost:5000/documents/"

# 5. ุงูุญุตูู ุนูู chunks
$ curl "http://localhost:5000/documents/{doc_id}/chunks"

# 6. ุฅุญุตุงุฆูุงุช
$ curl "http://localhost:5000/documents/stats/summary"
```


***

## ๐ ููุฎุต ุดุงูู ูุฃูู 10 ููุฏูููุงุช

### **ุงูููุฏูููุงุช ุงููุธุฑูุฉ (1-3):**

| ุงูููุฏูู | ุงููุฏุฉ | ุงููุญุชูู ุงูุฑุฆูุณู |
| :-- | :-- | :-- |
| **1. About the Course** | 5:52 | ุงููุดููุฉ: ุงููุฌูุฉ ุจูู Data Science ู Software Engineering<br>ุงูุญู: ุจูุงุก RAG application ูุงูู<br>ุงูููุณูุฉ: ุฃูุช ูููุฏุณ ุจุฑูุฌูุงุช ุฃููุงู |
| **2. What will we build** | 3:37 | ุงูู |


---

# ุงููู

ุณุฃููู ุงูููุฎุต ุงูุดุงูู:

***

## ๐ ููุฎุต ุดุงูู ูุฃูู 10 ููุฏูููุงุช

### **ุงูููุฏูููุงุช ุงููุธุฑูุฉ (1-3):**

| ุงูููุฏูู | ุงููุฏุฉ | ุงููุญุชูู ุงูุฑุฆูุณู |
| :-- | :-- | :-- |
| **1. About the Course** | 5:52 | ุงููุดููุฉ: ุงููุฌูุฉ ุจูู Data Science ู Software Engineering<br>ุงูุญู: ุจูุงุก RAG application ูุงูู<br>ุงูููุณูุฉ: ุฃูุช ูููุฏุณ ุจุฑูุฌูุงุช ุฃููุงู |
| **2. What will we build** | 3:37 | ุงููุนูุงุฑูุฉ ุงููุงููุฉ ูู RAG<br>ุงููุฑุงุญู: Upload โ Process โ Index โ Search โ Answer<br>4 APIs ุฑุฆูุณูุฉ |
| **3. Setup Tools** | 18:00 | Python 3.10 + MiniConda<br>VS Code + Git<br>ุชุญุณูู Terminal |

### **ุงูููุฏูููุงุช ุงูุชุทุจูููุฉ (4-10):**

#### **๐ฆ ุงููุฑุญูุฉ ุงูุฃููู: Project Foundation (4-6)**

**ุงูููุฏูู 4: Project Architecture (26 ุฏูููุฉ)**

- โ Branch: `tut-001`
- ๐ฏ ุงูุจููุฉ ุงููุนูุงุฑูุฉ ุงููุงููุฉ
- ๐ ูููู ุงููุดุฑูุน ุงูุฃุณุงุณู
- โ๏ธ ูููุงุช ุงูุฅุนุฏุงุฏ (.env, .gitignore, requirements.txt)

**ุงูููุฏูู 5: Welcome to FastAPI (15 ุฏูููุฉ)**

- โ Branch: `tut-002`
- ๐ฏ ุฃูู API endpoint
- ๐ Auto-documentation (Swagger UI)
- ๐ง Type hints \& Pydantic

```python
# ุงูููุฏ ุงูุฃุณุงุณู
from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def welcome():
    return {"message": "Hello World!"}
```

**ุงูููุฏูู 6: Nested Routes + Env Values (15 ุฏูููุฉ)**

- โ Branch: `tut-003`
- ๐ฏ ุชูุธูู Routes ุจุงุณุชุฎุฏุงู APIRouter
- ๐ ุฅุฏุงุฑุฉ Environment Variables
- ๐ ูุตู ุงูุฅุนุฏุงุฏุงุช ุนู ุงูููุฏ

```python
# config.py
class Settings(BaseSettings):
    app_name: str = "mini-RAG"
    openai_api_key: str
    mongodb_url: str
    
    class Config:
        env_file = ".env"

settings = Settings()
```


***

#### **๐ค ุงููุฑุญูุฉ ุงูุซุงููุฉ: File Handling (7-8)**

**ุงูููุฏูู 7: Uploading a File (1:23 ุณุงุนุฉ)**

- โ Branch: `tut-004`
- ๐ฏ File upload ูู FastAPI
- โ๏ธ Validation (type, size)
- ๐พ ุญูุธ ุขูู ูููููุงุช
- โ๏ธ Error handling

```python
# ุงููุธุงุฆู ุงูุฑุฆูุณูุฉ
- validate_file_extension()
- validate_file_size()
- save_file()
- generate_unique_filename()
```

**ุงูููุฏูู 8: File Processing (42 ุฏูููุฉ)**

- โ Branch: `tut-005`
- ๐ฏ ุงุณุชุฎุฑุงุฌ ุงููุตูุต (PDF, DOCX, TXT)
- ๐ช Text chunking strategies
- ๐งน ุชูุธูู ุงููุตูุต
- ๐ Metadata extraction

```python
# Text Chunking
chunks = text_processor.chunk_text(
    text,
    chunk_size=1000,
    overlap=200
)
# ุงููุชูุฌุฉ: ูุงุฆูุฉ ูู chunks ูุน metadata
```


***

#### **๐๏ธ ุงููุฑุญูุฉ ุงูุซุงูุซุฉ: Database Integration (9-10)**

**ุงูููุฏูู 9: Docker - MongoDB - Motor (38 ุฏูููุฉ)**

- โ Branch: `tut-006`
- ๐ณ Docker Compose setup
- ๐ MongoDB configuration
- โก Motor (async driver)
- ๐ Connection management

```yaml
# docker-compose.yml
services:
  mongodb:
    image: mongo:6.0
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
```

**ุงูููุฏูู 10: Mongo Schemes and Models (53 ุฏูููุฉ)**

- โ Branch: `tut-007`
- ๐ Pydantic models ููุชุญูู
- ๐๏ธ Collections design (documents, chunks)
- ๐ CRUD operations
- ๐ Database indexes ููุฃุฏุงุก

```python
# Document Model
class DocumentInDB(BaseModel):
    id: PyObjectId
    filename: str
    status: str
    chunks: List[Dict]
    
# Repository Pattern
doc_repo = DocumentRepository()
doc_id = await doc_repo.create(document)
```


***

## ๐ฏ ุงููุชุงุฆุฌ ุงูููุงุฆูุฉ ุจุนุฏ 10 ููุฏูููุงุช

### **ูุง ุชู ุฅูุฌุงุฒู:**

โ **Backend API ูุงูู** ุจุงุณุชุฎุฏุงู FastAPI
โ **File Upload System** ูุน validation
โ **Text Extraction** ูู PDF, DOCX, TXT
โ **Text Chunking** ุจุทุฑู ูุชูุฏูุฉ
โ **MongoDB Integration** ูุน Docker
โ **Repository Pattern** ููุชุนุงูู ูุน ุงูุจูุงูุงุช
โ **Background Processing** ูููุนุงูุฌุฉ ุบูุฑ ุงููุชุฒุงููุฉ
โ **Error Handling** ุดุงูู
โ **Auto Documentation** (Swagger UI)

### **ุงูุจููุฉ ุงูููุงุฆูุฉ ูููุดุฑูุน:**

```
mini-rag/
โโโ main.py                          # ููุทุฉ ุงูุฏุฎูู
โโโ config.py                        # ุงูุฅุนุฏุงุฏุงุช
โโโ requirements.txt                 # ุงูููุชุจุงุช
โโโ .env                            # ุงููุชุบูุฑุงุช ุงูุจูุฆูุฉ
โโโ database/
โ   โโโ connection.py               # ุฅุฏุงุฑุฉ ุงูุงุชุตุงู
โ   โโโ indexes.py                  # Database indexes
โ   โโโ repositories/
โ       โโโ document_repository.py  # CRUD ูููุณุชูุฏุงุช
โ       โโโ chunk_repository.py     # CRUD ููู chunks
โโโ models/
โ   โโโ document.py                 # Document models
โ   โโโ chunk.py                    # Chunk models
โโโ routes/
โ   โโโ documents.py                # Document endpoints
โ   โโโ queries.py                  # Query endpoints
โโโ helpers/
โ   โโโ file_handler.py            # ูุนุงูุฌุฉ ุงููููุงุช
โ   โโโ text_processor.py          # ูุนุงูุฌุฉ ุงููุตูุต
โโโ docker/
โ   โโโ docker-compose.yml         # Docker services
โ   โโโ .env                       # Docker environment
โโโ uploads/                        # ูุฌูุฏ ุงููููุงุช ุงููุฑููุนุฉ
```


### **ุงูู APIs ุงููุชุงุญุฉ:**

```bash
# Document Management
POST   /documents/upload           # ุฑูุน ูุณุชูุฏ
GET    /documents/                 # ูุงุฆูุฉ ุงููุณุชูุฏุงุช
GET    /documents/{id}             # ูุณุชูุฏ ูุญุฏุฏ
DELETE /documents/{id}             # ุญุฐู ูุณุชูุฏ
GET    /documents/{id}/chunks      # chunks ุงููุณุชูุฏ
GET    /documents/{id}/text        # ูุต ุงููุณุชูุฏ
GET    /documents/stats/summary    # ุฅุญุตุงุฆูุงุช

# System
GET    /                           # ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ
GET    /health                     # ูุญุต ุงูุตุญุฉ
GET    /docs                       # Swagger UI
```


### **ุงูุชูููุงุช ุงููุณุชุฎุฏูุฉ:**

| ุงูุชูููุฉ | ุงูุงุณุชุฎุฏุงู |
| :-- | :-- |
| **FastAPI** | Web Framework |
| **Uvicorn** | ASGI Server |
| **MongoDB** | Database |
| **Motor** | Async MongoDB Driver |
| **Pydantic** | Data Validation |
| **PyPDF2** | PDF Processing |
| **python-docx** | Word Processing |
| **Docker** | Containerization |
| **aiofiles** | Async File I/O |


***

## ๐ ุงูุฎุทูุงุช ุงููุงุฏูุฉ (ุงูููุฏูููุงุช 11-25)

### **ุงููุฑุญูุฉ ุงููุงุฏูุฉ: RAG Core Features**

- **11. Mongo Indexing** - ุชุญุณูู ุฃุฏุงุก ุงูุจุญุซ
- **12. Data Pipeline** - ุชุญุณููุงุช ุนูู ุงููุนุงูุฌุฉ
- **13. Checkpoint-1** - ูุฑุงุฌุนุฉ ูุง ุชู ุฅูุฌุงุฒู
- **14. LLM Factory** - ุฏูุฌ Large Language Models
- **15. Vector DB** - QDrant ููู embeddings
- **16. Semantic Search** - ุงูุจุญุซ ุงูุฏูุงูู
- **17. Augmented Answers** - ุชูููุฏ ุงูุฅุฌุงุจุงุช
- **18-19. Local LLM** - Ollama ููุชุดุบูู ุงููุญูู
- **20-21. PostgreSQL + PGVector** - ุงูุงูุชูุงู ููุงุนุฏุฉ ุจูุงูุงุช ุฃููู
- **22-23. Deployment** - ูุดุฑ ุงูุชุทุจูู
- **24-25. Celery** - ูุนุงูุฌุฉ ุงูููุงู ุงูุซูููุฉ

***

## ๐ก ูุตุงุฆุญ ูููุชุงุจุนุฉ

### **ููุชุทุจูู ุงูุนููู:**

1. โ๏ธ ุงูุชุจ ุงูููุฏ ุจููุณู - ูุง ุชูุณุฎ ููุท
2. ๐ ุฌุฑุจ ูู endpoint ูู Swagger UI
3. ๐ ุชุนูุฏ ูุณุฑ ุงูููุฏ ูููู ุงูุฃุฎุทุงุก
4. ๐ ุฃุถู logging ูุชุชุจุน ุงูุชูููุฐ
5. ๐งช ุงุฎุชุจุฑ ุจูููุงุช ูุฎุชููุฉ (PDF, Word, Text)

### **ููููู ุงูุนููู:**

- ๐ ุงูุฑุฃ FastAPI documentation
- ๐ ุชุนูู MongoDB queries
- ๐ณ ุงููู Docker networking
- ๐ ุฏุฑุณ async/await ูู Python
- ๐ฏ ุงููู Repository Pattern

***

## ๐ ุงูุฎูุงุตุฉ

ุจุนุฏ ุฃูู 10 ููุฏูููุงุชุ ุฃุตุจุญ ูุฏูู:

- โ **Foundation ูุงูู** ูุชุทุจูู Production-ready
- โ **ููู ุนููู** ูู Web Development ูุน Python
- โ **ุฎุจุฑุฉ ุนูููุฉ** ูู Database Integration
- โ **ููุงุฑุงุช** ูู File Processing ู Text Analysis
- โ **ูุงุนุฏุฉ ูููุฉ** ูุจูุงุก RAG application ูุชูุฏู

**ุงูู 15 ููุฏูู ุงููุงุฏูุฉ** ุณุชุจูู ุนูู ูุฐุง ุงูุฃุณุงุณ ูุฅุถุงูุฉ:

- ๐ค ุงูุฐูุงุก ุงูุงุตุทูุงุนู (LLM Integration)
- ๐ ุงูุจุญุซ ุงูุฏูุงูู (Semantic Search)
- ๐ Vector Databases
- ๐ Deployment ู Production Features

**ุงุณุชูุฑ ูู ุงูุชุนูู ูุงูุชุทุจูู!** ๐ช

---

# ุดุฑุญ ุชูุตููู ูุฏููู ููููุฏูููุงุช 11-15 ูู ุณูุณูุฉ mini-RAG


***

## ๐ฌ ุงูููุฏูู ุงูุญุงุฏู ุนุดุฑ: Mongo Indexing (1:13 ุณุงุนุฉ)

**Branch:** `tut-008`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููู ุฃูููุฉ Indexes ูู MongoDB
- ุฃููุงุน Indexes ุงููุฎุชููุฉ
- ุชุญุณูู ุฃุฏุงุก ุงูุงุณุชุนูุงูุงุช
- Query optimization strategies


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง ูุญุชุงุฌ Indexesุ

- **ุงูุณุฑุนุฉ**: ุชุญุณูู ุณุฑุนุฉ ุงูุจุญุซ ูู O(n) ุฅูู O(log n)
- **Efficiency**: ุชูููู ุงุณุชููุงู ุงูููุงุฑุฏ
- **Scalability**: ุฃุฏุงุก ุฃูุถู ูุน ุฒูุงุฏุฉ ุงูุจูุงูุงุช
- **Production Ready**: ุถุฑูุฑู ูุฃู ุชุทุจูู ุฅูุชุงุฌู


#### 2. ุฃููุงุน Indexes ูู MongoDB:

1. **Single Field Index** - ุนูู ุญูู ูุงุญุฏ
2. **Compound Index** - ุนูู ุนุฏุฉ ุญููู
3. **Text Index** - ููุจุญุซ ุงููุตู
4. **Geospatial Index** - ููุจูุงูุงุช ุงูุฌุบุฑุงููุฉ
5. **Unique Index** - ูุถูุงู ุนุฏู ุงูุชูุฑุงุฑ

### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู database/indexes.py (ูุญุฏุซ ูุดุงูู):

```python
from database.connection import db
from config import settings
import logging

logger = logging.getLogger(__name__)

async def create_indexes():
    """
    ุฅูุดุงุก ุฌููุน ุงูู indexes ูููุดุฑูุน
    """
    logger.info("๐ง Creating database indexes...")
    
    try:
        # ========== Documents Collection ==========
        await create_documents_indexes()
        
        # ========== Chunks Collection ==========
        await create_chunks_indexes()
        
        logger.info("โ All indexes created successfully")
        
    except Exception as e:
        logger.error(f"โ Error creating indexes: {str(e)}")
        raise

async def create_documents_indexes():
    """
    ุฅูุดุงุก indexes ูู documents collection
    """
    collection = db.get_collection(settings.documents_collection)
    
    # 1. Index ุนูู filename - ููุจุญุซ ุจุงูุงุณู
    await collection.create_index(
        "filename",
        name="idx_filename"
    )
    logger.info("โ Created index: filename")
    
    # 2. Index ุนูู status - ููุชุตููุฉ ุญุณุจ ุงูุญุงูุฉ
    await collection.create_index(
        "status",
        name="idx_status"
    )
    logger.info("โ Created index: status")
    
    # 3. Index ุนูู upload_date (ุชูุงุฒูู) - ููุชุฑุชูุจ ุงูุฒููู
    await collection.create_index(
        [("upload_date", -1)],
        name="idx_upload_date_desc"
    )
    logger.info("โ Created index: upload_date (desc)")
    
    # 4. Compound Index ุนูู status + upload_date
    # ูููุฏ ููุงุณุชุนูุงูุงุช: "ุฌููุน ุงููุณุชูุฏุงุช ุงููุนุงูุฌุฉ ูุฑุชุจุฉ ุจุงูุชุงุฑูุฎ"
    await collection.create_index(
        [
            ("status", 1),
            ("upload_date", -1)
        ],
        name="idx_status_upload_date"
    )
    logger.info("โ Created compound index: status + upload_date")
    
    # 5. Index ุนูู content_type - ููุชุตููุฉ ุญุณุจ ููุน ุงูููู
    await collection.create_index(
        "content_type",
        name="idx_content_type"
    )
    logger.info("โ Created index: content_type")
    
    # 6. Text Index ุนูู filename ู clean_text - ููุจุญุซ ุงููุตู ุงููุงูู
    await collection.create_index(
        [
            ("filename", "text"),
            ("clean_text", "text")
        ],
        name="idx_text_search",
        weights={
            "filename": 10,  # ูุฒู ุฃุนูู ููุงุณู
            "clean_text": 1
        },
        default_language="english"
    )
    logger.info("โ Created text index: filename + clean_text")
    
    # 7. Sparse Index ุนูู processed_at - ููุท ูููุณุชูุฏุงุช ุงููุนุงูุฌุฉ
    await collection.create_index(
        "processed_at",
        name="idx_processed_at",
        sparse=True  # ููุท documents ุงูุชู ููุง processed_at
    )
    logger.info("โ Created sparse index: processed_at")
    
    # 8. Index ุนูู chunk_count - ููุฅุญุตุงุฆูุงุช
    await collection.create_index(
        "chunk_count",
        name="idx_chunk_count",
        sparse=True
    )
    logger.info("โ Created index: chunk_count")

async def create_chunks_indexes():
    """
    ุฅูุดุงุก indexes ูู chunks collection
    """
    collection = db.get_collection(settings.chunks_collection)
    
    # 1. Index ุนูู document_id - ููุญุตูู ุนูู chunks ูุณุชูุฏ ูุนูู
    await collection.create_index(
        "document_id",
        name="idx_document_id"
    )
    logger.info("โ Created index: document_id")
    
    # 2. Compound Unique Index ุนูู document_id + chunk_id
    # ูุถูุงู ุนุฏู ุชูุฑุงุฑ ููุณ ุงูู chunk ูู ููุณ ุงููุณุชูุฏ
    await collection.create_index(
        [
            ("document_id", 1),
            ("chunk_id", 1)
        ],
        name="idx_document_chunk_unique",
        unique=True
    )
    logger.info("โ Created unique compound index: document_id + chunk_id")
    
    # 3. Index ุนูู created_at - ููุชุฑุชูุจ ุงูุฒููู
    await collection.create_index(
        [("created_at", -1)],
        name="idx_created_at_desc"
    )
    logger.info("โ Created index: created_at")
    
    # 4. Index ุนูู length - ููุจุญุซ ุจุญุฌู ุงูู chunk
    await collection.create_index(
        "length",
        name="idx_length"
    )
    logger.info("โ Created index: length")
    
    # 5. Text Index ุนูู text - ููุจุญุซ ุงููุตู ูู ุงูู chunks
    await collection.create_index(
        [("text", "text")],
        name="idx_text_search_chunks",
        default_language="english"
    )
    logger.info("โ Created text index: chunks text")
    
    # 6. Sparse Index ุนูู embedding - ููู chunks ุงูุชู ููุง embeddings
    await collection.create_index(
        "embedding",
        name="idx_embedding",
        sparse=True
    )
    logger.info("โ Created sparse index: embedding")

async def drop_all_indexes():
    """
    ุญุฐู ุฌููุน ุงูู indexes (ููุตูุงูุฉ)
    """
    logger.warning("โ๏ธ  Dropping all indexes...")
    
    # Documents
    doc_collection = db.get_collection(settings.documents_collection)
    await doc_collection.drop_indexes()
    
    # Chunks
    chunk_collection = db.get_collection(settings.chunks_collection)
    await chunk_collection.drop_indexes()
    
    logger.info("โ All indexes dropped")

async def list_indexes():
    """
    ุนุฑุถ ุฌููุน ุงูู indexes ุงูููุฌูุฏุฉ
    """
    # Documents indexes
    doc_collection = db.get_collection(settings.documents_collection)
    doc_indexes = await doc_collection.list_indexes().to_list(None)
    
    logger.info("๐ Documents Collection Indexes:")
    for idx in doc_indexes:
        logger.info(f"  - {idx['name']}: {idx.get('key', {})}")
    
    # Chunks indexes
    chunk_collection = db.get_collection(settings.chunks_collection)
    chunk_indexes = await chunk_collection.list_indexes().to_list(None)
    
    logger.info("๐ Chunks Collection Indexes:")
    for idx in chunk_indexes:
        logger.info(f"  - {idx['name']}: {idx.get('key', {})}")
    
    return {
        "documents": doc_indexes,
        "chunks": chunk_indexes
    }
```


#### 2. ุงุณุชุฎุฏุงู Text Search:

```python
# ููู database/repositories/document_repository.py (ุฅุถุงูุฉ)

async def search_documents(self, query: str, limit: int = 10) -> List[DocumentInDB]:
    """
    ุงูุจุญุซ ุงููุตู ูู ุงููุณุชูุฏุงุช
    
    Args:
        query: ูุต ุงูุจุญุซ
        limit: ุงูุญุฏ ุงูุฃูุตู ูููุชุงุฆุฌ
        
    Returns:
        ูุงุฆูุฉ ุงููุณุชูุฏุงุช ุงููุทุงุจูุฉ
    """
    cursor = self.collection.find(
        {
            "$text": {
                "$search": query
            }
        },
        {
            "score": {"$meta": "textScore"}  # ุงูุญุตูู ุนูู ููุงุท ุงููุทุงุจูุฉ
        }
    ).sort(
        [("score", {"$meta": "textScore"})]  # ุชุฑุชูุจ ุญุณุจ ุงูููุงุกูุฉ
    ).limit(limit)
    
    documents = []
    async for doc in cursor:
        documents.append(DocumentInDB(**doc))
    
    return documents
```


#### 3. Query Performance Analysis:

```python
# ููู database/query_analyzer.py
from database.connection import db
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

async def explain_query(
    collection_name: str,
    query: Dict[str, Any],
    sort: Dict[str, int] = None
) -> Dict:
    """
    ุชุญููู ุฃุฏุงุก ุงุณุชุนูุงู
    
    Args:
        collection_name: ุงุณู ุงูู collection
        query: ุงูุงุณุชุนูุงู
        sort: ุงูุชุฑุชูุจ (ุงุฎุชูุงุฑู)
        
    Returns:
        ุชูุงุตูู ุงูุชูููุฐ
    """
    collection = db.get_collection(collection_name)
    
    # ุจูุงุก ุงูุงุณุชุนูุงู
    cursor = collection.find(query)
    if sort:
        cursor = cursor.sort(list(sort.items()))
    
    # ุชุญููู ุงูุงุณุชุนูุงู
    explain_result = await cursor.explain()
    
    # ุงุณุชุฎุฑุงุฌ ุงููุนูููุงุช ุงููููุฉ
    execution_stats = explain_result.get('executionStats', {})
    
    analysis = {
        "query": query,
        "execution_time_ms": execution_stats.get('executionTimeMillis', 0),
        "documents_examined": execution_stats.get('totalDocsExamined', 0),
        "documents_returned": execution_stats.get('nReturned', 0),
        "index_used": explain_result.get('queryPlanner', {}).get('winningPlan', {}).get('inputStage', {}).get('indexName', 'COLLSCAN'),
        "efficient": execution_stats.get('totalDocsExamined', 0) == execution_stats.get('nReturned', 0)
    }
    
    # Log ุงูุชุญููู
    logger.info(f"Query Analysis:")
    logger.info(f"  Time: {analysis['execution_time_ms']}ms")
    logger.info(f"  Examined: {analysis['documents_examined']} docs")
    logger.info(f"  Returned: {analysis['documents_returned']} docs")
    logger.info(f"  Index: {analysis['index_used']}")
    logger.info(f"  Efficient: {analysis['efficient']}")
    
    return analysis

# ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงู
async def analyze_common_queries():
    """
    ุชุญููู ุงูุงุณุชุนูุงูุงุช ุงูุดุงุฆุนุฉ
    """
    # 1. ุงูุจุญุซ ุญุณุจ status
    await explain_query(
        "documents",
        {"status": "processed"},
        {"upload_date": -1}
    )
    
    # 2. ุงูุจุญุซ ุญุณุจ filename
    await explain_query(
        "documents",
        {"filename": {"$regex": "report", "$options": "i"}}
    )
    
    # 3. ุงูุจุญุซ ุงููุตู
    await explain_query(
        "documents",
        {"$text": {"$search": "machine learning"}}
    )
```


#### 4. ุฅุถุงูุฉ endpoint ููุฅุญุตุงุฆูุงุช:

```python
# ุชุญุฏูุซ routes/documents.py

@router.get("/search")
async def search_documents(
    q: str = Query(..., min_length=3, description="ูุต ุงูุจุญุซ"),
    limit: int = Query(10, ge=1, le=50)
):
    """
    ุงูุจุญุซ ุงููุตู ูู ุงููุณุชูุฏุงุช
    
    Args:
        q: ูุต ุงูุจุญุซ
        limit: ุนุฏุฏ ุงููุชุงุฆุฌ
    
    Returns:
        ุงููุณุชูุฏุงุช ุงููุทุงุจูุฉ
    """
    documents = await doc_repo.search_documents(q, limit)
    
    return {
        "query": q,
        "count": len(documents),
        "results": [
            DocumentResponse(
                id=str(doc.id),
                filename=doc.filename,
                file_size=doc.file_size,
                content_type=doc.content_type,
                upload_date=doc.upload_date,
                status=doc.status,
                chunk_count=doc.chunk_count or 0
            )
            for doc in documents
        ]
    }

@router.get("/admin/indexes")
async def list_all_indexes():
    """
    ุนุฑุถ ุฌููุน ุงูู indexes (ูููุทูุฑูู)
    """
    from database.indexes import list_indexes
    return await list_indexes()

@router.post("/admin/reindex")
async def recreate_indexes():

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู:

***

## ๐ฌ ุงูููุฏูู ุงูุญุงุฏู ุนุดุฑ: Mongo Indexing (ุชูููุฉ)

#### 4. ุฅุถุงูุฉ endpoints ููุฅุฏุงุฑุฉ (ุชูููุฉ):
```python
# ุชุญุฏูุซ routes/documents.py (ุชูููุฉ)

@router.post("/admin/reindex")
async def recreate_indexes():
    """
    ุฅุนุงุฏุฉ ุฅูุดุงุก ุฌููุน ุงูู indexes
    """
    from database.indexes import drop_all_indexes, create_indexes
    
    try:
        await drop_all_indexes()
        await create_indexes()
        return {"message": "Indexes recreated successfully"}
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error recreating indexes: {str(e)}"
        )

@router.get("/admin/query-performance")
async def analyze_query_performance():
    """
    ุชุญููู ุฃุฏุงุก ุงูุงุณุชุนูุงูุงุช
    """
    from database.query_analyzer import analyze_common_queries
    await analyze_common_queries()
    return {"message": "Check logs for analysis results"}
```


#### 5. Best Practices ููู Indexes:

```python
# ูุตุงุฆุญ ูููุฉ ูู ุงูุชุนูููุงุช

"""
โ DO's:
1. Create indexes ุนูู ุงูุญููู ุงููุณุชุฎุฏูุฉ ูู WHERE/FILTER
2. Use compound indexes ููุงุณุชุนูุงูุงุช ุงููุชูุฑุฑุฉ
3. Monitor index usage ูุน explain()
4. Keep indexes ุนูู ุงูุญุฏ ุงูุฃุฏูู ุงููุทููุจ

โ DON'Ts:
1. ูุง ุชููุดุฆ indexes ุนูู ูู ุญูู
2. ูุง ุชุณุชุฎุฏู indexes ุนูู collections ุตุบูุฑุฉ (<1000 document)
3. ูุง ุชูุณู indexes ูู Write-heavy collections (ุจุทุก ุงููุชุงุจุฉ)
4. ูุง ุชุณุชุฎุฏู text indexes ููุจุญุซ ุงูุจุณูุท

๐ Performance Impact:
- Read operations: ุฃุณุฑุน 10-100x ูุน indexes ููุงุณุจ
- Write operations: ุฃุจุทุฃ 5-10% ููู index ุฅุถุงูู
- Storage: ูู index ูุณุชููู ูุณุงุญุฉ ุฅุถุงููุฉ
"""
```


#### 6. ุงุฎุชุจุงุฑ ุงูู Indexes:

```bash
# ุงุฎุชุจุงุฑ ุงูุจุญุซ ุงููุตู
$ curl "http://localhost:5000/documents/search?q=machine+learning&limit=5"

# ุนุฑุถ ุงูู indexes
$ curl "http://localhost:5000/documents/admin/indexes"

# ุชุญููู ุงูุฃุฏุงุก
$ curl "http://localhost:5000/documents/admin/query-performance"

# ุฅุนุงุฏุฉ ุจูุงุก ุงูู indexes
$ curl -X POST "http://localhost:5000/documents/admin/reindex"
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู ุนุดุฑ: Data Pipeline Enhancements (43 ุฏูููุฉ)

**Branch:** `tut-008` (ููุณ branch ุงูููุฏูู 11)

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชุญุณูู pipeline ูุนุงูุฌุฉ ุงูุจูุงูุงุช
- ุฅุถุงูุฉ Progress tracking
- Better error handling
- Retry mechanisms


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง ูุญุชุงุฌ Pipeline Enhancementsุ

- **Reliability**: ูุนุงูุฌุฉ ุฃุฎุทุงุก ุฃูุถู
- **Monitoring**: ุชุชุจุน ุงูุชูุฏู
- **Scalability**: ูุนุงูุฌุฉ ูููุงุช ูุจูุฑุฉ
- **User Experience**: ุชุญุฏูุซุงุช ูู ุงูููุช ุงููุนูู


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู models/processing_status.py:

```python
from pydantic import BaseModel, Field
from typing import Optional, Dict, Any
from datetime import datetime
from enum import Enum

class ProcessingStage(str, Enum):
    """
    ูุฑุงุญู ุงููุนุงูุฌุฉ
    """
    UPLOADED = "uploaded"
    EXTRACTING_TEXT = "extracting_text"
    CLEANING_TEXT = "cleaning_text"
    CHUNKING = "chunking"
    SAVING_CHUNKS = "saving_chunks"
    COMPLETED = "completed"
    FAILED = "failed"

class ProcessingProgress(BaseModel):
    """
    ุชุชุจุน ุชูุฏู ุงููุนุงูุฌุฉ
    """
    document_id: str
    stage: ProcessingStage
    progress_percentage: float = Field(0.0, ge=0.0, le=100.0)
    current_step: Optional[str] = None
    total_steps: Optional[int] = None
    completed_steps: Optional[int] = 0
    started_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    def update_progress(
        self, 
        stage: ProcessingStage, 
        percentage: float,
        current_step: str = None
    ):
        """
        ุชุญุฏูุซ ุงูุชูุฏู
        """
        self.stage = stage
        self.progress_percentage = percentage
        self.current_step = current_step
        self.updated_at = datetime.now()
        
        if stage == ProcessingStage.COMPLETED:
            self.progress_percentage = 100.0
        elif stage == ProcessingStage.FAILED:
            self.progress_percentage = 0.0
```


#### 2. ููู helpers/enhanced_text_processor.py:

```python
from helpers.text_processor import TextProcessor
from models.processing_status import ProcessingProgress, ProcessingStage
from typing import List, Dict, Callable
import asyncio
import logging

logger = logging.getLogger(__name__)

class EnhancedTextProcessor(TextProcessor):
    """
    ูุนุงูุฌ ูุตูุต ูุญุณูู ูุน ุชุชุจุน ุงูุชูุฏู
    """
    
    def __init__(self, progress_callback: Callable = None):
        super().__init__()
        self.progress_callback = progress_callback
    
    async def process_with_progress(
        self,
        file_path: str,
        file_extension: str,
        document_id: str
    ) -> Dict:
        """
        ูุนุงูุฌุฉ ูุน ุชุชุจุน ุงูุชูุฏู
        
        Args:
            file_path: ูุณุงุฑ ุงูููู
            file_extension: ุงูุชุฏุงุฏ ุงูููู
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            ูุชุงุฆุฌ ุงููุนุงูุฌุฉ
        """
        progress = ProcessingProgress(
            document_id=document_id,
            stage=ProcessingStage.UPLOADED
        )
        
        try:
            # ุงููุฑุญูุฉ 1: ุงุณุชุฎุฑุงุฌ ุงููุต
            await self._update_progress(
                progress,
                ProcessingStage.EXTRACTING_TEXT,
                10.0,
                "Extracting text from file"
            )
            
            raw_text = self.extract_text(file_path, file_extension)
            
            await self._update_progress(
                progress,
                ProcessingStage.EXTRACTING_TEXT,
                30.0,
                f"Extracted {len(raw_text)} characters"
            )
            
            # ุงููุฑุญูุฉ 2: ุชูุธูู ุงููุต
            await self._update_progress(
                progress,
                ProcessingStage.CLEANING_TEXT,
                40.0,
                "Cleaning text"
            )
            
            clean_text = self.clean_text(raw_text)
            
            await self._update_progress(
                progress,
                ProcessingStage.CLEANING_TEXT,
                50.0,
                "Text cleaned"
            )
            
            # ุงููุฑุญูุฉ 3: ุงูุชูุณูู
            await self._update_progress(
                progress,
                ProcessingStage.CHUNKING,
                60.0,
                "Chunking text"
            )
            
            chunks = self.chunk_text(
                clean_text,
                chunk_size=1000,
                overlap=200
            )
            
            await self._update_progress(
                progress,
                ProcessingStage.CHUNKING,
                80.0,
                f"Created {len(chunks)} chunks"
            )
            
            # ุงููุฑุญูุฉ 4: ุงูุงูุชูุงู
            await self._update_progress(
                progress,
                ProcessingStage.COMPLETED,
                100.0,
                "Processing completed"
            )
            
            return {
                "raw_text": raw_text,
                "clean_text": clean_text,
                "chunks": chunks,
                "stats": {
                    "raw_length": len(raw_text),
                    "clean_length": len(clean_text),
                    "chunk_count": len(chunks)
                }
            }
            
        except Exception as e:
            logger.error(f"Processing error: {str(e)}")
            progress.stage = ProcessingStage.FAILED
            progress.error_message = str(e)
            await self._update_progress(
                progress,
                ProcessingStage.FAILED,
                0.0,
                f"Failed: {str(e)}"
            )
            raise
    
    async def _update_progress(
        self,
        progress: ProcessingProgress,
        stage: ProcessingStage,
        percentage: float,
        message: str
    ):
        """
        ุชุญุฏูุซ ูุฅุฑุณุงู ุงูุชูุฏู
        """
        progress.update_progress(stage, percentage, message)
        
        logger.info(
            f"[{progress.document_id}] "
            f"{stage.value}: {percentage}% - {message}"
        )
        
        # ุงุณุชุฏุนุงุก callback ุฅุฐุง ูุงู ููุฌูุฏุงู
        if self.progress_callback:
            await self.progress_callback(progress)
```


#### 3. ููู helpers/retry_handler.py:

```python
from typing import Callable, Any, Type
from functools import wraps
import asyncio
import logging

logger = logging.getLogger(__name__)

def retry_async(
    max_attempts: int = 3,
    delay: float = 1.0,
    backoff: float = 2.0,
    exceptions: tuple = (Exception,)
):
    """
    Decorator ูููุญุงููุฉ ูุฑุฉ ุฃุฎุฑู ุนูุฏ ุงููุดู
    
    Args:
        max_attempts: ุงูุญุฏ ุงูุฃูุตู ูููุญุงููุงุช
        delay: ุงูุงูุชุธุงุฑ ุงูุฃููู (ุซูุงูู)
        backoff: ูุนุงูู ุงูุฒูุงุฏุฉ ููุงูุชุธุงุฑ
        exceptions: ุฃููุงุน ุงูุฃุฎุทุงุก ูููุญุงููุฉ ูุฑุฉ ุฃุฎุฑู
    """
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            current_delay = delay
            last_exception = None
            
            for attempt in range(1, max_attempts + 1):
                try:
                    return await func(*args, **kwargs)
                
                except exceptions as e:
                    last_exception = e
                    
                    if attempt < max_attempts:
                        logger.warning(
                            f"Attempt {attempt}/{max_attempts} failed: {str(e)}. "
                            f"Retrying in {current_delay}s..."
                        )
                        await asyncio.sleep(current_delay)
                        current_delay *= backoff
                    else:
                        logger.error(
                            f"All {max_attempts} attempts failed. "
                            f"Last error: {str(e)}"
                        )
            
            raise last_exception
        
        return wrapper
    return decorator

# ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงู
@retry_async(max_attempts=3, delay=2.0, backoff=2.0)
async def process_with_retry(document_id: str):
    """
    ูุนุงูุฌุฉ ูุน ุฅุนุงุฏุฉ ุงููุญุงููุฉ
    """
    # ุนูููุฉ ูุฏ ุชูุดู
    pass
```


#### 4. ุชุญุฏูุซ routes/documents.py ูุน ุงูุชุญุณููุงุช:

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, status, BackgroundTasks, Query
from fastapi.responses import StreamingResponse
from helpers.enhanced_text_processor import EnhancedTextProcessor
from helpers.retry_handler import retry_async
from models.processing_status import ProcessingProgress, ProcessingStage
from typing import List, Optional, AsyncGenerator
import asyncio
import json

router = APIRouter(
    prefix="/documents",
    tags=["documents"]
)

# ูุงููุณ ูุชุชุจุน ุงูุชูุฏู ูู ุงูุฐุงูุฑุฉ (ููุชุทููุฑ)
# ูู ุงูุฅูุชุงุฌ: ุงุณุชุฎุฏู Redis ุฃู WebSocket
progress_tracker: Dict[str, ProcessingProgress] = {}

async def progress_callback(progress: ProcessingProgress):
    """
    ุญูุธ ุงูุชูุฏู
    """
    progress_tracker[progress.document_id] = progress

@retry_async(max_attempts=3, delay=2.0)
async def process_document_enhanced(document_id: str):
    """
    ูุนุงูุฌุฉ ูุญุณููุฉ ูุน retry
    """
    try:
        # ุงูุญุตูู ุนูู ุงููุณุชูุฏ
        document = await doc_repo.get_by_id(document_id)
        if not document:
            return
        
        # ุฅูุดุงุก ูุนุงูุฌ ูุญุณูู
        processor = EnhancedTextProcessor(progress_callback=progress_callback)
        
        # ุงููุนุงูุฌุฉ ูุน ุชุชุจุน ุงูุชูุฏู
        file_ext = os.path.splitext(document.filename)[1]
        result = await processor.process_with_progress(
            document.file_path,
            file_ext,
            document_id
        )
        
        # ุญูุธ ุงูู chunks
        chunks_to_create = []
        for chunk_data in result['chunks']:
            chunk = ChunkCreate(
                document_id=document_id,
                chunk_id=chunk_data['chunk_id'],
                text=chunk_data['text'],
                start_char=chunk_data['start_char'],
                end_char=chunk_data['end_char'],
                length=chunk_data['length']
            )
            chunks_to_create.append(chunk)
        
        if chunks_to_create:
            await chunk_repo.create_many(chunks_to_create)
        
        # ุชุญุฏูุซ ุงููุณุชูุฏ
        update_data = DocumentUpdate(
            status="processed",
            raw_text=result['raw_text'],
            clean_text=result['clean_text'],
            chunk_count=len(result['chunks']),
            metadata=result['stats']
        )
        await doc_repo.update(document_id, update_data)
        
    except Exception as e:
        await doc_repo.update_status(document_id, "failed", error=str(e))
        raise

@router.get("/{document_id}/progress")
async def get_processing_progress(document_id: str):
    """
    ุงูุญุตูู ุนูู ุชูุฏู ุงููุนุงูุฌุฉ
    
    Args:
        

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู:

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู ุนุดุฑ: Data Pipeline Enhancements (ุชูููุฉ)

#### 4. ุชุญุฏูุซ routes/documents.py (ุชูููุฉ):
```python
@router.get("/{document_id}/progress")
async def get_processing_progress(document_id: str):
    """
    ุงูุญุตูู ุนูู ุชูุฏู ุงููุนุงูุฌุฉ
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
    
    Returns:
        ุญุงูุฉ ุงูุชูุฏู ุงูุญุงููุฉ
    """
    if document_id not in progress_tracker:
        # ุงูุญุตูู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        document = await doc_repo.get_by_id(document_id)
        
        if not document:
            raise HTTPException(404, "Document not found")
        
        # ุฅูุดุงุก progress ูู ุญุงูุฉ ุงููุณุชูุฏ
        return {
            "document_id": document_id,
            "stage": document.status,
            "progress_percentage": 100.0 if document.status == "processed" else 0.0,
            "message": f"Status: {document.status}"
        }
    
    progress = progress_tracker[document_id]
    
    return {
        "document_id": progress.document_id,
        "stage": progress.stage.value,
        "progress_percentage": progress.progress_percentage,
        "current_step": progress.current_step,
        "started_at": progress.started_at.isoformat(),
        "updated_at": progress.updated_at.isoformat(),
        "error_message": progress.error_message
    }

@router.get("/{document_id}/progress/stream")
async def stream_processing_progress(document_id: str):
    """
    Stream ุชูุฏู ุงููุนุงูุฌุฉ (Server-Sent Events)
    """
    async def generate() -> AsyncGenerator[str, None]:
        """
        ุชูููุฏ events ููุชูุฏู
        """
        while True:
            if document_id in progress_tracker:
                progress = progress_tracker[document_id]
                
                # ุฅุฑุณุงู ุงูุจูุงูุงุช
                data = {
                    "stage": progress.stage.value,
                    "progress": progress.progress_percentage,
                    "message": progress.current_step
                }
                
                yield f"data: {json.dumps(data)}\n\n"
                
                # ุฅููุงุก Stream ุนูุฏ ุงูุงูุชูุงู ุฃู ุงููุดู
                if progress.stage in [ProcessingStage.COMPLETED, ProcessingStage.FAILED]:
                    break
            
            await asyncio.sleep(0.5)  # ุชุญุฏูุซ ูู ูุตู ุซุงููุฉ
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )
```


#### 5. ุงุฎุชุจุงุฑ ุงูุชุญุณููุงุช:

```bash
# ุฑูุน ูุณุชูุฏ
$ curl -X POST "http://localhost:5000/documents/upload" \
  -F "file=@large_document.pdf"
# Response: {"id": "abc123", ...}

# ุชุชุจุน ุงูุชูุฏู
$ curl "http://localhost:5000/documents/abc123/progress"

# Stream ุงูุชูุฏู (ูู ูุชุตูุญ ุฃู EventSource)
# http://localhost:5000/documents/abc123/progress/stream
```


#### 6. Frontend Example (JavaScript):

```javascript
// ูุซุงู ุนูู ุงุณุชุฎุฏุงู Server-Sent Events
const docId = "abc123";
const eventSource = new EventSource(
    `http://localhost:5000/documents/${docId}/progress/stream`
);

eventSource.onmessage = (event) => {
    const data = JSON.parse(event.data);
    console.log(`Stage: ${data.stage}`);
    console.log(`Progress: ${data.progress}%`);
    console.log(`Message: ${data.message}`);
    
    // ุชุญุฏูุซ UI
    updateProgressBar(data.progress);
    
    if (data.stage === 'completed' || data.stage === 'failed') {
        eventSource.close();
    }
};
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูุซ ุนุดุฑ: Checkpoint-1 | What have we learned? (27 ุฏูููุฉ)

**Branch:** `tut-008`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ูุฑุงุฌุนุฉ ุดุงููุฉ ููุง ุชู ุฅูุฌุงุฒู
- ุญู ุงููุดุงูู ุงูุดุงุฆุนุฉ
- ูุตุงุฆุญ ูุฃูุถู ุงูููุงุฑุณุงุช
- ุงูุงุณุชุนุฏุงุฏ ูููุฑุญูุฉ ุงููุงุฏูุฉ


### ุงููุญุชูู ุงูุฃุณุงุณู:

#### 1. ูุง ุชู ุฅูุฌุงุฒู ุญุชู ุงูุขู:

```python
"""
โ ุงูููููุงุช ุงูููุชููุฉ:
โโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ

1. Backend Infrastructure:
   - FastAPI application
   - MongoDB integration
   - Docker setup
   - Repository pattern

2. File Management:
   - Upload with validation
   - Multiple file types support
   - Secure file storage
   - File deletion

3. Text Processing:
   - PDF extraction
   - DOCX extraction
   - Text cleaning
   - Chunking strategies

4. Database Operations:
   - CRUD for documents
   - CRUD for chunks
   - Indexes for performance
   - Query optimization

5. Advanced Features:
   - Background processing
   - Progress tracking
   - Retry mechanisms
   - Error handling
"""
```


#### 2. ุงููุดุงูู ุงูุดุงุฆุนุฉ ูุญููููุง:

```python
# ููู docs/troubleshooting.md

"""
๐ง ูุดุงูู ุดุงุฆุนุฉ ูุญููููุง:

1. MongoDB Connection Failed:
   โ ุงููุดููุฉ: pymongo.errors.ServerSelectionTimeoutError
   โ ุงูุญู:
      - ุชุฃูุฏ ูู ุชุดุบูู Docker: docker compose ps
      - ุชุญูู ูู ุงููููุฐ: netstat -an | grep 27017
      - ุฑุงุฌุน credentials ูู .env

2. File Upload Error (413):
   โ ุงููุดููุฉ: File too large
   โ ุงูุญู:
      - ุฒูุงุฏุฉ max_file_size ูู FileHandler
      - ุฅุถุงูุฉ nginx config ูููููุงุช ุงููุจูุฑุฉ

3. Text Extraction Fails:
   โ ุงููุดููุฉ: Can't extract from PDF
   โ ุงูุญู:
      - ุชุญูู ูู ุชุซุจูุช PyPDF2 ุจุดูู ุตุญูุญ
      - ุจุนุถ PDFs ูุญููุฉ: ุงุณุชุฎุฏู pypdf ุฃู pdfplumber

4. Background Task Not Running:
   โ ุงููุดููุฉ: Document stuck in "uploaded"
   โ ุงูุญู:
      - ุชุญูู ูู logs: uvicorn main:app --log-level debug
      - ุชุฃูุฏ ูู ุนุฏู ูุฌูุฏ exceptions ูู process_document

5. Chunks Not Saved:
   โ ุงููุดููุฉ: chunk_count = 0
   โ ุงูุญู:
      - ุชุญูู ูู unique index ุนูู (document_id, chunk_id)
      - ุฑุงุฌุน permissions ุนูู uploads directory
"""
```


#### 3. Code Review Checklist:

```python
# ููู docs/code_review_checklist.py

"""
๐ Code Review Checklist:

โก Error Handling:
  โก Try-catch ูู ุฌููุน async functions
  โก HTTPException ูุน status codes ููุงุณุจุฉ
  โก Logging ููุฃุฎุทุงุก

โก Validation:
  โก Pydantic models ููู input
  โก Query parameters ูุน constraints
  โก File validation (type, size)

โก Database:
  โก Indexes ุนูู ุงูุญููู ุงููุณุชุฎุฏูุฉ
  โก Connection pool configured
  โก Proper error handling

โก Performance:
  โก Async operations ุญูุซูุง ุฃููู
  โก Pagination ููููุงุฆู ุงูุทูููุฉ
  โก Caching ููุจูุงูุงุช ุงููุชูุฑุฑุฉ

โก Security:
  โก Environment variables ููู secrets
  โก Input sanitization
  โก File upload restrictions

โก Documentation:
  โก Docstrings ููู function
  โก API documentation (Swagger)
  โก README updated
"""
```


#### 4. Performance Benchmarks:

```python
# ููู tests/benchmark.py
import time
import asyncio
from database.repositories.document_repository import DocumentRepository

async def benchmark_queries():
    """
    ููุงุณ ุฃุฏุงุก ุงูุงุณุชุนูุงูุงุช
    """
    repo = DocumentRepository()
    
    # 1. Get by ID
    start = time.time()
    doc = await repo.get_by_id("some_id")
    print(f"Get by ID: {(time.time() - start) * 1000:.2f}ms")
    
    # 2. List with pagination
    start = time.time()
    docs = await repo.get_all(skip=0, limit=10)
    print(f"List 10 docs: {(time.time() - start) * 1000:.2f}ms")
    
    # 3. Search
    start = time.time()
    results = await repo.search_documents("test", limit=10)
    print(f"Text search: {(time.time() - start) * 1000:.2f}ms")
    
    # 4. Count
    start = time.time()
    count = await repo.count()
    print(f"Count: {(time.time() - start) * 1000:.2f}ms")

"""
๐ฏ Expected Performance:
โโโโโโโโโโโโโโโโโโโโโโโ
Get by ID:       < 5ms
List 10 docs:    < 10ms
Text search:     < 50ms
Count:           < 5ms
File upload:     < 1s (per MB)
Text extraction: < 2s (per page)
"""
```


#### 5. ุงูุงุณุชุนุฏุงุฏ ูููุฑุญูุฉ ุงููุงุฏูุฉ:

```python
"""
๐ ุงููุฑุญูุฉ ุงููุงุฏูุฉ: RAG Core

1. LLM Integration:
   - OpenAI API
   - Local models (Ollama)
   - Prompt engineering

2. Vector Databases:
   - Embeddings generation
   - QDrant setup
   - Similarity search

3. Semantic Search:
   - Vector similarity
   - Hybrid search
   - Ranking algorithms

4. Answer Generation:
   - Context retrieval
   - Prompt construction
   - Response formatting

๐ ุงููุชุทูุจุงุช:
- OpenAI API key
- ููู Embeddings
- ูุนุฑูุฉ ุจู Vector operations
"""
```


***

## ๐ฌ ุงูููุฏูู ุงูุฑุงุจุน ุนุดุฑ: LLM Factory (2:01 ุณุงุนุฉ)

**Branch:** `tut-008` (ุจุฏุงูุฉ ุงูุชุบููุฑุงุช ุงููุจูุฑุฉ)

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุฏูุฌ Large Language Models
- ุฅูุดุงุก LLM Factory pattern
- ุงุณุชุฎุฏุงู OpenAI API
- ุฅุฏุงุฑุฉ API keys ุจุดูู ุขูู


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ูุง ูู LLM Factory Patternุ

- **Abstraction**: ูุงุฌูุฉ ููุญุฏุฉ ููุฎุชูู LLMs
- **Flexibility**: ุณูููุฉ ุงูุชุจุฏูู ุจูู ุงูููุงุฐุฌ
- **Testability**: ุฅููุงููุฉ testing ุจุฏูู API calls
- **Extensibility**: ุฅุถุงูุฉ ููุงุฐุฌ ุฌุฏูุฏุฉ ุจุณูููุฉ


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ requirements.txt:

```txt
# ุงูููุชุจุงุช ุงูุณุงุจูุฉ...
openai==1.12.0           # OpenAI API
tiktoken==0.5.2          # Token counting
tenacity==8.2.3          # Retry logic
python-dotenv==1.0.0     # Environment variables
```


#### 2. ุชุญุฏูุซ config.py:

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # Application
    app_name: str = "mini-RAG"
    app_version: str = "1.0.0"
    debug: bool = False
    
    # MongoDB
    mongodb_url: str
    database_name: str = "minirag"
    documents_collection: str = "documents"
    chunks_collection: str = "chunks"
    
    # OpenAI
    openai_api_key: str
    openai_model: str = "gpt-3.5-turbo"
    openai_embedding_model: str = "text-embedding-ada-002"
    openai_max_tokens: int = 2000
    openai_temperature: float = 0.7
    
    # LLM Settings
    llm_timeout: int = 30
    llm_max_retries: int = 3
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```


#### 3. ููู models/llm_models.py:

```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from enum import Enum

class LLMProvider(str, Enum):
    """
    ูุฒูุฏู LLM ุงููุฏุนูููู
    """
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    OLLAMA = "ollama"
    HUGGINGFACE = "huggingface"

class Message(BaseModel):
    """
    ุฑุณุงูุฉ ูู ุงููุญุงุฏุซุฉ
    """
    role: str = Field(..., description="ุฏูุฑ ุงููุฑุณู: system/user/assistant")
    content: str = Field(..., description="ูุญุชูู ุงูุฑุณุงูุฉ")

class ChatRequest(BaseModel):
    """
    ุทูุจ ูุญุงุฏุซุฉ
    """
    messages: List[Message]
    model: Optional[str] = None
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    max_tokens: Optional[int] = None
    stream: bool = False

class ChatResponse(BaseModel):
    """
    ุงุณุชุฌุงุจุฉ ุงููุญุงุฏุซุฉ
    """
    content: str
    model: str
    tokens_used: int
    finish_reason: str
    metadata: Dict[str, Any] = Field(default_factory=dict)

class EmbeddingRequest(BaseModel):
    """
    ุทูุจ embedding
    """
    text: str = Field(..., min_length=1)
    model: Optional[str] = None

class EmbeddingResponse(BaseModel):
    """
    ุงุณุชุฌุงุจุฉ embedding
    """
    embedding: List[float]
    model: str
    tokens_used: int
```


#### 4. ููู llm/base_llm.py:

```python
from abc import ABC, abstractmethod
from typing

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู:

***

## ๐ฌ ุงูููุฏูู ุงูุฑุงุจุน ุนุดุฑ: LLM Factory (ุชูููุฉ)

#### 4. ููู llm/base_llm.py:
```python
from abc import ABC, abstractmethod
from typing import List, AsyncGenerator
from models.llm_models import ChatRequest, ChatResponse, EmbeddingRequest, EmbeddingResponse

class BaseLLM(ABC):
    """
    Abstract base class ูุฌููุน LLM providers
    """
    
    def __init__(self, api_key: str, model: str = None):
        self.api_key = api_key
        self.model = model
    
    @abstractmethod
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        ุฅุฑุณุงู ุฑุณุงุฆู ูุงูุญุตูู ุนูู ุฑุฏ
        
        Args:
            request: ุทูุจ ุงููุญุงุฏุซุฉ
            
        Returns:
            ุงุณุชุฌุงุจุฉ LLM
        """
        pass
    
    @abstractmethod
    async def chat_stream(
        self, 
        request: ChatRequest
    ) -> AsyncGenerator[str, None]:
        """
        ูุญุงุฏุซุฉ ูุน streaming
        
        Args:
            request: ุทูุจ ุงููุญุงุฏุซุฉ
            
        Yields:
            ุฃุฌุฒุงุก ุงููุต ุงููุชุฏูู
        """
        pass
    
    @abstractmethod
    async def get_embedding(
        self, 
        request: EmbeddingRequest
    ) -> EmbeddingResponse:
        """
        ุงูุญุตูู ุนูู embedding ูููุต
        
        Args:
            request: ุทูุจ embedding
            
        Returns:
            vector embedding
        """
        pass
    
    @abstractmethod
    async def count_tokens(self, text: str) -> int:
        """
        ุญุณุงุจ ุนุฏุฏ tokens
        
        Args:
            text: ุงููุต
            
        Returns:
            ุนุฏุฏ tokens
        """
        pass
```


#### 5. ููู llm/openai_llm.py:

```python
from llm.base_llm import BaseLLM
from models.llm_models import ChatRequest, ChatResponse, EmbeddingRequest, EmbeddingResponse
from openai import AsyncOpenAI
from typing import AsyncGenerator
from tenacity import retry, stop_after_attempt, wait_exponential
import tiktoken
import logging

logger = logging.getLogger(__name__)

class OpenAILLM(BaseLLM):
    """
    OpenAI LLM implementation
    """
    
    def __init__(
        self, 
        api_key: str, 
        model: str = "gpt-3.5-turbo",
        embedding_model: str = "text-embedding-ada-002"
    ):
        super().__init__(api_key, model)
        self.client = AsyncOpenAI(api_key=api_key)
        self.embedding_model = embedding_model
        self.tokenizer = tiktoken.encoding_for_model(model)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        ูุญุงุฏุซุฉ ูุน OpenAI
        """
        try:
            # ุชุญุถูุฑ ุงูุฑุณุงุฆู
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            # ุงุณุชุฏุนุงุก API
            response = await self.client.chat.completions.create(
                model=request.model or self.model,
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
            )
            
            # ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช
            choice = response.choices[0]
            
            return ChatResponse(
                content=choice.message.content,
                model=response.model,
                tokens_used=response.usage.total_tokens,
                finish_reason=choice.finish_reason,
                metadata={
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens
                }
            )
            
        except Exception as e:
            logger.error(f"OpenAI chat error: {str(e)}")
            raise
    
    async def chat_stream(
        self, 
        request: ChatRequest
    ) -> AsyncGenerator[str, None]:
        """
        ูุญุงุฏุซุฉ ูุน streaming
        """
        try:
            messages = [
                {"role": msg.role, "content": msg.content}
                for msg in request.messages
            ]
            
            stream = await self.client.chat.completions.create(
                model=request.model or self.model,
                messages=messages,
                temperature=request.temperature,
                max_tokens=request.max_tokens,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"OpenAI stream error: {str(e)}")
            raise
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10)
    )
    async def get_embedding(
        self, 
        request: EmbeddingRequest
    ) -> EmbeddingResponse:
        """
        ุงูุญุตูู ุนูู embedding
        """
        try:
            response = await self.client.embeddings.create(
                model=request.model or self.embedding_model,
                input=request.text
            )
            
            embedding_data = response.data[0]
            
            return EmbeddingResponse(
                embedding=embedding_data.embedding,
                model=response.model,
                tokens_used=response.usage.total_tokens
            )
            
        except Exception as e:
            logger.error(f"OpenAI embedding error: {str(e)}")
            raise
    
    async def count_tokens(self, text: str) -> int:
        """
        ุญุณุงุจ tokens ุจุงุณุชุฎุฏุงู tiktoken
        """
        try:
            tokens = self.tokenizer.encode(text)
            return len(tokens)
        except Exception as e:
            logger.error(f"Token counting error: {str(e)}")
            # ุชูุฏูุฑ ุชูุฑูุจู: ูู 4 ุฃุญุฑู = 1 token
            return len(text) // 4
```


#### 6. ููู llm/llm_factory.py:

```python
from llm.base_llm import BaseLLM
from llm.openai_llm import OpenAILLM
from models.llm_models import LLMProvider
from config import settings
import logging

logger = logging.getLogger(__name__)

class LLMFactory:
    """
    Factory ูุฅูุดุงุก LLM instances
    """
    
    _instances = {}  # Cache ููู instances
    
    @staticmethod
    def create(
        provider: LLMProvider = LLMProvider.OPENAI,
        **kwargs
    ) -> BaseLLM:
        """
        ุฅูุดุงุก LLM instance
        
        Args:
            provider: ููุน ุงููุฒูุฏ
            **kwargs: ูุนุงููุงุช ุฅุถุงููุฉ
            
        Returns:
            LLM instance
        """
        # ุงูุชุญูู ูู Cache
        cache_key = f"{provider.value}_{kwargs.get('model', 'default')}"
        
        if cache_key in LLMFactory._instances:
            logger.info(f"Using cached LLM instance: {cache_key}")
            return LLMFactory._instances[cache_key]
        
        # ุฅูุดุงุก instance ุฌุฏูุฏ
        if provider == LLMProvider.OPENAI:
            instance = OpenAILLM(
                api_key=kwargs.get('api_key') or settings.openai_api_key,
                model=kwargs.get('model') or settings.openai_model,
                embedding_model=kwargs.get('embedding_model') or settings.openai_embedding_model
            )
        
        # ูููู ุฅุถุงูุฉ providers ุฃุฎุฑู ููุง
        # elif provider == LLMProvider.ANTHROPIC:
        #     instance = AnthropicLLM(...)
        # elif provider == LLMProvider.OLLAMA:
        #     instance = OllamaLLM(...)
        
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        # ุญูุธ ูู Cache
        LLMFactory._instances[cache_key] = instance
        logger.info(f"Created new LLM instance: {cache_key}")
        
        return instance
    
    @staticmethod
    def clear_cache():
        """
        ูุณุญ ุงูู cache
        """
        LLMFactory._instances.clear()
        logger.info("LLM cache cleared")
```


#### 7. ุฅุถุงูุฉ routes/llm.py:

```python
from fastapi import APIRouter, HTTPException, status
from fastapi.responses import StreamingResponse
from llm.llm_factory import LLMFactory
from models.llm_models import (
    LLMProvider, 
    ChatRequest, 
    ChatResponse,
    EmbeddingRequest,
    EmbeddingResponse,
    Message
)
from typing import AsyncGenerator
import logging

router = APIRouter(
    prefix="/llm",
    tags=["LLM"]
)

logger = logging.getLogger(__name__)

@router.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """
    ูุญุงุฏุซุฉ ูุน LLM
    
    Args:
        request: ุทูุจ ุงููุญุงุฏุซุฉ
    
    Returns:
        ุงุณุชุฌุงุจุฉ LLM
    """
    try:
        # ุฅูุดุงุก LLM instance
        llm = LLMFactory.create(LLMProvider.OPENAI)
        
        # ุฅุฑุณุงู ุงูุทูุจ
        response = await llm.chat(request)
        
        return response
        
    except Exception as e:
        logger.error(f"Chat error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"LLM error: {str(e)}"
        )

@router.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    ูุญุงุฏุซุฉ ูุน streaming
    """
    async def generate() -> AsyncGenerator[str, None]:
        try:
            llm = LLMFactory.create(LLMProvider.OPENAI)
            
            async for chunk in llm.chat_stream(request):
                yield chunk
                
        except Exception as e:
            logger.error(f"Stream error: {str(e)}")
            yield f"\n\nError: {str(e)}"
    
    return StreamingResponse(
        generate(),
        media_type="text/plain"
    )

@router.post("/embedding", response_model=EmbeddingResponse)
async def get_embedding(request: EmbeddingRequest):
    """
    ุงูุญุตูู ุนูู embedding ูููุต
    """
    try:
        llm = LLMFactory.create(LLMProvider.OPENAI)
        response = await llm.get_embedding(request)
        return response
        
    except Exception as e:
        logger.error(f"Embedding error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Embedding error: {str(e)}"
        )

@router.post("/tokens/count")
async def count_tokens(text: str):
    """
    ุญุณุงุจ ุนุฏุฏ tokens
    """
    try:
        llm = LLMFactory.create(LLMProvider.OPENAI)
        token_count = await llm.count_tokens(text)
        
        return {
            "text_length": len(text),
            "token_count": token_count,
            "ratio": len(text) / token_count if token_count > 0 else 0
        }
        
    except Exception as e:
        logger.error(f"Token counting error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error: {str(e)}"
        )
```


#### 8. ุชุญุฏูุซ main.py:

```python
from fastapi import FastAPI
from config import settings
from database.connection import db
from database.indexes import create_indexes
from routes import documents, queries, llm  # ุฅุถุงูุฉ llm
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version
)

# ุชุถููู ุงูู routers
app.include_router(documents.router)
app.include_router(queries.router)
app.include_router(llm.router)  # LLM routes

@app.on_event("startup")
async def startup_event():
    logger.info(f"๐ Starting {settings.app_name}")
    await db.connect_to_database()
    await create_indexes()
    logger.info("โ Application started")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("๐ Shutting down...")
    await db.close_database_connection()
    logger.info("โ Shutdown complete")
```


#### 9. ุงุฎุชุจุงุฑ LLM Integration:

```bash
# ุงุฎุชุจุงุฑ ุงููุญุงุฏุซุฉ
$ curl -X POST "http://localhost:5000/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "What is RAG in AI?"}
    ],
    "temperature": 0.7
  }'

# Response:
{
  "content": "RAG stands for Retrieval-Augmented Generation...",
  "model": "gpt-3.5-turbo",
  "tokens_used": 150,
  "finish_reason": "stop"
}

# ุงุฎุชุจุงุฑ embedding
$ curl -X POST "http://localhost:5000/llm/embedding" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Machine learning is amazing"
  }'

# ุญุณุงุจ tokens
$ curl -X POST "http://localhost:5000/llm/tokens/count?text=Hello+World"
```


***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ ุนุดุฑ: Vector DB Factory | QDrant (1:02 ุณุงุนุฉ)

**Branch:** `tut-009`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููู Vector Databases
- ุฅุนุฏุงุฏ QDrant
- ุชุฎุฒูู Embeddings
- Similarity Search


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ูุง ูู Vector Databaseุ

- **ุชุฎุฒูู Embeddings**: vectors ุนุฏุฏูุฉ ุชูุซู ุงููุนูู
- **Similarity Search**: ุงูุจุญุซ ุจุงูุชุดุงุจู ุงูุฏูุงูู
- **High Performance**: ูุญุณูู ููุจุญุซ ูู vectors

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ููููุฏูู ุงูุฎุงูุณ ุนุดุฑ:

***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ ุนุดุฑ: Vector DB Factory | QDrant (ุชูููุฉ)

### ุงูููุงููู ุงูุฃุณุงุณูุฉ (ุชูููุฉ):

#### 2. ููุงุฐุง QDrantุ

- **Performance**: ุณุฑูุน ุฌุฏุงู ูู similarity search
- **Open Source**: ูุฌุงูู ูููุชูุญ ุงููุตุฏุฑ
- **Docker Support**: ุณูู ุงูุชุดุบูู
- **Rich Features**: filtering, payload, collections


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ docker/docker-compose.yml:

```yaml
version: '3.8'

services:
  # MongoDB (ููุฌูุฏ ุณุงุจูุงู)
  mongodb:
    image: mongo:6.0
    # ... ุงูุฅุนุฏุงุฏุงุช ุงูุณุงุจูุฉ

  # QDrant Vector Database
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: minirag-qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC
    volumes:
      - qdrant_storage:/qdrant/storage
    networks:
      - minirag-network
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/"]
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  mongodb_data:
  mongodb_config:
  qdrant_storage:  # ุชุฎุฒูู QDrant

networks:
  minirag-network:
    driver: bridge
```


#### 2. ุชุญุฏูุซ requirements.txt:

```txt
# ุงูููุชุจุงุช ุงูุณุงุจูุฉ...
qdrant-client==1.7.0      # QDrant client
numpy==1.24.3             # ููุนูููุงุช ุนูู vectors
```


#### 3. ุชุญุฏูุซ config.py:

```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    # ... ุงูุฅุนุฏุงุฏุงุช ุงูุณุงุจูุฉ
    
    # QDrant Settings
    qdrant_url: str = "http://localhost:6333"
    qdrant_collection_name: str = "documents_chunks"
    qdrant_vector_size: int = 1536  # OpenAI embedding size
    qdrant_distance: str = "Cosine"  # ุฃู "Euclidean" ุฃู "Dot"
    
    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```


#### 4. ููู vectordb/base_vectordb.py:

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional

class BaseVectorDB(ABC):
    """
    Abstract base class ูุฌููุน Vector DBs
    """
    
    @abstractmethod
    async def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine"
    ):
        """
        ุฅูุดุงุก collection ุฌุฏูุฏ
        """
        pass
    
    @abstractmethod
    async def insert_vectors(
        self,
        collection_name: str,
        vectors: List[List[float]],
        payloads: List[Dict[str, Any]],
        ids: Optional[List[str]] = None
    ):
        """
        ุฅุฏุฑุงุฌ vectors
        """
        pass
    
    @abstractmethod
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        limit: int = 10,
        score_threshold: Optional[float] = None,
        filter_dict: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        ุงูุจุญุซ ุจุงูุชุดุงุจู
        """
        pass
    
    @abstractmethod
    async def delete_by_filter(
        self,
        collection_name: str,
        filter_dict: Dict[str, Any]
    ):
        """
        ุญุฐู vectors ุญุณุจ filter
        """
        pass
    
    @abstractmethod
    async def get_collection_info(self, collection_name: str) -> Dict:
        """
        ุงูุญุตูู ุนูู ูุนูููุงุช collection
        """
        pass
```


#### 5. ููู vectordb/qdrant_db.py:

```python
from vectordb.base_vectordb import BaseVectorDB
from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance, 
    VectorParams, 
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue
)
from typing import List, Dict, Any, Optional
import logging
import uuid

logger = logging.getLogger(__name__)

class QdrantDB(BaseVectorDB):
    """
    QDrant implementation
    """
    
    def __init__(self, url: str):
        self.client = QdrantClient(url=url)
        logger.info(f"Connected to QDrant at {url}")
    
    async def create_collection(
        self,
        collection_name: str,
        vector_size: int,
        distance: str = "Cosine"
    ):
        """
        ุฅูุดุงุก collection
        """
        try:
            # ุชุญููู distance string ุฅูู enum
            distance_map = {
                "Cosine": Distance.COSINE,
                "Euclidean": Distance.EUCLIDEAN,
                "Dot": Distance.DOT
            }
            
            distance_metric = distance_map.get(distance, Distance.COSINE)
            
            # ุงูุชุญูู ูู ูุฌูุฏ collection
            collections = self.client.get_collections().collections
            collection_names = [col.name for col in collections]
            
            if collection_name in collection_names:
                logger.info(f"Collection '{collection_name}' already exists")
                return
            
            # ุฅูุดุงุก collection
            self.client.create_collection(
                collection_name=collection_name,
                vectors_config=VectorParams(
                    size=vector_size,
                    distance=distance_metric
                )
            )
            
            logger.info(f"โ Created collection '{collection_name}'")
            
        except Exception as e:
            logger.error(f"Error creating collection: {str(e)}")
            raise
    
    async def insert_vectors(
        self,
        collection_name: str,
        vectors: List[List[float]],
        payloads: List[Dict[str, Any]],
        ids: Optional[List[str]] = None
    ):
        """
        ุฅุฏุฑุงุฌ vectors ูุน payload
        """
        try:
            # ุฅูุดุงุก IDs ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
            if ids is None:
                ids = [str(uuid.uuid4()) for _ in vectors]
            
            # ุฅูุดุงุก points
            points = []
            for i, (vector, payload, point_id) in enumerate(zip(vectors, payloads, ids)):
                point = PointStruct(
                    id=point_id,
                    vector=vector,
                    payload=payload
                )
                points.append(point)
            
            # ุฅุฏุฑุงุฌ ูู QDrant
            self.client.upsert(
                collection_name=collection_name,
                points=points
            )
            
            logger.info(f"โ Inserted {len(points)} vectors into '{collection_name}'")
            
        except Exception as e:
            logger.error(f"Error inserting vectors: {str(e)}")
            raise
    
    async def search(
        self,
        collection_name: str,
        query_vector: List[float],
        limit: int = 10,
        score_threshold: Optional[float] = None,
        filter_dict: Optional[Dict] = None
    ) -> List[Dict[str, Any]]:
        """
        ุงูุจุญุซ ุจุงูุชุดุงุจู
        """
        try:
            # ุจูุงุก filter ุฅุฐุง ูุงู ููุฌูุฏุงู
            query_filter = None
            if filter_dict:
                conditions = []
                for key, value in filter_dict.items():
                    conditions.append(
                        FieldCondition(
                            key=key,
                            match=MatchValue(value=value)
                        )
                    )
                query_filter = Filter(must=conditions)
            
            # ุงูุจุญุซ
            search_results = self.client.search(
                collection_name=collection_name,
                query_vector=query_vector,
                limit=limit,
                score_threshold=score_threshold,
                query_filter=query_filter
            )
            
            # ุชุญููู ุงููุชุงุฆุฌ
            results = []
            for hit in search_results:
                result = {
                    "id": hit.id,
                    "score": hit.score,
                    "payload": hit.payload
                }
                results.append(result)
            
            logger.info(f"Found {len(results)} results")
            
            return results
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            raise
    
    async def delete_by_filter(
        self,
        collection_name: str,
        filter_dict: Dict[str, Any]
    ):
        """
        ุญุฐู vectors
        """
        try:
            # ุจูุงุก filter
            conditions = []
            for key, value in filter_dict.items():
                conditions.append(
                    FieldCondition(
                        key=key,
                        match=MatchValue(value=value)
                    )
                )
            
            delete_filter = Filter(must=conditions)
            
            # ุงูุญุฐู
            self.client.delete(
                collection_name=collection_name,
                points_selector=delete_filter
            )
            
            logger.info(f"Deleted vectors with filter: {filter_dict}")
            
        except Exception as e:
            logger.error(f"Delete error: {str(e)}")
            raise
    
    async def get_collection_info(self, collection_name: str) -> Dict:
        """
        ูุนูููุงุช collection
        """
        try:
            info = self.client.get_collection(collection_name)
            
            return {
                "name": collection_name,
                "vectors_count": info.vectors_count,
                "points_count": info.points_count,
                "status": info.status,
                "vector_size": info.config.params.vectors.size,
                "distance": info.config.params.vectors.distance
            }
            
        except Exception as e:
            logger.error(f"Error getting collection info: {str(e)}")
            raise
```


#### 6. ููู vectordb/vectordb_factory.py:

```python
from vectordb.base_vectordb import BaseVectorDB
from vectordb.qdrant_db import QdrantDB
from config import settings
from enum import Enum
import logging

logger = logging.getLogger(__name__)

class VectorDBProvider(str, Enum):
    """
    ูุฒูุฏู Vector DB ุงููุฏุนูููู
    """
    QDRANT = "qdrant"
    PINECONE = "pinecone"
    WEAVIATE = "weaviate"
    MILVUS = "milvus"

class VectorDBFactory:
    """
    Factory ูุฅูุดุงุก Vector DB instances
    """
    
    _instance = None
    
    @staticmethod
    def create(
        provider: VectorDBProvider = VectorDBProvider.QDRANT,
        **kwargs
    ) -> BaseVectorDB:
        """
        ุฅูุดุงุก Vector DB instance
        """
        # Singleton pattern
        if VectorDBFactory._instance is not None:
            return VectorDBFactory._instance
        
        if provider == VectorDBProvider.QDRANT:
            instance = QdrantDB(
                url=kwargs.get('url') or settings.qdrant_url
            )
        
        # ูููู ุฅุถุงูุฉ providers ุฃุฎุฑู
        # elif provider == VectorDBProvider.PINECONE:
        #     instance = PineconeDB(...)
        
        else:
            raise ValueError(f"Unsupported vector DB: {provider}")
        
        VectorDBFactory._instance = instance
        logger.info(f"Created VectorDB instance: {provider.value}")
        
        return instance
    
    @staticmethod
    def reset():
        """
        ุฅุนุงุฏุฉ ุชุนููู instance
        """
        VectorDBFactory._instance = None
```


#### 7. ููู services/embedding_service.py:

```python
from llm.llm_factory import LLMFactory
from models.llm_models import LLMProvider, EmbeddingRequest
from vectordb.vectordb_factory import VectorDBFactory, VectorDBProvider
from database.repositories.chunk_repository import ChunkRepository
from config import settings
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class EmbeddingService:
    """
    ุฎุฏูุฉ ุฅุฏุงุฑุฉ embeddings
    """
    
    def __init__(self):
        self.llm = LLMFactory.create(LLMProvider.OPENAI)
        self.vectordb = VectorDBFactory.create(VectorDBProvider.QDRANT)
        self.chunk_repo = ChunkRepository()
    
    async def initialize_collection(self):
        """
        ุฅูุดุงุก collection ูู QDrant
        """
        await self.vectordb.create_collection(
            collection_name=settings.qdrant_collection_name,
            vector_size=settings.qdrant_vector_size,
            distance=settings.qdrant_distance
        )
    
    async def embed_and_store_chunks(self, document_id: str) -> int:
        """
        ุฅูุดุงุก embeddings ููู chunks ูุชุฎุฒูููุง
        
        Args:
            document_id: ูุนุฑู ุงููุณุชูุฏ
            
        Returns:
            ุนุฏุฏ chunks ุงูููุนุงูุฌุฉ
        """
        try:
            # ุงูุญุตูู ุนูู chunks
            chunks = await self.chunk_repo.get_by_document_id(document_id)
            
            if not chunks:
                logger.warning(f"No chunks found for document {document_id}")
                return 0
            
            logger.info(f"Processing {len(chunks)} chunks for embeddings")
            
            # ูุนุงูุฌุฉ ูู chunk
            vectors = []
            payloads = []
            ids = []
            
            for chunk in chunks:
                # ุฅูุดุงุก embedding
                embedding_request = EmbeddingRequest(text=chunk.text)
                embedding_response = await self.llm.get_embedding(embedding_request)
                
                # ุชุญุถูุฑ ุงูุจูุงูุงุช
                vectors.append(embedding_response.embedding)
                payloads.append({
                    "document_id": document_id,
                    "chunk_id": chunk.chunk_id,
                    "text": chunk.text,
                    "length": chunk.length
                })
                ids.append(str(chunk.id))

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู:

***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ ุนุดุฑ: Vector DB Factory | QDrant (ุชูููุฉ)

#### 7. ููู services/embedding_service.py (ุชูููุฉ):
```python
            # ... ุชุงุจุน ูู ุงูุณุงุจู
            
            # ุชุฎุฒูู ูู QDrant
            await self.vectordb.insert_vectors(
                collection_name=settings.qdrant_collection_name,
                vectors=vectors,
                payloads=payloads,
                ids=ids
            )
            
            # ุชุญุฏูุซ chunks ูู MongoDB ุจุฃู ููุง embeddings
            for chunk in chunks:
                await self.chunk_repo.update_embedding(
                    str(chunk.id),
                    embedding_response.embedding  # ุญูุธ ุขุฎุฑ embedding ููุซุงู
                )
            
            logger.info(f"โ Stored {len(chunks)} embeddings for document {document_id}")
            
            return len(chunks)
            
        except Exception as e:
            logger.error(f"Error embedding chunks: {str(e)}")
            raise
    
    async def search_similar_chunks(
        self,
        query: str,
        limit: int = 5,
        document_id: Optional[str] = None,
        score_threshold: float = 0.7
    ) -> List[Dict]:
        """
        ุงูุจุญุซ ุนู chunks ูุดุงุจูุฉ
        
        Args:
            query: ูุต ุงูุจุญุซ
            limit: ุนุฏุฏ ุงููุชุงุฆุฌ
            document_id: ุชุตููุฉ ุญุณุจ ูุณุชูุฏ ูุนูู
            score_threshold: ุงูุญุฏ ุงูุฃุฏูู ููุชุดุงุจู
            
        Returns:
            ูุงุฆูุฉ chunks ุงููุดุงุจูุฉ
        """
        try:
            # ุฅูุดุงุก embedding ููุงุณุชุนูุงู
            embedding_request = EmbeddingRequest(text=query)
            embedding_response = await self.llm.get_embedding(embedding_request)
            
            # ุจูุงุก filter
            filter_dict = None
            if document_id:
                filter_dict = {"document_id": document_id}
            
            # ุงูุจุญุซ ูู QDrant
            results = await self.vectordb.search(
                collection_name=settings.qdrant_collection_name,
                query_vector=embedding_response.embedding,
                limit=limit,
                score_threshold=score_threshold,
                filter_dict=filter_dict
            )
            
            logger.info(f"Found {len(results)} similar chunks for query: '{query}'")
            
            return results
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            raise
    
    async def delete_document_embeddings(self, document_id: str):
        """
        ุญุฐู ุฌููุน embeddings ูุณุชูุฏ
        """
        try:
            await self.vectordb.delete_by_filter(
                collection_name=settings.qdrant_collection_name,
                filter_dict={"document_id": document_id}
            )
            
            logger.info(f"Deleted embeddings for document {document_id}")
            
        except Exception as e:
            logger.error(f"Error deleting embeddings: {str(e)}")
            raise
```


#### 8. ุชุญุฏูุซ routes/documents.py ูุน Embeddings:

```python
from services.embedding_service import EmbeddingService

# ุฅุถุงูุฉ instance
embedding_service = EmbeddingService()

@router.post("/{document_id}/embed")
async def create_embeddings(document_id: str):
    """
    ุฅูุดุงุก embeddings ูููุณุชูุฏ
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
    
    Returns:
        ุนุฏุฏ embeddings ุงููููุดุฃุฉ
    """
    # ุงูุชุญูู ูู ุงููุณุชูุฏ
    document = await doc_repo.get_by_id(document_id)
    
    if not document:
        raise HTTPException(404, "Document not found")
    
    if document.status != "processed":
        raise HTTPException(
            400,
            f"Document must be processed first. Current status: {document.status}"
        )
    
    try:
        # ุฅูุดุงุก embeddings
        count = await embedding_service.embed_and_store_chunks(document_id)
        
        return {
            "document_id": document_id,
            "embeddings_created": count,
            "message": "Embeddings created successfully"
        }
        
    except Exception as e:
        raise HTTPException(
            500,
            f"Error creating embeddings: {str(e)}"
        )

@router.delete("/{document_id}/embeddings")
async def delete_embeddings(document_id: str):
    """
    ุญุฐู embeddings ุงููุณุชูุฏ
    """
    try:
        await embedding_service.delete_document_embeddings(document_id)
        
        return {"message": "Embeddings deleted successfully"}
        
    except Exception as e:
        raise HTTPException(
            500,
            f"Error deleting embeddings: {str(e)}"
        )
```


#### 9. ุฅุถุงูุฉ routes/search.py:

```python
from fastapi import APIRouter, HTTPException, Query
from services.embedding_service import EmbeddingService
from typing import Optional

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

embedding_service = EmbeddingService()

@router.get("/semantic")
async def semantic_search(
    q: str = Query(..., min_length=3, description="ูุต ุงูุจุญุซ"),
    limit: int = Query(5, ge=1, le=20, description="ุนุฏุฏ ุงููุชุงุฆุฌ"),
    document_id: Optional[str] = None,
    threshold: float = Query(0.7, ge=0.0, le=1.0, description="ุงูุญุฏ ุงูุฃุฏูู ููุชุดุงุจู")
):
    """
    ุงูุจุญุซ ุงูุฏูุงูู ูู ุงููุณุชูุฏุงุช
    
    Args:
        q: ูุต ุงูุจุญุซ
        limit: ุนุฏุฏ ุงููุชุงุฆุฌ
        document_id: ุชุตููุฉ ุญุณุจ ูุณุชูุฏ ูุนูู
        threshold: ุงูุญุฏ ุงูุฃุฏูู ูุฏุฑุฌุฉ ุงูุชุดุงุจู
    
    Returns:
        chunks ุงููุดุงุจูุฉ ูุน ุฏุฑุฌุงุช ุงูุชุดุงุจู
    """
    try:
        results = await embedding_service.search_similar_chunks(
            query=q,
            limit=limit,
            document_id=document_id,
            score_threshold=threshold
        )
        
        return {
            "query": q,
            "count": len(results),
            "results": [
                {
                    "chunk_id": result["payload"]["chunk_id"],
                    "document_id": result["payload"]["document_id"],
                    "text": result["payload"]["text"],
                    "similarity_score": result["score"],
                    "length": result["payload"]["length"]
                }
                for result in results
            ]
        }
        
    except Exception as e:
        raise HTTPException(
            500,
            f"Search error: {str(e)}"
        )
```


#### 10. ุชุญุฏูุซ main.py:

```python
from fastapi import FastAPI
from config import settings
from database.connection import db
from database.indexes import create_indexes
from services.embedding_service import EmbeddingService
from routes import documents, queries, llm, search  # ุฅุถุงูุฉ search
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version
)

# ุชุถููู ุงูู routers
app.include_router(documents.router)
app.include_router(queries.router)
app.include_router(llm.router)
app.include_router(search.router)  # Search routes

@app.on_event("startup")
async def startup_event():
    logger.info(f"๐ Starting {settings.app_name}")
    
    # MongoDB
    await db.connect_to_database()
    await create_indexes()
    
    # QDrant
    embedding_service = EmbeddingService()
    await embedding_service.initialize_collection()
    
    logger.info("โ Application started")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("๐ Shutting down...")
    await db.close_database_connection()
    logger.info("โ Shutdown complete")
```


#### 11. ุงุฎุชุจุงุฑ ูุงูู ููู Workflow:

```bash
# 1. ุชุดุบูู Docker (MongoDB + QDrant)
$ cd docker
$ docker compose up -d

# 2. ุชุดุบูู ุงูุชุทุจูู
$ cd ..
$ uvicorn main:app --reload

# 3. ุฑูุน ูุณุชูุฏ
$ curl -X POST "http://localhost:5000/documents/upload" \
  -F "file=@document.pdf"
# Response: {"id": "doc123", "status": "uploaded"}

# ุงูุชุธุฑ ุญุชู ูุชู ุงููุนุงูุฌุฉ (status = "processed")

# 4. ุฅูุดุงุก embeddings
$ curl -X POST "http://localhost:5000/documents/doc123/embed"
# Response: {"embeddings_created": 25}

# 5. ุงูุจุญุซ ุงูุฏูุงูู
$ curl "http://localhost:5000/search/semantic?q=machine+learning&limit=5"

# Response:
{
  "query": "machine learning",
  "count": 5,
  "results": [
    {
      "chunk_id": 0,
      "document_id": "doc123",
      "text": "Machine learning is a subset of AI...",
      "similarity_score": 0.92,
      "length": 450
    },
    ...
  ]
}
```


***

## ๐ ููุฎุต ุงูููุฏูููุงุช 11-15

### **ูุง ุชู ุฅูุฌุงุฒู:**

| ุงูููุฏูู | ุงููุฏุฉ | ุงูุฅูุฌุงุฒุงุช ุงูุฑุฆูุณูุฉ |
| :-- | :-- | :-- |
| **11. Mongo Indexing** | 1:13 | โ Indexes ูุชุนุฏุฏุฉ ููุฃุฏุงุก<br>โ Text search<br>โ Query optimization<br>โ Performance analysis |
| **12. Pipeline Enhancements** | 0:43 | โ Progress tracking<br>โ Retry mechanisms<br>โ Server-Sent Events<br>โ Better error handling |
| **13. Checkpoint-1** | 0:27 | โ ูุฑุงุฌุนุฉ ุดุงููุฉ<br>โ ุญู ุงููุดุงูู<br>โ Best practices<br>โ Code review |
| **14. LLM Factory** | 2:01 | โ OpenAI integration<br>โ Factory pattern<br>โ Chat \& Embeddings<br>โ Token counting |
| **15. Vector DB (QDrant)** | 1:02 | โ QDrant setup<br>โ Embeddings storage<br>โ Similarity search<br>โ Vector operations |

### **ุงูุชูููุงุช ุงูุฌุฏูุฏุฉ:**

```python
"""
๐ฆ ุงูููุชุจุงุช ุงููุถุงูุฉ:
โโโโโโโโโโโโโโโโโโโโโโ
- openai==1.12.0          # LLM integration
- tiktoken==0.5.2         # Token counting
- tenacity==8.2.3         # Retry logic
- qdrant-client==1.7.0    # Vector database
- numpy==1.24.3           # Vector operations
"""
```


### **ุงูู APIs ุงูุฌุฏูุฏุฉ:**

```bash
# LLM Operations
POST   /llm/chat                    # ูุญุงุฏุซุฉ ูุน LLM
POST   /llm/chat/stream            # ูุญุงุฏุซุฉ ูุน streaming
POST   /llm/embedding              # ุฅูุดุงุก embedding
POST   /llm/tokens/count           # ุญุณุงุจ tokens

# Embeddings Operations
POST   /documents/{id}/embed       # ุฅูุดุงุก embeddings ูููุณุชูุฏ
DELETE /documents/{id}/embeddings  # ุญุฐู embeddings

# Semantic Search
GET    /search/semantic            # ุงูุจุญุซ ุงูุฏูุงูู

# Monitoring
GET    /documents/{id}/progress    # ุชุชุจุน ุงูุชูุฏู
GET    /documents/{id}/progress/stream  # Stream ุงูุชูุฏู
```


### **ุงูุจููุฉ ุงููุญุฏุซุฉ:**

```
mini-rag/
โโโ llm/                          # LLM integrations
โ   โโโ base_llm.py              # Abstract base
โ   โโโ openai_llm.py            # OpenAI implementation
โ   โโโ llm_factory.py           # Factory pattern
โโโ vectordb/                     # Vector DB integrations
โ   โโโ base_vectordb.py         # Abstract base
โ   โโโ qdrant_db.py             # QDrant implementation
โ   โโโ vectordb_factory.py      # Factory pattern
โโโ services/                     # Business logic services
โ   โโโ embedding_service.py     # Embedding management
โโโ models/
โ   โโโ llm_models.py            # LLM models
โ   โโโ processing_status.py     # Progress tracking
โโโ routes/
    โโโ llm.py                   # LLM endpoints
    โโโ search.py                # Search endpoints
```


### **ุงูุฎุทูุงุช ุงููุงุฏูุฉ (16-25):**

```python
"""
๐ฎ ูุง ุณูุฃุชู ูู ุงูููุฏูููุงุช ุงููุงุฏูุฉ:

16. Semantic Search (1:24 ุณุงุนุฉ)
    - Hybrid search (text + semantic)
    - Ranking algorithms
    - Search optimization

17. Augmented Answers (1:17 ุณุงุนุฉ)
    - RAG pipeline ูุงูู
    - Context retrieval
    - Answer generation
    - Citation management

18-19. Ollama Local LLM
    - ุชุดุบูู ููุงุฐุฌ ูุญููุฉ
    - Privacy & cost reduction
    - Offline capabilities

20-21. PostgreSQL + PGVector
    - ุงูุงูุชูุงู ูู MongoDB
    - PGVector ููู embeddings
    - Migration strategies

22-23. Deployment
    - Production deployment
    - Docker optimization
    - Monitoring & logging

24-25. Celery
    - Task queues
    - Background jobs
    - Scalability
"""
```


### **๐ ูุตุงุฆุญ ููุงุณุชูุงุฏุฉ ุงููุตูู:**

1. **ุฌุฑูุจ ุจููุณู**: ูุง ุชูุชูู ุจุงููุดุงูุฏุฉุ ุงูุชุจ ุงูููุฏ
2. **ุงุฎุชุจุฑ Embeddings**: ุฌุฑุจ ูุตูุต ูุฎุชููุฉ ูุดุงูุฏ ุงููุชุงุฆุฌ
3. **ุฑุงูุจ ุงูุฃุฏุงุก**: ุงุณุชุฎุฏู QDrant dashboard
4. **ุงูุฑุฃ ุงูุชูุซูู**: OpenAI ู QDrant docs ูููุฉ
5. **ุงุณุชูุดู**: ุบููุฑ parameters ูุดุงูุฏ ุงูุชุฃุซูุฑ

***

**๐ ุชูุงูููุง! ุฃูููุช 15 ููุฏูู ูู mini-RAG**

ุงูุขู ูุฏูู:

- โ Backend ูุงูู ูุน FastAPI
- โ Database layer (MongoDB + QDrant)
- โ LL

---

# ุงููู ุงูุจุงูู

ุณุฃููู ุดุฑุญ ุจุงูู ุงูููุฏูููุงุช ูู 16 ุฅูู 25 ุจุงูุชูุตูู:

***

# ุดุฑุญ ุชูุตููู ูุฏููู ููููุฏูููุงุช 16-25 ูู ุณูุณูุฉ mini-RAG


***

## ๐ฌ ุงูููุฏูู ุงูุณุงุฏุณ ุนุดุฑ: Semantic Search (1:24 ุณุงุนุฉ)

**Branch:** `tut-010`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููู Semantic vs Keyword Search
- ุชุทุจูู Hybrid Search
- ุชุญุณูู ูุชุงุฆุฌ ุงูุจุญุซ
- Ranking algorithms


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ุงููุฑู ุจูู ุฃููุงุน ุงูุจุญุซ:

```python
"""
๐ ููุงุฑูุฉ ุฃููุงุน ุงูุจุญุซ:

1. Keyword Search (ุงูุชูููุฏู):
   - ูุจุญุซ ุนู ูููุงุช ูุทุงุจูุฉ ุจุงูุถุจุท
   - ุณุฑูุน ููู ูุญุฏูุฏ
   - ูุซุงู: "machine learning" ูู ูุฌุฏ "ML" ุฃู "artificial intelligence"

2. Semantic Search (ุงูุฏูุงูู):
   - ูููู ุงููุนูู ูุงูุณูุงู
   - ูุฌุฏ ูุชุงุฆุฌ ูุดุงุจูุฉ ุฏูุงููุงู
   - ูุซุงู: "machine learning" ูุฌุฏ "ML", "AI", "deep learning"

3. Hybrid Search:
   - ูุฌูุน ุจูู ุงูุงุซููู
   - ุฃูุถู ุงููุชุงุฆุฌ
   - ูุณุชุฎุฏูู ูู mini-RAG
"""
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู services/search_service.py:

```python
from typing import List, Dict, Optional
from services.embedding_service import EmbeddingService
from database.repositories.document_repository import DocumentRepository
from database.repositories.chunk_repository import ChunkRepository
import logging

logger = logging.getLogger(__name__)

class SearchService:
    """
    ุฎุฏูุฉ ุงูุจุญุซ ุงููุชูุฏู
    """
    
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.doc_repo = DocumentRepository()
        self.chunk_repo = ChunkRepository()
    
    async def hybrid_search(
        self,
        query: str,
        limit: int = 10,
        semantic_weight: float = 0.7,
        keyword_weight: float = 0.3,
        document_id: Optional[str] = None
    ) -> List[Dict]:
        """
        ุจุญุซ ูุฌูู (ุฏูุงูู + ูููุงุช ููุชุงุญูุฉ)
        
        Args:
            query: ูุต ุงูุจุญุซ
            limit: ุนุฏุฏ ุงููุชุงุฆุฌ
            semantic_weight: ูุฒู ุงูุจุญุซ ุงูุฏูุงูู (0-1)
            keyword_weight: ูุฒู ุงูุจุญุซ ุจุงููููุงุช (0-1)
            document_id: ุชุตููุฉ ุญุณุจ ูุณุชูุฏ
            
        Returns:
            ูุชุงุฆุฌ ูุฏูุฌุฉ ููุฑุชุจุฉ
        """
        try:
            # 1. ุงูุจุญุซ ุงูุฏูุงูู (Semantic)
            semantic_results = await self.embedding_service.search_similar_chunks(
                query=query,
                limit=limit * 2,  # ูุฃุฎุฐ ุฃูุซุฑ ููุฏูุฌ
                document_id=document_id,
                score_threshold=0.5
            )
            
            # 2. ุงูุจุญุซ ุจุงููููุงุช ุงูููุชุงุญูุฉ (Keyword)
            keyword_results = await self._keyword_search(
                query=query,
                limit=limit * 2,
                document_id=document_id
            )
            
            # 3. ุฏูุฌ ุงููุชุงุฆุฌ
            merged_results = self._merge_and_rank(
                semantic_results,
                keyword_results,
                semantic_weight,
                keyword_weight
            )
            
            # 4. ุฅุฑุฌุงุน ุฃูุถู ุงููุชุงุฆุฌ
            return merged_results[:limit]
            
        except Exception as e:
            logger.error(f"Hybrid search error: {str(e)}")
            raise
    
    async def _keyword_search(
        self,
        query: str,
        limit: int,
        document_id: Optional[str] = None
    ) -> List[Dict]:
        """
        ุงูุจุญุซ ุจุงููููุงุช ุงูููุชุงุญูุฉ ูู MongoDB
        """
        try:
            # ุงูุจุญุซ ูู documents
            documents = await self.doc_repo.search_documents(query, limit)
            
            # ุฌูุน chunks ูู ูุฐู ุงููุณุชูุฏุงุช
            results = []
            for doc in documents:
                chunks = await self.chunk_repo.get_by_document_id(str(doc.id))
                
                for chunk in chunks:
                    # ุญุณุงุจ relevance score ุจุณูุท
                    score = self._calculate_keyword_score(query, chunk.text)
                    
                    if score > 0:
                        results.append({
                            "id": str(chunk.id),
                            "score": score,
                            "payload": {
                                "document_id": str(doc.id),
                                "chunk_id": chunk.chunk_id,
                                "text": chunk.text,
                                "length": chunk.length
                            },
                            "source": "keyword"
                        })
            
            # ุชุฑุชูุจ ุญุณุจ Score
            results.sort(key=lambda x: x["score"], reverse=True)
            
            return results[:limit]
            
        except Exception as e:
            logger.error(f"Keyword search error: {str(e)}")
            return []
    
    def _calculate_keyword_score(self, query: str, text: str) -> float:
        """
        ุญุณุงุจ ุฏุฑุฌุฉ ุงูุชุทุงุจู ูููููุงุช ุงูููุชุงุญูุฉ
        """
        # ุชุญููู ูู lowercase
        query_lower = query.lower()
        text_lower = text.lower()
        
        # ุชูุณูู ููููุงุช
        query_words = set(query_lower.split())
        text_words = set(text_lower.split())
        
        # ุญุณุงุจ ุงูุชูุงุทุน
        common_words = query_words.intersection(text_words)
        
        if not query_words:
            return 0.0
        
        # ูุณุจุฉ ุงูุชุทุงุจู
        match_ratio = len(common_words) / len(query_words)
        
        # bonus ุฅุฐุง ูุงู ุงููุต ูุญุชูู ุนูู ุงูุฌููุฉ ูุงููุฉ
        if query_lower in text_lower:
            match_ratio += 0.5
        
        return min(match_ratio, 1.0)
    
    def _merge_and_rank(
        self,
        semantic_results: List[Dict],
        keyword_results: List[Dict],
        semantic_weight: float,
        keyword_weight: float
    ) -> List[Dict]:
        """
        ุฏูุฌ ูุชุฑุชูุจ ุงููุชุงุฆุฌ
        """
        # ูุงููุณ ูุชุฌููุน ุงููุชุงุฆุฌ ุญุณุจ chunk_id
        merged = {}
        
        # ุฅุถุงูุฉ ุงููุชุงุฆุฌ ุงูุฏูุงููุฉ
        for result in semantic_results:
            chunk_id = result["payload"]["chunk_id"]
            merged[chunk_id] = {
                **result,
                "semantic_score": result["score"],
                "keyword_score": 0.0,
                "combined_score": result["score"] * semantic_weight
            }
        
        # ุฅุถุงูุฉ/ุชุญุฏูุซ ุงููุชุงุฆุฌ ุงููููุงุชูุฉ
        for result in keyword_results:
            chunk_id = result["payload"]["chunk_id"]
            
            if chunk_id in merged:
                # ููุฌูุฏ: ุชุญุฏูุซ
                merged[chunk_id]["keyword_score"] = result["score"]
                merged[chunk_id]["combined_score"] += result["score"] * keyword_weight
            else:
                # ุฌุฏูุฏ: ุฅุถุงูุฉ
                merged[chunk_id] = {
                    **result,
                    "semantic_score": 0.0,
                    "keyword_score": result["score"],
                    "combined_score": result["score"] * keyword_weight
                }
        
        # ุชุญููู ููุงุฆูุฉ ูุชุฑุชูุจ
        results = list(merged.values())
        results.sort(key=lambda x: x["combined_score"], reverse=True)
        
        return results
    
    async def rerank_results(
        self,
        query: str,
        results: List[Dict],
        strategy: str = "diversity"
    ) -> List[Dict]:
        """
        ุฅุนุงุฏุฉ ุชุฑุชูุจ ุงููุชุงุฆุฌ
        
        Args:
            query: ุงูุงุณุชุนูุงู ุงูุฃุตูู
            results: ุงููุชุงุฆุฌ
            strategy: ุงุณุชุฑุงุชูุฌูุฉ ุงูุชุฑุชูุจ (diversity, relevance, recency)
        """
        if strategy == "diversity":
            # ุชูููุน ุงููุชุงุฆุฌ ูู ูุณุชูุฏุงุช ูุฎุชููุฉ
            return self._rerank_by_diversity(results)
        elif strategy == "recency":
            # ุงูุฃุญุฏุซ ุฃููุงู
            return self._rerank_by_recency(results)
        else:
            # ุงูุฃูุซุฑ ุตูุฉ (default)
            return results
    
    def _rerank_by_diversity(self, results: List[Dict]) -> List[Dict]:
        """
        ุชุฑุชูุจ ุจุงูุชูููุน - ูุชุงุฆุฌ ูู ูุณุชูุฏุงุช ูุฎุชููุฉ
        """
        reranked = []
        seen_documents = set()
        
        # ุงููุฑูุฑ ุงูุฃูู: ูุชูุฌุฉ ูุงุญุฏุฉ ูู ูู ูุณุชูุฏ
        for result in results:
            doc_id = result["payload"]["document_id"]
            if doc_id not in seen_documents:
                reranked.append(result)
                seen_documents.add(doc_id)
        
        # ุงููุฑูุฑ ุงูุซุงูู: ุจุงูู ุงููุชุงุฆุฌ
        for result in results:
            if result not in reranked:
                reranked.append(result)
        
        return reranked
    
    def _rerank_by_recency(self, results: List[Dict]) -> List[Dict]:
        """
        ุชุฑุชูุจ ุญุณุจ ุงูุชุงุฑูุฎ (ุงูุฃุญุฏุซ ุฃููุงู)
        """
        # ูุญุชุงุฌ ูู metadata ุจุงูุชุงุฑูุฎ
        # ููุง ูุฌุฑุฏ ูุซุงู
        return sorted(
            results,
            key=lambda x: x["payload"].get("created_at", 0),
            reverse=True
        )
```


#### 2. ุชุญุฏูุซ routes/search.py:

```python
from fastapi import APIRouter, HTTPException, Query
from services.search_service import SearchService
from typing import Optional, List
from enum import Enum

router = APIRouter(
    prefix="/search",
    tags=["Search"]
)

search_service = SearchService()

class SearchStrategy(str, Enum):
    """
    ุงุณุชุฑุงุชูุฌูุงุช ุงูุจุญุซ
    """
    SEMANTIC = "semantic"
    KEYWORD = "keyword"
    HYBRID = "hybrid"

class RerankStrategy(str, Enum):
    """
    ุงุณุชุฑุงุชูุฌูุงุช ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ
    """
    RELEVANCE = "relevance"
    DIVERSITY = "diversity"
    RECENCY = "recency"

@router.get("/")
async def search(
    q: str = Query(..., min_length=2, description="ูุต ุงูุจุญุซ"),
    strategy: SearchStrategy = SearchStrategy.HYBRID,
    limit: int = Query(10, ge=1, le=50),
    document_id: Optional[str] = None,
    semantic_weight: float = Query(0.7, ge=0.0, le=1.0),
    rerank: Optional[RerankStrategy] = None
):
    """
    ุงูุจุญุซ ุงููุชูุฏู ูู ุงููุณุชูุฏุงุช
    
    Args:
        q: ูุต ุงูุจุญุซ
        strategy: ุงุณุชุฑุงุชูุฌูุฉ ุงูุจุญุซ
        limit: ุนุฏุฏ ุงููุชุงุฆุฌ
        document_id: ุชุตููุฉ ุญุณุจ ูุณุชูุฏ
        semantic_weight: ูุฒู ุงูุจุญุซ ุงูุฏูุงูู
        rerank: ุงุณุชุฑุงุชูุฌูุฉ ุฅุนุงุฏุฉ ุงูุชุฑุชูุจ
    """
    try:
        # ุชูููุฐ ุงูุจุญุซ ุญุณุจ ุงูุงุณุชุฑุงุชูุฌูุฉ
        if strategy == SearchStrategy.HYBRID:
            results = await search_service.hybrid_search(
                query=q,
                limit=limit,
                semantic_weight=semantic_weight,
                keyword_weight=1.0 - semantic_weight,
                document_id=document_id
            )
        elif strategy == SearchStrategy.SEMANTIC:
            results = await search_service.embedding_service.search_similar_chunks(
                query=q,
                limit=limit,
                document_id=document_id
            )
            # ุชูุณูู ุงููุชุงุฆุฌ
            results = [{
                **r,
                "semantic_score": r["score"],
                "keyword_score": 0.0,
                "combined_score": r["score"]
            } for r in results]
        else:  # KEYWORD
            results = await search_service._keyword_search(
                query=q,
                limit=limit,
                document_id=document_id
            )
        
        # ุฅุนุงุฏุฉ ุชุฑุชูุจ ุฅุฐุง ุทููุจ
        if rerank:
            results = await search_service.rerank_results(
                query=q,
                results=results,
                strategy=rerank.value
            )
        
        # ุชูุณูู ุงูุงุณุชุฌุงุจุฉ
        return {
            "query": q,
            "strategy": strategy.value,
            "count": len(results),
            "results": [
                {
                    "chunk_id": r["payload"]["chunk_id"],
                    "document_id": r["payload"]["document_id"],
                    "text": r["payload"]["text"][:500] + "..." if len(r["payload"]["text"]) > 500 else r["payload"]["text"],
                    "scores": {
                        "semantic": r.get("semantic_score", 0.0),
                        "keyword": r.get("keyword_score", 0.0),
                        "combined": r.get("combined_score", r.get("score", 0.0))
                    }
                }
                for r in results
            ]
        }
        
    except Exception as e:
        logger.error(f"Search error: {str(e)}")
        raise HTTPException(500, f"Search error: {str(e)}")
```


#### 3. ุงุฎุชุจุงุฑ ุงูุจุญุซ ุงููุชูุฏู:

```bash
# ุจุญุซ ูุฌูู
$ curl "http://localhost:5000/search/?q=machine+learning&strategy=hybrid&limit=5"

# ุจุญุซ ุฏูุงูู ููุท
$ curl "http://localhost:5000/search/?q=AI+models&strategy=semantic"

# ุจุญุซ ุจุงููููุงุช ููุท
$ curl "http://localhost:5000/search/?q=neural+network&strategy=keyword"

# ุจุญุซ ูุน ุฅุนุงุฏุฉ ุชุฑุชูุจ
$ curl "http://localhost:5000/search/?q=deep+learning&rerank=diversity"

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุจุงูู ุงูููุฏูููุงุช:

***

## ๐ฌ ุงูููุฏูู ุงูุณุงุจุน ุนุดุฑ: Augmented Answers (1:17 ุณุงุนุฉ)
**Branch:** `tut-011`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:
- ุฅุชูุงู RAG pipeline ุงููุงูู
- Context retrieval
- Answer generation
- Citation management

### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ูุง ูู RAG ุงููุงููุ
```python
"""
๐ RAG Pipeline:

1. User Question โ "What is machine learning?"

2. Retrieval:
   - Search similar chunks (semantic)
   - Get relevant context

3. Augmentation:
   - Combine question + context
   - Build prompt for LLM

4. Generation:
   - Send to LLM
   - Get answer based on context

5. Response:
   - Answer with citations
   - Source references
"""
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู models/rag_models.py:

```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any

class ContextChunk(BaseModel):
    """
    ูุทุนุฉ ูู ุงูุณูุงู
    """
    chunk_id: int
    document_id: str
    text: str
    score: float
    metadata: Dict[str, Any] = Field(default_factory=dict)

class RAGRequest(BaseModel):
    """
    ุทูุจ RAG
    """
    question: str = Field(..., min_length=5, description="ุงูุณุคุงู")
    max_context_chunks: int = Field(5, ge=1, le=10, description="ุนุฏุฏ chunks ููุณูุงู")
    temperature: float = Field(0.7, ge=0.0, le=2.0)
    include_sources: bool = Field(True, description="ุฅุถุงูุฉ ุงููุตุงุฏุฑ")
    document_id: Optional[str] = None

class Citation(BaseModel):
    """
    ุงุณุชุดูุงุฏ/ูุตุฏุฑ
    """
    chunk_id: int
    document_id: str
    text_snippet: str
    relevance_score: float

class RAGResponse(BaseModel):
    """
    ุงุณุชุฌุงุจุฉ RAG
    """
    question: str
    answer: str
    citations: List[Citation]
    context_used: int
    confidence: float
    metadata: Dict[str, Any] = Field(default_factory=dict)
```


#### 2. ููู services/rag_service.py:

```python
from services.search_service import SearchService
from llm.llm_factory import LLMFactory
from models.llm_models import LLMProvider, ChatRequest, Message
from models.rag_models import RAGRequest, RAGResponse, Citation, ContextChunk
from typing import List
import logging

logger = logging.getLogger(__name__)

class RAGService:
    """
    ุฎุฏูุฉ RAG ุงููุงููุฉ
    """
    
    def __init__(self):
        self.search_service = SearchService()
        self.llm = LLMFactory.create(LLMProvider.OPENAI)
        
        # System prompt ููู RAG
        self.system_prompt = """ุฃูุช ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุจูุงุกู ุนูู ุงููุณุชูุฏุงุช ุงูููุฏูุฉ.

ููุงุนุฏ ุงูุฅุฌุงุจุฉ:
1. ุฃุฌุจ ููุท ุจูุงุกู ุนูู ุงููุนูููุงุช ุงูููุฌูุฏุฉ ูู ุงูุณูุงู ุงูููุฏู
2. ุฅุฐุง ูู ููู ุงูุณูุงู ูุงููุงูุ ูู "ูุง ุฃููู ูุนูููุงุช ูุงููุฉ ููุฅุฌุงุจุฉ"
3. ูู ุฏูููุงู ููุญุฏุฏุงู ูู ุฅุฌุงุจุชู
4. ุงุณุชุฎุฏู ูุบุฉ ูุงุถุญุฉ ููููููุฉ
5. ุฅุฐุง ูุงูุช ุงูุฅุฌุงุจุฉ ุบูุฑ ูุชุฃูุฏ ูููุงุ ูุถุญ ุฐูู

ุชุฐูุฑ: ูุตุฏุงููุชู ุชุนุชูุฏ ุนูู ุฏูุฉ ุฅุฌุงุจุงุชู."""
    
    async def generate_answer(self, request: RAGRequest) -> RAGResponse:
        """
        ุชูููุฏ ุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู RAG
        
        Args:
            request: ุทูุจ RAG
            
        Returns:
            ุฅุฌุงุจุฉ ูุน ูุตุงุฏุฑ
        """
        try:
            # ุงูุฎุทูุฉ 1: ุงุณุชุฑุฌุงุน ุงูุณูุงู
            logger.info(f"Retrieving context for: '{request.question}'")
            
            context_chunks = await self._retrieve_context(
                question=request.question,
                max_chunks=request.max_context_chunks,
                document_id=request.document_id
            )
            
            if not context_chunks:
                return RAGResponse(
                    question=request.question,
                    answer="ุนุฐุฑุงูุ ูู ุฃุฌุฏ ูุนูููุงุช ุฐุงุช ุตูุฉ ูู ุงููุณุชูุฏุงุช ุงููุชุงุญุฉ ููุฅุฌุงุจุฉ ุนูู ุณุคุงูู.",
                    citations=[],
                    context_used=0,
                    confidence=0.0,
                    metadata={"error": "No relevant context found"}
                )
            
            # ุงูุฎุทูุฉ 2: ุจูุงุก ุงูู prompt
            logger.info(f"Building prompt with {len(context_chunks)} chunks")
            
            prompt = self._build_prompt(
                question=request.question,
                context_chunks=context_chunks
            )
            
            # ุงูุฎุทูุฉ 3: ุชูููุฏ ุงูุฅุฌุงุจุฉ
            logger.info("Generating answer from LLM")
            
            chat_request = ChatRequest(
                messages=[
                    Message(role="system", content=self.system_prompt),
                    Message(role="user", content=prompt)
                ],
                temperature=request.temperature,
                max_tokens=1000
            )
            
            llm_response = await self.llm.chat(chat_request)
            
            # ุงูุฎุทูุฉ 4: ุงุณุชุฎุฑุงุฌ ุงููุตุงุฏุฑ
            citations = []
            if request.include_sources:
                citations = self._extract_citations(context_chunks)
            
            # ุงูุฎุทูุฉ 5: ุญุณุงุจ ุงูุซูุฉ
            confidence = self._calculate_confidence(
                context_chunks,
                llm_response.content
            )
            
            return RAGResponse(
                question=request.question,
                answer=llm_response.content,
                citations=citations,
                context_used=len(context_chunks),
                confidence=confidence,
                metadata={
                    "model": llm_response.model,
                    "tokens_used": llm_response.tokens_used,
                    "avg_context_score": sum(c.score for c in context_chunks) / len(context_chunks)
                }
            )
            
        except Exception as e:
            logger.error(f"RAG error: {str(e)}")
            raise
    
    async def _retrieve_context(
        self,
        question: str,
        max_chunks: int,
        document_id: Optional[str] = None
    ) -> List[ContextChunk]:
        """
        ุงุณุชุฑุฌุงุน ุงูุณูุงู ุฐู ุงูุตูุฉ
        """
        # ุงูุจุญุซ ุงููุฌูู
        search_results = await self.search_service.hybrid_search(
            query=question,
            limit=max_chunks,
            semantic_weight=0.8,  # ุชุฑููุฒ ุฃูุจุฑ ุนูู ุงูุฏูุงูู
            keyword_weight=0.2,
            document_id=document_id
        )
        
        # ุชุญููู ูู ContextChunk objects
        context_chunks = []
        for result in search_results:
            chunk = ContextChunk(
                chunk_id=result["payload"]["chunk_id"],
                document_id=result["payload"]["document_id"],
                text=result["payload"]["text"],
                score=result.get("combined_score", result.get("score", 0.0)),
                metadata={
                    "semantic_score": result.get("semantic_score", 0.0),
                    "keyword_score": result.get("keyword_score", 0.0)
                }
            )
            context_chunks.append(chunk)
        
        return context_chunks
    
    def _build_prompt(
        self,
        question: str,
        context_chunks: List[ContextChunk]
    ) -> str:
        """
        ุจูุงุก ุงูู prompt ุงููุงูู
        """
        # ุจูุงุก ุงูุณูุงู
        context_parts = []
        for i, chunk in enumerate(context_chunks, 1):
            context_parts.append(f"[ูุตุฏุฑ {i}]\n{chunk.text}\n")
        
        context_text = "\n".join(context_parts)
        
        # ุจูุงุก ุงูู prompt
        prompt = f"""ุงูุณูุงู ุงููุชุงุญ:

{context_text}

---

ุจูุงุกู ุนูู ุงูุณูุงู ุฃุนูุงูุ ุฃุฌุจ ุนูู ุงูุณุคุงู ุงูุชุงูู:

ุงูุณุคุงู: {question}

ุงูุฅุฌุงุจุฉ:"""
        
        return prompt
    
    def _extract_citations(
        self,
        context_chunks: List[ContextChunk]
    ) -> List[Citation]:
        """
        ุงุณุชุฎุฑุงุฌ ุงูุงุณุชุดูุงุฏุงุช ูู chunks
        """
        citations = []
        
        for chunk in context_chunks:
            # ุฃุฎุฐ ุฃูู 200 ุญุฑู ูู snippet
            snippet = chunk.text[:200]
            if len(chunk.text) > 200:
                snippet += "..."
            
            citation = Citation(
                chunk_id=chunk.chunk_id,
                document_id=chunk.document_id,
                text_snippet=snippet,
                relevance_score=chunk.score
            )
            citations.append(citation)
        
        return citations
    
    def _calculate_confidence(
        self,
        context_chunks: List[ContextChunk],
        answer: str
    ) -> float:
        """
        ุญุณุงุจ ุฏุฑุฌุฉ ุงูุซูุฉ ูู ุงูุฅุฌุงุจุฉ
        """
        if not context_chunks:
            return 0.0
        
        # ุนูุงูู ุงูุซูุฉ:
        # 1. ูุชูุณุท scores ุงูู chunks
        avg_score = sum(c.score for c in context_chunks) / len(context_chunks)
        
        # 2. ุนุฏุฏ chunks ุงููุณุชุฎุฏูุฉ (ุงููุฒูุฏ = ุฃูุถู)
        chunk_factor = min(len(context_chunks) / 5.0, 1.0)
        
        # 3. ุทูู ุงูุฅุฌุงุจุฉ (ููุณุช ูุตูุฑุฉ ุฌุฏุงู)
        length_factor = min(len(answer) / 200.0, 1.0)
        
        # 4. ุนุฏู ุงุญุชูุงุก ุงูุฅุฌุงุจุฉ ุนูู "ูุง ุฃุนุฑู" ุฃู ูุดุงุจู
        uncertainty_phrases = [
            "ูุง ุฃุนุฑู",
            "ูุง ุฃููู ูุนูููุงุช",
            "ุบูุฑ ูุชุฃูุฏ",
            "ูุง ูููููู",
            "i don't know",
            "i'm not sure"
        ]
        
        has_uncertainty = any(
            phrase in answer.lower() 
            for phrase in uncertainty_phrases
        )
        
        uncertainty_factor = 0.3 if has_uncertainty else 1.0
        
        # ุงูุญุณุงุจ ุงูููุงุฆู
        confidence = (
            avg_score * 0.4 +
            chunk_factor * 0.3 +
            length_factor * 0.2 +
            uncertainty_factor * 0.1
        )
        
        return round(min(confidence, 1.0), 2)
```


#### 3. ุฅุถุงูุฉ routes/rag.py:

```python
from fastapi import APIRouter, HTTPException, status
from fastapi.responses import StreamingResponse
from services.rag_service import RAGService
from models.rag_models import RAGRequest, RAGResponse
from typing import AsyncGenerator
import json
import logging

router = APIRouter(
    prefix="/rag",
    tags=["RAG"]
)

logger = logging.getLogger(__name__)
rag_service = RAGService()

@router.post("/answer", response_model=RAGResponse)
async def generate_answer(request: RAGRequest):
    """
    ุชูููุฏ ุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู RAG
    
    Args:
        request: ุทูุจ RAG ูุน ุงูุณุคุงู ูุงูุฅุนุฏุงุฏุงุช
    
    Returns:
        ุฅุฌุงุจุฉ ูุน ูุตุงุฏุฑ ูุงุณุชุดูุงุฏุงุช
    """
    try:
        response = await rag_service.generate_answer(request)
        return response
        
    except Exception as e:
        logger.error(f"RAG error: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Error generating answer: {str(e)}"
        )

@router.post("/answer/stream")
async def generate_answer_stream(request: RAGRequest):
    """
    ุชูููุฏ ุฅุฌุงุจุฉ ูุน streaming
    """
    async def generate() -> AsyncGenerator[str, None]:
        try:
            # ุงุณุชุฑุฌุงุน ุงูุณูุงู
            context_chunks = await rag_service._retrieve_context(
                question=request.question,
                max_chunks=request.max_context_chunks,
                document_id=request.document_id
            )
            
            if not context_chunks:
                yield json.dumps({"error": "No context found"}) + "\n"
                return
            
            # ุจูุงุก prompt
            prompt = rag_service._build_prompt(
                request.question,
                context_chunks
            )
            
            # ุฅุฑุณุงู metadata ุฃููุงู
            metadata = {
                "context_chunks": len(context_chunks),
                "question": request.question
            }
            yield f"data: {json.dumps(metadata)}\n\n"
            
            # Stream ุงูุฅุฌุงุจุฉ
            from models.llm_models import ChatRequest, Message
            
            chat_request = ChatRequest(
                messages=[
                    Message(role="system", content=rag_service.system_prompt),
                    Message(role="user", content=prompt)
                ],
                temperature=request.temperature,
                stream=True
            )
            
            async for chunk in rag_service.llm.chat_stream(chat_request):
                yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            
            # ุฅุฑุณุงู citations ูู ุงูููุงูุฉ
            if request.include_sources:
                citations = rag_service._extract_citations(context_chunks)
                citations_data = {
                    "citations": [c.dict() for c in citations]
                }
                yield f"data: {json.dumps(citations_data)}\n\n"
            
        except Exception as e:
            logger.error(f"Stream error: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุจุงูู ุงูููุฏูููุงุช:

***

## ๐ฌ ุงูููุฏูู ุงูุณุงุจุน ุนุดุฑ: Augmented Answers (ุชูููุฉ)

#### 4. ุงุฎุชุจุงุฑ RAG ุงููุงูู:
```bash
# ุชูููุฏ ุฅุฌุงุจุฉ
$ curl -X POST "http://localhost:5000/rag/answer" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is machine learning?",
    "max_context_chunks": 5,
    "temperature": 0.7,
    "include_sources": true
  }'

# Response:
{
  "question": "What is machine learning?",
  "answer": "Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed...",
  "citations": [
    {
      "chunk_id": 0,
      "document_id": "doc123",
      "text_snippet": "Machine learning algorithms use statistical techniques...",
      "relevance_score": 0.92
    }
  ],
  "context_used": 5,
  "confidence": 0.87,
  "metadata": {
    "model": "gpt-3.5-turbo",
    "tokens_used": 450
  }
}
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู ุนุดุฑ: Checkpoint-2 | Fixes (21 ุฏูููุฉ)

**Branch:** `tut-012`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ูุฑุงุฌุนุฉ ุงููุฑุญูุฉ ุงูุซุงููุฉ
- ุฅุตูุงุญ ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ
- ุชุญุณููุงุช ูุชูุธูู ุงูููุฏ


### ุงููุญุชูู ุงูุฃุณุงุณู:

#### 1. ุงูุฃุฎุทุงุก ุงูุดุงุฆุนุฉ ูุญููููุง:

```python
# ููู docs/checkpoint2_fixes.md

"""
๐ง ุฅุตูุงุญุงุช Checkpoint 2:

1. Embedding Errors:
   โ ุงููุดููุฉ: Embeddings ุชูุดู ูููุตูุต ุงูุทูููุฉ
   โ ุงูุญู: ุชูุณูู ุงููุต ูุจู embedding
   
   # ูุจู
   embedding = await llm.get_embedding(long_text)
   
   # ุจุนุฏ
   if len(long_text) > 8000:
       long_text = long_text[:8000]
   embedding = await llm.get_embedding(long_text)

2. QDrant Connection Issues:
   โ ุงููุดููุฉ: Connection timeout
   โ ุงูุญู: ุฒูุงุฏุฉ timeout ูุฅุถุงูุฉ retry
   
   client = QdrantClient(
       url=url,
       timeout=30,
       prefer_grpc=False
   )

3. Search Returns Empty:
   โ ุงููุดููุฉ: ุงูุจุญุซ ูุง ูุฑุฌุน ูุชุงุฆุฌ
   โ ุงูุญู: ุชูููู score_threshold
   
   # ูุจู
   results = search(query, score_threshold=0.9)
   
   # ุจุนุฏ
   results = search(query, score_threshold=0.5)

4. Context Too Long for LLM:
   โ ุงููุดููุฉ: Prompt ูุชุฌุงูุฒ max_tokens
   โ ุงูุญู: ุชูููุต ุงูุณูุงู
   
   # ุญุณุงุจ tokens ูุชูููุต ุฅุฐุง ูุฒู
   total_tokens = await llm.count_tokens(prompt)
   if total_tokens > 3000:
       # ุชูููู ุนุฏุฏ chunks
       context_chunks = context_chunks[:3]

5. Citations Missing:
   โ ุงููุดููุฉ: ุงููุตุงุฏุฑ ูุง ุชุธูุฑ
   โ ุงูุญู: ุชุฃูุฏ ูู include_sources=True
"""
```


***

## ๐ฌ ุงูููุฏูู ุงูุชุงุณุน ุนุดุฑ: Ollama Local LLM Server (50 ุฏูููุฉ)

**Branch:** `tut-012` (ุงุณุชูุฑุงุฑ)

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชุดุบูู LLM ูุญููุงู
- ุงุณุชุฎุฏุงู Ollama
- ุชูููู ุงูุชูููุฉ ูุงูุงุนุชูุงุฏ ุนูู APIs ุฎุงุฑุฌูุฉ


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง Ollamaุ

```python
"""
๐ก ูููุฒุงุช Ollama:

โ ูุฌุงูู ุชูุงูุงู
โ ูุนูู offline
โ ูุง ุญุฏูุฏ ููุงุณุชุฎุฏุงู
โ ุฎุตูุตูุฉ ูุงููุฉ
โ ููุงุฐุฌ ูุชุนุฏุฏุฉ (Llama, Mistral, etc.)

โ๏ธ ุงูุนููุจ:
- ูุญุชุงุฌ GPU ููู
- ุฃุจุทุฃ ูู OpenAI
- ุฌูุฏุฉ ุฃูู ููููุงู
"""
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุดุบูู Ollama ูุน Colab + Ngrok:

```python
# ููู notebooks/ollama_setup.ipynb

"""
๐ Colab Notebook ูุชุดุบูู Ollama

# 1. ุชุซุจูุช Ollama
!curl https://ollama.ai/install.sh | sh

# 2. ุชุดุบูู Ollama ูู ุงูุฎูููุฉ
import subprocess
import time

ollama_process = subprocess.Popen(
    ['ollama', 'serve'],
    stdout=subprocess.PIPE,
    stderr=subprocess.PIPE
)
time.sleep(5)

# 3. ุชุญููู ูููุฐุฌ
!ollama pull llama2:7b

# 4. ุชุซุจูุช ngrok
!pip install pyngrok

# 5. ุฅูุดุงุก tunnel
from pyngrok import ngrok

# ุฅูุดุงุก tunnel ูููููุฐ 11434 (Ollama default)
public_url = ngrok.connect(11434, "http")
print(f"๐ Ollama URL: {public_url}")

# ุงุณุชุฎุฏู ูุฐุง URL ูู .env:
# OLLAMA_BASE_URL=https://xxxx-xx-xx-xxx-xxx.ngrok.io
"""
```


#### 2. ููู llm/ollama_llm.py:

```python
from llm.base_llm import BaseLLM
from models.llm_models import ChatRequest, ChatResponse, EmbeddingRequest, EmbeddingResponse
from typing import AsyncGenerator
import aiohttp
import logging

logger = logging.getLogger(__name__)

class OllamaLLM(BaseLLM):
    """
    Ollama LLM implementation
    """
    
    def __init__(
        self, 
        base_url: str = "http://localhost:11434",
        model: str = "llama2",
        embedding_model: str = "nomic-embed-text"
    ):
        super().__init__(api_key="", model=model)  # Ollama ูุง ูุญุชุงุฌ API key
        self.base_url = base_url.rstrip('/')
        self.embedding_model = embedding_model
    
    async def chat(self, request: ChatRequest) -> ChatResponse:
        """
        ูุญุงุฏุซุฉ ูุน Ollama
        """
        try:
            # ุชุญููู messages ูู prompt
            prompt = self._messages_to_prompt(request.messages)
            
            # ุงุณุชุฏุนุงุก Ollama API
            url = f"{self.base_url}/api/generate"
            
            payload = {
                "model": request.model or self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": request.temperature,
                    "num_predict": request.max_tokens or 2000
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status != 200:
                        raise Exception(f"Ollama error: {response.status}")
                    
                    result = await response.json()
            
            return ChatResponse(
                content=result["response"],
                model=result["model"],
                tokens_used=result.get("eval_count", 0),
                finish_reason="stop",
                metadata={
                    "total_duration": result.get("total_duration"),
                    "load_duration": result.get("load_duration")
                }
            )
            
        except Exception as e:
            logger.error(f"Ollama chat error: {str(e)}")
            raise
    
    async def chat_stream(
        self, 
        request: ChatRequest
    ) -> AsyncGenerator[str, None]:
        """
        ูุญุงุฏุซุฉ ูุน streaming
        """
        try:
            prompt = self._messages_to_prompt(request.messages)
            url = f"{self.base_url}/api/generate"
            
            payload = {
                "model": request.model or self.model,
                "prompt": prompt,
                "stream": True,
                "options": {
                    "temperature": request.temperature
                }
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    async for line in response.content:
                        if line:
                            import json
                            data = json.loads(line)
                            if "response" in data:
                                yield data["response"]
                            
                            if data.get("done", False):
                                break
                                
        except Exception as e:
            logger.error(f"Ollama stream error: {str(e)}")
            raise
    
    async def get_embedding(
        self, 
        request: EmbeddingRequest
    ) -> EmbeddingResponse:
        """
        ุงูุญุตูู ุนูู embedding
        """
        try:
            url = f"{self.base_url}/api/embeddings"
            
            payload = {
                "model": request.model or self.embedding_model,
                "prompt": request.text
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(url, json=payload) as response:
                    if response.status != 200:
                        raise Exception(f"Ollama error: {response.status}")
                    
                    result = await response.json()
            
            return EmbeddingResponse(
                embedding=result["embedding"],
                model=request.model or self.embedding_model,
                tokens_used=len(request.text.split())  # ุชูุฏูุฑ
            )
            
        except Exception as e:
            logger.error(f"Ollama embedding error: {str(e)}")
            raise
    
    async def count_tokens(self, text: str) -> int:
        """
        ุญุณุงุจ tokens (ุชูุฑูุจู)
        """
        # Ollama ูุง ูููุฑ token counting ุฏููู
        # ุชูุฏูุฑ: ูู 4 ุฃุญุฑู = 1 token
        return len(text) // 4
    
    def _messages_to_prompt(self, messages: List) -> str:
        """
        ุชุญููู messages ูู prompt ูุงุญุฏ
        """
        prompt_parts = []
        
        for msg in messages:
            role = msg.role
            content = msg.content
            
            if role == "system":
                prompt_parts.append(f"System: {content}\n")
            elif role == "user":
                prompt_parts.append(f"User: {content}\n")
            elif role == "assistant":
                prompt_parts.append(f"Assistant: {content}\n")
        
        prompt_parts.append("Assistant:")
        
        return "\n".join(prompt_parts)
```


#### 3. ุชุญุฏูุซ llm/llm_factory.py:

```python
from llm.base_llm import BaseLLM
from llm.openai_llm import OpenAILLM
from llm.ollama_llm import OllamaLLM
from models.llm_models import LLMProvider
from config import settings

class LLMProvider(str, Enum):
    OPENAI = "openai"
    OLLAMA = "ollama"  # ุฅุถุงูุฉ

class LLMFactory:
    @staticmethod
    def create(
        provider: LLMProvider = LLMProvider.OPENAI,
        **kwargs
    ) -> BaseLLM:
        if provider == LLMProvider.OPENAI:
            instance = OpenAILLM(
                api_key=kwargs.get('api_key') or settings.openai_api_key,
                model=kwargs.get('model') or settings.openai_model
            )
        
        elif provider == LLMProvider.OLLAMA:
            instance = OllamaLLM(
                base_url=kwargs.get('base_url') or settings.ollama_base_url,
                model=kwargs.get('model') or settings.ollama_model
            )
        
        else:
            raise ValueError(f"Unsupported provider: {provider}")
        
        return instance
```


#### 4. ุชุญุฏูุซ config.py:

```python
class Settings(BaseSettings):
    # ... ุฅุนุฏุงุฏุงุช ุณุงุจูุฉ
    
    # Ollama Settings
    ollama_base_url: str = "http://localhost:11434"
    ollama_model: str = "llama2"
    ollama_embedding_model: str = "nomic-embed-text"
    
    # Default LLM Provider
    default_llm_provider: str = "openai"  # ุฃู "ollama"
```


#### 5. ุงุฎุชุจุงุฑ Ollama:

```bash
# ุชุดุบูู Ollama ูุญููุงู
$ ollama serve

# ูู terminal ุขุฎุฑ: ุชุญููู ูููุฐุฌ
$ ollama pull llama2

# ุงุฎุชุจุงุฑ
$ curl -X POST "http://localhost:5000/llm/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "provider": "ollama"
  }'
```


***

## ๐ฌ ุงูููุฏูู ุงูุนุดุฑูู: From Mongo to Postgres + SQLAlchemy \& Alembic (2:17 ุณุงุนุฉ)

**Branch:** `tut-013`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุงูุงูุชูุงู ูู MongoDB ุฅูู PostgreSQL
- ุงุณุชุฎุฏุงู SQLAlchemy ORM
- Database migrations ูุน Alembic
- ุฃุณุจุงุจ ุงูุชุบููุฑ


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง PostgreSQLุ

```python
"""
๐ MongoDB vs PostgreSQL:

MongoDB (NoSQL):
โ ูุฑููุฉ ูู ุงูุจูุงูุงุช
โ ุณูู ุงูุจุฏุงูุฉ
โ ูุง ูุฏุนู JOIN ููู
โ Transactions ูุญุฏูุฏุฉ
โ ูุง ุชูุฌุฏ PGVector native

PostgreSQL (SQL):
โ ACID compliance
โ Relations ู JOINs ูููุฉ
โ PGVector extension ููู embeddings
โ ุฃุฏุงุก ุฃูุถู ููุจูุงูุงุช ุงููู

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุจุงูู ุงูููุฏูููุงุช:

***

## ๐ฌ ุงูููุฏูู ุงูุนุดุฑูู: From Mongo to Postgres (ุชูููุฉ)

### ุงูููุงููู ุงูุฃุณุงุณูุฉ (ุชูููุฉ):

#### 1. ููุงุฐุง PostgreSQLุ (ุชูููุฉ)
```python
"""
PostgreSQL (SQL):
โ ACID compliance
โ Relations ู JOINs ูููุฉ
โ PGVector extension ููู embeddings
โ ุฃุฏุงุก ุฃูุถู ููุจูุงูุงุช ุงูููุธูุฉ
โ Mature ecosystem
โ Free & Open Source

ุงููุฑุงุฑ: PostgreSQL ุฃูุถู ูู Production RAG
"""
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ docker/docker-compose.yml:

```yaml
version: '3.8'

services:
  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    container_name: minirag-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - minirag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # PGAdmin (Web UI ุงุฎุชูุงุฑู)
  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: minirag-pgadmin
    restart: unless-stopped
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
    depends_on:
      - postgres
    networks:
      - minirag-network

volumes:
  postgres_data:
  qdrant_storage:

networks:
  minirag-network:
    driver: bridge
```


#### 2. ุชุญุฏูุซ requirements.txt:

```txt
# ... ุงูููุชุจุงุช ุงูุณุงุจูุฉ
sqlalchemy==2.0.25        # ORM
asyncpg==0.29.0          # PostgreSQL async driver
alembic==1.13.1          # Database migrations
psycopg2-binary==2.9.9   # PostgreSQL sync driver
```


#### 3. ููู database/postgres_connection.py:

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import declarative_base
from config import settings
import logging

logger = logging.getLogger(__name__)

# Base class ููู models
Base = declarative_base()

# Engine
engine = create_async_engine(
    settings.postgres_url,
    echo=settings.debug,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True
)

# Session factory
AsyncSessionLocal = async_sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def init_db():
    """
    ุฅูุดุงุก ุงูุฌุฏุงูู
    """
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
    logger.info("โ Database tables created")

async def get_session() -> AsyncSession:
    """
    Dependency ููุญุตูู ุนูู session
    """
    async with AsyncSessionLocal() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()
```


#### 4. ููู database/models.py:

```python
from sqlalchemy import Column, Integer, String, Text, DateTime, Float, Boolean, ForeignKey, Index
from sqlalchemy.orm import relationship
from sqlalchemy.dialects.postgresql import ARRAY
from datetime import datetime
from database.postgres_connection import Base

class Document(Base):
    """
    ุฌุฏูู ุงููุณุชูุฏุงุช
    """
    __tablename__ = "documents"
    
    id = Column(Integer, primary_key=True, index=True)
    filename = Column(String(255), nullable=False, index=True)
    file_path = Column(String(500), nullable=False)
    file_size = Column(Integer, nullable=False)
    content_type = Column(String(100), nullable=False)
    
    status = Column(String(50), nullable=False, default="uploaded", index=True)
    raw_text = Column(Text, nullable=True)
    clean_text = Column(Text, nullable=True)
    
    upload_date = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    processed_at = Column(DateTime, nullable=True)
    
    chunk_count = Column(Integer, default=0)
    error_message = Column(Text, nullable=True)
    
    # Relationship
    chunks = relationship("Chunk", back_populates="document", cascade="all, delete-orphan")
    
    # Indexes
    __table_args__ = (
        Index('idx_status_upload', 'status', 'upload_date'),
    )

class Chunk(Base):
    """
    ุฌุฏูู ุงูู chunks
    """
    __tablename__ = "chunks"
    
    id = Column(Integer, primary_key=True, index=True)
    document_id = Column(Integer, ForeignKey("documents.id", ondelete="CASCADE"), nullable=False, index=True)
    chunk_id = Column(Integer, nullable=False)
    
    text = Column(Text, nullable=False)
    start_char = Column(Integer, nullable=False)
    end_char = Column(Integer, nullable=False)
    length = Column(Integer, nullable=False)
    
    # Embedding (ุณูุชู ูููู ูู PGVector ูุงุญูุงู)
    has_embedding = Column(Boolean, default=False)
    
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationship
    document = relationship("Document", back_populates="chunks")
    
    # Indexes
    __table_args__ = (
        Index('idx_document_chunk', 'document_id', 'chunk_id', unique=True),
    )
```


#### 5. ููู alembic.ini:

```ini
[alembic]
script_location = alembic
prepend_sys_path = .
sqlalchemy.url = postgresql+asyncpg://user:password@localhost:5432/minirag

[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
```


#### 6. ุฅูุดุงุก Migrations:

```bash
# ุชููุฆุฉ Alembic
$ alembic init alembic

# ุฅูุดุงุก migration ุฃููู
$ alembic revision --autogenerate -m "Initial schema"

# ุชุทุจูู migrations
$ alembic upgrade head

# ุงูุชุฑุงุฌุน
$ alembic downgrade -1
```


#### 7. ููู database/repositories/document_repository_pg.py:

```python
from sqlalchemy import select, func, or_
from sqlalchemy.ext.asyncio import AsyncSession
from database.models import Document, Chunk
from typing import List, Optional
import logging

logger = logging.getLogger(__name__)

class DocumentRepositoryPG:
    """
    Repository ูููุณุชูุฏุงุช ูุน PostgreSQL
    """
    
    def __init__(self, session: AsyncSession):
        self.session = session
    
    async def create(self, document_data: dict) -> Document:
        """
        ุฅูุดุงุก ูุณุชูุฏ
        """
        document = Document(**document_data)
        self.session.add(document)
        await self.session.flush()
        return document
    
    async def get_by_id(self, document_id: int) -> Optional[Document]:
        """
        ุงูุญุตูู ุนูู ูุณุชูุฏ
        """
        result = await self.session.execute(
            select(Document).where(Document.id == document_id)
        )
        return result.scalar_one_or_none()
    
    async def get_all(
        self,
        skip: int = 0,
        limit: int = 10,
        status: Optional[str] = None
    ) -> List[Document]:
        """
        ุงูุญุตูู ุนูู ูุงุฆูุฉ ุงููุณุชูุฏุงุช
        """
        query = select(Document)
        
        if status:
            query = query.where(Document.status == status)
        
        query = query.order_by(Document.upload_date.desc())
        query = query.offset(skip).limit(limit)
        
        result = await self.session.execute(query)
        return result.scalars().all()
    
    async def update(self, document_id: int, update_data: dict) -> bool:
        """
        ุชุญุฏูุซ ูุณุชูุฏ
        """
        document = await self.get_by_id(document_id)
        if not document:
            return False
        
        for key, value in update_data.items():
            if hasattr(document, key):
                setattr(document, key, value)
        
        await self.session.flush()
        return True
    
    async def delete(self, document_id: int) -> bool:
        """
        ุญุฐู ูุณุชูุฏ
        """
        document = await self.get_by_id(document_id)
        if not document:
            return False
        
        await self.session.delete(document)
        await self.session.flush()
        return True
    
    async def count(self, status: Optional[str] = None) -> int:
        """
        ุนุฏ ุงููุณุชูุฏุงุช
        """
        query = select(func.count(Document.id))
        
        if status:
            query = query.where(Document.status == status)
        
        result = await self.session.execute(query)
        return result.scalar()
    
    async def search(self, query: str, limit: int = 10) -> List[Document]:
        """
        ุงูุจุญุซ ูู ุงููุณุชูุฏุงุช
        """
        search_query = select(Document).where(
            or_(
                Document.filename.ilike(f"%{query}%"),
                Document.clean_text.ilike(f"%{query}%")
            )
        ).limit(limit)
        
        result = await self.session.execute(search_query)
        return result.scalars().all()
```


***

## ๐ฌ ุงูููุฏูู ุงูุญุงุฏู ูุงูุนุดุฑูู: The Way to PGVector (3:39 ุณุงุนุฉ)

**Branch:** `tut-014`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชุซุจูุช PGVector extension
- ุฏูุฌ Embeddings ูู PostgreSQL
- Vector similarity search
- ุงูุงูุชูุงู ูู QDrant


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุซุจูุช PGVector:

```sql
-- ูู PostgreSQL
CREATE EXTENSION IF NOT EXISTS vector;

-- ุงูุชุญูู
SELECT * FROM pg_extension WHERE extname = 'vector';
```


#### 2. ุชุญุฏูุซ database/models.py:

```python
from sqlalchemy import Column, Integer, Text, DateTime, ForeignKey, Index
from sqlalchemy.dialects.postgresql import ARRAY
from pgvector.sqlalchemy import Vector  # ุฅุถุงูุฉ
from database.postgres_connection import Base

class Chunk(Base):
    """
    ุฌุฏูู ุงูู chunks ูุน embeddings
    """
    __tablename__ = "chunks"
    
    id = Column(Integer, primary_key=True, index=True)
    document_id = Column(Integer, ForeignKey("documents.id", ondelete="CASCADE"), nullable=False, index=True)
    chunk_id = Column(Integer, nullable=False)
    
    text = Column(Text, nullable=False)
    start_char = Column(Integer, nullable=False)
    end_char = Column(Integer, nullable=False)
    length = Column(Integer, nullable=False)
    
    # Embedding vector (1536 dimensions for OpenAI)
    embedding = Column(Vector(1536), nullable=True)
    
    created_at = Column(DateTime, default=datetime.utcnow, nullable=False, index=True)
    
    # Relationship
    document = relationship("Document", back_populates="chunks")
    
    # Indexes
    __table_args__ = (
        Index('idx_document_chunk', 'document_id', 'chunk_id', unique=True),
        # Vector index ููุจุญุซ ุงูุณุฑูุน
        Index('idx_embedding_vector', 'embedding', postgresql_using='ivfflat', postgresql_with={'lists': 100}),
    )
```


#### 3. ููู services/pgvector_service.py:

```python
from sqlalchemy import select, func, text
from sqlalchemy.ext.asyncio import AsyncSession
from database.models import Chunk
from llm.llm_factory import LLMFactory
from models.llm_models import LLMProvider, EmbeddingRequest
from typing import List, Dict
import logging

logger = logging.getLogger(__name__)

class PGVectorService:
    """
    ุฎุฏูุฉ PGVector ููุจุญุซ ุงูุฏูุงูู
    """
    
    def __init__(self, session: AsyncSession):
        self.session = session
        self.llm = LLMFactory.create(LLMProvider.OPENAI)
    
    async def embed_and_store_chunks(self, document_id: int) -> int:
        """
        ุฅูุดุงุก embeddings ููู chunks ูุญูุธูุง
        """
        try:
            # ุงูุญุตูู ุนูู chunks ุจุฏูู embeddings
            result = await self.session.execute(
                select(Chunk).where(
                    Chunk.document_id == document_id,
                    Chunk.embedding.is_(None)
                )
            )
            chunks = result.scalars().all()
            
            if not chunks:
                logger.warning(f"No chunks to embed for document {document_id}")
                return 0
            
            logger.info(f"Embedding {len(chunks)} chunks")
            
            # ูุนุงูุฌุฉ ูู chunk
            for chunk in chunks:
                # ุฅูุดุงุก embedding
                embedding_request = EmbeddingRequest(text=chunk.text)
                embedding_response = await self.llm.get_embedding(embedding_request)
                
                # ุญูุธ embedding
                chunk.embedding = embedding_response.embedding
            
            await self.session.flush()
            
            logger.info(f"โ Embedded {len(chunks)} chunks")
            return len(chunks)
            
        

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุจุงูู ุงูููุฏูููุงุช:

***

## ๐ฌ ุงูููุฏูู ุงูุญุงุฏู ูุงูุนุดุฑูู: PGVector (ุชูููุฉ)

#### 3. ููู services/pgvector_service.py (ุชูููุฉ):
```python
        except Exception as e:
            logger.error(f"Error embedding chunks: {str(e)}")
            raise
    
    async def search_similar(
        self,
        query: str,
        limit: int = 5,
        document_id: Optional[int] = None,
        distance_threshold: float = 1.0
    ) -> List[Dict]:
        """
        ุงูุจุญุซ ุจุงูุชุดุงุจู ุงูุฏูุงูู
        
        Args:
            query: ูุต ุงูุจุญุซ
            limit: ุนุฏุฏ ุงููุชุงุฆุฌ
            document_id: ุชุตููุฉ ุญุณุจ ูุณุชูุฏ
            distance_threshold: ุงูุญุฏ ุงูุฃูุตู ูููุณุงูุฉ
        """
        try:
            # ุฅูุดุงุก embedding ููุงุณุชุนูุงู
            embedding_request = EmbeddingRequest(text=query)
            embedding_response = await self.llm.get_embedding(embedding_request)
            query_vector = embedding_response.embedding
            
            # ุจูุงุก query
            # ุงุณุชุฎุฏุงู cosine distance: 1 - cosine_similarity
            query_sql = select(
                Chunk,
                (1 - Chunk.embedding.cosine_distance(query_vector)).label('similarity')
            ).where(
                Chunk.embedding.is_not(None)
            )
            
            # ุชุตููุฉ ุญุณุจ document
            if document_id:
                query_sql = query_sql.where(Chunk.document_id == document_id)
            
            # ุชุตููุฉ ุญุณุจ distance
            query_sql = query_sql.where(
                Chunk.embedding.cosine_distance(query_vector) < distance_threshold
            )
            
            # ุชุฑุชูุจ ูุชุญุฏูุฏ
            query_sql = query_sql.order_by(
                Chunk.embedding.cosine_distance(query_vector)
            ).limit(limit)
            
            result = await self.session.execute(query_sql)
            rows = result.all()
            
            # ุชูุณูู ุงููุชุงุฆุฌ
            results = []
            for chunk, similarity in rows:
                results.append({
                    "chunk_id": chunk.chunk_id,
                    "document_id": chunk.document_id,
                    "text": chunk.text,
                    "similarity": float(similarity),
                    "length": chunk.length
                })
            
            logger.info(f"Found {len(results)} similar chunks")
            return results
            
        except Exception as e:
            logger.error(f"Search error: {str(e)}")
            raise
```


#### 4. Migration Script:

```python
# ููู scripts/migrate_qdrant_to_pgvector.py

"""
ููู ุงูุจูุงูุงุช ูู QDrant ุฅูู PGVector
"""

import asyncio
from database.postgres_connection import AsyncSessionLocal
from vectordb.qdrant_db import QdrantDB
from database.models import Chunk
from sqlalchemy import select
import logging

logger = logging.getLogger(__name__)

async def migrate_embeddings():
    """
    ููู embeddings ูู QDrant ุฅูู PostgreSQL
    """
    logger.info("๐ Starting migration from QDrant to PGVector")
    
    # ุงูุงุชุตุงู ุจู QDrant
    qdrant = QdrantDB(url="http://localhost:6333")
    
    # ุงูุญุตูู ุนูู ุฌููุน points
    collection_info = await qdrant.get_collection_info("documents_chunks")
    logger.info(f"Total points in QDrant: {collection_info['points_count']}")
    
    async with AsyncSessionLocal() as session:
        # ุงูุญุตูู ุนูู chunks ุจุฏูู embeddings
        result = await session.execute(
            select(Chunk).where(Chunk.embedding.is_(None))
        )
        chunks = result.scalars().all()
        
        logger.info(f"Chunks to migrate: {len(chunks)}")
        
        migrated = 0
        for chunk in chunks:
            try:
                # ุงูุจุญุซ ุนู embedding ูู QDrant
                # (ูุญุชุงุฌ ูุชุนุฏูู ุญุณุจ ุทุฑููุฉ ุชุฎุฒููู)
                # ููุง ููุชุฑุถ ุฃู chunk.id ููุฌูุฏ ูู QDrant
                
                # ููููู ุงุณุชุฎุฏุงู search ุฃู get point
                # ุซู ูุณุฎ ุงูู embedding
                
                # ูุซุงู:
                # point = qdrant.client.retrieve(
                #     collection_name="documents_chunks",
                #     ids=[str(chunk.id)]
                # )
                # if point:
                #     chunk.embedding = point[0].vector
                #     migrated += 1
                
                pass  # ุงุณุชุจุฏู ุจุงูููุฏ ุงููุนูู
                
            except Exception as e:
                logger.error(f"Error migrating chunk {chunk.id}: {str(e)}")
        
        await session.commit()
        logger.info(f"โ Migrated {migrated} embeddings")

if __name__ == "__main__":
    asyncio.run(migrate_embeddings())
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู ูุงูุนุดุฑูู: App Deployment 1/2 (2:17 ุณุงุนุฉ)

**Branch:** `tut-015`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ุชุญุถูุฑ ุงูุชุทุจูู ููู Production
- Docker optimization
- Environment management
- Security best practices


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู Dockerfile (ูุญุณูู):

```dockerfile
# Stage 1: Builder
FROM python:3.10-slim as builder

WORKDIR /app

# ุชุซุจูุช dependencies ููู build
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libpq-dev \
    && rm -rf /var/lib/apt/lists/*

# ูุณุฎ requirements
COPY requirements.txt .

# ุชุซุจูุช dependencies
RUN pip install --no-cache-dir --user -r requirements.txt

# Stage 2: Runtime
FROM python:3.10-slim

WORKDIR /app

# ุชุซุจูุช runtime dependencies ููุท
RUN apt-get update && apt-get install -y \
    libpq5 \
    && rm -rf /var/lib/apt/lists/*

# ูุณุฎ Python packages ูู builder
COPY --from=builder /root/.local /root/.local

# ุฅุถุงูุฉ ููู PATH
ENV PATH=/root/.local/bin:$PATH

# ูุณุฎ ุงูููุฏ
COPY . .

# ุฅูุดุงุก user ุบูุฑ root
RUN useradd -m -u 1000 appuser && \
    chown -R appuser:appuser /app

USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import requests; requests.get('http://localhost:8000/health')"

# ุชุดุบูู ุงูุชุทุจูู
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```


#### 2. ููู docker-compose.prod.yml:

```yaml
version: '3.8'

services:
  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    networks:
      - minirag-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis (ููู caching ูุงูู rate limiting)
  redis:
    image: redis:7-alpine
    restart: always
    command: redis-server --requirepass ${REDIS_PASSWORD}
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - minirag-network

  # API Application
  api:
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - minirag-network
    volumes:
      - ./uploads:/app/uploads
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx (Reverse Proxy)
  nginx:
    image: nginx:alpine
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    depends_on:
      - api
    networks:
      - minirag-network

volumes:
  postgres_data:
  redis_data:

networks:
  minirag-network:
    driver: bridge
```


#### 3. ููู nginx/nginx.conf:

```nginx
events {
    worker_connections 1024;
}

http {
    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
    
    upstream api {
        server api:8000;
    }
    
    server {
        listen 80;
        server_name api.yourdomain.com;
        
        # Redirect to HTTPS
        return 301 https://$server_name$request_uri;
    }
    
    server {
        listen 443 ssl http2;
        server_name api.yourdomain.com;
        
        # SSL certificates
        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        
        # SSL settings
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        
        # Max upload size
        client_max_body_size 50M;
        
        # Security headers
        add_header X-Frame-Options "SAMEORIGIN" always;
        add_header X-Content-Type-Options "nosniff" always;
        add_header X-XSS-Protection "1; mode=block" always;
        
        location / {
            limit_req zone=api_limit burst=20 nodelay;
            
            proxy_pass http://api;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 60s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # Health check endpoint (no rate limit)
        location /health {
            proxy_pass http://api;
        }
    }
}
```


#### 4. ููู .env.production:

```bash
# Application
APP_NAME=mini-RAG
APP_VERSION=1.0.0
DEBUG=false
ENVIRONMENT=production

# Database
POSTGRES_USER=minirag_user
POSTGRES_PASSWORD=super_secure_password_here
POSTGRES_DB=minirag_prod
DATABASE_URL=postgresql+asyncpg://minirag_user:super_secure_password_here@postgres:5432/minirag_prod

# Redis
REDIS_PASSWORD=redis_secure_password
REDIS_URL=redis://:redis_secure_password@redis:6379/0

# OpenAI
OPENAI_API_KEY=sk-your-production-key-here

# Security
SECRET_KEY=your-super-secret-key-here-minimum-32-chars
ALLOWED_HOSTS=api.yourdomain.com,yourdomain.com

# CORS
CORS_ORIGINS=https://yourdomain.com,https://app.yourdomain.com

# Rate Limiting
RATE_LIMIT_PER_MINUTE=60
```


#### 5. ุชุญุฏูุซ main.py ููู Production:

```python
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from config import settings
import logging
import time

# ุฅุนุฏุงุฏ logging
logging.basicConfig(
    level=logging.INFO if not settings.debug else logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Rate limiter
limiter = Limiter(key_func=get_remote_address)

# ุฅูุดุงุก ุงูุชุทุจูู
app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    docs_url="/docs" if settings.debug else None,  # ุฅุฎูุงุก docs ูู production
    redoc_url="/redoc" if settings.debug else None
)

# Middleware ููู rate limiting
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins.split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip compression
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Request timing middleware
@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

# Error handling
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global error: {str(exc)}", exc_info=True)
    
    if settings.debug:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุจุงูู ุงูููุฏูููุงุช:

***

## ๐ฌ ุงูููุฏูู ุงูุซุงูู ูุงูุนุดุฑูู: Deployment 1/2 (ุชูููุฉ)

#### 5. ุชุญุฏูุซ main.py ููู Production (ุชูููุฉ):
```python
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"Global error: {str(exc)}", exc_info=True)
    
    if settings.debug:
        return JSONResponse(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            content={"detail": str(exc), "type": type(exc).__name__}
        )
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": "Internal server error"}
    )

# Health check
@app.get("/health")
@limiter.exempt  # ูุง rate limit ุนูู health check
async def health():
    """ูุญุต ุตุญุฉ ุงูุชุทุจูู"""
    return {
        "status": "healthy",
        "environment": settings.environment,
        "version": settings.app_version
    }

# Startup
@app.on_event("startup")
async def startup():
    logger.info(f"๐ Starting {settings.app_name} in {settings.environment} mode")
    await db.connect_to_database()
    await init_db()
    logger.info("โ Application started successfully")

# Shutdown
@app.on_event("shutdown")
async def shutdown():
    logger.info("๐ Shutting down application")
    await db.close_database_connection()
    logger.info("โ Shutdown complete")
```


#### 6. ูุดุฑ ุงูุชุทุจูู:

```bash
# ุจูุงุก ูุชุดุบูู
$ docker-compose -f docker-compose.prod.yml up -d --build

# ุนุฑุถ logs
$ docker-compose -f docker-compose.prod.yml logs -f api

# ุฅููุงู
$ docker-compose -f docker-compose.prod.yml down
```


***

## ๐ฌ ุงูููุฏูู ุงูุซุงูุซ ูุงูุนุดุฑูู: App Deployment 2/2 (1:49 ุณุงุนุฉ)

**Branch:** `tut-015`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- Monitoring \& Logging
- Backup strategies
- CI/CD pipeline
- Performance optimization


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุฅุถุงูุฉ Prometheus \& Grafana:

```yaml
# ุชุญุฏูุซ docker-compose.prod.yml

  # Prometheus
  prometheus:
    image: prom/prometheus:latest
    restart: always
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - minirag-network

  # Grafana
  grafana:
    image: grafana/grafana:latest
    restart: always
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
    depends_on:
      - prometheus
    networks:
      - minirag-network

volumes:
  prometheus_data:
  grafana_data:
```


#### 2. ููู prometheus/prometheus.yml:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'fastapi'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics'

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']
```


#### 3. ุฅุถุงูุฉ Metrics endpoint:

```python
# ููู monitoring/metrics.py
from prometheus_client import Counter, Histogram, generate_latest, REGISTRY
from fastapi import APIRouter
from fastapi.responses import Response

router = APIRouter()

# Metrics
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

rag_queries = Counter(
    'rag_queries_total',
    'Total RAG queries'
)

documents_uploaded = Counter(
    'documents_uploaded_total',
    'Total documents uploaded'
)

@router.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return Response(
        content=generate_latest(REGISTRY),
        media_type="text/plain"
    )
```


#### 4. Middleware ููู Metrics:

```python
# ูู main.py
from monitoring.metrics import request_count, request_duration
import time

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    
    response = await call_next(request)
    
    duration = time.time() - start_time
    
    # ุชุณุฌูู metrics
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    request_duration.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    return response
```


#### 5. Backup Script:

```bash
#!/bin/bash
# ููู scripts/backup.sh

# ุฅุนุฏุงุฏุงุช
BACKUP_DIR="/backups"
DATE=$(date +%Y%m%d_%H%M%S)
DB_NAME="minirag_prod"
DB_USER="minirag_user"

# ุฅูุดุงุก ูุฌูุฏ backup
mkdir -p $BACKUP_DIR

# Backup PostgreSQL
echo "๐ฆ Backing up PostgreSQL..."
docker exec minirag-postgres pg_dump -U $DB_USER $DB_NAME | gzip > "$BACKUP_DIR/db_$DATE.sql.gz"

# Backup uploads folder
echo "๐ฆ Backing up uploads..."
tar -czf "$BACKUP_DIR/uploads_$DATE.tar.gz" ./uploads

# ุญุฐู backups ุฃูุฏู ูู 7 ุฃูุงู
echo "๐งน Cleaning old backups..."
find $BACKUP_DIR -name "*.gz" -mtime +7 -delete

echo "โ Backup completed: $DATE"
```


#### 6. CI/CD ูุน GitHub Actions:

```yaml
# ููู .github/workflows/deploy.yml
name: Deploy to Production

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-asyncio
      
      - name: Run tests
        run: pytest tests/

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v3
      
      - name: Deploy to server
        uses: appleboy/ssh-action@master
        with:
          host: ${{ secrets.SERVER_HOST }}
          username: ${{ secrets.SERVER_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          script: |
            cd /app/mini-rag
            git pull origin main
            docker-compose -f docker-compose.prod.yml down
            docker-compose -f docker-compose.prod.yml up -d --build
            docker-compose -f docker-compose.prod.yml exec api alembic upgrade head
```


***

## ๐ฌ ุงูููุฏูู ุงูุฑุงุจุน ูุงูุนุดุฑูู: Celery Basics 1/2 (2:01 ุณุงุนุฉ)

**Branch:** `tut-016`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- ููู Task Queues
- ุฅุนุฏุงุฏ Celery
- Background tasks
- Task monitoring


### ุงูููุงููู ุงูุฃุณุงุณูุฉ:

#### 1. ููุงุฐุง Celeryุ

```python
"""
๐ฏ ูุดุงูู Background Tasks ุงูุจุณูุทุฉ:

โ FastAPI BackgroundTasks:
   - ุชุนูู ูู ููุณ ุงูู process
   - ุฅุฐุง ุฃูุนูุฏ ุชุดุบูู ุงูุฎุงุฏูุ ุชุถูุน ุงูููุงู
   - ูุง ููุฌุฏ retry mechanism
   - ูุง ููุฌุฏ monitoring

โ Celery:
   - ููุงู ูู processes ูููุตูุฉ
   - Persistent (ุชูุญูุธ ูู Redis/RabbitMQ)
   - Retry automatic
   - Monitoring ูุน Flower
   - Scheduling (Celery Beat)
   - Scalable (ุนุฏุฉ workers)
"""
```


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ุชุญุฏูุซ requirements.txt:

```txt
# ... ุงูููุชุจุงุช ุงูุณุงุจูุฉ
celery==5.3.4
redis==5.0.1
flower==2.0.1  # ููู monitoring
```


#### 2. ููู celery_app.py:

```python
from celery import Celery
from config import settings
import logging

logger = logging.getLogger(__name__)

# ุฅูุดุงุก Celery app
celery_app = Celery(
    "minirag",
    broker=settings.redis_url,
    backend=settings.redis_url,
    include=[
        'tasks.document_tasks',
        'tasks.embedding_tasks',
        'tasks.cleanup_tasks'
    ]
)

# ุฅุนุฏุงุฏุงุช Celery
celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=30 * 60,  # 30 ุฏูููุฉ
    task_soft_time_limit=25 * 60,  # 25 ุฏูููุฉ
    worker_prefetch_multiplier=1,
    worker_max_tasks_per_child=1000,
)

# Task routing
celery_app.conf.task_routes = {
    'tasks.document_tasks.*': {'queue': 'documents'},
    'tasks.embedding_tasks.*': {'queue': 'embeddings'},
    'tasks.cleanup_tasks.*': {'queue': 'maintenance'},
}

logger.info("โ Celery app configured")
```


#### 3. ููู tasks/document_tasks.py:

```python
from celery_app import celery_app
from database.postgres_connection import AsyncSessionLocal
from services.rag_service import RAGService
from helpers.enhanced_text_processor import EnhancedTextProcessor
import logging

logger = logging.getLogger(__name__)

@celery_app.task(
    bind=True,
    name='process_document',
    max_retries=3,
    default_retry_delay=60
)
def process_document_task(self, document_id: int):
    """
    ูุนุงูุฌุฉ ูุณุชูุฏ ูู ุงูุฎูููุฉ
    
    Args:
        document_id: ูุนุฑู ุงููุณุชูุฏ
    """
    try:
        logger.info(f"๐ Processing document {document_id}")
        
        # ุชุญุฏูุซ ุงูุญุงูุฉ
        self.update_state(
            state='PROCESSING',
            meta={'document_id': document_id, 'step': 'extracting_text'}
        )
        
        # ุงููุนุงูุฌุฉ (ุณูุญุชุงุฌ ูู async wrapper)
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def process():
            async with AsyncSessionLocal() as session:
                # ุงูุญุตูู ุนูู ุงููุณุชูุฏ
                from database.repositories.document_repository_pg import DocumentRepositoryPG
                doc_repo = DocumentRepositoryPG(session)
                
                document = await doc_repo.get_by_id(document_id)
                if not document:
                    raise Exception("Document not found")
                
                # ุงุณุชุฎุฑุงุฌ ููุนุงูุฌุฉ
                processor = EnhancedTextProcessor()
                file_ext = os.path.splitext(document.filename)[1]
                
                result = await processor.process_with_progress(
                    document.file_path,
                    file_ext,
                    str(document_id)
                )
                
                # ุญูุธ chunks
                from database.models import Chunk
                for chunk_data in result['chunks']:
                    chunk = Chunk(
                        document_id=document_id,
                        chunk_id=chunk_data['chunk_id'],
                        text=chunk_data['text'],
                        start_char=chunk_data['start_char'],
                        end_char=chunk_data['end_char'],
                        length=chunk_data['length']
                    )
                    session.add(chunk)
                
                # ุชุญุฏูุซ ุงููุณุชูุฏ
                await doc_repo.update(document_id, {
                    'status': 'processed',
                    'clean_text': result['clean_text'],
                    'chunk_count': len(result['chunks'])
                })
                
                await session.commit()
                
                logger.info(f"โ Document {document_id} processed successfully")
                
                return {
                    'document_id': document_id,
                    'chunks_created': len(result['chunks'])
                }
        
        result = loop.run_until_complete(process())
        return result
        
    except Exception as exc:
        logger.error(f"โ Error processing document {document_id}: {str(exc)}")
        
        # Retry
        raise self.retry(exc=exc)

@celery_app.task(name='delete_document')
def delete_document_task(document_id: int):
    """
    ุญุฐู ูุณุชูุฏ ููููุงุชู
    """
    try:
        logger.info(f"๐๏ธ Deleting document {document_id}")
        
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def delete():
            async with AsyncSessionLocal() as session:
                doc_repo = DocumentRepositoryPG(session)
                
                document = await doc_repo.get_by_id(document_id)
                if document:
                    # ุญุฐู ุงูููู
                    import os
                    if os.path.exists(document.file_path):
                        os.remove(document.file

---

# ุงููู

ุณุฃููู ุงูุดุฑุญ ุงูุชูุตููู ูุขุฎุฑ ููุฏููููู:

***

## ๐ฌ ุงูููุฏูู ุงูุฑุงุจุน ูุงูุนุดุฑูู: Celery Basics 1/2 (ุชูููุฉ)

#### 3. ููู tasks/document_tasks.py (ุชูููุฉ):
```python
                if document:
                    # ุญุฐู ุงูููู
                    import os
                    if os.path.exists(document.file_path):
                        os.remove(document.file_path)
                    
                    # ุญุฐู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
                    await doc_repo.delete(document_id)
                    await session.commit()
                
                logger.info(f"โ Document {document_id} deleted")
        
        loop.run_until_complete(delete())
        return {'document_id': document_id, 'status': 'deleted'}
        
    except Exception as exc:
        logger.error(f"โ Error deleting document: {str(exc)}")
        raise
```


#### 4. ููู tasks/embedding_tasks.py:

```python
from celery_app import celery_app
from database.postgres_connection import AsyncSessionLocal
from services.pgvector_service import PGVectorService
import logging

logger = logging.getLogger(__name__)

@celery_app.task(
    bind=True,
    name='create_embeddings',
    max_retries=3
)
def create_embeddings_task(self, document_id: int):
    """
    ุฅูุดุงุก embeddings ูููุณุชูุฏ
    """
    try:
        logger.info(f"๐ง Creating embeddings for document {document_id}")
        
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def create_embeddings():
            async with AsyncSessionLocal() as session:
                pgvector_service = PGVectorService(session)
                
                count = await pgvector_service.embed_and_store_chunks(document_id)
                await session.commit()
                
                return count
        
        count = loop.run_until_complete(create_embeddings())
        
        logger.info(f"โ Created {count} embeddings for document {document_id}")
        
        return {
            'document_id': document_id,
            'embeddings_created': count
        }
        
    except Exception as exc:
        logger.error(f"โ Error creating embeddings: {str(exc)}")
        raise self.retry(exc=exc)
```


#### 5. ุชุญุฏูุซ routes/documents.py ูุงุณุชุฎุฏุงู Celery:

```python
from tasks.document_tasks import process_document_task
from tasks.embedding_tasks import create_embeddings_task

@router.post("/upload", response_model=DocumentResponse)
async def upload_document(file: UploadFile = File(...)):
    """
    ุฑูุน ูุณุชูุฏ - ุงููุนุงูุฌุฉ ุณุชุชู ุนุจุฑ Celery
    """
    try:
        # ุญูุธ ุงูููู
        file_path, file_size = await file_handler.save_file(file)
        
        # ุฅูุดุงุก ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        async with AsyncSessionLocal() as session:
            doc_repo = DocumentRepositoryPG(session)
            
            document = await doc_repo.create({
                'filename': file.filename,
                'file_path': file_path,
                'file_size': file_size,
                'content_type': file.content_type,
                'status': 'uploaded'
            })
            
            await session.commit()
            document_id = document.id
        
        # ุฅุฑุณุงู ูููุนุงูุฌุฉ ุนุจุฑ Celery
        task = process_document_task.delay(document_id)
        
        return {
            "id": document_id,
            "filename": file.filename,
            "status": "uploaded",
            "task_id": task.id,
            "message": "Document uploaded, processing in background"
        }
        
    except Exception as e:
        raise HTTPException(500, f"Error: {str(e)}")

@router.get("/tasks/{task_id}")
async def get_task_status(task_id: str):
    """
    ุงูุญุตูู ุนูู ุญุงูุฉ ุงููููุฉ
    """
    from celery.result import AsyncResult
    
    task = AsyncResult(task_id, app=celery_app)
    
    return {
        "task_id": task_id,
        "status": task.state,
        "result": task.result if task.ready() else None,
        "info": task.info
    }
```


#### 6. ุชุดุบูู Celery:

```bash
# Worker ูููุณุชูุฏุงุช
$ celery -A celery_app worker -Q documents -l info -n documents@%h

# Worker ููู embeddings
$ celery -A celery_app worker -Q embeddings -l info -n embeddings@%h

# Flower ููู monitoring
$ celery -A celery_app flower --port=5555
```


***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ ูุงูุนุดุฑูู: Advanced Celery 2/2 (1:49 ุณุงุนุฉ)

**Branch:** `tut-017`

### ุงูุฃูุฏุงู ุงูุชุนููููุฉ:

- Celery Beat ููููุงู ุงููุฌุฏููุฉ
- Task chains ู groups
- Error handling ูุชูุฏู
- Performance optimization


### ุงูุฃููุงุฏ ุงูุชูุตูููุฉ:

#### 1. ููู tasks/cleanup_tasks.py:

```python
from celery_app import celery_app
from database.postgres_connection import AsyncSessionLocal
from datetime import datetime, timedelta
import logging
import os

logger = logging.getLogger(__name__)

@celery_app.task(name='cleanup_old_documents')
def cleanup_old_documents_task():
    """
    ุญุฐู ุงููุณุชูุฏุงุช ุงููุฏููุฉ (ุฃูุซุฑ ูู 30 ููู)
    """
    try:
        logger.info("๐งน Starting cleanup of old documents")
        
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def cleanup():
            async with AsyncSessionLocal() as session:
                from sqlalchemy import select
                from database.models import Document
                
                # ุงูุนุซูุฑ ุนูู ุงููุณุชูุฏุงุช ุงููุฏููุฉ
                cutoff_date = datetime.utcnow() - timedelta(days=30)
                
                result = await session.execute(
                    select(Document).where(
                        Document.upload_date < cutoff_date,
                        Document.status == 'failed'
                    )
                )
                
                old_documents = result.scalars().all()
                
                deleted_count = 0
                for doc in old_documents:
                    # ุญุฐู ุงูููู
                    if os.path.exists(doc.file_path):
                        os.remove(doc.file_path)
                    
                    # ุญุฐู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
                    await session.delete(doc)
                    deleted_count += 1
                
                await session.commit()
                
                logger.info(f"โ Cleaned up {deleted_count} old documents")
                return deleted_count
        
        count = loop.run_until_complete(cleanup())
        return {'deleted': count}
        
    except Exception as exc:
        logger.error(f"โ Cleanup error: {str(exc)}")
        raise

@celery_app.task(name='generate_statistics')
def generate_statistics_task():
    """
    ุชูููุฏ ุฅุญุตุงุฆูุงุช ููููุฉ
    """
    try:
        logger.info("๐ Generating statistics")
        
        import asyncio
        loop = asyncio.get_event_loop()
        
        async def generate_stats():
            async with AsyncSessionLocal() as session:
                from sqlalchemy import select, func
                from database.models import Document, Chunk
                
                # ุฅุญุตุงุฆูุงุช ุงููุณุชูุฏุงุช
                total_docs = await session.execute(
                    select(func.count(Document.id))
                )
                total_docs = total_docs.scalar()
                
                # ุฅุญุตุงุฆูุงุช chunks
                total_chunks = await session.execute(
                    select(func.count(Chunk.id))
                )
                total_chunks = total_chunks.scalar()
                
                # ุงููุณุชูุฏุงุช ุงููุนุงูุฌุฉ ุงูููู
                today = datetime.utcnow().date()
                processed_today = await session.execute(
                    select(func.count(Document.id)).where(
                        func.date(Document.processed_at) == today
                    )
                )
                processed_today = processed_today.scalar()
                
                stats = {
                    'date': str(today),
                    'total_documents': total_docs,
                    'total_chunks': total_chunks,
                    'processed_today': processed_today
                }
                
                logger.info(f"๐ Stats: {stats}")
                return stats
        
        return loop.run_until_complete(generate_stats())
        
    except Exception as exc:
        logger.error(f"โ Stats error: {str(exc)}")
        raise
```


#### 2. ุฅุนุฏุงุฏ Celery Beat:

```python
# ูู celery_app.py
from celery.schedules import crontab

celery_app.conf.beat_schedule = {
    # ุชูุธูู ูููู ูู ุงูุณุงุนุฉ 2 ุตุจุงุญุงู
    'cleanup-daily': {
        'task': 'cleanup_old_documents',
        'schedule': crontab(hour=2, minute=0),
    },
    
    # ุฅุญุตุงุฆูุงุช ููููุฉ ูู ุงูุณุงุนุฉ 23:55
    'daily-statistics': {
        'task': 'generate_statistics',
        'schedule': crontab(hour=23, minute=55),
    },
    
    # ูุญุต ุตุญุฉ ูู ุณุงุนุฉ
    'health-check': {
        'task': 'system_health_check',
        'schedule': crontab(minute=0),  # ูู ุณุงุนุฉ
    },
}
```


#### 3. Task Chains - ูุนุงูุฌุฉ ูุชุณูุณูุฉ:

```python
from celery import chain, group

@celery_app.task(name='full_document_pipeline')
def full_document_pipeline_task(document_id: int):
    """
    Pipeline ูุงูู ูููุณุชูุฏ
    """
    # ุฅูุดุงุก chain ูู ุงูููุงู
    pipeline = chain(
        process_document_task.s(document_id),
        create_embeddings_task.s(),
        notify_completion_task.s()
    )
    
    return pipeline.apply_async()

@celery_app.task(name='notify_completion')
def notify_completion_task(result: dict):
    """
    ุฅุดุนุงุฑ ุจุฅุชูุงู ุงููุนุงูุฌุฉ
    """
    logger.info(f"๐ Pipeline completed: {result}")
    
    # ูููู ุฅุฑุณุงู email ุฃู webhook ููุง
    return {'status': 'notified', 'result': result}
```


#### 4. Task Groups - ูุนุงูุฌุฉ ูุชูุงุฒูุฉ:

```python
@celery_app.task(name='batch_process_documents')
def batch_process_documents_task(document_ids: list):
    """
    ูุนุงูุฌุฉ ุนุฏุฉ ูุณุชูุฏุงุช ุจุงูุชูุงุฒู
    """
    # ุฅูุดุงุก group ูู ุงูููุงู
    job = group(
        process_document_task.s(doc_id)
        for doc_id in document_ids
    )
    
    return job.apply_async()
```


#### 5. Error Handling ุงููุชูุฏู:

```python
from celery.exceptions import SoftTimeLimitExceeded

@celery_app.task(
    bind=True,
    name='safe_process_document',
    max_retries=5,
    autoretry_for=(Exception,),
    retry_backoff=True,
    retry_backoff_max=600,  # 10 ุฏูุงุฆู
    retry_jitter=True
)
def safe_process_document_task(self, document_id: int):
    """
    ูุนุงูุฌุฉ ูุน error handling ูุชูุฏู
    """
    try:
        # ุงููุนุงูุฌุฉ
        result = process_document_task(document_id)
        return result
        
    except SoftTimeLimitExceeded:
        # ุชู ุชุฌุงูุฒ ุงูููุช ุงููุญุฏุฏ
        logger.warning(f"โฐ Task timeout for document {document_id}")
        
        # ุชุญุฏูุซ ุญุงูุฉ ุงููุณุชูุฏ
        # ... update status to 'timeout'
        
        # ุนุฏู ุฅุนุงุฏุฉ ุงููุญุงููุฉ
        raise
        
    except Exception as exc:
        # log ุงูุชูุงุตูู
        logger.error(
            f"โ Error in attempt {self.request.retries + 1}: {str(exc)}",
            exc_info=True
        )
        
        # ุฅุฐุง ูุตููุง ููุญุฏ ุงูุฃูุตู ูู ุงููุญุงููุงุช
        if self.request.retries >= self.max_retries:
            logger.error(f"๐ซ Max retries reached for document {document_id}")
            
            # ุชุญุฏูุซ ุญุงูุฉ ุงููุณุชูุฏ
            # ... update status to 'failed'
            
            return {'status': 'failed', 'error': str(exc)}
        
        # ุฅุนุงุฏุฉ ุงููุญุงููุฉ
        raise self.retry(exc=exc, countdown=60 * (self.request.retries + 1))
```


#### 6. Monitoring ูุน Flower:

```python
# ููู flowerconfig.py
import os

# Basic Auth
basic_auth = [os.getenv('FLOWER_USER', 'admin') + ':' + os.getenv('FLOWER_PASSWORD', 'admin')]

# Broker URL
broker_api = os.getenv('REDIS_URL', 'redis://localhost:6379/0')

# Port
port = 5555

# Enable events
persistent = True

# Database
db = 'flower.db'
```


#### 7. ุชุญุฏูุซ docker-compose.prod.yml:

```yaml
  # Celery Workers
  celery-documents:
    build: .
    command: celery -A celery_app worker -Q documents -l info -n documents@%h
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    volumes:
      - ./uploads:/app/uploads
    networks:
      - minirag-network

  celery-embeddings:
    build: .
    command: celery -A celery_app worker -Q embeddings -l info -n embeddings@%h
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    networks:
      - minirag-network

  # Celery Beat
  celery-beat:
    build: .
    command: celery -A celery_app beat -l info
    restart: always
    depends_on:
      - redis
      - postgres
    environment

---

# ุงููู

ุณุฃููู ูุฃุฎุชู ุงูุดุฑุญ ุจููุฎุต ุดุงูู:

***

## ๐ฌ ุงูููุฏูู ุงูุฎุงูุณ ูุงูุนุดุฑูู: Advanced Celery (ุชูููุฉ)

#### 7. ุชุญุฏูุซ docker-compose.prod.yml (ุชูููุฉ):
```yaml
  celery-beat:
    build: .
    command: celery -A celery_app beat -l info
    restart: always
    depends_on:
      - redis
      - postgres
    environment:
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
    networks:
      - minirag-network

  # Flower Dashboard
  flower:
    build: .
    command: celery -A celery_app flower --conf=flowerconfig.py
    restart: always
    ports:
      - "5555:5555"
    depends_on:
      - redis
      - celery-documents
    environment:
      - REDIS_URL=${REDIS_URL}
      - FLOWER_USER=${FLOWER_USER}
      - FLOWER_PASSWORD=${FLOWER_PASSWORD}
    networks:
      - minirag-network
```


#### 8. Performance Optimization:

```python
# ูู celery_app.py

# ุชุญุณููุงุช ุงูุฃุฏุงุก
celery_app.conf.update(
    # ุงุณุชุฎุฏุงู compression ูููุชุงุฆุฌ
    result_compression='gzip',
    
    # ุชูููู ุญุฌู ุงูุฑุณุงุฆู
    task_compression='gzip',
    
    # ุชุญุณูู connection pooling
    broker_pool_limit=10,
    
    # Task acknowledgment
    task_acks_late=True,
    
    # Worker optimization
    worker_prefetch_multiplier=4,
    worker_max_tasks_per_child=100,
    
    # Result backend optimization
    result_expires=3600,  # ุณุงุนุฉ ูุงุญุฏุฉ
    result_backend_transport_options={
        'master_name': 'mymaster',
    },
)
```


***

# ๐ ุงูููุฎุต ุงูุดุงูู ุงูููุงุฆู ูุณูุณูุฉ mini-RAG

## ๐ฏ ูุง ุชู ุฅูุฌุงุฒู ูู 25 ููุฏูู:

### **ุงููุฑุญูุฉ ุงูุฃููู: Foundation (ุงูููุฏูููุงุช 1-10)**

| ุงููุฑุญูุฉ | ุงูููุฏูููุงุช | ุงูุฅูุฌุงุฒุงุช |
| :-- | :-- | :-- |
| **Setup** | 1-3 | โ ููู ุงููุดุฑูุน<br>โ ุฅุนุฏุงุฏ ุจูุฆุฉ ุงูุนูู<br>โ Python + Conda + Tools |
| **Backend** | 4-6 | โ FastAPI setup<br>โ Project architecture<br>โ Routes \& config |
| **Files** | 7-8 | โ File upload<br>โ Text extraction<br>โ Chunking |
| **Database** | 9-10 | โ MongoDB + Docker<br>โ Models \& schemas<br>โ CRUD operations |

### **ุงููุฑุญูุฉ ุงูุซุงููุฉ: RAG Core (ุงูููุฏูููุงุช 11-19)**

| ุงููุฑุญูุฉ | ุงูููุฏูููุงุช | ุงูุฅูุฌุงุฒุงุช |
| :-- | :-- | :-- |
| **Optimization** | 11-13 | โ Database indexes<br>โ Pipeline enhancements<br>โ Checkpoint \& fixes |
| **AI Integration** | 14-15 | โ OpenAI LLM<br>โ QDrant vector DB<br>โ Embeddings |
| **Search** | 16-17 | โ Semantic search<br>โ Hybrid search<br>โ RAG answers |
| **Advanced** | 18-19 | โ Ollama local LLM<br>โ Cost optimization |

### **ุงููุฑุญูุฉ ุงูุซุงูุซุฉ: Production (ุงูููุฏูููุงุช 20-25)**

| ุงููุฑุญูุฉ | ุงูููุฏูููุงุช | ุงูุฅูุฌุงุฒุงุช |
| :-- | :-- | :-- |
| **Migration** | 20-21 | โ PostgreSQL<br>โ PGVector<br>โ Alembic migrations |
| **Deployment** | 22-23 | โ Docker optimization<br>โ Nginx + SSL<br>โ Monitoring |
| **Scalability** | 24-25 | โ Celery workers<br>โ Task queues<br>โ Scheduling |


***

## ๐๏ธ ุงูุจููุฉ ุงูููุงุฆูุฉ ุงููุงููุฉ ูููุดุฑูุน:

```
mini-rag/
โโโ main.py                          # ููุทุฉ ุงูุฏุฎูู
โโโ celery_app.py                    # Celery configuration
โโโ config.py                        # ุฅุนุฏุงุฏุงุช ุงูุชุทุจูู
โโโ Dockerfile                       # Docker image
โโโ docker-compose.prod.yml          # Production setup
โโโ requirements.txt                 # Python dependencies
โโโ alembic.ini                      # Migration config
โ
โโโ database/
โ   โโโ postgres_connection.py      # Database connection
โ   โโโ models.py                    # SQLAlchemy models
โ   โโโ indexes.py                   # Database indexes
โ   โโโ repositories/
โ       โโโ document_repository_pg.py
โ       โโโ chunk_repository.py
โ
โโโ models/
โ   โโโ llm_models.py               # LLM Pydantic models
โ   โโโ rag_models.py               # RAG models
โ   โโโ processing_status.py        # Progress tracking
โ
โโโ llm/
โ   โโโ base_llm.py                 # Abstract LLM
โ   โโโ openai_llm.py               # OpenAI implementation
โ   โโโ ollama_llm.py               # Ollama implementation
โ   โโโ llm_factory.py              # Factory pattern
โ
โโโ vectordb/
โ   โโโ base_vectordb.py            # Abstract vector DB
โ   โโโ qdrant_db.py                # QDrant implementation
โ   โโโ vectordb_factory.py         # Factory pattern
โ
โโโ services/
โ   โโโ embedding_service.py        # Embeddings management
โ   โโโ search_service.py           # Hybrid search
โ   โโโ rag_service.py              # RAG pipeline
โ   โโโ pgvector_service.py         # PGVector operations
โ
โโโ routes/
โ   โโโ documents.py                # Document endpoints
โ   โโโ search.py                   # Search endpoints
โ   โโโ llm.py                      # LLM endpoints
โ   โโโ rag.py                      # RAG endpoints
โ
โโโ tasks/
โ   โโโ document_tasks.py           # Document processing
โ   โโโ embedding_tasks.py          # Embedding creation
โ   โโโ cleanup_tasks.py            # Maintenance tasks
โ
โโโ helpers/
โ   โโโ file_handler.py             # File operations
โ   โโโ text_processor.py           # Text processing
โ   โโโ enhanced_text_processor.py  # Advanced processing
โ   โโโ retry_handler.py            # Retry logic
โ
โโโ monitoring/
โ   โโโ metrics.py                  # Prometheus metrics
โ
โโโ tests/
โ   โโโ test_documents.py
โ   โโโ test_search.py
โ   โโโ test_rag.py
โ
โโโ docker/
โ   โโโ docker-compose.yml          # Development
โ   โโโ .env.example
โ
โโโ nginx/
โ   โโโ nginx.conf                  # Reverse proxy
โ   โโโ ssl/
โ
โโโ prometheus/
โ   โโโ prometheus.yml
โ
โโโ grafana/
โ   โโโ dashboards/
โ
โโโ scripts/
    โโโ backup.sh
    โโโ migrate_qdrant_to_pgvector.py
```


***

## ๐ ุงูููุงุฑุงุช ุงูููุชุณุจุฉ:

### **Backend Development:**

- โ FastAPI framework
- โ Async/await ูู Python
- โ REST API design
- โ Authentication \& Authorization
- โ Error handling


### **Database:**

- โ MongoDB (NoSQL)
- โ PostgreSQL (SQL)
- โ SQLAlchemy ORM
- โ Database migrations (Alembic)
- โ Indexes \& optimization


### **AI \& ML:**

- โ Large Language Models
- โ Embeddings \& vectors
- โ Semantic search
- โ RAG architecture
- โ Prompt engineering


### **DevOps:**

- โ Docker \& Docker Compose
- โ Nginx reverse proxy
- โ CI/CD pipelines
- โ Monitoring (Prometheus + Grafana)
- โ Logging \& debugging


### **Production:**

- โ Task queues (Celery)
- โ Caching (Redis)
- โ Scalability patterns
- โ Security best practices
- โ Backup strategies

***

## ๐ ุงูู APIs ุงูููุงุฆูุฉ (ูุงููุฉ):

```bash
# Documents Management
POST   /documents/upload              # ุฑูุน ูุณุชูุฏ
GET    /documents/                    # ูุงุฆูุฉ ุงููุณุชูุฏุงุช
GET    /documents/{id}                # ูุณุชูุฏ ูุญุฏุฏ
DELETE /documents/{id}                # ุญุฐู ูุณุชูุฏ
GET    /documents/{id}/chunks         # chunks ุงููุณุชูุฏ
GET    /documents/{id}/text           # ุงููุต ุงููุงูู
GET    /documents/{id}/progress       # ุชูุฏู ุงููุนุงูุฌุฉ
POST   /documents/{id}/embed          # ุฅูุดุงุก embeddings
GET    /documents/stats/summary       # ุฅุญุตุงุฆูุงุช

# Search
GET    /search/                       # ุจุญุซ ูุฌูู
GET    /search/semantic               # ุจุญุซ ุฏูุงูู

# LLM Operations
POST   /llm/chat                      # ูุญุงุฏุซุฉ
POST   /llm/chat/stream              # ูุญุงุฏุซุฉ ูุน streaming
POST   /llm/embedding                # ุฅูุดุงุก embedding
POST   /llm/tokens/count             # ุญุณุงุจ tokens

# RAG
POST   /rag/answer                    # ุชูููุฏ ุฅุฌุงุจุฉ
POST   /rag/answer/stream            # ุฅุฌุงุจุฉ ูุน streaming

# Tasks
GET    /tasks/{task_id}              # ุญุงูุฉ ุงููููุฉ

# Monitoring
GET    /health                        # ูุญุต ุงูุตุญุฉ
GET    /metrics                       # Prometheus metrics

# Admin
GET    /admin/indexes                 # ุนุฑุถ indexes
POST   /admin/reindex                 # ุฅุนุงุฏุฉ ุจูุงุก indexes
```


***

## ๐ก ุฃูุถู ุงูููุงุฑุณุงุช ุงููุชุนููุฉ:

### **1. Code Organization:**

```python
โ Repository Pattern ููู database
โ Factory Pattern ููู LLMs
โ Service Layer ููู business logic
โ Dependency Injection
```


### **2. Error Handling:**

```python
โ Try-except ูู ูู async function
โ Custom exceptions
โ Proper logging
โ Retry mechanisms
```


### **3. Performance:**

```python
โ Database indexes
โ Connection pooling
โ Caching ูุน Redis
โ Async operations
โ Background tasks
```


### **4. Security:**

```python
โ Environment variables ููู secrets
โ Input validation
โ Rate limiting
โ HTTPS + SSL
โ CORS configuration
```


***

## ๐ ุงูุชูุงูู ุงูููุงุฆูุฉ!

**ุฃูุช ุงูุขู ูุฏูู:**

- โ **Production-ready RAG application** ูุงูู
- โ **ููู ุนููู** ูู Full Stack AI Development
- โ **ุฎุจุฑุฉ ุนูููุฉ** ูู ุจูุงุก ุชุทุจููุงุช AI
- โ **ููุงุฑุงุช** ูุงุจูุฉ ููุชุทุจูู ูู ุงูุณูู
- โ **Portfolio project** ููู

**ุงูุฎุทูุงุช ุงููุงุฏูุฉ:**

1. ๐ง **ุทูุฑ ุงููุดุฑูุน** - ุฃุถู features ุฎุงุตุฉ ุจู
2. ๐ **ุงูุดุฑู** - ุถุนู ุนูู cloud (AWS, GCP, Azure)
3. ๐ **ูุซููู** - ุงูุชุจ ููุงูุงุช ุนู ูุง ุชุนููุช
4. ๐ผ **ุดุงุฑูู** - ุฃุถูู ูู portfolio ูLinkedIn
5. ๐ **ุนููู ุบูุฑู** - ุดุงุฑู ุงููุนุฑูุฉ ูุน ุงููุฌุชูุน

***

## ๐ ูุตุงุฏุฑ ุฅุถุงููุฉ ููุชุนูู:

- **FastAPI:** https://fastapi.tiangolo.com/
- **Celery:** https://docs.celeryq.dev/
- **PostgreSQL:** https://www.postgresql.org/docs/
- **OpenAI:** https://platform.openai.com/docs
- **LangChain:** https://python.langchain.com/
- **QDrant:** https://qdrant.tech/documentation/

***

**๐ ุฃุชููู ุฃู ูููู ูุฐุง ุงูุดุฑุญ ูุฏ ุฃูุงุฏู ูู ุฑุญูุชู ูุชุนูู RAG ู Full Stack AI Development!**

**ุจุงูุชูููู ูู ูุดุงุฑูุนู ุงููุงุฏูุฉ! ๐๐ช**

