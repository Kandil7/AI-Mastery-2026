# Phase 4: Advanced LLM Engineering & Fine-tuning

**Objective:** Master techniques for adapting and optimizing LLMs for specific tasks.

## Key Topics:

*   **Prompt Engineering:** Mastering advanced techniques for crafting effective prompts, including few-shot learning, zero-shot prompting, and chain-of-thought (CoT) prompting to guide LLMs towards better reasoning and output.
*   **Fine-tuning:** Deep dive into various fine-tuning methods for LLMs such as LoRA (Low-Rank Adaptation) and QLoRA, focusing on parameter-efficient fine-tuning (PEFT) techniques to adapt models to specific tasks with minimal computational cost.
*   **LLM Evaluation:** Understanding and implementing metrics for evaluating the quality of LLM outputs, especially in RAG systems (faithfulness, relevance, coherence), and utilizing "LLM-as-a-judge" approaches for automated evaluation.
*   **Agentic Frameworks:** Introduction to the design and implementation of basic AI agent architectures, exploring how LLMs can be empowered with tools and memory to perform complex tasks, including multi-agent systems.

## Deliverables:

*   Implementation of fine-tuning adapters and related LLM engineering utilities within [`src/llm/`](../../src/llm/).
*   Completion of practical notebooks demonstrating LLM fine-tuning, advanced prompting techniques, and agentic workflows in [`research/`](../../research/) (e.g., `research/rag_engine/`, `research/week12-fine-tuning/`).
*   In-depth understanding and documentation of advanced LLM concepts by reviewing relevant sections in [`docs/02_core_concepts/deep_dives/`](../02_core_concepts/deep_dives/README.md), focusing on prompt engineering and fine-tuning.
*   Successful completion of practical exercises, including [`docs/04_tutorials/exercises/level4_reranking.md`](../04_tutorials/exercises/level4_reranking.md) (for improving retrieval quality) and [`docs/04_tutorials/exercises/level5_evaluation.md`](../04_tutorials/exercises/level5_evaluation.md) (for comprehensive RAG system assessment).
