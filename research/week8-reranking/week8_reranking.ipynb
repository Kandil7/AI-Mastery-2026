{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ† Week 8: Re-ranking\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Understand why re-ranking improves retrieval\n",
                "2. Implement cross-encoder re-ranker\n",
                "3. Evaluate before/after re-ranking\n",
                "4. Optimize latency vs quality trade-offs\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import List, Tuple"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## Why Re-ranking?\n",
                "\n",
                "| Stage | Model | Speed | Quality |\n",
                "|-------|-------|-------|--------|\n",
                "| Retrieval | Bi-encoder | Fast | Good |\n",
                "| Re-ranking | Cross-encoder | Slow | Excellent |\n",
                "\n",
                "**Two-stage approach:**\n",
                "1. Fast retrieval: Get top-100 candidates\n",
                "2. Slow re-ranking: Score top-100 â†’ return top-10"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleCrossEncoder:\n",
                "    \"\"\"Simulated cross-encoder for demo.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        # Simulated model weights\n",
                "        self.keyword_boost = 0.3\n",
                "    \n",
                "    def score(self, query: str, document: str) -> float:\n",
                "        \"\"\"Score query-document pair.\"\"\"\n",
                "        # Simulate cross-encoder scoring\n",
                "        query_words = set(query.lower().split())\n",
                "        doc_words = set(document.lower().split())\n",
                "        \n",
                "        # Overlap score\n",
                "        overlap = len(query_words & doc_words)\n",
                "        \n",
                "        # Length normalization\n",
                "        norm = len(query_words) + len(doc_words)\n",
                "        \n",
                "        # Add some randomness for realism\n",
                "        np.random.seed(hash(query + document) % 2**32)\n",
                "        noise = np.random.uniform(-0.1, 0.1)\n",
                "        \n",
                "        return (overlap / norm * 2) + self.keyword_boost + noise\n",
                "    \n",
                "    def rerank(self, query: str, documents: List[str], top_k: int = 5) -> List[Tuple[int, float, str]]:\n",
                "        \"\"\"Re-rank documents by relevance to query.\"\"\"\n",
                "        scores = []\n",
                "        for i, doc in enumerate(documents):\n",
                "            score = self.score(query, doc)\n",
                "            scores.append((i, score, doc))\n",
                "        \n",
                "        scores.sort(key=lambda x: x[1], reverse=True)\n",
                "        return scores[:top_k]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test re-ranking\n",
                "documents = [\n",
                "    \"Python programming for beginners\",\n",
                "    \"Machine learning with Python tutorial\",\n",
                "    \"Deep learning neural networks\",\n",
                "    \"Python data science handbook\",\n",
                "    \"Natural language processing basics\"\n",
                "]\n",
                "\n",
                "# Initial retrieval order (simulated)\n",
                "initial_order = [2, 4, 0, 1, 3]  # Suboptimal order\n",
                "\n",
                "reranker = SimpleCrossEncoder()\n",
                "query = \"Python machine learning\"\n",
                "\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Initial Order:\")\n",
                "for i, idx in enumerate(initial_order):\n",
                "    print(f\"  {i+1}. {documents[idx]}\")\n",
                "\n",
                "# Re-rank\n",
                "retrieved_docs = [documents[i] for i in initial_order]\n",
                "reranked = reranker.rerank(query, retrieved_docs, top_k=5)\n",
                "\n",
                "print(\"\\nAfter Re-ranking:\")\n",
                "for i, (orig_idx, score, doc) in enumerate(reranked):\n",
                "    print(f\"  {i+1}. [{score:.3f}] {doc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 NDCG Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dcg(relevances, k):\n",
                "    \"\"\"Discounted Cumulative Gain.\"\"\"\n",
                "    return sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances[:k]))\n",
                "\n",
                "def ndcg(retrieved_relevances, ideal_relevances, k):\n",
                "    \"\"\"Normalized DCG.\"\"\"\n",
                "    ideal_sorted = sorted(ideal_relevances, reverse=True)\n",
                "    idcg = dcg(ideal_sorted, k)\n",
                "    if idcg == 0:\n",
                "        return 0\n",
                "    return dcg(retrieved_relevances, k) / idcg\n",
                "\n",
                "# Example: relevance scores (3=highly relevant, 0=not relevant)\n",
                "before_rerank = [0, 1, 3, 2, 1]  # Bad order\n",
                "after_rerank = [3, 2, 1, 1, 0]   # Good order\n",
                "ideal = [3, 2, 1, 1, 0]\n",
                "\n",
                "print(f\"NDCG@5 before re-ranking: {ndcg(before_rerank, ideal, 5):.4f}\")\n",
                "print(f\"NDCG@5 after re-ranking: {ndcg(after_rerank, ideal, 5):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test cross-encoder\n",
                "    ce = SimpleCrossEncoder()\n",
                "    score = ce.score(\"python\", \"python programming\")\n",
                "    assert score > 0\n",
                "    print(\"âœ“ Cross-encoder scoring test passed\")\n",
                "    \n",
                "    # Test NDCG perfect score\n",
                "    assert abs(ndcg([3, 2, 1], [3, 2, 1], 3) - 1.0) < 0.01\n",
                "    print(\"âœ“ NDCG perfect score test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Interview Prep\n",
                "---\n",
                "\n",
                "### Q1: Bi-encoder vs Cross-encoder?\n",
                "**Answer:** Bi-encoder: encodes query/doc separately, fast. Cross-encoder: encodes together, slow but more accurate.\n",
                "\n",
                "### Q2: How to optimize re-ranking latency?\n",
                "**Answer:** Limit candidates (top-50), use distillation, batch processing, caching.\n",
                "\n",
                "---\n",
                "# Section 5: Deliverable\n",
                "---\n",
                "\n",
                "**Created:** `reranker_demo.py`\n",
                "\n",
                "**Next Week:** LLM Orchestration & Streaming"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}