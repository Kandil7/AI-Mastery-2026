{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ”„ Week 8: Reranking for Improved Retrieval\n",
                "\n",
                "This notebook covers reranking techniques to improve retrieval quality.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Why Reranking?](#1-why-reranking)\n",
                "2. [Cross-Encoder Reranking](#2-cross-encoder-reranking)\n",
                "3. [LLM-Based Reranking](#3-llm-based-reranking)\n",
                "4. [Reciprocal Rank Fusion](#4-reciprocal-rank-fusion)\n",
                "5. [Diversity Reranking (MMR)](#5-diversity-reranking-mmr)\n",
                "6. [Building a Reranking Pipeline](#6-building-a-reranking-pipeline)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Why Reranking?\n",
                "\n",
                "### 1.1 The Two-Stage Retrieval Paradigm\n",
                "\n",
                "```\n",
                "Query â†’ [Stage 1: Fast Retrieval] â†’ Top-100 â†’ [Stage 2: Reranking] â†’ Top-10\n",
                "              (BM25, Dense)                    (Cross-Encoder, LLM)\n",
                "```\n",
                "\n",
                "**Why Two Stages?**\n",
                "\n",
                "| Stage | Speed | Accuracy | Use Case |\n",
                "|-------|-------|----------|----------|\n",
                "| Initial Retrieval | âš¡ Fast | Good | Narrow down from millions |\n",
                "| Reranking | ğŸ¢ Slow | Excellent | Refine top candidates |\n",
                "\n",
                "### 1.2 Reranking Methods Overview\n",
                "\n",
                "| Method | How It Works | Pros | Cons |\n",
                "|--------|--------------|------|------|\n",
                "| **Cross-Encoder** | BERT scores query-doc pairs | Most accurate | Slow |\n",
                "| **LLM Reranking** | Prompt LLM to rank | Zero-shot, semantic | Expensive |\n",
                "| **RRF** | Combine multiple rankings | Simple, effective | Needs multiple rankings |\n",
                "| **MMR** | Maximize relevance & diversity | Reduces redundancy | Needs embeddings |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '../..')\n",
                "\n",
                "import numpy as np\n",
                "from dataclasses import dataclass\n",
                "from typing import List, Dict, Optional\n",
                "\n",
                "# Import our reranking module\n",
                "from src.reranking import (\n",
                "    CrossEncoderReranker,\n",
                "    LLMReranker,\n",
                "    ReciprocalRankFusion,\n",
                "    DiversityReranker,\n",
                "    RerankConfig,\n",
                ")\n",
                "from src.reranking.reranking import Document\n",
                "\n",
                "print(\"âœ… Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Cross-Encoder Reranking\n",
                "\n",
                "### 2.1 Understanding Cross-Encoders\n",
                "\n",
                "**Bi-Encoder (Initial Retrieval)**:\n",
                "- Encodes query and document separately\n",
                "- Compare via dot product\n",
                "- Fast but limited interaction\n",
                "\n",
                "```\n",
                "Query â†’ [Encoder] â†’ q_vec\n",
                "                            â†’ Dot Product â†’ Score\n",
                "Doc   â†’ [Encoder] â†’ d_vec\n",
                "```\n",
                "\n",
                "**Cross-Encoder (Reranking)**:\n",
                "- Processes query and document together\n",
                "- Deep cross-attention between tokens\n",
                "- Rich interaction, more accurate\n",
                "\n",
                "```\n",
                "[CLS] query tokens [SEP] document tokens [SEP] â†’ [BERT] â†’ Score\n",
                "```\n",
                "\n",
                "### 2.2 Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample documents (simulating initial retrieval results)\n",
                "documents = [\n",
                "    Document(id=\"1\", content=\"Machine learning is a subset of artificial intelligence.\", score=0.85),\n",
                "    Document(id=\"2\", content=\"Deep learning uses neural networks with many layers.\", score=0.82),\n",
                "    Document(id=\"3\", content=\"Natural language processing helps computers understand text.\", score=0.78),\n",
                "    Document(id=\"4\", content=\"Neural networks are inspired by the human brain.\", score=0.75),\n",
                "    Document(id=\"5\", content=\"Python is popular for data science applications.\", score=0.70),\n",
                "]\n",
                "\n",
                "query = \"How do neural networks learn from data?\"\n",
                "\n",
                "# Initialize cross-encoder reranker\n",
                "cross_encoder = CrossEncoderReranker()\n",
                "\n",
                "# Rerank\n",
                "results = cross_encoder.rerank(query, documents, top_k=3)\n",
                "\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Cross-Encoder Reranking Results:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for r in results:\n",
                "    rank_change = r.original_rank - r.final_rank\n",
                "    change_str = f\"(+{rank_change})\" if rank_change > 0 else f\"({rank_change})\" if rank_change < 0 else \"(=)\"\n",
                "    \n",
                "    print(f\"\\nRank {r.final_rank} {change_str}: {r.document.content}\")\n",
                "    print(f\"  Original Score: {r.original_score:.3f} â†’ Rerank Score: {r.rerank_score:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 When to Use Cross-Encoders\n",
                "\n",
                "âœ… **Good for:**\n",
                "- Search quality is critical\n",
                "- Small candidate set (< 100 documents)\n",
                "- Complex queries requiring deep understanding\n",
                "\n",
                "âŒ **Not good for:**\n",
                "- Real-time with many documents\n",
                "- Very fast response requirements\n",
                "- Initial retrieval from large corpus"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. LLM-Based Reranking\n",
                "\n",
                "### 3.1 Concept\n",
                "\n",
                "Use a large language model to assess relevance through prompting.\n",
                "\n",
                "**Strategies:**\n",
                "\n",
                "| Strategy | Description | Example |\n",
                "|----------|-------------|----------|\n",
                "| **Pointwise** | Score each doc independently | \"Rate 0-10...\" |\n",
                "| **Pairwise** | Compare two docs | \"Which is more relevant?\" |\n",
                "| **Listwise** | Rank entire list | \"Order by relevance...\" |\n",
                "\n",
                "### 3.2 Pointwise Prompt Example\n",
                "\n",
                "```\n",
                "Given the query and document, rate relevance from 0-10.\n",
                "\n",
                "Query: How do neural networks learn?\n",
                "Document: Deep learning uses neural networks with many layers.\n",
                "\n",
                "Relevance (0-10): 8\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LLM Reranker (conceptual - uses fallback without actual LLM)\n",
                "llm_reranker = LLMReranker(strategy=\"pointwise\")\n",
                "\n",
                "# Demonstrate the concept\n",
                "print(\"LLM-Based Reranking Concept\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\"\"\n",
                "In production, the LLM would receive prompts like:\n",
                "\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚ Given the following query and document, rate the       â”‚\n",
                "â”‚ relevance on a scale of 0-10.                          â”‚\n",
                "â”‚                                                        â”‚\n",
                "â”‚ Query: How do neural networks learn from data?         â”‚\n",
                "â”‚                                                        â”‚\n",
                "â”‚ Document: Deep learning uses neural networks with      â”‚\n",
                "â”‚ many layers.                                           â”‚\n",
                "â”‚                                                        â”‚\n",
                "â”‚ Relevance score (0-10):                                â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "\n",
                "The LLM would respond with a score, which is then used\n",
                "to rerank the documents.\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Reciprocal Rank Fusion\n",
                "\n",
                "### 4.1 The RRF Formula\n",
                "\n",
                "RRF combines multiple rankings without needing score normalization:\n",
                "\n",
                "$$RRF(d) = \\sum_{r \\in R} \\frac{1}{k + rank_r(d)}$$\n",
                "\n",
                "Where:\n",
                "- $d$ = document\n",
                "- $R$ = set of rankings\n",
                "- $k$ = constant (usually 60)\n",
                "- $rank_r(d)$ = rank of document d in ranking r\n",
                "\n",
                "### 4.2 Why RRF Works\n",
                "\n",
                "| Property | Explanation |\n",
                "|----------|-------------|\n",
                "| **No normalization** | Ranks are comparable across systems |\n",
                "| **Diminishing returns** | Top ranks contribute more |\n",
                "| **Robust** | One bad ranker doesn't dominate |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RRF Example\n",
                "rrf = ReciprocalRankFusion(k=60)\n",
                "\n",
                "# Simulate different retrieval rankings\n",
                "# BM25 ranking\n",
                "bm25_docs = [\n",
                "    Document(id=\"1\", content=\"Machine learning is AI.\", score=0.9),\n",
                "    Document(id=\"2\", content=\"Deep learning uses neural nets.\", score=0.8),\n",
                "    Document(id=\"3\", content=\"NLP processes text.\", score=0.7),\n",
                "    Document(id=\"4\", content=\"Neural networks learn patterns.\", score=0.6),\n",
                "]\n",
                "\n",
                "# Dense retrieval ranking (different order)\n",
                "dense_docs = [\n",
                "    Document(id=\"4\", content=\"Neural networks learn patterns.\", score=0.85),\n",
                "    Document(id=\"2\", content=\"Deep learning uses neural nets.\", score=0.82),\n",
                "    Document(id=\"1\", content=\"Machine learning is AI.\", score=0.75),\n",
                "    Document(id=\"3\", content=\"NLP processes text.\", score=0.70),\n",
                "]\n",
                "\n",
                "# Apply RRF\n",
                "fused_results = rrf.fuse([bm25_docs, dense_docs], top_k=4)\n",
                "\n",
                "print(\"Reciprocal Rank Fusion Results\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\\nBM25 Ranking:   [1, 2, 3, 4]\")\n",
                "print(\"Dense Ranking:  [4, 2, 1, 3]\")\n",
                "print(\"\\nFused Ranking:\")\n",
                "\n",
                "for r in fused_results:\n",
                "    print(f\"  Rank {r.final_rank}: Doc {r.document.id} (RRF Score: {r.rerank_score:.4f})\")\n",
                "\n",
                "# Explain RRF calculation\n",
                "print(\"\\nRRF Score Calculation (k=60):\")\n",
                "print(\"-\" * 40)\n",
                "print(\"Doc 2: 1/(60+2) + 1/(60+2) = 0.0323\")\n",
                "print(\"Doc 1: 1/(60+1) + 1/(60+3) = 0.0322\")\n",
                "print(\"Doc 4: 1/(60+4) + 1/(60+1) = 0.0320\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Diversity Reranking (MMR)\n",
                "\n",
                "### 5.1 The Problem with Redundancy\n",
                "\n",
                "Top results often say the same thing differently:\n",
                "```\n",
                "1. Neural networks learn from data.\n",
                "2. Neural nets are trained on data.\n",
                "3. Networks learn patterns from datasets.\n",
                "â† All similar!\n",
                "```\n",
                "\n",
                "### 5.2 Maximal Marginal Relevance (MMR)\n",
                "\n",
                "MMR balances relevance with diversity:\n",
                "\n",
                "$$MMR = \\arg\\max_{d \\in R} \\left[ \\lambda \\cdot Sim(d, q) - (1-\\lambda) \\cdot \\max_{d_s \\in S} Sim(d, d_s) \\right]$$\n",
                "\n",
                "Where:\n",
                "- $d$ = candidate document\n",
                "- $q$ = query\n",
                "- $S$ = already selected documents\n",
                "- $\\lambda$ = relevance/diversity trade-off (0.5-0.8 typical)\n",
                "\n",
                "### 5.3 Î» Parameter Effect\n",
                "\n",
                "| Î» Value | Behavior |\n",
                "|---------|----------|\n",
                "| 1.0 | Pure relevance (no diversity) |\n",
                "| 0.5 | Balanced |\n",
                "| 0.0 | Maximum diversity |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Diversity Reranking Example\n",
                "diversity_reranker = DiversityReranker(lambda_param=0.7)\n",
                "\n",
                "# Documents with some redundancy\n",
                "redundant_docs = [\n",
                "    Document(id=\"1\", content=\"Neural networks learn patterns from data.\", score=0.9),\n",
                "    Document(id=\"2\", content=\"Neural nets are trained on datasets to learn.\", score=0.88),\n",
                "    Document(id=\"3\", content=\"Deep learning uses layers of neurons.\", score=0.85),\n",
                "    Document(id=\"4\", content=\"Networks learn from examples in data.\", score=0.82),\n",
                "    Document(id=\"5\", content=\"Computer vision recognizes images.\", score=0.75),\n",
                "]\n",
                "\n",
                "query = \"How do neural networks learn?\"\n",
                "\n",
                "# Apply MMR\n",
                "diverse_results = diversity_reranker.rerank(query, redundant_docs, top_k=3)\n",
                "\n",
                "print(\"Diversity Reranking (MMR) Results\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"Î» = 0.7 (70% relevance, 30% diversity)\\n\")\n",
                "\n",
                "print(\"Original Top 3:\")\n",
                "for i, d in enumerate(redundant_docs[:3], 1):\n",
                "    print(f\"  {i}. {d.content}\")\n",
                "\n",
                "print(\"\\nDiversified Top 3:\")\n",
                "for r in diverse_results:\n",
                "    print(f\"  {r.final_rank}. {r.document.content}\")\n",
                "    print(f\"     MMR: {r.rerank_score:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Building a Reranking Pipeline\n",
                "\n",
                "### 6.1 Multi-Stage Pipeline\n",
                "\n",
                "Combine multiple rerankers for best results:\n",
                "\n",
                "```\n",
                "Query â†’ BM25 (1000) â†’ Dense (100) â†’ Cross-Encoder (20) â†’ MMR (10)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.reranking.reranking import RerankingPipeline\n",
                "\n",
                "# Build pipeline\n",
                "pipeline = RerankingPipeline()\n",
                "pipeline.add_stage(cross_encoder)  # First: accuracy\n",
                "pipeline.add_stage(diversity_reranker)  # Then: diversity\n",
                "\n",
                "print(\"Reranking Pipeline Architecture\")\n",
                "print(\"=\" * 60)\n",
                "print(\"\"\"\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚  Initial Results â”‚ (from retrieval)\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚  Cross-Encoder   â”‚ Stage 1: Improve relevance\n",
                "â”‚     Reranker     â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚    Diversity     â”‚ Stage 2: Add variety\n",
                "â”‚   Reranker (MMR) â”‚\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "         â–¼\n",
                "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
                "â”‚   Final Results  â”‚ High quality + diverse\n",
                "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 Best Practices\n",
                "\n",
                "| Aspect | Recommendation |\n",
                "|--------|----------------|\n",
                "| **Initial retrieval** | Use fast methods (BM25, bi-encoder) |\n",
                "| **Top-K for reranking** | 50-200 documents |\n",
                "| **Cross-encoder model** | ms-marco-MiniLM-L-6-v2 (fast), ms-marco-electra-base (accurate) |\n",
                "| **MMR Î»** | Start with 0.7, tune based on use case |\n",
                "| **Latency budget** | Allocate 60% to retrieval, 40% to reranking |\n",
                "\n",
                "### 6.3 Common Pitfalls\n",
                "\n",
                "âŒ **Don't:**\n",
                "- Rerank too many documents (expensive)\n",
                "- Ignore diversity for QA systems\n",
                "- Use cross-encoder for initial retrieval\n",
                "\n",
                "âœ… **Do:**\n",
                "- Monitor reranking latency\n",
                "- Evaluate with nDCG, MRR\n",
                "- A/B test reranking strategies"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ğŸ“ Summary\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Cross-Encoder**: Most accurate, processes query+doc together\n",
                "2. **LLM Reranking**: Zero-shot, semantic understanding\n",
                "3. **RRF**: Simple, effective fusion without score normalization\n",
                "4. **MMR**: Balances relevance with diversity\n",
                "\n",
                "### When to Use What\n",
                "\n",
                "| Scenario | Best Method |\n",
                "|----------|-------------|\n",
                "| High accuracy needed | Cross-Encoder |\n",
                "| Multiple retrieval systems | RRF |\n",
                "| Avoiding redundancy | MMR |\n",
                "| Limited training data | LLM Reranking |\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. Try the [Evaluation Module](../week10-evaluation/) to measure improvements\n",
                "2. Build a complete [RAG Pipeline](../week9-orchestration/) with reranking\n",
                "3. Explore production deployment in [Week 13](../week13-deployment/)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}