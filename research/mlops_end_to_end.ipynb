{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MLOps End-to-End: From Training to Deployment\n",
                "\n",
                "This notebook demonstrates a complete MLOps workflow using the AI-Mastery-2026 toolkit.\n",
                "\n",
                "**What you'll learn:**\n",
                "1. Train a model from scratch using toolkit components\n",
                "2. Save the model for production use\n",
                "3. Serve the model via API\n",
                "4. Make predictions and monitor performance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'numpy'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_classification\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.datasets import make_classification\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Generate synthetic dataset\n",
                "X, y = make_classification(\n",
                "    n_samples=1000,\n",
                "    n_features=10,\n",
                "    n_informative=8,\n",
                "    n_redundant=2,\n",
                "    n_classes=2,\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "# Split data\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Standardize\n",
                "scaler = StandardScaler()\n",
                "X_train = scaler.fit_transform(X_train)\n",
                "X_test = scaler.transform(X_test)\n",
                "\n",
                "print(f\"Training samples: {X_train.shape[0]}\")\n",
                "print(f\"Test samples: {X_test.shape[0]}\")\n",
                "print(f\"Features: {X_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: Train From-Scratch Neural Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from src.ml.deep_learning import (\n",
                "    NeuralNetwork, Dense, Activation, Dropout, \n",
                "    BatchNormalization, CrossEntropyLoss\n",
                ")\n",
                "\n",
                "# Build model\n",
                "model = NeuralNetwork()\n",
                "model.add(Dense(10, 64, weight_init='he'))\n",
                "model.add(BatchNormalization(64))\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(0.3))\n",
                "model.add(Dense(64, 32, weight_init='he'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dense(32, 2, weight_init='xavier'))\n",
                "model.add(Activation('softmax'))\n",
                "\n",
                "# Compile\n",
                "model.compile(loss=CrossEntropyLoss(), learning_rate=0.01)\n",
                "\n",
                "# Summary\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train the model\n",
                "history = model.fit(\n",
                "    X_train, y_train,\n",
                "    epochs=100,\n",
                "    batch_size=32,\n",
                "    validation_data=(X_test, y_test),\n",
                "    verbose=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "axes[0].plot(history['loss'], label='Train Loss')\n",
                "axes[0].plot(history['val_loss'], label='Val Loss')\n",
                "axes[0].set_xlabel('Epoch')\n",
                "axes[0].set_ylabel('Loss')\n",
                "axes[0].legend()\n",
                "axes[0].set_title('Training Loss')\n",
                "\n",
                "axes[1].plot(history['accuracy'], label='Train Acc')\n",
                "axes[1].plot(history['val_accuracy'], label='Val Acc')\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('Accuracy')\n",
                "axes[1].legend()\n",
                "axes[1].set_title('Training Accuracy')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 3: Evaluate Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import seaborn as sns\n",
                "\n",
                "# Evaluate\n",
                "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
                "print(f\"Test Loss: {test_loss:.4f}\")\n",
                "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
                "\n",
                "# Get predictions\n",
                "y_pred = model.predict(X_test)\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 4: Save Model for Production"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "# Create models directory\n",
                "os.makedirs('../models', exist_ok=True)\n",
                "\n",
                "# Save neural network parameters\n",
                "model_data = {\n",
                "    'layer_params': [layer.get_params() for layer in model.layers],\n",
                "    'layer_types': [layer.__class__.__name__ for layer in model.layers],\n",
                "    'scaler_mean': scaler.mean_,\n",
                "    'scaler_scale': scaler.scale_,\n",
                "    'metadata': {\n",
                "        'model_type': 'NeuralNetwork',\n",
                "        'n_features': 10,\n",
                "        'n_classes': 2,\n",
                "        'test_accuracy': float(test_acc),\n",
                "        'test_loss': float(test_loss)\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save with joblib\n",
                "model_path = '../models/mlops_demo_model.joblib'\n",
                "joblib.dump(model_data, model_path)\n",
                "print(f\"Model saved to: {model_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 5: Start Docker Services\n",
                "\n",
                "Run these commands in terminal:\n",
                "\n",
                "```bash\n",
                "# Start all services\n",
                "docker-compose up -d\n",
                "\n",
                "# Check status\n",
                "docker-compose ps\n",
                "\n",
                "# View API logs\n",
                "docker-compose logs -f api\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 6: Test API Endpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "\n",
                "API_URL = \"http://localhost:8000\"\n",
                "\n",
                "# Health check\n",
                "response = requests.get(f\"{API_URL}/health\")\n",
                "print(\"Health Check:\")\n",
                "print(response.json())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List available models\n",
                "response = requests.get(f\"{API_URL}/models\")\n",
                "print(\"\\nAvailable Models:\")\n",
                "print(response.json())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Make prediction\n",
                "sample = X_test[0].tolist()\n",
                "\n",
                "response = requests.post(\n",
                "    f\"{API_URL}/predict\",\n",
                "    json={\"features\": sample, \"model_name\": \"classification_model\"}\n",
                ")\n",
                "\n",
                "print(\"\\nPrediction Result:\")\n",
                "result = response.json()\n",
                "print(f\"  Prediction: {result['predictions']}\")\n",
                "print(f\"  Actual: {y_test[0]}\")\n",
                "print(f\"  Processing time: {result['processing_time']*1000:.2f}ms\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Batch prediction\n",
                "batch_samples = X_test[:10].tolist()\n",
                "\n",
                "response = requests.post(\n",
                "    f\"{API_URL}/predict/batch\",\n",
                "    json={\"features\": batch_samples, \"model_id\": \"classification_model\"}\n",
                ")\n",
                "\n",
                "print(\"\\nBatch Prediction:\")\n",
                "result = response.json()\n",
                "print(f\"  Predictions: {result['predictions']}\")\n",
                "print(f\"  Actual: {y_test[:10].tolist()}\")\n",
                "print(f\"  Accuracy: {np.mean(np.array(result['predictions']) == y_test[:10]):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 7: View Metrics\n",
                "\n",
                "Access monitoring dashboards:\n",
                "\n",
                "- **Prometheus**: http://localhost:9090\n",
                "- **Grafana**: http://localhost:3000 (admin/admin)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get API metrics\n",
                "response = requests.get(f\"{API_URL}/metrics\")\n",
                "print(\"API Metrics:\")\n",
                "print(response.json())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "In this notebook, we completed the full MLOps cycle:\n",
                "\n",
                "1. ✅ **Data Preparation** - Generated and preprocessed data\n",
                "2. ✅ **Model Training** - Built neural network from scratch\n",
                "3. ✅ **Evaluation** - Assessed model performance\n",
                "4. ✅ **Model Saving** - Serialized for production\n",
                "5. ✅ **API Deployment** - Served via FastAPI in Docker\n",
                "6. ✅ **Predictions** - Made real-time predictions\n",
                "7. ✅ **Monitoring** - Tracked metrics with Prometheus/Grafana\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "- Add model versioning\n",
                "- Implement A/B testing\n",
                "- Set up CI/CD pipeline\n",
                "- Add data validation\n",
                "- Configure alerting"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
