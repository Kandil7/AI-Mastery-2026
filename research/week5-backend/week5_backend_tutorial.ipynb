{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîß Week 5: Backend Development with FastAPI\n",
                "\n",
                "This notebook covers building production-ready ML APIs with FastAPI.\n",
                "\n",
                "## Table of Contents\n",
                "1. [FastAPI Fundamentals](#1-fastapi-fundamentals)\n",
                "2. [Request/Response Models](#2-requestresponse-models)\n",
                "3. [Model Serving](#3-model-serving)\n",
                "4. [Async Processing](#4-async-processing)\n",
                "5. [Error Handling](#5-error-handling)\n",
                "6. [API Best Practices](#6-api-best-practices)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. FastAPI Fundamentals\n",
                "\n",
                "### 1.1 Why FastAPI?\n",
                "\n",
                "| Feature | Benefit |\n",
                "|---------|--------|\n",
                "| **Fast** | Built on Starlette/uvicorn, very high performance |\n",
                "| **Type hints** | Automatic validation and documentation |\n",
                "| **Async support** | Native async/await for I/O-bound operations |\n",
                "| **OpenAPI** | Auto-generated Swagger/ReDoc documentation |\n",
                "| **Pydantic** | Data validation with Python type hints |\n",
                "\n",
                "### 1.2 Basic Application Structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic FastAPI application structure\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel\n",
                "from typing import List, Optional\n",
                "import uvicorn\n",
                "\n",
                "# Create app instance\n",
                "app = FastAPI(\n",
                "    title=\"ML API\",\n",
                "    description=\"Machine Learning Model Serving API\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Health check endpoint\n",
                "@app.get(\"/health\")\n",
                "async def health_check():\n",
                "    \"\"\"Check if API is running.\"\"\"\n",
                "    return {\"status\": \"healthy\"}\n",
                "\n",
                "# Root endpoint\n",
                "@app.get(\"/\")\n",
                "async def root():\n",
                "    return {\"message\": \"Welcome to the ML API\"}\n",
                "\n",
                "print(\"‚úÖ Basic FastAPI app structure defined!\")\n",
                "print(\"Run with: uvicorn app:app --reload\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Request/Response Models\n",
                "\n",
                "### 2.1 Pydantic Models\n",
                "\n",
                "Pydantic models define the schema for request/response data with automatic validation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field, validator\n",
                "from typing import List, Optional\n",
                "from enum import Enum\n",
                "\n",
                "# Enum for model types\n",
                "class ModelType(str, Enum):\n",
                "    CLASSIFICATION = \"classification\"\n",
                "    REGRESSION = \"regression\"\n",
                "    EMBEDDING = \"embedding\"\n",
                "\n",
                "# Request model for predictions\n",
                "class PredictionRequest(BaseModel):\n",
                "    \"\"\"\n",
                "    Request schema for ML predictions.\n",
                "    \n",
                "    Pydantic automatically validates:\n",
                "    - Type correctness\n",
                "    - Required fields\n",
                "    - Value constraints\n",
                "    \"\"\"\n",
                "    text: str = Field(..., min_length=1, description=\"Input text for prediction\")\n",
                "    model_type: ModelType = Field(default=ModelType.CLASSIFICATION)\n",
                "    top_k: int = Field(default=5, ge=1, le=100, description=\"Number of results\")\n",
                "    include_confidence: bool = Field(default=True)\n",
                "    \n",
                "    @validator('text')\n",
                "    def text_not_empty(cls, v):\n",
                "        if not v.strip():\n",
                "            raise ValueError('Text cannot be empty or whitespace only')\n",
                "        return v.strip()\n",
                "    \n",
                "    class Config:\n",
                "        schema_extra = {\n",
                "            \"example\": {\n",
                "                \"text\": \"This product is amazing!\",\n",
                "                \"model_type\": \"classification\",\n",
                "                \"top_k\": 3\n",
                "            }\n",
                "        }\n",
                "\n",
                "# Response model\n",
                "class PredictionResult(BaseModel):\n",
                "    label: str\n",
                "    confidence: float = Field(..., ge=0, le=1)\n",
                "\n",
                "class PredictionResponse(BaseModel):\n",
                "    \"\"\"Response schema for ML predictions.\"\"\"\n",
                "    request_id: str\n",
                "    predictions: List[PredictionResult]\n",
                "    model_version: str\n",
                "    processing_time_ms: float\n",
                "\n",
                "# Test validation\n",
                "try:\n",
                "    valid_request = PredictionRequest(text=\"Hello world\", top_k=3)\n",
                "    print(f\"‚úÖ Valid request: {valid_request}\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Validation error: {e}\")\n",
                "\n",
                "try:\n",
                "    invalid_request = PredictionRequest(text=\"\", top_k=200)\n",
                "except Exception as e:\n",
                "    print(f\"‚úÖ Caught invalid request: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 API Endpoint with Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import uuid\n",
                "import time\n",
                "\n",
                "@app.post(\"/predict\", response_model=PredictionResponse)\n",
                "async def predict(request: PredictionRequest):\n",
                "    \"\"\"\n",
                "    Generate predictions for input text.\n",
                "    \n",
                "    - **text**: Input text to classify\n",
                "    - **model_type**: Type of model to use\n",
                "    - **top_k**: Number of predictions to return\n",
                "    \"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Simulate prediction (replace with actual model)\n",
                "    predictions = [\n",
                "        PredictionResult(label=\"positive\", confidence=0.85),\n",
                "        PredictionResult(label=\"neutral\", confidence=0.10),\n",
                "        PredictionResult(label=\"negative\", confidence=0.05),\n",
                "    ][:request.top_k]\n",
                "    \n",
                "    processing_time = (time.time() - start_time) * 1000\n",
                "    \n",
                "    return PredictionResponse(\n",
                "        request_id=str(uuid.uuid4()),\n",
                "        predictions=predictions,\n",
                "        model_version=\"1.0.0\",\n",
                "        processing_time_ms=processing_time\n",
                "    )\n",
                "\n",
                "print(\"‚úÖ Prediction endpoint defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Model Serving\n",
                "\n",
                "### 3.1 Loading Models at Startup\n",
                "\n",
                "Use lifespan events to load models once at startup."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from contextlib import asynccontextmanager\n",
                "\n",
                "# Global model registry\n",
                "class ModelRegistry:\n",
                "    \"\"\"Singleton to hold loaded models.\"\"\"\n",
                "    models: dict = {}\n",
                "    \n",
                "    @classmethod\n",
                "    def load_model(cls, name: str, model):\n",
                "        cls.models[name] = model\n",
                "        print(f\"Loaded model: {name}\")\n",
                "    \n",
                "    @classmethod\n",
                "    def get_model(cls, name: str):\n",
                "        return cls.models.get(name)\n",
                "\n",
                "@asynccontextmanager\n",
                "async def lifespan(app: FastAPI):\n",
                "    \"\"\"Lifespan context manager for startup/shutdown.\"\"\"\n",
                "    # Startup: Load models\n",
                "    print(\"üöÄ Starting up...\")\n",
                "    \n",
                "    # Load your ML models here\n",
                "    # ModelRegistry.load_model(\"classifier\", load_classifier())\n",
                "    # ModelRegistry.load_model(\"embedder\", load_embedder())\n",
                "    \n",
                "    yield  # Application runs here\n",
                "    \n",
                "    # Shutdown: Cleanup\n",
                "    print(\"üëã Shutting down...\")\n",
                "    ModelRegistry.models.clear()\n",
                "\n",
                "# Create app with lifespan\n",
                "app_with_lifespan = FastAPI(lifespan=lifespan)\n",
                "\n",
                "print(\"‚úÖ Lifespan management configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Dependency Injection for Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import Depends\n",
                "\n",
                "# Dependency to get model\n",
                "def get_classifier():\n",
                "    \"\"\"Dependency injection for classifier model.\"\"\"\n",
                "    model = ModelRegistry.get_model(\"classifier\")\n",
                "    if model is None:\n",
                "        raise HTTPException(\n",
                "            status_code=503,\n",
                "            detail=\"Model not loaded\"\n",
                "        )\n",
                "    return model\n",
                "\n",
                "# Use dependency in endpoint\n",
                "@app.post(\"/classify\")\n",
                "async def classify_text(\n",
                "    request: PredictionRequest,\n",
                "    model = Depends(get_classifier)\n",
                "):\n",
                "    \"\"\"\n",
                "    Classify text using injected model.\n",
                "    \n",
                "    Dependencies are resolved automatically by FastAPI.\n",
                "    \"\"\"\n",
                "    # In real code: result = model.predict(request.text)\n",
                "    return {\"result\": \"predicted\"}\n",
                "\n",
                "print(\"‚úÖ Dependency injection configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Async Processing\n",
                "\n",
                "### 4.1 Background Tasks"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import BackgroundTasks\n",
                "\n",
                "def log_prediction(request_id: str, text: str, result: dict):\n",
                "    \"\"\"Background task to log predictions.\"\"\"\n",
                "    # In real code: write to database or log file\n",
                "    print(f\"Logged prediction {request_id}: {text[:50]}...\")\n",
                "\n",
                "@app.post(\"/predict_async\")\n",
                "async def predict_with_logging(\n",
                "    request: PredictionRequest,\n",
                "    background_tasks: BackgroundTasks\n",
                "):\n",
                "    \"\"\"Prediction with async logging.\"\"\"\n",
                "    request_id = str(uuid.uuid4())\n",
                "    \n",
                "    # Make prediction\n",
                "    result = {\"label\": \"positive\", \"confidence\": 0.9}\n",
                "    \n",
                "    # Add logging as background task (doesn't block response)\n",
                "    background_tasks.add_task(\n",
                "        log_prediction, \n",
                "        request_id, \n",
                "        request.text, \n",
                "        result\n",
                "    )\n",
                "    \n",
                "    return {\"request_id\": request_id, \"result\": result}\n",
                "\n",
                "print(\"‚úÖ Background tasks configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Batch Processing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing import List\n",
                "\n",
                "class BatchPredictionRequest(BaseModel):\n",
                "    texts: List[str] = Field(..., min_items=1, max_items=100)\n",
                "    model_type: ModelType = ModelType.CLASSIFICATION\n",
                "\n",
                "class BatchPredictionResponse(BaseModel):\n",
                "    request_id: str\n",
                "    results: List[PredictionResult]\n",
                "    total_items: int\n",
                "    processing_time_ms: float\n",
                "\n",
                "@app.post(\"/predict/batch\", response_model=BatchPredictionResponse)\n",
                "async def predict_batch(request: BatchPredictionRequest):\n",
                "    \"\"\"\n",
                "    Process multiple predictions in a batch.\n",
                "    \n",
                "    More efficient than individual requests for multiple items.\n",
                "    \"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Batch process (replace with actual model batch inference)\n",
                "    results = [\n",
                "        PredictionResult(label=\"positive\", confidence=0.8 + i*0.01)\n",
                "        for i, _ in enumerate(request.texts)\n",
                "    ]\n",
                "    \n",
                "    processing_time = (time.time() - start_time) * 1000\n",
                "    \n",
                "    return BatchPredictionResponse(\n",
                "        request_id=str(uuid.uuid4()),\n",
                "        results=results,\n",
                "        total_items=len(request.texts),\n",
                "        processing_time_ms=processing_time\n",
                "    )\n",
                "\n",
                "print(\"‚úÖ Batch processing configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Error Handling\n",
                "\n",
                "### 5.1 Custom Exception Handlers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import Request\n",
                "from fastapi.responses import JSONResponse\n",
                "\n",
                "# Custom exception\n",
                "class ModelError(Exception):\n",
                "    \"\"\"Custom exception for model errors.\"\"\"\n",
                "    def __init__(self, message: str, model_name: str):\n",
                "        self.message = message\n",
                "        self.model_name = model_name\n",
                "\n",
                "# Exception handler\n",
                "@app.exception_handler(ModelError)\n",
                "async def model_error_handler(request: Request, exc: ModelError):\n",
                "    return JSONResponse(\n",
                "        status_code=500,\n",
                "        content={\n",
                "            \"error\": \"model_error\",\n",
                "            \"message\": exc.message,\n",
                "            \"model\": exc.model_name,\n",
                "            \"path\": str(request.url)\n",
                "        }\n",
                "    )\n",
                "\n",
                "# Global exception handler\n",
                "@app.exception_handler(Exception)\n",
                "async def global_exception_handler(request: Request, exc: Exception):\n",
                "    return JSONResponse(\n",
                "        status_code=500,\n",
                "        content={\n",
                "            \"error\": \"internal_error\",\n",
                "            \"message\": \"An unexpected error occurred\",\n",
                "            \"detail\": str(exc) if app.debug else None\n",
                "        }\n",
                "    )\n",
                "\n",
                "print(\"‚úÖ Exception handlers configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. API Best Practices\n",
                "\n",
                "### 6.1 Rate Limiting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from collections import defaultdict\n",
                "from datetime import datetime, timedelta\n",
                "\n",
                "class RateLimiter:\n",
                "    \"\"\"Simple in-memory rate limiter.\"\"\"\n",
                "    def __init__(self, max_requests: int = 100, window_seconds: int = 60):\n",
                "        self.max_requests = max_requests\n",
                "        self.window_seconds = window_seconds\n",
                "        self.requests = defaultdict(list)\n",
                "    \n",
                "    def is_allowed(self, client_id: str) -> bool:\n",
                "        now = datetime.now()\n",
                "        window_start = now - timedelta(seconds=self.window_seconds)\n",
                "        \n",
                "        # Remove old requests\n",
                "        self.requests[client_id] = [\n",
                "            t for t in self.requests[client_id] if t > window_start\n",
                "        ]\n",
                "        \n",
                "        if len(self.requests[client_id]) >= self.max_requests:\n",
                "            return False\n",
                "        \n",
                "        self.requests[client_id].append(now)\n",
                "        return True\n",
                "\n",
                "rate_limiter = RateLimiter(max_requests=100, window_seconds=60)\n",
                "\n",
                "# Middleware for rate limiting\n",
                "from fastapi import Request\n",
                "\n",
                "@app.middleware(\"http\")\n",
                "async def rate_limit_middleware(request: Request, call_next):\n",
                "    client_ip = request.client.host\n",
                "    \n",
                "    if not rate_limiter.is_allowed(client_ip):\n",
                "        return JSONResponse(\n",
                "            status_code=429,\n",
                "            content={\"error\": \"Rate limit exceeded\"}\n",
                "        )\n",
                "    \n",
                "    return await call_next(request)\n",
                "\n",
                "print(\"‚úÖ Rate limiting configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 6.2 API Versioning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from fastapi import APIRouter\n",
                "\n",
                "# Create versioned routers\n",
                "router_v1 = APIRouter(prefix=\"/api/v1\", tags=[\"v1\"])\n",
                "router_v2 = APIRouter(prefix=\"/api/v2\", tags=[\"v2\"])\n",
                "\n",
                "@router_v1.get(\"/models\")\n",
                "async def list_models_v1():\n",
                "    \"\"\"V1: List available models.\"\"\"\n",
                "    return {\"models\": [\"classifier-v1\"]}\n",
                "\n",
                "@router_v2.get(\"/models\")\n",
                "async def list_models_v2():\n",
                "    \"\"\"V2: List available models with metadata.\"\"\"\n",
                "    return {\n",
                "        \"models\": [\n",
                "            {\"name\": \"classifier-v2\", \"version\": \"2.0\", \"type\": \"classification\"}\n",
                "        ]\n",
                "    }\n",
                "\n",
                "# Include routers in app\n",
                "app.include_router(router_v1)\n",
                "app.include_router(router_v2)\n",
                "\n",
                "print(\"‚úÖ API versioning configured!\")\n",
                "print(\"Endpoints: /api/v1/models and /api/v2/models\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "### Key Concepts\n",
                "\n",
                "| Concept | Best Practice |\n",
                "|---------|---------------|\n",
                "| **Request Models** | Use Pydantic with validators |\n",
                "| **Model Loading** | Load at startup with lifespan |\n",
                "| **Dependencies** | Use Depends() for injection |\n",
                "| **Background Tasks** | Non-blocking async operations |\n",
                "| **Error Handling** | Custom exception handlers |\n",
                "| **Rate Limiting** | Protect against abuse |\n",
                "| **Versioning** | Maintain backward compatibility |\n",
                "\n",
                "### Production Checklist\n",
                "\n",
                "- [ ] Health check endpoint\n",
                "- [ ] Request validation with Pydantic\n",
                "- [ ] Model loading at startup\n",
                "- [ ] Proper error handling\n",
                "- [ ] Rate limiting\n",
                "- [ ] Logging and monitoring\n",
                "- [ ] API versioning\n",
                "- [ ] CORS configuration\n",
                "- [ ] Authentication (if needed)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}