{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ Week 13: Production Deployment\n",
                "\n",
                "This notebook covers deploying ML models to production.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Deployment Fundamentals](#1-deployment-fundamentals)\n",
                "2. [Model Serving](#2-model-serving)\n",
                "3. [Containerization](#3-containerization)\n",
                "4. [Scaling Strategies](#4-scaling-strategies)\n",
                "5. [Monitoring](#5-monitoring)\n",
                "6. [Production Checklist](#6-production-checklist)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Deployment Fundamentals\n",
                "\n",
                "### 1.1 Deployment Options\n",
                "\n",
                "| Option | Pros | Cons | Use Case |\n",
                "|--------|------|------|----------|\n",
                "| **REST API** | Simple, standard | Latency overhead | General purpose |\n",
                "| **gRPC** | Fast, typed | More complex | Internal services |\n",
                "| **Serverless** | No infra, auto-scale | Cold starts | Low traffic |\n",
                "| **Edge** | Low latency | Limited compute | IoT, mobile |\n",
                "\n",
                "### 1.2 Deployment Architecture\n",
                "\n",
                "```\n",
                "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "‚îÇ                    LOAD BALANCER                        ‚îÇ\n",
                "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                          ‚îÇ\n",
                "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "        ‚ñº                 ‚ñº                 ‚ñº\n",
                "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "  ‚îÇ API Pod  ‚îÇ     ‚îÇ API Pod  ‚îÇ     ‚îÇ API Pod  ‚îÇ\n",
                "  ‚îÇ  + Model ‚îÇ     ‚îÇ  + Model ‚îÇ     ‚îÇ  + Model ‚îÇ\n",
                "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "        ‚îÇ                 ‚îÇ                 ‚îÇ\n",
                "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "                          ‚ñº\n",
                "                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
                "                   ‚îÇ   Model      ‚îÇ\n",
                "                   ‚îÇ   Registry   ‚îÇ\n",
                "                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Model Serving\n",
                "\n",
                "### 2.1 FastAPI Production Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Production FastAPI Application\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel\n",
                "from typing import List, Optional\n",
                "import time\n",
                "import uuid\n",
                "\n",
                "app = FastAPI(\n",
                "    title=\"ML Model API\",\n",
                "    description=\"Production ML model serving API\",\n",
                "    version=\"1.0.0\"\n",
                ")\n",
                "\n",
                "# Request/Response Models\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str\n",
                "    model_version: str = \"latest\"\n",
                "\n",
                "class PredictionResponse(BaseModel):\n",
                "    request_id: str\n",
                "    prediction: str\n",
                "    confidence: float\n",
                "    model_version: str\n",
                "    latency_ms: float\n",
                "\n",
                "@app.get(\"/health\")\n",
                "async def health():\n",
                "    \"\"\"Health check endpoint for load balancer.\"\"\"\n",
                "    return {\"status\": \"healthy\", \"timestamp\": time.time()}\n",
                "\n",
                "@app.get(\"/ready\")\n",
                "async def ready():\n",
                "    \"\"\"Readiness check - is the model loaded?\"\"\"\n",
                "    # Check if model is loaded\n",
                "    model_loaded = True  # Replace with actual check\n",
                "    if not model_loaded:\n",
                "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
                "    return {\"status\": \"ready\"}\n",
                "\n",
                "@app.post(\"/predict\", response_model=PredictionResponse)\n",
                "async def predict(request: PredictionRequest):\n",
                "    \"\"\"Main prediction endpoint.\"\"\"\n",
                "    start_time = time.time()\n",
                "    \n",
                "    # Generate prediction (replace with actual model)\n",
                "    prediction = \"positive\"\n",
                "    confidence = 0.95\n",
                "    \n",
                "    latency = (time.time() - start_time) * 1000\n",
                "    \n",
                "    return PredictionResponse(\n",
                "        request_id=str(uuid.uuid4()),\n",
                "        prediction=prediction,\n",
                "        confidence=confidence,\n",
                "        model_version=request.model_version,\n",
                "        latency_ms=latency\n",
                "    )\n",
                "\n",
                "print(\"‚úÖ Production API defined!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Model Loading Optimization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import threading\n",
                "\n",
                "class ModelManager:\n",
                "    \"\"\"\n",
                "    Singleton model manager with lazy loading and versioning.\n",
                "    \"\"\"\n",
                "    _instance = None\n",
                "    _lock = threading.Lock()\n",
                "    \n",
                "    def __new__(cls):\n",
                "        if cls._instance is None:\n",
                "            with cls._lock:\n",
                "                if cls._instance is None:\n",
                "                    cls._instance = super().__new__(cls)\n",
                "                    cls._instance._initialized = False\n",
                "        return cls._instance\n",
                "    \n",
                "    def __init__(self):\n",
                "        if self._initialized:\n",
                "            return\n",
                "        self.models = {}\n",
                "        self.current_version = None\n",
                "        self._initialized = True\n",
                "    \n",
                "    def load_model(self, version: str, model_path: str):\n",
                "        \"\"\"Load a model version.\"\"\"\n",
                "        print(f\"Loading model v{version} from {model_path}...\")\n",
                "        # Simulate model loading\n",
                "        self.models[version] = f\"model_{version}\"\n",
                "        self.current_version = version\n",
                "        print(f\"‚úÖ Model v{version} loaded\")\n",
                "    \n",
                "    def get_model(self, version: str = \"latest\"):\n",
                "        \"\"\"Get a model by version.\"\"\"\n",
                "        if version == \"latest\":\n",
                "            version = self.current_version\n",
                "        return self.models.get(version)\n",
                "    \n",
                "    def switch_version(self, version: str):\n",
                "        \"\"\"Hot-swap to a different model version.\"\"\"\n",
                "        if version in self.models:\n",
                "            self.current_version = version\n",
                "            print(f\"Switched to model v{version}\")\n",
                "        else:\n",
                "            raise ValueError(f\"Model v{version} not loaded\")\n",
                "\n",
                "# Example usage\n",
                "manager = ModelManager()\n",
                "manager.load_model(\"1.0\", \"/models/v1\")\n",
                "manager.load_model(\"2.0\", \"/models/v2\")\n",
                "manager.switch_version(\"2.0\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Containerization\n",
                "\n",
                "### 3.1 Dockerfile for ML"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dockerfile_content = '''\n",
                "# Multi-stage build for smaller image\n",
                "FROM python:3.10-slim as builder\n",
                "\n",
                "WORKDIR /app\n",
                "COPY requirements.txt .\n",
                "\n",
                "# Install dependencies\n",
                "RUN pip install --no-cache-dir -r requirements.txt\n",
                "\n",
                "# Final stage\n",
                "FROM python:3.10-slim\n",
                "\n",
                "WORKDIR /app\n",
                "\n",
                "# Copy installed packages\n",
                "COPY --from=builder /usr/local/lib/python3.10/site-packages /usr/local/lib/python3.10/site-packages\n",
                "\n",
                "# Copy application\n",
                "COPY . .\n",
                "\n",
                "# Health check\n",
                "HEALTHCHECK --interval=30s --timeout=10s \\\\\n",
                "  CMD curl -f http://localhost:8000/health || exit 1\n",
                "\n",
                "# Non-root user\n",
                "RUN useradd -m appuser && chown -R appuser /app\n",
                "USER appuser\n",
                "\n",
                "EXPOSE 8000\n",
                "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
                "'''\n",
                "\n",
                "print(\"Optimized Dockerfile:\")\n",
                "print(dockerfile_content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Docker Compose for Development"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "docker_compose = '''\n",
                "version: \"3.8\"\n",
                "\n",
                "services:\n",
                "  api:\n",
                "    build: .\n",
                "    ports:\n",
                "      - \"8000:8000\"\n",
                "    environment:\n",
                "      - MODEL_PATH=/models/latest\n",
                "      - LOG_LEVEL=info\n",
                "    volumes:\n",
                "      - ./models:/models:ro\n",
                "    deploy:\n",
                "      resources:\n",
                "        limits:\n",
                "          memory: 4G\n",
                "        reservations:\n",
                "          memory: 2G\n",
                "    healthcheck:\n",
                "      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n",
                "      interval: 30s\n",
                "      timeout: 10s\n",
                "      retries: 3\n",
                "\n",
                "  redis:\n",
                "    image: redis:7-alpine\n",
                "    ports:\n",
                "      - \"6379:6379\"\n",
                "\n",
                "  prometheus:\n",
                "    image: prom/prometheus\n",
                "    ports:\n",
                "      - \"9090:9090\"\n",
                "    volumes:\n",
                "      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n",
                "'''\n",
                "\n",
                "print(\"Docker Compose:\")\n",
                "print(docker_compose)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Scaling Strategies\n",
                "\n",
                "### 4.1 Horizontal Scaling\n",
                "\n",
                "| Strategy | Description | When to Use |\n",
                "|----------|-------------|-------------|\n",
                "| **Replicas** | Multiple identical pods | Stateless services |\n",
                "| **Load Balancing** | Distribute requests | High traffic |\n",
                "| **Auto-scaling** | Scale based on metrics | Variable load |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Kubernetes HPA configuration\n",
                "hpa_config = '''\n",
                "apiVersion: autoscaling/v2\n",
                "kind: HorizontalPodAutoscaler\n",
                "metadata:\n",
                "  name: ml-api-hpa\n",
                "spec:\n",
                "  scaleTargetRef:\n",
                "    apiVersion: apps/v1\n",
                "    kind: Deployment\n",
                "    name: ml-api\n",
                "  minReplicas: 2\n",
                "  maxReplicas: 10\n",
                "  metrics:\n",
                "    - type: Resource\n",
                "      resource:\n",
                "        name: cpu\n",
                "        target:\n",
                "          type: Utilization\n",
                "          averageUtilization: 70\n",
                "    - type: Pods\n",
                "      pods:\n",
                "        metric:\n",
                "          name: requests_per_second\n",
                "        target:\n",
                "          type: AverageValue\n",
                "          averageValue: 100\n",
                "'''\n",
                "\n",
                "print(\"Kubernetes HPA:\")\n",
                "print(hpa_config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Optimization Techniques\n",
                "\n",
                "| Technique | Speedup | Effort | Notes |\n",
                "|-----------|---------|--------|-------|\n",
                "| **Batching** | 2-10x | Low | Combine requests |\n",
                "| **Caching** | 10-100x | Low | Cache frequent queries |\n",
                "| **Quantization** | 2-4x | Medium | Reduce precision |\n",
                "| **Distillation** | 2-10x | High | Train smaller model |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple caching with TTL\n",
                "from functools import lru_cache\n",
                "from datetime import datetime, timedelta\n",
                "import hashlib\n",
                "\n",
                "class PredictionCache:\n",
                "    def __init__(self, ttl_seconds: int = 3600, max_size: int = 1000):\n",
                "        self.cache = {}\n",
                "        self.ttl = timedelta(seconds=ttl_seconds)\n",
                "        self.max_size = max_size\n",
                "    \n",
                "    def _hash_input(self, text: str) -> str:\n",
                "        return hashlib.md5(text.encode()).hexdigest()\n",
                "    \n",
                "    def get(self, text: str):\n",
                "        key = self._hash_input(text)\n",
                "        if key in self.cache:\n",
                "            entry, timestamp = self.cache[key]\n",
                "            if datetime.now() - timestamp < self.ttl:\n",
                "                return entry\n",
                "            del self.cache[key]\n",
                "        return None\n",
                "    \n",
                "    def set(self, text: str, value):\n",
                "        if len(self.cache) >= self.max_size:\n",
                "            # Evict oldest\n",
                "            oldest = min(self.cache, key=lambda k: self.cache[k][1])\n",
                "            del self.cache[oldest]\n",
                "        \n",
                "        key = self._hash_input(text)\n",
                "        self.cache[key] = (value, datetime.now())\n",
                "\n",
                "cache = PredictionCache(ttl_seconds=3600)\n",
                "print(\"‚úÖ Caching layer ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Monitoring\n",
                "\n",
                "### 5.1 Key Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prometheus metrics with custom buckets\n",
                "metrics_code = '''\n",
                "from prometheus_client import Counter, Histogram, Gauge\n",
                "\n",
                "# Request metrics\n",
                "REQUEST_COUNT = Counter(\n",
                "    \"model_requests_total\",\n",
                "    \"Total prediction requests\",\n",
                "    [\"model_version\", \"status\"]\n",
                ")\n",
                "\n",
                "REQUEST_LATENCY = Histogram(\n",
                "    \"model_request_latency_seconds\",\n",
                "    \"Request latency in seconds\",\n",
                "    [\"model_version\"],\n",
                "    buckets=[.01, .025, .05, .075, .1, .25, .5, .75, 1.0, 2.5]\n",
                ")\n",
                "\n",
                "# Model metrics\n",
                "MODEL_LOAD_TIME = Gauge(\n",
                "    \"model_load_time_seconds\",\n",
                "    \"Time to load model\",\n",
                "    [\"model_version\"]\n",
                ")\n",
                "\n",
                "PREDICTION_CONFIDENCE = Histogram(\n",
                "    \"prediction_confidence\",\n",
                "    \"Distribution of prediction confidence scores\",\n",
                "    buckets=[.1, .2, .3, .4, .5, .6, .7, .8, .9, 1.0]\n",
                ")\n",
                "'''\n",
                "\n",
                "print(\"Prometheus Metrics:\")\n",
                "print(metrics_code)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Production Checklist\n",
                "\n",
                "### Pre-Deployment\n",
                "- [ ] Model validated on production-like data\n",
                "- [ ] Performance benchmarks meet SLAs\n",
                "- [ ] Docker image built and tested\n",
                "- [ ] Health/readiness endpoints working\n",
                "- [ ] Rollback plan documented\n",
                "\n",
                "### Deployment\n",
                "- [ ] Canary deployment configured\n",
                "- [ ] Auto-scaling policies set\n",
                "- [ ] Load balancer configured\n",
                "- [ ] SSL/TLS enabled\n",
                "\n",
                "### Post-Deployment\n",
                "- [ ] Monitoring dashboards set up\n",
                "- [ ] Alerts configured\n",
                "- [ ] Logging aggregation working\n",
                "- [ ] Model drift detection enabled"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **Use health checks** - Load balancers need them\n",
                "2. **Cache aggressively** - Huge latency improvements\n",
                "3. **Monitor everything** - Can't fix what you can't see\n",
                "4. **Plan for failure** - Graceful degradation\n",
                "\n",
                "### Production Architecture\n",
                "\n",
                "```\n",
                "Users ‚Üí CDN ‚Üí Load Balancer ‚Üí API Pods ‚Üí Model\n",
                "                    ‚Üì\n",
                "              Cache Layer\n",
                "                    ‚Üì\n",
                "              Monitoring\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}