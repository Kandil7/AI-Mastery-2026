{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "67cada25",
            "metadata": {},
            "source": [
                "# RAG Pipeline Deep Dive (Comprehensive Analysis)\n",
                "\n",
                "This notebook provides a comprehensive exploration of the RAG (Retrieval-Augmented Generation) pipeline in the RAG Engine Mini project. It covers the complete flow from document ingestion to response generation, including all intermediate processing steps.\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will understand:\n",
                "- How documents are ingested and processed in the RAG pipeline\n",
                "- The multi-modal processing capabilities of the system\n",
                "- Hierarchical chunking strategies and their benefits\n",
                "- Knowledge graph extraction and storage\n",
                "- Embedding generation and caching mechanisms\n",
                "- Background task processing with Celery\n",
                "\n",
                "## Prerequisites\n",
                "\n",
                "This notebook assumes familiarity with:\n",
                "- Basic RAG concepts\n",
                "- Python async programming\n",
                "- Document processing workflows"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "069e36c9",
            "metadata": {},
            "source": [
                "## Step 0: Setup and Imports\n",
                "\n",
                "We add the project root to `sys.path` so imports work when running from the notebook directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6d6c2a29",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import asyncio\n",
                "import hashlib\n",
                "from pathlib import Path\n",
                "\n",
                "# Find rag-engine-mini root by walking upwards\n",
                "current = Path.cwd().resolve()\n",
                "repo_root = None\n",
                "for parent in [current, *current.parents]:\n",
                "    if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
                "        repo_root = parent\n",
                "        break\n",
                "\n",
                "if repo_root is None:\n",
                "    raise RuntimeError(\"Could not locate rag-engine-mini root for imports\")\n",
                "\n",
                "sys.path.insert(0, str(repo_root))\n",
                "\n",
                "print(\"Repo root:\", repo_root)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9995f3a1",
            "metadata": {},
            "source": [
                "## Step 1: Understanding the Core Components\n",
                "\n",
                "Let's examine the core components of the RAG pipeline by importing and inspecting them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c833c74a",
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.core.config import settings\n",
                "from src.core.bootstrap import get_container\n",
                "from src.domain.entities import TenantId, DocumentId, ChunkSpec\n",
                "from src.application.services.chunking import chunk_hierarchical\n",
                "\n",
                "# Get the dependency container\n",
                "container = get_container()\n",
                "\n",
                "# List some of the key services available\n",
                "services = list(container._services.keys())\n",
                "print(\"Available services in container:\", services[:10], \"...\")\n",
                "\n",
                "# Focus on the key services for our RAG pipeline\n",
                "print(\"\\nKey RAG services:\")\n",
                "print(\"- Document Repository:\", type(container.get(\"document_repo\")))\n",
                "print(\"- Document Reader:\", type(container.get(\"document_reader\")))\n",
                "print(\"- Text Extractor:\", type(container.get(\"text_extractor\")))\n",
                "print(\"- Cached Embeddings:\", type(container.get(\"cached_embeddings\")))\n",
                "print(\"- Chunk Dedup Repository:\", type(container.get(\"chunk_dedup_repo\")))\n",
                "print(\"- Vector Store:\", type(container.get(\"vector_store\")))\n",
                "print(\"- LLM:\", type(container.get(\"llm\")))\n",
                "print(\"- Graph Extractor:\", type(container.get(\"graph_extractor\")))\n",
                "print(\"- Graph Repository:\", type(container.get(\"graph_repo\")))\n",
                "print(\"- Vision Service:\", type(container.get(\"vision_service\")))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d7b166b",
            "metadata": {},
            "source": [
                "## Step 2: Simulating Document Ingestion Process\n",
                "\n",
                "Let's simulate the document ingestion process step-by-step, following the same pattern as the actual pipeline."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "12527a49",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate a simplified version of the document indexing process\n",
                "\n",
                "def simulate_document_ingestion_pipeline():\n",
                "    \"\"\"\n",
                "    Simulates the document ingestion pipeline with explanations for each step.\n",
                "    This mirrors the actual index_document Celery task but simplified for demonstration.\n",
                "    \"\"\"\n",
                "    print(\"Starting simulated document ingestion pipeline...\")\n",
                "    \n",
                "    # Get services from container\n",
                "    document_repo = container[\"document_repo\"]\n",
                "    document_reader = container[\"document_reader\"]\n",
                "    text_extractor = container[\"text_extractor\"]\n",
                "    cached_embeddings = container[\"cached_embeddings\"]\n",
                "    chunk_dedup_repo = container[\"chunk_dedup_repo\"]\n",
                "    vector_store = container[\"vector_store\"]\n",
                "    llm = container[\"llm\"]\n",
                "    graph_extractor = container[\"graph_extractor\"]\n",
                "    graph_repo = container[\"graph_repo\"]\n",
                "    \n",
                "    print(\"✓ Services initialized\")\n",
                "    \n",
                "    # Step 1: Create a mock document entity\n",
                "    tenant = TenantId(\"demo-tenant\")\n",
                "    doc_id = DocumentId(\"demo-doc-id\")\n",
                "    \n",
                "    print(f\"\\n1. Processing document: {doc_id.value} for tenant: {tenant.value}\")\n",
                "    \n",
                "    # Step 2: Mock document content (in real scenario, this would come from document_reader)\n",
                "    mock_document_content = \"\"\"\n",
                "    Artificial Intelligence and Machine Learning Overview\n",
                "    \n",
                "    Introduction to AI\n",
                "    Artificial Intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.\n",
                "    \n",
                "    Machine Learning Concepts\n",
                "    Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence.\n",
                "    \n",
                "    Deep Learning Applications\n",
                "    Deep learning is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.\n",
                "    \n",
                "    Future of AI\n",
                "    The future of AI holds many possibilities, from autonomous vehicles to personalized medicine. However, ethical considerations must be taken into account as AI systems become more powerful.\n",
                "    \"\"\"\n",
                "    \n",
                "    print(\"2. Document content loaded (simulated)\")\n",
                "    \n",
                "    # Step 3: Document Summary Generation\n",
                "    print(\"3. Generating document summary with LLM...\")\n",
                "    summary_prompt = (\n",
                "        \"Summarize the following document content in 1-2 sentences to provide context for RAG chunks. \"\n",
                "        \"Output ONLY the summary sentences:\\n\\n\"\n",
                "        f\"{mock_document_content[:12000]}\"\n",
                "    )\n",
                "    doc_summary = \"This document provides an overview of AI, ML, and deep learning concepts along with their applications and ethical considerations.\"\n",
                "    print(f\"   Generated summary: {doc_summary}\")\n",
                "    \n",
                "    # Step 4: Hierarchical Chunking\n",
                "    print(\"4. Performing hierarchical chunking...\")\n",
                "    spec = ChunkSpec(strategy=\"hierarchical\", parent_size=2048, child_size=512)\n",
                "    hierarchy = chunk_hierarchical(mock_document_content, spec)\n",
                "    print(f\"   Created {len(hierarchy)} chunk pairs (parent-child)\")\n",
                "    \n",
                "    # Display first few chunks as examples\n",
                "    print(\"   Example chunks:\")\n",
                "    for i, item in enumerate(hierarchy[:2]):\n",
                "        print(f\"     Parent {i+1}: '{item['parent_text'][:50]}...'\")\n",
                "        print(f\"     Child {i+1}:  '{item['child_text'][:50]}...'\\n\")\n",
                "    \n",
                "    # Step 5: Batch Embedding\n",
                "    print(\"5. Generating embeddings for chunks...\")\n",
                "    child_texts = [h[\"child_text\"] for h in hierarchy]\n",
                "    unique_child_texts = list(set(child_texts))\n",
                "    \n",
                "    # In a real system, we'd call: unique_vectors = cached_embeddings.embed_many(unique_child_texts)\n",
                "    # For simulation, we'll create mock embeddings\n",
                "    mock_vectors = [[0.1] * 384 for _ in unique_child_texts]  # 384-dim mock vectors\n",
                "    vec_map = dict(zip(unique_child_texts, mock_vectors))\n",
                "    \n",
                "    print(f\"   Generated embeddings for {len(unique_child_texts)} unique chunks\")\n",
                "    \n",
                "    # Step 6: Storage Operations\n",
                "    print(\"6. Storing chunks and vectors...\")\n",
                "    chunk_ids_in_order = []\n",
                "    handled_parents = set()\n",
                "    \n",
                "    for idx, item in enumerate(hierarchy):\n",
                "        c_text = item[\"child_text\"]\n",
                "        p_text = item[\"parent_text\"]\n",
                "        \n",
                "        # Generate hashes for parent and child\n",
                "        p_hash = hashlib.sha256(p_text.encode()).hexdigest()[:16]\n",
                "        c_hash = hashlib.sha256(c_text.encode()).hexdigest()[:16]\n",
                "        \n",
                "        # Simulate parent chunk creation\n",
                "        p_id = f\"parent_{p_hash}\"\n",
                "        if p_id not in handled_parents:\n",
                "            # In real system: p_id = chunk_dedup_repo.upsert_chunk_store(...)\n",
                "            print(f\"   Created parent chunk: {p_id}\")\n",
                "            \n",
                "            # Extract and store graph triplets for parent\n",
                "            # In real system: triplets = graph_extractor.extract_triplets(p_text)\n",
                "            print(f\"   Extracted and stored graph triplets for parent\")\n",
                "            handled_parents.add(p_id)\n",
                "        \n",
                "        # Simulate child chunk creation\n",
                "        c_id = f\"child_{c_hash}_{idx}\"\n",
                "        # In real system: c_id = chunk_dedup_repo.upsert_chunk_store(...)\n",
                "        print(f\"   Created child chunk: {c_id}\")\n",
                "        \n",
                "        chunk_ids_in_order.append(c_id)\n",
                "        \n",
                "        # Simulate vector storage\n",
                "        # In real system: vector_store.upsert_points(ids=[c_id], vectors=[vec_map[c_text]], ...)\n",
                "        print(f\"   Stored vector for chunk: {c_id}\")\n",
                "    \n",
                "    print(f\"\\n✓ Pipeline completed successfully!\")\n",
                "    print(f\"  Total chunks processed: {len(chunk_ids_in_order)}\")\n",
                "    print(f\"  Document summary: {doc_summary[:60]}...\")\n",
                "    \n",
                "    return {\n",
                "        \"chunks_created\": len(chunk_ids_in_order),\n",
                "        \"document_summary\": doc_summary,\n",
                "        \"chunk_ids\": chunk_ids_in_order\n",
                "    }\n",
                "\n",
                "# Run the simulation\n",
                "result = simulate_document_ingestion_pipeline()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "53bbc0dc",
            "metadata": {},
            "source": [
                "## Step 3: Exploring the Chunk Deduplication Mechanism\n",
                "\n",
                "The RAG engine implements a sophisticated chunk deduplication mechanism to prevent storing identical content multiple times."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9f59bca3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_chunk_deduplication():\n",
                "    \"\"\"\n",
                "    Demonstrates the chunk deduplication mechanism.\n",
                "    \"\"\"\n",
                "    print(\"Demonstrating chunk deduplication mechanism:\")\n",
                "    \n",
                "    # Example texts, including some duplicates\n",
                "    texts = [\n",
                "        \"Artificial Intelligence is transforming industries worldwide.\",\n",
                "        \"Machine learning enables computers to learn from experience.\",\n",
                "        \"Artificial Intelligence is transforming industries worldwide.\",  # Duplicate\n",
                "        \"Deep learning uses neural networks with multiple layers.\",\n",
                "        \"Machine learning enables computers to learn from experience.\"   # Duplicate\n",
                "    ]\n",
                "    \n",
                "    print(\"\\nOriginal texts:\")\n",
                "    for i, text in enumerate(texts):\n",
                "        print(f\"  {i+1}. {text}\")\n",
                "    \n",
                "    # Calculate hashes (same as _chunk_hash function in tasks.py)\n",
                "    def chunk_hash(text: str) -> str:\n",
                "        \"\"\"Generate SHA256 hash of normalized text.\"\"\"\n",
                "        normalized = \" \".join(text.split())\n",
                "        return hashlib.sha256(normalized.encode(\"utf-8\")).hexdigest()\n",
                "    \n",
                "    hashes = [chunk_hash(text) for text in texts]\n",
                "    \n",
                "    print(\"\\nHashes calculated:\")\n",
                "    for i, (text, h) in enumerate(zip(texts, hashes)):\n",
                "        print(f\"  {i+1}. {h[:16]}... <- '{text[:30]}...'\")\n",
                "    \n",
                "    # Identify unique texts\n",
                "    unique_texts = list(set(texts))\n",
                "    unique_hashes = [chunk_hash(text) for text in unique_texts]\n",
                "    \n",
                "    print(f\"\\nAfter deduplication:\")\n",
                "    print(f\"  Original count: {len(texts)}\")\n",
                "    print(f\"  Unique count: {len(unique_texts)}\")\n",
                "    print(f\"  Saved: {len(texts) - len(unique_texts)} duplicate chunks\")\n",
                "    \n",
                "    print(\"\\nUnique texts:\")\n",
                "    for i, text in enumerate(unique_texts):\n",
                "        print(f\"  {i+1}. {text}\")\n",
                "\n",
                "demonstrate_chunk_deduplication()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4623ea56",
            "metadata": {},
            "source": [
                "## Step 4: Examining the Hierarchical Chunking Strategy\n",
                "\n",
                "The RAG engine uses hierarchical chunking to maintain context while enabling granular retrieval."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b082f27f",
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_hierarchical_chunking():\n",
                "    \"\"\"\n",
                "    Demonstrates the hierarchical chunking strategy.\n",
                "    \"\"\"\n",
                "    print(\"Demonstrating hierarchical chunking strategy:\")\n",
                "    \n",
                "    # Example content to chunk\n",
                "    sample_content = \"\"\"\n",
                "    Chapter 1: Introduction to Neural Networks\n",
                "    \n",
                "    Neural networks are computing systems inspired by the human brain. They consist of interconnected nodes or neurons in various layers. Each connection has a weight that adjusts as learning proceeds.\n",
                "    \n",
                "    Section 1.1: Perceptron Model\n",
                "    The perceptron is the simplest form of a neural network, consisting of a single neuron. It takes multiple inputs, applies weights to them, sums them up, and passes the result through an activation function.\n",
                "    \n",
                "    Section 1.2: Multilayer Perceptrons\n",
                "    Multilayer perceptrons extend the simple perceptron by adding hidden layers between the input and output layers. This allows the network to learn more complex patterns and relationships in the data.\n",
                "    \n",
                "    Activation Functions\n",
                "    Activation functions determine the output of a neural network. Common activation functions include sigmoid, tanh, and ReLU. Each has its own advantages and disadvantages depending on the use case.\n",
                "    \"\"\"\n",
                "    \n",
                "    print(\"Sample content to chunk:\")\n",
                "    print(sample_content.strip())\n",
                "    \n",
                "    # Define chunk specifications\n",
                "    spec = ChunkSpec(strategy=\"hierarchical\", parent_size=200, child_size=100)\n",
                "    \n",
                "    # Simulate hierarchical chunking (in real system, this would call chunk_hierarchical)\n",
                "    # For demonstration, we'll manually create a hierarchy\n",
                "    \n",
                "    # Parents (larger context chunks)\n",
                "    parents = [\n",
                "        sample_content[0:300],  # Introduction to Neural Networks + Section 1.1\n",
                "        sample_content[200:500],  # Section 1.1 + Section 1.2\n",
                "        sample_content[400:]  # Section 1.2 + Activation Functions\n",
                "    ]\n",
                "    \n",
                "    # Children (focused chunks)\n",
                "    children = [\n",
                "        sample_content[0:100],   # Intro sentence\n",
                "        sample_content[100:200], # Neural network definition\n",
                "        sample_content[200:300], # Perceptron details\n",
                "        sample_content[300:400], # Multilayer details\n",
                "        sample_content[400:500], # Activation functions intro\n",
                "        sample_content[500:]     # Activation function details\n",
                "    ]\n",
                "    \n",
                "    # Create hierarchy pairs\n",
                "    hierarchy = []\n",
                "    for child in children:\n",
                "        # Find the most appropriate parent for each child\n",
                "        if \"Introduction\" in child or \"Neural\" in child:\n",
                "            hierarchy.append({\"parent_text\": parents[0], \"child_text\": child})\n",
                "        elif \"Perceptron\" in child:\n",
                "            # Determine which parent contains this child\n",
                "            if \"Multilayer\" in child:\n",
                "                hierarchy.append({\"parent_text\": parents[2], \"child_text\": child})\n",
                "            else:\n",
                "                hierarchy.append({\"parent_text\": parents[0], \"child_text\": child})\n",
                "        else:\n",
                "            hierarchy.append({\"parent_text\": parents[2], \"child_text\": child})\n",
                "    \n",
                "    print(f\"\\nCreated {len(hierarchy)} parent-child pairs:\")\n",
                "    \n",
                "    for i, pair in enumerate(hierarchy):\n",
                "        print(f\"\\nPair {i+1}:\")\n",
                "        print(f\"  Parent ({len(pair['parent_text'])} chars): {pair['parent_text'][:60]}...\")\n",
                "        print(f\"  Child ({len(pair['child_text'])} chars):  {pair['child_text'][:60]}...\")\n",
                "    \n",
                "    print(f\"\\nBenefits of hierarchical chunking:\")\n",
                "    print(\"• Granular retrieval: Search for specific details in child chunks\")\n",
                "    print(\"• Context preservation: Access broader context via parent chunks\")\n",
                "    print(\"• Efficient storage: Related content grouped together\")\n",
                "    print(\"• Flexible querying: Choose appropriate granularity for different questions\")\n",
                "\n",
                "demonstrate_hierarchical_chunking()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "55082522",
            "metadata": {},
            "source": [
                "## Step 5: Understanding the Knowledge Graph Extraction\n",
                "\n",
                "The RAG engine extracts structured knowledge from text and stores it as graph triplets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3670633d",
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_knowledge_graph_extraction():\n",
                "    \"\"\"\n",
                "    Demonstrates the knowledge graph extraction concept.\n",
                "    \"\"\"\n",
                "    print(\"Demonstrating knowledge graph extraction:\")\n",
                "    \n",
                "    # Sample text with extractable relationships\n",
                "    sample_text = (\n",
                "        \"Apple Inc. is headquartered in Cupertino, California. Tim Cook is the CEO of Apple Inc. \"\n",
                "        \"Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne. The iPhone was developed by Apple Inc.\"\n",
                "    )\n",
                "    \n",
                "    print(f\"\\nInput text: {sample_text}\")\n",
                "    \n",
                "    # Simulate triplet extraction\n",
                "    # In a real system, graph_extractor.extract_triplets(text) would return actual triplets\n",
                "    extracted_triplets = [\n",
                "        (\"Apple Inc.\", \"headquartered_in\", \"Cupertino, California\"),\n",
                "        (\"Tim Cook\", \"CEO_of\", \"Apple Inc.\"),\n",
                "        (\"Apple Inc.\", \"founded_by\", \"Steve Jobs\"),\n",
                "        (\"Apple Inc.\", \"founded_by\", \"Steve Wozniak\"),\n",
                "        (\"Apple Inc.\", \"founded_by\", \"Ronald Wayne\"),\n",
                "        (\"iPhone\", \"developed_by\", \"Apple Inc.\")\n",
                "    ]\n",
                "    \n",
                "    print(f\"\\nExtracted {len(extracted_triplets)} triplets:\")\n",
                "    for i, triplet in enumerate(extracted_triplets, 1):\n",
                "        print(f\"  {i}. ({triplet[0]}, {triplet[1]}, {triplet[2]})\")\n",
                "    \n",
                "    print(f\"\\nBenefits of knowledge graph extraction:\")\n",
                "    print(\"• Structured knowledge representation\")\n",
                "    print(\"• Ability to answer relationship-based questions\")\n",
                "    print(\"• Enhanced reasoning capabilities\")\n",
                "    print(\"• Improved retrieval for fact-based queries\")\n",
                "    \n",
                "    # Show how triplets could be used for enhanced retrieval\n",
                "    print(f\"\\nExample query processing:\")\n",
                "    print(\"Q: Who founded Apple Inc.?\")\n",
                "    print(\"A: Based on triplets: Steve Jobs, Steve Wozniak, Ronald Wayne\")\n",
                "    \n",
                "    print(\"\\nQ: What did Apple Inc. develop?\")\n",
                "    print(\"A: Based on triplets: iPhone\")\n",
                "\n",
                "demonstrate_knowledge_graph_extraction()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "291bce05",
            "metadata": {},
            "source": [
                "## Step 6: Understanding the Caching Strategy\n",
                "\n",
                "The RAG engine implements a multi-layer caching strategy for performance optimization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c7f659b",
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_caching_strategy():\n",
                "    \"\"\"\n",
                "    Demonstrates the caching strategy used in the RAG pipeline.\n",
                "    \"\"\"\n",
                "    print(\"Demonstrating caching strategy:\")\n",
                "    \n",
                "    print(f\"\\nThe RAG engine uses multiple caching layers:\")\n",
                "    \n",
                "    print(f\"\\n1. Embedding Cache:\")\n",
                "    print(\"   • Stores computed embeddings to avoid recomputation\")\n",
                "    print(\"   • Uses document/content hashing for quick lookup\")\n",
                "    print(\"   • Reduces API costs and improves response time\")\n",
                "    \n",
                "    # Example of embedding caching\n",
                "    sample_texts = [\n",
                "        \"Machine learning is a subset of artificial intelligence\",\n",
                "        \"Deep learning uses neural networks with multiple layers\",\n",
                "        \"Machine learning is a subset of artificial intelligence\"  # Duplicate\n",
                "    ]\n",
                "    \n",
                "    print(f\"\\n   Example: Processing {len(sample_texts)} texts with caching\")\n",
                "    unique_texts = list(set(sample_texts))\n",
                "    print(f\"   Unique texts after deduplication: {len(unique_texts)}\")\n",
                "    print(f\"   Embeddings computed: {len(unique_texts)} (not {len(sample_texts)})\")\n",
                "    print(f\"   Cost saving: {(len(sample_texts) - len(unique_texts))/len(sample_texts)*100:.0f}% fewer embeddings\")\n",
                "    \n",
                "    print(f\"\\n2. Document Chunk Cache:\")\n",
                "    print(\"   • Stores processed document chunks to avoid re-processing\")\n",
                "    print(\"   • Maintains parent-child relationships\")\n",
                "    print(\"   • Enables faster document retrieval\")\n",
                "    \n",
                "    print(f\"\\n3. Query Result Cache:\")\n",
                "    print(\"   • Caches frequent query results\")\n",
                "    print(\"   • Improves response time for repeated questions\")\n",
                "    print(\"   • Reduces computational overhead\")\n",
                "    \n",
                "    print(f\"\\n4. LLM Response Cache:\")\n",
                "    print(\"   • Caches responses for similar prompts\")\n",
                "    print(\"   • Reduces API usage and latency\")\n",
                "    print(\"   • Maintains quality while improving performance\")\n",
                "\n",
                "demonstrate_caching_strategy()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1b4d2f3d",
            "metadata": {},
            "source": [
                "## Step 7: Reviewing the Complete Pipeline Architecture\n",
                "\n",
                "Let's review the complete RAG pipeline architecture with all components working together."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c4e4f5a",
            "metadata": {},
            "outputs": [],
            "source": [
                "def summarize_rag_pipeline():\n",
                "    \"\"\"\n",
                "    Summarizes the complete RAG pipeline architecture.\n",
                "    \"\"\"\n",
                "    print(\"SUMMARY: Complete RAG Pipeline Architecture\")\n",
                "    print(\"=\" * 50)\n",
                "    \n",
                "    print(\"\\nINGESTION PHASE:\")\n",
                "    print(\"1. Document Upload → Stored in file system\")\n",
                "    print(\"2. Document Processing → Extract text, tables, images\")\n",
                "    print(\"3. Multi-modal Processing → Describe images with vision models\")\n",
                "    print(\"4. Document Summarization → Generate context summary with LLM\")\n",
                "    print(\"5. Hierarchical Chunking → Create parent-child chunk relationships\")\n",
                "    print(\"6. Knowledge Extraction → Extract graph triplets from content\")\n",
                "    print(\"7. Embedding Generation → Convert chunks to vector representations\")\n",
                "    print(\"8. Storage → Save chunks to DB, vectors to vector store, graphs to graph DB\")\n",
                "    \n",
                "    print(\"\\nRETRIEVAL PHASE:\")\n",
                "    print(\"1. Query Processing → Parse and potentially expand user query\")\n",
                "    print(\"2. Hybrid Search → Combine semantic and keyword-based retrieval\")\n",
                "    print(\"3. Re-ranking → Improve relevance with cross-encoder models\")\n",
                "    print(\"4. Context Assembly → Gather relevant chunks with parent context\")\n",
                "    \n",
                "    print(\"\\nGENERATION PHASE:\")\n",
                "    print(\"1. Prompt Construction → Build context-aware prompt with retrieved content\")\n",
                "    print(\"2. LLM Generation → Generate response using LLM\")\n",
                "    print(\"3. Verification → Validate response accuracy if enabled\")\n",
                "    print(\"4. Response Delivery → Return answer with sources and metadata\")\n",
                "    \n",
                "    print(\"\\nKEY FEATURES:\")\n",
                "    print(\"✓ Multi-tenant isolation\")\n",
                "    print(\"✓ Multi-modal content support\")\n",
                "    print(\"✓ Hierarchical chunking for context preservation\")\n",
                "    print(\"✓ Knowledge graph integration\")\n",
                "    print(\"✓ Embedding caching for cost optimization\")\n",
                "    print(\"✓ Asynchronous processing with Celery\")\n",
                "    print(\"✓ Comprehensive observability\")\n",
                "    print(\"✓ Enterprise-grade security\")\n",
                "    \n",
                "    print(\"\\nPERFORMANCE OPTIMIZATIONS:\")\n",
                "    print(\"• Chunk deduplication to reduce storage/compute\")\n",
                "    print(\"• Batch embedding operations\")\n",
                "    print(\"• Multi-level caching strategy\")\n",
                "    print(\"• Asynchronous processing for scalability\")\n",
                "    print(\"• Connection pooling for databases\")\n",
                "    \n",
                "summarize_rag_pipeline()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5b4d2f3d",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "This notebook provided a comprehensive exploration of the RAG pipeline in the RAG Engine Mini project. We examined:\n",
                "\n",
                "1. The complete document ingestion pipeline\n",
                "2. Multi-modal processing capabilities\n",
                "3. Hierarchical chunking strategies\n",
                "4. Knowledge graph extraction and storage\n",
                "5. Caching mechanisms for performance\n",
                "6. The complete architecture with all components integrated\n",
                "\n",
                "The RAG Engine demonstrates enterprise-grade practices for building scalable, reliable, and performant RAG systems. The combination of hierarchical chunking, knowledge graph integration, multi-modal processing, and comprehensive caching creates a robust foundation for production RAG applications."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10"
        }
    }
}