{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ“ Notebook 11: Agentic RAG & Planning (ReAct)\n",
    "\n",
    "Welcome to the **Pinnacle** of RAG Engineering. In this notebook, we transform our passive retrieval engine into an **Autonomous Agent**.\n",
    "\n",
    "### What is an Agent?\n",
    "Instead of a simple linear pipeline (`Question -> Search -> Answer`), an Agent follows a **ReAct** (Reason + Act) loop:\n",
    "1. **Thought**: Analyze the question and decide what information is missing.\n",
    "2. **Action**: Choose a tool (e.g., Vector Search, Web Search, Calculator).\n",
    "3. **Observation**: Read the tool's output.\n",
    "4. **Repeat**: Continue until a definitive answer can be synthesized.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Implement a manual ReAct loop from scratch.\n",
    "2. Orchestrate multiple tools (Hybrid Search + Tavily Web Search).\n",
    "3. Understand \"Agentic Planning\" vs. static routing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:16.805388Z",
     "iopub.status.busy": "2026-01-31T14:06:16.805388Z",
     "iopub.status.idle": "2026-01-31T14:06:16.816386Z",
     "shell.execute_reply": "2026-01-31T14:06:16.814387Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root = os.getcwd()\n",
    "rag_root = None\n",
    "\n",
    "while True:\n",
    "    candidate = os.path.join(root, 'sprints', 'rag_engine', 'rag-engine-mini')\n",
    "    if os.path.isdir(os.path.join(candidate, 'src')):\n",
    "        rag_root = candidate\n",
    "        break\n",
    "    if os.path.basename(root) == 'rag-engine-mini' and os.path.isdir(os.path.join(root, 'src')):\n",
    "        rag_root = root\n",
    "        break\n",
    "    parent = os.path.dirname(root)\n",
    "    if parent == root:\n",
    "        break\n",
    "    root = parent\n",
    "\n",
    "if rag_root:\n",
    "    sys.path.insert(0, rag_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:16.819913Z",
     "iopub.status.busy": "2026-01-31T14:06:16.819913Z",
     "iopub.status.idle": "2026-01-31T14:06:28.419497Z",
     "shell.execute_reply": "2026-01-31T14:06:28.418213Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from src.core.bootstrap import get_container\n",
    "from src.application.use_cases.ask_question_hybrid import AskQuestionHybridUseCase\n",
    "\n",
    "container = get_container()\n",
    "use_case: AskQuestionHybridUseCase | None = container.get(\"ask_hybrid_use_case\")\n",
    "llm = container.get(\"llm\")\n",
    "\n",
    "if use_case is None:\n",
    "    print(\"ask_hybrid_use_case not configured; skipping live workflow steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining Tools\n",
    "\n",
    "Our agent will have two primary tools: `internal_knowledge` (RAG) and `web_search` (Tavily)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:28.422531Z",
     "iopub.status.busy": "2026-01-31T14:06:28.421508Z",
     "iopub.status.idle": "2026-01-31T14:06:28.430834Z",
     "shell.execute_reply": "2026-01-31T14:06:28.430834Z"
    }
   },
   "outputs": [],
   "source": [
    "def internal_knowledge_tool(query: str):\n",
    "    print(f\"[Tool] Searching internal RAG for: {query}\")\n",
    "    if use_case is None:\n",
    "        return \"RAG backend not configured; skipping retrieval.\"\n",
    "    result = use_case.execute_retrieval_only(tenant_id=\"default\", question=query)\n",
    "    return \"\\n\".join([getattr(c, \"text\", getattr(c, \"content\", \"\")) for c in result[:3]])\n",
    "\n",
    "\n",
    "def web_search_tool(query: str):\n",
    "    print(f\"[Tool] Searching the web for: {query}\")\n",
    "    # Note: Requires TAVILY_API_KEY in .env\n",
    "    from src.adapters.web_search.tavily_search import TavilySearch\n",
    "    from src.core.config import settings\n",
    "    searcher = TavilySearch(api_key=settings.tavily_api_key)\n",
    "    return \"\\n\".join([r.content for r in searcher.search(query)[:3]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The ReAct Loop (The Brain)\n",
    "\n",
    "We use the LLM to decide which tool to use. If the internal search fails or the question is about current events, it should \"plan\" to use the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:28.433833Z",
     "iopub.status.busy": "2026-01-31T14:06:28.433833Z",
     "iopub.status.idle": "2026-01-31T14:06:28.439863Z",
     "shell.execute_reply": "2026-01-31T14:06:28.439188Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an Autonomous Research Agent.\n",
    "Available Tools:\n",
    "1. internal_knowledge: Use for technical documentation, physics, or specific data.\n",
    "2. web_search: Use for current events, news, or if internal search fails.\n",
    "\n",
    "PROCESS:\n",
    "THOUGHT: Explain why you are choosing a tool or finalizing the answer.\n",
    "ACTION: tool_name(query)\n",
    "OBSERVATION: Output from the tool.\n",
    "FINAL ANSWER: Your final synthesis.\n",
    "\"\"\"\n",
    "\n",
    "def run_agent(question: str):\n",
    "    prompt = f\"{SYSTEM_PROMPT}\\nQuestion: {question}\\nTHOUGHT:\"\n",
    "\n",
    "    # For this demo, we do a single-step ReAct. In production, this would be a while-loop.\n",
    "    if llm is None or not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"LLM not configured; set OPENAI_API_KEY to run this demo.\"\n",
    "\n",
    "    response = llm.generate(prompt)\n",
    "    print(response)\n",
    "\n",
    "    if \"ACTION: internal_knowledge\" in response:\n",
    "        query = response.split(\"(\")[1].split(\")\")[0]\n",
    "        obs = internal_knowledge_tool(query)\n",
    "        final = llm.generate(f\"{prompt}\\n{response}\\nOBSERVATION: {obs}\\nTHOUGHT: I have the data. FINAL ANSWER:\")\n",
    "        return final\n",
    "\n",
    "    if \"ACTION: web_search\" in response:\n",
    "        query = response.split(\"(\")[1].split(\")\")[0]\n",
    "        obs = web_search_tool(query)\n",
    "        final = llm.generate(f\"{prompt}\\n{response}\\nOBSERVATION: {obs}\\nTHOUGHT: I have the search results. FINAL ANSWER:\")\n",
    "        return final\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Live Execution\n",
    "\n",
    "Observe how the agent \"decides\" based on the type of question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:28.443827Z",
     "iopub.status.busy": "2026-01-31T14:06:28.443827Z",
     "iopub.status.idle": "2026-01-31T14:06:28.449327Z",
     "shell.execute_reply": "2026-01-31T14:06:28.448346Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Test 1: Internal Topic ---\n",
      "LLM not configured; set OPENAI_API_KEY to run this demo.\n",
      "\n",
      "--- Test 2: Current Events ---\n",
      "LLM not configured; set OPENAI_API_KEY to run this demo.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Test 1: Internal Topic ---\")\n",
    "print(run_agent(\"What is the red planet?\"))\n",
    "\n",
    "print(\"\\n--- Test 2: Current Events ---\")\n",
    "print(run_agent(\"What is the latest AI news from today?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Why this matters\n",
    "\n",
    "Static RAG is limited by its retrieval algorithm. **Agentic RAG** creates a dynamic researcher that can solve multi-step problems, check its own errors, and explore new information sources autonomously.\n",
    "\n",
    "**Congratulations!** You have mastered the most advanced pattern in current AI Engineering. ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
