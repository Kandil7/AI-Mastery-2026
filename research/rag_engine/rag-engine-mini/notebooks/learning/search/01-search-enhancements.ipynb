{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Enhancements - Interactive Learning\n",
    "# تحسينات البحث - تعلم تفاعلي\n",
    "\n",
    "This notebook covers:\n",
    "- Trie data structure for autocomplete\n",
    "- Auto-suggest types (document, query, topic)\n",
    "- Query expansion strategies\n",
    "- Faceted search implementation\n",
    "\n",
    "يغطي هذا المفكرة:\n",
    "- هيكل البيانات Trie للإكمال التلقائي\n",
    "- أنواع الاقتراح (مستند، استعلام، موضوع)\n",
    "- استراتيجيات توسيع الاستعلامات\n",
    "- تنفيذ البحث المجزوء"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Trie Data Structure\n",
    "## الجزء 1: هيكل البيانات Trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trie implementation for efficient autocomplete\n",
    "from collections import defaultdict\n",
    "\n",
    "class TrieNode:\n",
    "    \"\"\"Node in trie data structure.\"\"\"\n",
    "    __slots__ = ['children', 'is_end', 'score']\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.children = defaultdict(TrieNode)\n",
    "        self.is_end = False\n",
    "        self.score = 0.0\n",
    "\n",
    "class Trie:\n",
    "    \"\"\"Trie data structure for prefix search.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "        self._size = 0\n",
    "    \n",
    "    def insert(self, word: str, score: float = 1.0):\n",
    "        \"\"\"Insert word into trie.\"\"\"\n",
    "        node = self.root\n",
    "        for char in word.lower():\n",
    "            node = node.children[char]\n",
    "        node.is_end = True\n",
    "        node.score = score\n",
    "        self._size += 1\n",
    "    \n",
    "    def autocomplete(self, prefix: str, limit: int = 10):\n",
    "        \"\"\"Get autocomplete suggestions for prefix.\"\"\"\n",
    "        node = self.root\n",
    "        \n",
    "        # Navigate to prefix end\n",
    "        for char in prefix.lower():\n",
    "            if char not in node.children:\n",
    "                return []\n",
    "            node = node.children[char]\n",
    "        \n",
    "        # Collect all words from this node\n",
    "        results = []\n",
    "        \n",
    "        def dfs(current_node, current_word):\n",
    "            if len(results) >= limit:\n",
    "                return\n",
    "            if current_node.is_end:\n",
    "                results.append((current_word, current_node.score))\n",
    "            for char, child_node in current_node.children.items():\n",
    "                dfs(child_node, current_word + char)\n",
    "        \n",
    "        dfs(node, prefix)\n",
    "        return results\n",
    "\n",
    "# Create and populate trie\n",
    "trie = Trie()\n",
    "documents = [\n",
    "    \"project_plan.pdf\", \"project_specs.docx\", \"project_timeline.pdf\",\n",
    "    \"rag_architecture.md\", \"rag_guide.pdf\", \"rag_tutorial.md\",\n",
    "    \"machine_learning.pdf\", \"deep_learning.pdf\", \"neural_networks.pdf\",\n",
    "]\n",
    "\n",
    "for doc in documents:\n",
    "    trie.insert(doc, score=1.0)\n",
    "\n",
    "print(f\"Trie populated with {trie._size} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test autocomplete\n",
    "test_prefixes = [\"proj\", \"rag\", \"machine\", \"deep\"]\n",
    "\n",
    "for prefix in test_prefixes:\n",
    "    print(f\"\\nPrefix: '{prefix}'\")\n",
    "    suggestions = trie.autocomplete(prefix, limit=5)\n",
    "    \n",
    "    for doc_name, score in suggestions:\n",
    "        print(f\"  - {doc_name} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement Synonym-Based Expansion\n",
    "### تمرين 1: تنفيذ التوسيع المستند إلى المرادفات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement synonym-based query expansion\n",
    "\n",
    "SYNONYMS = {\n",
    "    \"rag\": [\"retrieval augmented generation\", \"retrieval-augmented\"],\n",
    "    \"vector\": [\"embedding\", \"representation\", \"feature vector\"],\n",
    "    \"search\": [\"query\", \"find\", \"retrieve\"],\n",
    "    \"document\": [\"file\", \"record\", \"entry\", \"paper\"],\n",
    "}\n",
    "\n",
    "def expand_with_synonyms(query: str) -> list:\n",
    "    \"\"\"Expand query using synonym mapping.\"\"\"\n",
    "    expanded = [query.lower()]\n",
    "    words = query.lower().split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in SYNONYMS:\n",
    "            for synonym in SYNONYMS[word]:\n",
    "                # Replace word with synonym in original query\n",
    "                expanded.append(\n",
    "                    \" \".join(synonym if w != word else w for w in words)\n",
    "                )\n",
    "    \n",
    "    return list(set(expanded))  # Remove duplicates\n",
    "\n",
    "# Test\n",
    "test_queries = [\"rag architecture\", \"vector search\", \"find document\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    expanded = expand_with_synonyms(query)\n",
    "    for exp in expanded:\n",
    "        print(f\"  - {exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution / الحل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "SYNONYMS = {\n",
    "    \"rag\": [\"retrieval augmented generation\", \"retrieval-augmented\"],\n",
    "    \"vector\": [\"embedding\", \"representation\", \"feature vector\"],\n",
    "    \"search\": [\"query\", \"find\", \"retrieve\"],\n",
    "    \"document\": [\"file\", \"record\", \"entry\", \"paper\"],\n",
    "}\n",
    "\n",
    "def expand_with_synonyms(query: str) -> list:\n",
    "    \"\"\"Expand query using synonym mapping.\"\"\"\n",
    "    expanded = [query.lower()]\n",
    "    words = query.lower().split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in SYNONYMS:\n",
    "            for synonym in SYNONYMS[word]:\n",
    "                expanded.append(\n",
    "                    \" \".join(synonym if w != word else w for w in words)\n",
    "                )\n",
    "    \n",
    "    return list(set(expanded))\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    expanded = expand_with_synonyms(query)\n",
    "    print(f\"Expansions: {len(expanded)}\")\n",
    "    for exp in expanded:\n",
    "        print(f\"  - {exp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Faceted Search\n",
    "## الجزء 2: البحث المجزوء"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement faceted search\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "sample_documents = [\n",
    "    {\"name\": \"doc1.pdf\", \"status\": \"indexed\", \"type\": \"pdf\", \"size_bytes\": 100*1024, \"date\": datetime(2024, 1, 15)},\n",
    "    {\"name\": \"doc2.pdf\", \"status\": \"indexed\", \"type\": \"pdf\", \"size_bytes\": 500*1024, \"date\": datetime(2024, 1, 20)},\n",
    "    {\"name\": \"doc3.txt\", \"status\": \"indexed\", \"type\": \"text\", \"size_bytes\": 10*1024, \"date\": datetime(2024, 1, 25)},\n",
    "    {\"name\": \"doc4.pdf\", \"status\": \"processing\", \"type\": \"pdf\", \"size_bytes\": 2000*1024, \"date\": datetime(2024, 1, 10)},\n",
    "    {\"name\": \"doc5.pdf\", \"status\": \"failed\", \"type\": \"pdf\", \"size_bytes\": 5000*1024, \"date\": datetime(2024, 1, 5)},\n",
    "]\n",
    "\n",
    "# Status facets\n",
    "status_counts = {}\n",
    "for doc in sample_documents:\n",
    "    status = doc['status']\n",
    "    status_counts[status] = status_counts.get(status, 0) + 1\n",
    "\n",
    "print(\"Status Facets:\")\n",
    "for status, count in sorted(status_counts.items()):\n",
    "    print(f\"  {status}: {count}\")\n",
    "\n",
    "# Content type facets\n",
    "type_counts = {}\n",
    "for doc in sample_documents:\n",
    "    dtype = doc['type']\n",
    "    type_counts[dtype] = type_counts.get(dtype, 0) + 1\n",
    "\n",
    "print(\"\\nContent Type Facets:\")\n",
    "for dtype, count in sorted(type_counts.items()):\n",
    "    print(f\"  {dtype}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size range facets\n",
    "RANGES = [\n",
    "    (\"0-100KB\", 0, 100 * 1024),\n",
    "    (\"100KB-1MB\", 100 * 1024, 1024 * 1024),\n",
    "    (\"1MB-10MB\", 1024 * 1024, 10 * 1024 * 1024),\n",
    "    (\"10MB+\", 10 * 1024 * 1024, float('inf')),\n",
    "]\n",
    "\n",
    "print(\"\\nSize Range Facets:\")\n",
    "for name, min_size, max_size in RANGES:\n",
    "    count = sum(1 for d in sample_documents if min_size <= d['size_bytes'] < max_size)\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date range facets\n",
    "now = datetime.now()\n",
    "RANGES = [\n",
    "    (\"Last 7 days\", now - timedelta(days=7), now),\n",
    "    (\"Last 30 days\", now - timedelta(days=30), now),\n",
    "    (\"Older than 30 days\", None, now - timedelta(days=30)),\n",
    "]\n",
    "\n",
    "print(\"\\nDate Range Facets:\")\n",
    "for name, min_date, max_date in RANGES:\n",
    "    if min_date:\n",
    "        count = sum(1 for d in sample_documents if min_date <= d['date'] < max_date)\n",
    "    else:\n",
    "        count = sum(1 for d in sample_documents if d['date'] < max_date)\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Facet\n",
    "### تمرين 2: جانب مخصص"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement custom facet calculation\n",
    "\n",
    "def compute_custom_facet(documents, field_name, extractor):\n",
    "    \"\"\"Compute custom facet based on field extractor.\"\"\"\n",
    "    facet_values = {}\n",
    "    \n",
    "    for doc in documents:\n",
    "        value = extractor(doc)\n",
    "        facet_values[value] = facet_values.get(value, 0) + 1\n",
    "    \n",
    "    return facet_values\n",
    "\n",
    "# Example: Extract document extension as facet\n",
    "def get_extension(doc):\n",
    "    return doc['name'].split('.')[-1] if '.' in doc['name'] else 'unknown'\n",
    "\n",
    "print(\"Custom Facet (File Extension):\")\n",
    "extension_facet = compute_custom_facet(sample_documents, 'extension', get_extension)\n",
    "for ext, count in sorted(extension_facet.items()):\n",
    "    print(f\"  {ext}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Performance Comparison\n",
    "## الجزء 3: مقارنة الأداء"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare trie vs linear search performance\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Generate large document list\n",
    "doc_names = [f\"document_{i}.pdf\" for i in range(10000)]\n",
    "\n",
    "# Build trie\n",
    "trie = Trie()\n",
    "for name in doc_names:\n",
    "    trie.insert(name)\n",
    "\n",
    "# Test prefixes\n",
    "test_prefixes = [\"doc\", \"document\", \"file\", \"report\"]\n",
    "\n",
    "print(\"Performance Comparison (10,000 documents):\\n\")\n",
    "print(f\"{'Prefix':<15} {'Trie (ms)':<15} {'Linear (ms)':<15} {'Speedup':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for prefix in test_prefixes:\n",
    "    # Trie search\n",
    "    start = time.time()\n",
    "    trie_results = trie.autocomplete(prefix, limit=10)\n",
    "    trie_time = (time.time() - start) * 1000\n",
    "    \n",
    "    # Linear search\n",
    "    start = time.time()\n",
    "    linear_results = [d for d in doc_names if d.lower().startswith(prefix)][:10]\n",
    "    linear_time = (time.time() - start) * 1000\n",
    "    \n",
    "    speedup = linear_time / trie_time if trie_time > 0 else 1.0\n",
    "    \n",
    "    print(f\"{prefix:<15} {trie_time:<15.2f} {linear_time:<15.2f} {speedup:<10.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Relevance Scoring\n",
    "## الجزء 4: الترتيب حسب الصلة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement relevance scoring for suggestions\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class Suggestion:\n",
    "    def __init__(self, text, s_type, score=1.0, last_accessed=None):\n",
    "        self.text = text\n",
    "        self.type = s_type\n",
    "        self.score = score\n",
    "        self.last_accessed = last_accessed\n",
    "    \n",
    "    def calculate_relevance(self, query, now=None):\n",
    "        \"\"\"Calculate relevance score based on multiple factors.\"\"\"\n",
    "        now = now or datetime.now()\n",
    "        \n",
    "        # Factor 1: Prefix match quality\n",
    "        if self.text.lower().startswith(query.lower()):\n",
    "            self.score *= 1.5  # Boost for prefix match\n",
    "        \n",
    "        # Factor 2: Recency\n",
    "        if self.last_accessed:\n",
    "            days_since = (now - self.last_accessed).total_seconds() / 86400\n",
    "            if days_since < 7:\n",
    "                self.score *= 1.2  # Recent documents get boost\n",
    "            elif days_since < 30:\n",
    "                self.score *= 1.05\n",
    "        \n",
    "        return self.score\n",
    "\n",
    "# Create suggestions with different relevance factors\n",
    "now = datetime.now()\n",
    "suggestions = [\n",
    "    Suggestion(\"project_plan.pdf\", \"document\", score=1.0, last_accessed=now - timedelta(days=1)),\n",
    "    Suggestion(\"project_specs.docx\", \"document\", score=1.0, last_accessed=now - timedelta(days=30)),\n",
    "    Suggestion(\"project_timeline.pdf\", \"document\", score=1.0, last_accessed=now - timedelta(days=60)),\n",
    "    Suggestion(\"retrieval augmented generation\", \"query\", score=0.9),\n",
    "    Suggestion(\"rag architecture\", \"query\", score=0.8),\n",
    "]\n",
    "\n",
    "query = \"proj\"\n",
    "print(f\"Relevance scoring for query: '{query}'\\n\")\n",
    "\n",
    "for s in suggestions:\n",
    "    relevance = s.calculate_relevance(query)\n",
    "    print(f\"{s.text:<40} {s.type:<10} {relevance:<10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary / الملخص\n",
    "\n",
    "**Key concepts covered / المفاهيم الرئيسية المشمولة:**\n",
    "\n",
    "1. **Trie data structure** - Efficient prefix-based autocomplete\n",
    "2. **Auto-suggest types** - Document, query, and topic suggestions\n",
    "3. **Query expansion** - Synonyms and LLM-based expansions\n",
    "4. **Faceted search** - Organizing results by categories\n",
    "5. **Performance optimization** - Trie vs linear search comparison\n",
    "6. **Relevance scoring** - Multi-factor scoring for better results\n",
    "\n",
    "**النقاط الرئيسية المشمولة:**\n",
    "\n",
    "1. **هيكل البيانات Trie** - إكمال تلقائي فعال بالبادئات\n",
    "2. **أنواع الاقتراح** - اقتراحات المستندات والاستعلامات والمواضيع\n",
    "3. **توسيع الاستعلامات** - المرادفات والتوسيع المستند إلى LLM\n",
    "4. **البحث المجزوء** - تنظيم النتائج حسب الفئات\n",
    "5. **تحسين الأداء** - مقارنة Trie مع البحث الخطي\n",
    "6. **الترتيب حسب الصلة** - تسجيل متعدد العوامل لنتائج أفضل"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
