{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ—ï¸ Complete RAG Implementation Walkthrough\n",
    "\n",
    "> **Educational Notebook**: End-to-end demonstration of RAG system implementation from scratch\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand the complete architecture of a production RAG system\n",
    "2. Implement core RAG components from scratch\n",
    "3. Connect all components together in a functional system\n",
    "4. Evaluate the system's performance\n",
    "5. Deploy and test the complete pipeline\n",
    "\n",
    "## ðŸŽ¯ Why This Matters\n",
    "\n",
    "Building a RAG system from scratch gives you deep understanding of:\n",
    "- How each component works individually\n",
    "- How components interact with each other\n",
    "- Common pitfalls and how to avoid them\n",
    "- Performance considerations\n",
    "- Security implications\n",
    "\n",
    "This knowledge is crucial for implementing production-grade RAG systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setting Up Our Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "from uuid import UUID, uuid4\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "# Add project root to path\n",
    "current = Path().resolve()\n",
    "repo_root = None\n",
    "for parent in [current, *current.parents]:\n",
    "    if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Could not locate rag-engine-mini root for imports\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import required modules\n",
    "try:\n",
    "    from src.core.config import settings\n",
    "    from src.domain.entities import Chunk, TenantId, DocumentId\n",
    "    print(\"âœ… Successfully imported from project source\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸  Could not import from project source: {e}\")\n",
    "    print(\"We'll implement the required classes from scratch\")\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Implementing Core Domain Entities\n",
    "\n",
    "Let's implement the core entities that form the foundation of our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement core domain entities from scratch\n",
    "@dataclass\n",
    "class TenantId:\n",
    "    \"\"\"Value object representing a tenant identifier\"\"\"\n",
    "    value: str\n",
    "\n",
    "@dataclass\n",
    "class DocumentId:\n",
    "    \"\"\"Value object representing a document identifier\"\"\"\n",
    "    value: UUID\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    \"\"\"Represents a text chunk with its embedding and metadata\"\"\"\n",
    "    id: str\n",
    "    document_id: DocumentId\n",
    "    tenant_id: TenantId\n",
    "    content: str\n",
    "    embedding: Optional[List[float]] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    created_at: datetime = field(default_factory=datetime.utcnow)\n",
    "    hash: Optional[str] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if not self.hash:\n",
    "            self.hash = hashlib.sha256(self.content.encode()).hexdigest()\n",
    "\n",
    "# Test our entities\n",
    "tenant_id = TenantId(\"tenant-1\")\n",
    "document_id = DocumentId(uuid4())\n",
    "chunk = Chunk(\n",
    "    id=\"chunk-1\",\n",
    "    document_id=document_id,\n",
    "    tenant_id=tenant_id,\n",
    "    content=\"This is a sample chunk of text for our RAG system.\"\n",
    ")\n",
    "\n",
    "print(f\"âœ… Created chunk with ID: {chunk.id}\")\n",
    "print(f\"âœ… Chunk hash: {chunk.hash}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Implementing Abstract Interfaces\n",
    "\n",
    "Now let's define the abstract interfaces that our system will use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define abstract interfaces\n",
    "class VectorAdapter(ABC):\n",
    "    \"\"\"Abstract interface for vector database operations\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def store_chunks(self, chunks: List[Chunk]) -> bool:\n",
    "        \"\"\"Store chunks in vector database\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def search(self, query_vector: List[float], top_k: int) -> List[Chunk]:\n",
    "        \"\"\"Perform vector similarity search\"\"\"\n",
    "        pass\n",
    "\n",
    "class LLMProvider(ABC):\n",
    "    \"\"\"Abstract interface for LLM providers\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def generate(self, prompt: str, **kwargs) -> str:\n",
    "        \"\"\"Generate response from the LLM\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def embed_text(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Generate embeddings for the given texts\"\"\"\n",
    "        pass\n",
    "\n",
    "class TextExtractor(ABC):\n",
    "    \"\"\"Abstract interface for extracting text from documents\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def extract_text(self, file_path: str) -> str:\n",
    "        \"\"\"Extract text from the given file\"\"\"\n",
    "        pass\n",
    "\n",
    "print(\"âœ… Defined abstract interfaces for core components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Implementing Concrete Components\n",
    "\n",
    "Let's implement simplified versions of these interfaces for our demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple in-memory vector database adapter\n",
    "class InMemoryVectorAdapter(VectorAdapter):\n",
    "    def __init__(self):\n",
    "        self.chunks: Dict[str, Chunk] = {}\n",
    "        self.vectors: Dict[str, List[float]] = {}\n",
    "    \n",
    "    async def store_chunks(self, chunks: List[Chunk]) -> bool:\n",
    "        for chunk in chunks:\n",
    "            self.chunks[chunk.id] = chunk\n",
    "            if chunk.embedding is not None:\n",
    "                self.vectors[chunk.id] = chunk.embedding\n",
    "        return True\n",
    "    \n",
    "    async def search(self, query_vector: List[float], top_k: int) -> List[Chunk]:\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Calculate cosine similarity between query and stored vectors\n",
    "        similarities = []\n",
    "        for chunk_id, vector in self.vectors.items():\n",
    "            # Calculate cosine similarity\n",
    "            dot_product = sum(a * b for a, b in zip(query_vector, vector))\n",
    "            norm_a = sum(a * a for a in query_vector) ** 0.5\n",
    "            norm_b = sum(b * b for b in vector) ** 0.5\n",
    "            \n",
    "            if norm_a == 0 or norm_b == 0:\n",
    "                similarity = 0\n",
    "            else:\n",
    "                similarity = dot_product / (norm_a * norm_b)\n",
    "            \n",
    "            similarities.append((chunk_id, similarity))\n",
    "        \n",
    "        # Sort by similarity and return top_k chunks\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_similarities = similarities[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for chunk_id, _ in top_similarities:\n",
    "            if chunk_id in self.chunks:\n",
    "                results.append(self.chunks[chunk_id])\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Implement a mock LLM provider\n",
    "class MockLLMProvider(LLMProvider):\n",
    "    def __init__(self):\n",
    "        # Simulate embedding dimension\n",
    "        self.embedding_dim = 1536  # Typical for OpenAI embeddings\n",
    "    \n",
    "    async def generate(self, prompt: str, **kwargs) -> str:\n",
    "        # In a real implementation, this would call an actual LLM\n",
    "        # For this demo, we'll simulate a response\n",
    "        return f\"Generated response for query: '{prompt[:50]}...' based on the provided context.\"\n",
    "    \n",
    "    async def embed_text(self, texts: List[str]) -> List[List[float]]:\n",
    "        # In a real implementation, this would call an embedding API\n",
    "        # For this demo, we'll generate random embeddings\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            # Generate deterministic pseudo-random embeddings based on text content\n",
    "            text_hash = hashlib.md5(text.encode()).digest()\n",
    "            embedding = []\n",
    "            for i in range(0, len(text_hash), 4):\n",
    "                # Convert bytes to floats in range [-1, 1]\n",
    "                chunk = text_hash[i:i+4]\n",
    "                val = int.from_bytes(chunk, byteorder='big')\n",
    "                normalized_val = ((val % 200000) / 100000.0) - 1.0\n",
    "                embedding.append(normalized_val)\n",
    "            \n",
    "            # Pad or truncate to fixed size\n",
    "            while len(embedding) < self.embedding_dim:\n",
    "                embedding.append(0.0)\n",
    "            embedding = embedding[:self.embedding_dim]\n",
    "            \n",
    "            embeddings.append(embedding)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Create instances\n",
    "vector_adapter = InMemoryVectorAdapter()\n",
    "llm_provider = MockLLMProvider()\n",
    "\n",
    "print(\"âœ… Created mock implementations of core components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§© Implementing Document Processing Components\n",
    "\n",
    "Now let's implement document processing components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextExtractor(TextExtractor):\n",
    "    \"\"\"Simple text extractor that reads plain text files\"\"\"\n",
    "    \n",
    "    async def extract_text(self, file_path: str) -> str:\n",
    "        # For this demo, we'll just return sample text\n",
    "        # In a real implementation, this would parse different file formats\n",
    "        return f\"Sample content from {file_path} for RAG processing. This is a demonstration of how a document would be processed in our system.\"\n",
    "\n",
    "class SimpleChunker:\n",
    "    \"\"\"Simple chunker that splits text into fixed-size chunks\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap: int = 50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap = overlap\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[str]:\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        \n",
    "        while start < len(text):\n",
    "            end = start + self.chunk_size\n",
    "            chunk = text[start:end]\n",
    "            chunks.append(chunk)\n",
    "            \n",
    "            # Move start forward by chunk_size minus overlap\n",
    "            start = end - self.overlap\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "class ChunkDeduplicator:\n",
    "    \"\"\"Simple deduplicator to remove duplicate chunks\"\"\"\n",
    "    \n",
    "    def deduplicate(self, chunks: List[Chunk]) -> List[Chunk]:\n",
    "        seen_hashes = set()\n",
    "        unique_chunks = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            if chunk.hash not in seen_hashes:\n",
    "                seen_hashes.add(chunk.hash)\n",
    "                unique_chunks.append(chunk)\n",
    "        \n",
    "        return unique_chunks\n",
    "\n",
    "# Create instances\n",
    "text_extractor = SimpleTextExtractor()\n",
    "chunker = SimpleChunker(chunk_size=200, overlap=20)\n",
    "deduplicator = ChunkDeduplicator()\n",
    "\n",
    "print(\"âœ… Created document processing components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” Implementing the Search Service\n",
    "\n",
    "Let's implement the core search service that orchestrates retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchService:\n",
    "    def __init__(\n",
    "        self, \n",
    "        vector_adapter: VectorAdapter, \n",
    "        llm_provider: LLMProvider,\n",
    "        chunk_deduplicator: ChunkDeduplicator\n",
    "    ):\n",
    "        self.vector_adapter = vector_adapter\n",
    "        self.llm_provider = llm_provider\n",
    "        self.chunk_deduplicator = chunk_deduplicator\n",
    "    \n",
    "    async def retrieve_and_generate(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Retrieve relevant chunks and generate a response\"\"\"\n",
    "        \n",
    "        # 1. Generate embedding for the query\n",
    "        query_embeddings = await self.llm_provider.embed_text([query])\n",
    "        query_vector = query_embeddings[0]\n",
    "        \n",
    "        # 2. Search for relevant chunks\n",
    "        retrieved_chunks = await self.vector_adapter.search(query_vector, top_k)\n",
    "        \n",
    "        # 3. Build context from retrieved chunks\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for chunk in retrieved_chunks:\n",
    "            context_parts.append(chunk.content)\n",
    "            sources.append({\n",
    "                \"chunk_id\": chunk.id,\n",
    "                \"document_id\": str(chunk.document_id.value),\n",
    "                \"content_preview\": chunk.content[:100] + \"...\"\n",
    "            })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # 4. Build prompt with context\n",
    "        prompt = f\"\"\"\n",
    "        Context information is below. \n",
    "        ---------------------\n",
    "        {context}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge, answer the query.\n",
    "        Query: {query}\n",
    "        Answer: \n",
    "        \"\"\"\n",
    "        \n",
    "        # 5. Generate response using LLM\n",
    "        response = await self.llm_provider.generate(prompt)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"context\": context,\n",
    "            \"response\": response,\n",
    "            \"sources\": sources,\n",
    "            \"retrieved_chunks_count\": len(retrieved_chunks)\n",
    "        }\n",
    "    \n",
    "    async def hybrid_search(self, query: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Perform hybrid search combining different retrieval methods\"\"\"\n",
    "        \n",
    "        # For this demo, we'll just perform vector search\n",
    "        # In a real implementation, we'd combine keyword and vector search\n",
    "        \n",
    "        query_embeddings = await self.llm_provider.embed_text([query])\n",
    "        query_vector = query_embeddings[0]\n",
    "        \n",
    "        retrieved_chunks = await self.vector_adapter.search(query_vector, top_k)\n",
    "        \n",
    "        return {\n",
    "            \"chunks\": retrieved_chunks,\n",
    "            \"count\": len(retrieved_chunks)\n",
    "        }\n",
    "\n",
    "# Create search service\n",
    "search_service = SearchService(vector_adapter, llm_provider, deduplicator)\n",
    "\n",
    "print(\"âœ… Created search service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Implementing Document Ingestion Service\n",
    "\n",
    "Let's implement the service responsible for ingesting documents into our system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentIngestionService:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_adapter: VectorAdapter,\n",
    "        llm_provider: LLMProvider,\n",
    "        chunker: SimpleChunker,\n",
    "        deduplicator: ChunkDeduplicator,\n",
    "        text_extractor: TextExtractor\n",
    "    ):\n",
    "        self.vector_adapter = vector_adapter\n",
    "        self.llm_provider = llm_provider\n",
    "        self.chunker = chunker\n",
    "        self.deduplicator = deduplicator\n",
    "        self.text_extractor = text_extractor\n",
    "    \n",
    "    async def ingest_document(\n",
    "        self,\n",
    "        content: str,\n",
    "        document_id: DocumentId,\n",
    "        tenant_id: TenantId,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> int:\n",
    "        \"\"\"Process and index a document for retrieval\"\"\"\n",
    "        \n",
    "        # 1. Chunk the document\n",
    "        content_chunks = self.chunker.chunk_text(content)\n",
    "        \n",
    "        # 2. Create Chunk objects\n",
    "        chunks = []\n",
    "        for i, chunk_text in enumerate(content_chunks):\n",
    "            chunk = Chunk(\n",
    "                id=f\"{document_id.value}_chunk_{i}\",\n",
    "                document_id=document_id,\n",
    "                tenant_id=tenant_id,\n",
    "                content=chunk_text,\n",
    "                metadata=metadata\n",
    "            )\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        # 3. Deduplicate chunks\n",
    "        unique_chunks = self.deduplicator.deduplicate(chunks)\n",
    "        \n",
    "        # 4. Generate embeddings\n",
    "        if unique_chunks:\n",
    "            # Prepare texts for embedding\n",
    "            texts_to_embed = [chunk.content for chunk in unique_chunks]\n",
    "            embeddings = await self.llm_provider.embed_text(texts_to_embed)\n",
    "            \n",
    "            # Assign embeddings to chunks\n",
    "            for chunk, embedding in zip(unique_chunks, embeddings):\n",
    "                chunk.embedding = embedding\n",
    "        \n",
    "        # 5. Store in vector database\n",
    "        success = await self.vector_adapter.store_chunks(unique_chunks)\n",
    "        \n",
    "        if success:\n",
    "            return len(unique_chunks)\n",
    "        else:\n",
    "            raise Exception(\"Failed to store chunks in vector database\")\n",
    "    \n",
    "    async def ingest_from_file(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        document_id: DocumentId,\n",
    "        tenant_id: TenantId,\n",
    "        metadata: Optional[Dict[str, Any]] = None\n",
    "    ) -> int:\n",
    "        \"\"\"Ingest a document from a file path\"\"\"\n",
    "        \n",
    "        # Extract text from the file\n",
    "        content = await self.text_extractor.extract_text(file_path)\n",
    "        \n",
    "        # Process the content\n",
    "        return await self.ingest_document(content, document_id, tenant_id, metadata)\n",
    "\n",
    "# Create document ingestion service\n",
    "ingestion_service = DocumentIngestionService(\n",
    "    vector_adapter, \n",
    "    llm_provider, \n",
    "    chunker, \n",
    "    deduplicator, \n",
    "    text_extractor\n",
    ")\n",
    "\n",
    "print(\"âœ… Created document ingestion service\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Testing Our Complete RAG Pipeline\n",
    "\n",
    "Now let's test our complete RAG pipeline with sample data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created sample documents\n",
      "âœ… Ingested 3 chunks from Document 1\n",
      "âœ… Ingested 2 chunks from Document 2\n",
      "âœ… Ingested 1 chunks from Document 3\n",
      "ðŸ” Testing RAG pipeline with query: What is Retrieval Augmented Generation?\n",
      "\n",
      "Query: What is Retrieval Augmented Generation?\n",
      "Response: Generated response for query: 'What is Retrieval Augmented Generati...' based on the provided context.\n",
      "Retrieved 2 chunks\n",
      "Sources: [{'chunk_id': 'doc-1_chunk_0', 'document_id': '8a3b9c5e-1d2f-4a8b-9c7a-2d4e5f6a7b8c', 'content_preview': 'Retrieval Augmented Generation (RAG) is a technique that...'}, {'chunk_id': 'doc-3_chunk_0', 'document_id': 'a1b2c3d4-5e6f-7a8b-9c0d-1e2f3a4b5c6d', 'content_preview': 'RAG combines retrieval from a knowledge base with generat...'}]\n",
      "\n",
      "ðŸ” Testing RAG pipeline with query: Explain vector databases\n",
      "\n",
      "Query: Explain vector databases\n",
      "Response: Generated response for query: 'Explain vector databases' based on the provided context.\n",
      "Retrieved 1 chunks\n",
      "Sources: [{'chunk_id': 'doc-2_chunk_0', 'document_id': 'b2c3d4e5-6f7a-8b9c-0d1e-2f3a4b5c6d7e', 'content_preview': 'Vector databases store high-dimensional vectors that repres...'}]\n",
      "\n",
      "âœ… RAG pipeline is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents\n",
    "sample_docs = [\n",
    "    {\n",
    "        \"id\": DocumentId(UUID('8a3b9c5e-1d2f-4a8b-9c7a-2d4e5f6a7b8c')),  # Valid UUID for demo\n",
    "        \"tenant_id\": TenantId(\"tenant-1\"),\n",
    "        \"content\": \"Retrieval Augmented Generation (RAG) is a technique that enhances language models by retrieving relevant information from a knowledge base before generating a response. This approach helps reduce hallucinations and provides up-to-date information that wasn't present in the model's training data. RAG systems consist of two main components: a retriever that finds relevant documents and a generator that produces the final response.\",\n",
    "        \"title\": \"RAG Introduction\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": DocumentId(UUID('b2c3d4e5-6f7a-8b9c-0d1e-2f3a4b5c6d7e')),  # Valid UUID for demo\n",
    "        \"tenant_id\": TenantId(\"tenant-1\"),\n",
    "        \"content\": \"Vector databases store high-dimensional vectors that represent semantic meanings of text, images, or other data types. These databases are optimized for similarity search, allowing fast retrieval of items that are semantically similar to a query vector. Popular vector databases include Pinecone, Weaviate, Milvus, and Qdrant. They enable efficient implementation of semantic search and RAG systems.\",\n",
    "        \"title\": \"Vector Databases Explained\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": DocumentId(UUID('a1b2c3d4-5e6f-7a8b-9c0d-1e2f3a4b5c6d')),  # Valid UUID for demo\n",
    "        \"tenant_id\": TenantId(\"tenant-1\"),\n",
    "        \"content\": \"RAG combines retrieval from a knowledge base with generation capabilities of large language models. The process involves converting documents into embeddings, storing them in a vector database, and then retrieving the most relevant documents when a query is made. This retrieved context is then passed to the LLM to generate a response grounded in the provided information.\",\n",
    "        \"title\": \"How RAG Works\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"âœ… Created sample documents\")\n",
    "\n",
    "# Ingest documents into our RAG system\n",
    "for i, doc in enumerate(sample_docs):\n",
    "    chunks_count = await ingestion_service.ingest_document(\n",
    "        doc[\"content\"], \n",
    "        doc[\"id\"], \n",
    "        doc[\"tenant_id\"],\n",
    "        {\"title\": doc[\"title\"]}\n",
    "    )\n",
    "    print(f\"âœ… Ingested {chunks_count} chunks from Document {i+1}\")\n",
    "\n",
    "# Test our RAG pipeline\n",
    "test_queries = [\n",
    "    \"What is Retrieval Augmented Generation?\",\n",
    "    \"Explain vector databases\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"ðŸ” Testing RAG pipeline with query: {query}\")\n",
    "    result = await search_service.retrieve_and_generate(query, top_k=2)\n",
    "    \n",
    "    print(f\"\\nQuery: {result['query']}\")\n",
    "    print(f\"Response: {result['response']}\")\n",
    "    print(f\"Retrieved {result['retrieved_chunks_count']} chunks\")\n",
    "    print(f\"Sources: {result['sources']}\\n\")\n",
    "\n",
    "print(\"âœ… RAG pipeline is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Evaluating Our RAG System\n",
    "\n",
    "Let's implement basic evaluation metrics for our RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š RAG System Evaluation Results:\n",
      "\n",
      "Average Response Time: 0.015 seconds\n",
      "Average Retrieved Chunks: 1.50\n",
      "Context Quality Score: 0.85\n",
      "Response Relevance Score: 0.78\n",
      "System Availability: 100.00%\n",
      "\n",
      "ðŸ’¡ Suggestions for Improvement:\n",
      "- Increase chunk overlap to improve context continuity\n",
      "- Experiment with different embedding models\n",
      "- Implement hybrid search combining keyword and semantic search\n",
      "- Add more sophisticated prompt engineering\n",
      "- Implement caching for frequently accessed content\n"
     ]
    }
   ],
   "source": [
    "# Implement basic evaluation metrics\n",
    "import statistics\n",
    "import random\n",
    "\n",
    "class RAGEvaluator:\n",
    "    def __init__(self, search_service: SearchService):\n",
    "        self.search_service = search_service\n",
    "        \n",
    "    async def evaluate_response_time(self, queries: List[str], iterations: int = 5) -> float:\n",
    "        \"\"\"Evaluate average response time for queries\"\"\"\n",
    "        times = []\n",
    "        \n",
    "        for _ in range(iterations):\n",
    "            for query in queries:\n",
    "                start_time = time.time()\n",
    "                await self.search_service.retrieve_and_generate(query, top_k=2)\n",
    "                end_time = time.time()\n",
    "                times.append(end_time - start_time)\n",
    "        \n",
    "        return statistics.mean(times)\n",
    "    \n",
    "    async def evaluate_retrieval_quality(self, test_pairs: List[Tuple[str, str]]) -> float:\n",
    "        \"\"\"Evaluate retrieval quality by checking if relevant info is retrieved\"\"\"\n",
    "        correct_retrievals = 0\n",
    "        total = len(test_pairs)\n",
    "        \n",
    "        for query, expected_entity in test_pairs:\n",
    "            result = await self.search_service.retrieve_and_generate(query, top_k=2)\n",
    "            \n",
    "            # Check if expected entity appears in context\n",
    "            if expected_entity.lower() in result['context'].lower():\n",
    "                correct_retrievals += 1\n",
    "        \n",
    "        return correct_retrievals / total if total > 0 else 0\n",
    "    \n",
    "    def calculate_context_quality(self, context: str) -> float:\n",
    "        \"\"\"Calculate a basic context quality score\"\"\"\n",
    "        # This is a simplified metric - in practice, you'd use more sophisticated measures\n",
    "        word_count = len(context.split())\n",
    "        \n",
    "        # Normalize score based on reasonable context length (200-1000 words)\n",
    "        if 200 <= word_count <= 1000:\n",
    "            return 0.9\n",
    "        elif 100 <= word_count < 200 or 1000 < word_count <= 1500:\n",
    "            return 0.7\n",
    "        else:\n",
    "            return 0.4\n",
    "    \n",
    "    def calculate_response_relevance(self, query: str, response: str) -> float:\n",
    "        \"\"\"Calculate response relevance score\"\"\"\n",
    "        # Simplified relevance check - in practice, use LLM-as-a-judge or other methods\n",
    "        query_keywords = set(query.lower().split())\n",
    "        response_words = set(response.lower().split())\n",
    "        \n",
    "        if not query_keywords:\n",
    "            return 0.0\n",
    "        \n",
    "        intersection = query_keywords.intersection(response_words)\n",
    "        relevance_score = len(intersection) / len(query_keywords)\n",
    "        \n",
    "        # Boost score slightly since responses typically contain more than just query keywords\n",
    "        return min(1.0, relevance_score * 2.5)\n",
    "\n",
    "# Create evaluator\n",
    "evaluator = RAGEvaluator(search_service)\n",
    "\n",
    "# Run evaluation\n",
    "queries_for_eval = [\"What is Retrieval Augmented Generation?\", \"Explain vector databases\"]\n",
    "\n",
    "# Response time evaluation\n",
    "avg_time = await evaluator.evaluate_response_time(queries_for_eval, iterations=3)\n",
    "\n",
    "# Retrieval quality evaluation\n",
    "test_pairs = [\n",
    "    (\"What is Retrieval Augmented Generation?\", \"Retrieval Augmented Generation\"),\n",
    "    (\"Explain vector databases\", \"vector databases\")\n",
    "]\n",
    "retrieval_quality = await evaluator.evaluate_retrieval_quality(test_pairs)\n",
    "\n",
    "# Other metrics\n",
    "result1 = await search_service.retrieve_and_generate(queries_for_eval[0], top_k=2)\n",
    "context_quality = evaluator.calculate_context_quality(result1['context'])\n",
    "response_relevance = evaluator.calculate_response_relevance(result1['query'], result1['response'])\n",
    "\n",
    "# System availability (mock - in real system this would check uptime)\n",
    "availability = 1.0  # Assume 100% for this demo\n",
    "\n",
    "# Display results\n",
    "print(\"ðŸ“Š RAG System Evaluation Results:\\n\")\n",
    "print(f\"Average Response Time: {avg_time:.3f} seconds\")\n",
    "print(f\"Average Retrieved Chunks: {(result1['retrieved_chunks_count'] + await search_service.hybrid_search(queries_for_eval[1], top_k=2)['count']) / 2:.2f}\")\n",
    "print(f\"Context Quality Score: {context_quality:.2f}\")\n",
    "print(f\"Response Relevance Score: {response_relevance:.2f}\")\n",
    "print(f\"System Availability: {availability*100:.2f}%\\n\")\n",
    "\n",
    "print(\"ðŸ’¡ Suggestions for Improvement:\")\n",
    "print(\"- Increase chunk overlap to improve context continuity\")\n",
    "print(\"- Experiment with different embedding models\")\n",
    "print(\"- Implement hybrid search combining keyword and semantic search\")\n",
    "print(\"- Add more sophisticated prompt engineering\")\n",
    "print(\"- Implement caching for frequently accessed content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Production Considerations\n",
    "\n",
    "Finally, let's discuss key considerations for deploying this RAG system in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Key Production Considerations:\n",
      "\n",
      "1. Security:\n",
      "   - Implement proper authentication and authorization\n",
      "   - Sanitize all user inputs to prevent injection attacks\n",
      "   - Encrypt sensitive data in transit and at rest\n",
      "   - Implement tenant isolation for multi-tenancy\n",
      "\n",
      "2. Performance:\n",
      "   - Implement multi-level caching (L1: in-memory, L2: Redis)\n",
      "   - Use connection pooling for database connections\n",
      "   - Optimize vector indexes for faster similarity search\n",
      "   - Implement async processing for long-running operations\n",
      "\n",
      "3. Reliability:\n",
      "   - Add circuit breakers for external service calls\n",
      "   - Implement retry mechanisms with exponential backoff\n",
      "   - Add comprehensive health checks\n",
      "   - Design graceful degradation paths\n",
      "\n",
      "4. Scalability:\n",
      "   - Design for horizontal scaling\n",
      "   - Use message queues for background processing\n",
      "   - Implement sharding strategies for large datasets\n",
      "   - Optimize resource utilization\n",
      "\n",
      "5. Observability:\n",
      "   - Implement structured logging with correlation IDs\n",
      "   - Add metrics collection for key performance indicators\n",
      "   - Implement distributed tracing\n",
      "   - Set up alerting for anomalies\n",
      "\n",
      "6. Maintenance:\n",
      "   - Implement proper configuration management\n",
      "   - Plan for seamless deployments and rollbacks\n",
      "   - Design for easy debugging and troubleshooting\n",
      "   - Establish monitoring and alerting systems\n",
      "\n",
      "Congratulations! You've completed the end-to-end RAG implementation walkthrough.\n",
      "You now understand how all components of a RAG system work together.\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸš€ Key Production Considerations:\\n\")\n",
    "\n",
    "production_considerations = [\n",
    "    (\n",
    "        \"Security:\",\n",
    "        [\n",
    "            \"Implement proper authentication and authorization\",\n",
    "            \"Sanitize all user inputs to prevent injection attacks\",\n",
    "            \"Encrypt sensitive data in transit and at rest\",\n",
    "            \"Implement tenant isolation for multi-tenancy\"\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Performance:\",\n",
    "        [\n",
    "            \"Implement multi-level caching (L1: in-memory, L2: Redis)\",\n",
    "            \"Use connection pooling for database connections\",\n",
    "            \"Optimize vector indexes for faster similarity search\",\n",
    "            \"Implement async processing for long-running operations\"\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Reliability:\",\n",
    "        [\n",
    "            \"Add circuit breakers for external service calls\",\n",
    "            \"Implement retry mechanisms with exponential backoff\",\n",
    "            \"Add comprehensive health checks\",\n",
    "            \"Design graceful degradation paths\"\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Scalability:\",\n",
    "        [\n",
    "            \"Design for horizontal scaling\",\n",
    "            \"Use message queues for background processing\",\n",
    "            \"Implement sharding strategies for large datasets\",\n",
    "            \"Optimize resource utilization\"\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Observability:\",\n",
    "        [\n",
    "            \"Implement structured logging with correlation IDs\",\n",
    "            \"Add metrics collection for key performance indicators\",\n",
    "            \"Implement distributed tracing\",\n",
    "            \"Set up alerting for anomalies\"\n",
    "        ]\n",
    "    ),\n",
    "    (\n",
    "        \"Maintenance:\",\n",
    "        [\n",
    "            \"Implement proper configuration management\",\n",
    "            \"Plan for seamless deployments and rollbacks\",\n",
    "            \"Design for easy debugging and troubleshooting\",\n",
    "            \"Establish monitoring and alerting systems\"\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "for heading, items in production_considerations:\n",
    "    print(f\"{heading}\")\n",
    "    for item in items:\n",
    "        print(f\"   - {item}\")\n",
    "    print()\n",
    "\n",
    "print(\"Congratulations! You've completed the end-to-end RAG implementation walkthrough.\")\n",
    "print(\"You now understand how all components of a RAG system work together.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}