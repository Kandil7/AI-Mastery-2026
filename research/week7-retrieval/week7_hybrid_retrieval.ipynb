{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Week 7: Hybrid Retrieval\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Understand lexical vs semantic search trade-offs\n",
                "2. Implement BM25 for lexical search\n",
                "3. Build hybrid retrieval combining both approaches\n",
                "4. Evaluate retrieval quality with precision/recall\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from collections import Counter\n",
                "import math\n",
                "import re"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## Lexical vs Semantic Search\n",
                "\n",
                "| Aspect | Lexical (BM25) | Semantic (Embeddings) |\n",
                "|--------|---------------|----------------------|\n",
                "| Matches | Exact keywords | Meaning/context |\n",
                "| \"bank account\" | ‚úÖ Finds exact | ‚úÖ Finds \"savings\" |\n",
                "| Synonyms | ‚ùå Misses | ‚úÖ Captures |\n",
                "| Rare terms | ‚úÖ Excellent | ‚ö†Ô∏è May miss |\n",
                "\n",
                "## Why Hybrid?\n",
                "Combines strengths of both: keyword precision + semantic understanding"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BM25:\n",
                "    \"\"\"BM25 lexical search implementation.\"\"\"\n",
                "    \n",
                "    def __init__(self, k1=1.5, b=0.75):\n",
                "        self.k1 = k1\n",
                "        self.b = b\n",
                "        self.docs = []\n",
                "        self.doc_freqs = []\n",
                "        self.idf = {}\n",
                "        self.avg_dl = 0\n",
                "    \n",
                "    def _tokenize(self, text):\n",
                "        return re.findall(r'\\w+', text.lower())\n",
                "    \n",
                "    def fit(self, documents):\n",
                "        self.docs = documents\n",
                "        self.doc_freqs = [Counter(self._tokenize(d)) for d in documents]\n",
                "        self.avg_dl = sum(len(df) for df in self.doc_freqs) / len(documents)\n",
                "        \n",
                "        # Calculate IDF\n",
                "        df = Counter()\n",
                "        for doc_freq in self.doc_freqs:\n",
                "            for term in doc_freq:\n",
                "                df[term] += 1\n",
                "        \n",
                "        N = len(documents)\n",
                "        for term, freq in df.items():\n",
                "            self.idf[term] = math.log((N - freq + 0.5) / (freq + 0.5) + 1)\n",
                "    \n",
                "    def search(self, query, top_k=5):\n",
                "        query_terms = self._tokenize(query)\n",
                "        scores = []\n",
                "        \n",
                "        for i, doc_freq in enumerate(self.doc_freqs):\n",
                "            score = 0\n",
                "            dl = sum(doc_freq.values())\n",
                "            \n",
                "            for term in query_terms:\n",
                "                if term in doc_freq:\n",
                "                    tf = doc_freq[term]\n",
                "                    idf = self.idf.get(term, 0)\n",
                "                    numerator = tf * (self.k1 + 1)\n",
                "                    denominator = tf + self.k1 * (1 - self.b + self.b * dl / self.avg_dl)\n",
                "                    score += idf * numerator / denominator\n",
                "            \n",
                "            scores.append((i, score))\n",
                "        \n",
                "        scores.sort(key=lambda x: x[1], reverse=True)\n",
                "        return scores[:top_k]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class HybridRetriever:\n",
                "    \"\"\"Combines BM25 and semantic search.\"\"\"\n",
                "    \n",
                "    def __init__(self, alpha=0.5):\n",
                "        self.alpha = alpha  # Weight for semantic\n",
                "        self.bm25 = BM25()\n",
                "        self.embeddings = []\n",
                "        self.docs = []\n",
                "    \n",
                "    def _embed(self, text):\n",
                "        # Simulated embedding\n",
                "        np.random.seed(hash(text) % 2**32)\n",
                "        return np.random.randn(384)\n",
                "    \n",
                "    def fit(self, documents):\n",
                "        self.docs = documents\n",
                "        self.bm25.fit(documents)\n",
                "        self.embeddings = [self._embed(d) for d in documents]\n",
                "    \n",
                "    def _normalize_scores(self, scores):\n",
                "        if not scores:\n",
                "            return scores\n",
                "        max_s = max(s for _, s in scores)\n",
                "        min_s = min(s for _, s in scores)\n",
                "        if max_s == min_s:\n",
                "            return [(i, 1.0) for i, _ in scores]\n",
                "        return [(i, (s - min_s) / (max_s - min_s)) for i, s in scores]\n",
                "    \n",
                "    def search(self, query, top_k=5):\n",
                "        # Lexical search\n",
                "        bm25_results = self.bm25.search(query, top_k=len(self.docs))\n",
                "        bm25_scores = dict(self._normalize_scores(bm25_results))\n",
                "        \n",
                "        # Semantic search\n",
                "        query_emb = self._embed(query)\n",
                "        semantic_scores = []\n",
                "        for i, emb in enumerate(self.embeddings):\n",
                "            sim = np.dot(query_emb, emb) / (np.linalg.norm(query_emb) * np.linalg.norm(emb))\n",
                "            semantic_scores.append((i, sim))\n",
                "        semantic_scores = dict(self._normalize_scores(semantic_scores))\n",
                "        \n",
                "        # Combine scores\n",
                "        combined = []\n",
                "        for i in range(len(self.docs)):\n",
                "            bm25_s = bm25_scores.get(i, 0)\n",
                "            sem_s = semantic_scores.get(i, 0)\n",
                "            combined_score = self.alpha * sem_s + (1 - self.alpha) * bm25_s\n",
                "            combined.append((i, combined_score, self.docs[i]))\n",
                "        \n",
                "        combined.sort(key=lambda x: x[1], reverse=True)\n",
                "        return combined[:top_k]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test hybrid retrieval\n",
                "documents = [\n",
                "    \"Machine learning is a subset of artificial intelligence\",\n",
                "    \"Deep neural networks power modern AI systems\",\n",
                "    \"Python is popular for data science and ML\",\n",
                "    \"Natural language processing enables text understanding\",\n",
                "    \"Vector databases store embeddings efficiently\"\n",
                "]\n",
                "\n",
                "retriever = HybridRetriever(alpha=0.5)\n",
                "retriever.fit(documents)\n",
                "\n",
                "query = \"AI and machine learning\"\n",
                "results = retriever.search(query, top_k=3)\n",
                "\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Hybrid Search Results:\")\n",
                "for idx, score, doc in results:\n",
                "    print(f\"  [{score:.4f}] {doc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Evaluation Metrics\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def precision_at_k(retrieved, relevant, k):\n",
                "    \"\"\"Precision@K: fraction of retrieved docs that are relevant.\"\"\"\n",
                "    retrieved_k = set(retrieved[:k])\n",
                "    relevant_set = set(relevant)\n",
                "    return len(retrieved_k & relevant_set) / k\n",
                "\n",
                "def recall_at_k(retrieved, relevant, k):\n",
                "    \"\"\"Recall@K: fraction of relevant docs that are retrieved.\"\"\"\n",
                "    retrieved_k = set(retrieved[:k])\n",
                "    relevant_set = set(relevant)\n",
                "    if not relevant_set:\n",
                "        return 0\n",
                "    return len(retrieved_k & relevant_set) / len(relevant_set)\n",
                "\n",
                "def mrr(retrieved, relevant):\n",
                "    \"\"\"Mean Reciprocal Rank.\"\"\"\n",
                "    for i, doc in enumerate(retrieved):\n",
                "        if doc in relevant:\n",
                "            return 1 / (i + 1)\n",
                "    return 0\n",
                "\n",
                "# Example\n",
                "retrieved = [0, 2, 1, 4, 3]\n",
                "relevant = [0, 1]\n",
                "\n",
                "print(f\"Precision@3: {precision_at_k(retrieved, relevant, 3):.2f}\")\n",
                "print(f\"Recall@3: {recall_at_k(retrieved, relevant, 3):.2f}\")\n",
                "print(f\"MRR: {mrr(retrieved, relevant):.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test BM25\n",
                "    bm25 = BM25()\n",
                "    bm25.fit([\"hello world\", \"world peace\"])\n",
                "    results = bm25.search(\"hello\", top_k=1)\n",
                "    assert results[0][0] == 0\n",
                "    print(\"‚úì BM25 search test passed\")\n",
                "    \n",
                "    # Test Precision\n",
                "    assert precision_at_k([0, 1, 2], [0, 1], 2) == 1.0\n",
                "    print(\"‚úì Precision@K test passed\")\n",
                "    \n",
                "    # Test MRR\n",
                "    assert mrr([2, 1, 0], [0]) == 1/3\n",
                "    print(\"‚úì MRR test passed\")\n",
                "    \n",
                "    print(\"\\nüéâ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: Interview Prep\n",
                "---\n",
                "\n",
                "### Q1: When would you use hybrid search?\n",
                "**Answer:** When you need both keyword precision (product names, codes) and semantic understanding (concepts, synonyms).\n",
                "\n",
                "### Q2: How do you tune the alpha parameter?\n",
                "**Answer:** Use a validation set with labeled relevance. Grid search over alpha values. Optimize for your target metric (NDCG, MRR).\n",
                "\n",
                "---\n",
                "# Section 6: Deliverable\n",
                "---\n",
                "\n",
                "**Created:** `hybrid_retriever.py` with BM25 + semantic search\n",
                "\n",
                "**Next Week:** Re-ranking"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}