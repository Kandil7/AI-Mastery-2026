{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Week 2: Probability & Statistics for ML\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Master probability fundamentals (distributions, expectation, variance)\n",
                "2. Understand Bayes' theorem and its applications\n",
                "3. Apply statistical concepts to ML problems\n",
                "4. Visualize and interpret probability distributions\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "from collections import Counter\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## Why Probability in ML?\n",
                "\n",
                "Machine Learning is fundamentally about **uncertainty**:\n",
                "- Models make **probabilistic predictions** (not certainties)\n",
                "- Training involves **maximum likelihood estimation**\n",
                "- Bayesian methods use **prior beliefs + data**\n",
                "\n",
                "## Key Concepts\n",
                "\n",
                "| Concept | Formula | Meaning |\n",
                "|---------|---------|--------|\n",
                "| Probability | $P(A) \\in [0, 1]$ | Likelihood of event A |\n",
                "| Expectation | $E[X] = \\sum x \\cdot P(x)$ | Average value |\n",
                "| Variance | $Var(X) = E[(X - \\mu)^2]$ | Spread around mean |\n",
                "| Bayes | $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$ | Update beliefs with evidence |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Basic Probability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def probability(events, target):\n",
                "    \"\"\"Calculate probability of target in events.\"\"\"\n",
                "    return sum(1 for e in events if e == target) / len(events)\n",
                "\n",
                "\n",
                "def joint_probability(events_a, events_b, target_a, target_b):\n",
                "    \"\"\"Calculate P(A and B).\"\"\"\n",
                "    count = sum(1 for a, b in zip(events_a, events_b) \n",
                "                if a == target_a and b == target_b)\n",
                "    return count / len(events_a)\n",
                "\n",
                "\n",
                "def conditional_probability(events_a, events_b, target_a, given_b):\n",
                "    \"\"\"Calculate P(A | B).\"\"\"\n",
                "    filtered = [(a, b) for a, b in zip(events_a, events_b) if b == given_b]\n",
                "    if len(filtered) == 0:\n",
                "        return 0\n",
                "    return sum(1 for a, b in filtered if a == target_a) / len(filtered)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Coin flips\n",
                "coin_flips = [0, 1, 1, 0, 1, 1, 0, 1, 0, 1]  # 0=Tails, 1=Heads\n",
                "\n",
                "p_heads = probability(coin_flips, 1)\n",
                "p_tails = probability(coin_flips, 0)\n",
                "\n",
                "print(f\"P(Heads) = {p_heads:.2f}\")\n",
                "print(f\"P(Tails) = {p_tails:.2f}\")\n",
                "print(f\"Sum = {p_heads + p_tails:.2f} (should be 1.00)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Expectation & Variance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def expectation(values):\n",
                "    \"\"\"Calculate expected value (mean).\"\"\"\n",
                "    return sum(values) / len(values)\n",
                "\n",
                "\n",
                "def variance(values):\n",
                "    \"\"\"Calculate population variance.\"\"\"\n",
                "    mu = expectation(values)\n",
                "    return sum((x - mu)**2 for x in values) / len(values)\n",
                "\n",
                "\n",
                "def std_dev(values):\n",
                "    \"\"\"Calculate standard deviation.\"\"\"\n",
                "    return variance(values) ** 0.5"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Dice rolls\n",
                "dice_rolls = [np.random.randint(1, 7) for _ in range(1000)]\n",
                "\n",
                "print(\"Dice Roll Statistics (1000 rolls):\")\n",
                "print(f\"  E[X] = {expectation(dice_rolls):.4f} (theoretical: 3.5)\")\n",
                "print(f\"  Var(X) = {variance(dice_rolls):.4f} (theoretical: 2.9167)\")\n",
                "print(f\"  Std(X) = {std_dev(dice_rolls):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Bayes' Theorem\n",
                "\n",
                "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
                "\n",
                "**Real-World Application: Spam Classification**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def bayes_theorem(p_a, p_b_given_a, p_b_given_not_a):\n",
                "    \"\"\"\n",
                "    Calculate P(A|B) using Bayes' theorem.\n",
                "    \n",
                "    Args:\n",
                "        p_a: Prior probability P(A)\n",
                "        p_b_given_a: Likelihood P(B|A)\n",
                "        p_b_given_not_a: P(B|not A)\n",
                "    \n",
                "    Returns:\n",
                "        Posterior probability P(A|B)\n",
                "    \"\"\"\n",
                "    # P(B) = P(B|A)*P(A) + P(B|not A)*P(not A)\n",
                "    p_b = p_b_given_a * p_a + p_b_given_not_a * (1 - p_a)\n",
                "    \n",
                "    # Bayes' theorem\n",
                "    p_a_given_b = (p_b_given_a * p_a) / p_b\n",
                "    return p_a_given_b"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 1: Spam Detection\n",
                "print(\"=\" * 50)\n",
                "print(\"SPAM DETECTION EXAMPLE\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Given:\n",
                "p_spam = 0.3                    # 30% of emails are spam\n",
                "p_word_given_spam = 0.8         # 80% of spam contains \"free\"\n",
                "p_word_given_ham = 0.1          # 10% of ham contains \"free\"\n",
                "\n",
                "# Question: Email contains \"free\" - what's P(Spam | \"free\")?\n",
                "p_spam_given_word = bayes_theorem(p_spam, p_word_given_spam, p_word_given_ham)\n",
                "\n",
                "print(f\"Prior P(Spam) = {p_spam:.0%}\")\n",
                "print(f\"P('free' | Spam) = {p_word_given_spam:.0%}\")\n",
                "print(f\"P('free' | Ham) = {p_word_given_ham:.0%}\")\n",
                "print(f\"\\nâ†’ P(Spam | 'free') = {p_spam_given_word:.1%}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example 2: Medical Test (Classic!)\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"MEDICAL TEST EXAMPLE\")\n",
                "print(\"=\" * 50)\n",
                "\n",
                "# Given:\n",
                "p_disease = 0.001               # 0.1% have the disease\n",
                "p_positive_given_disease = 0.99 # 99% sensitivity (true positive)\n",
                "p_positive_given_healthy = 0.05 # 5% false positive\n",
                "\n",
                "# Question: Test is positive - what's P(Disease | Positive)?\n",
                "p_disease_given_positive = bayes_theorem(\n",
                "    p_disease, p_positive_given_disease, p_positive_given_healthy\n",
                ")\n",
                "\n",
                "print(f\"Disease prevalence: {p_disease:.1%}\")\n",
                "print(f\"Test sensitivity: {p_positive_given_disease:.0%}\")\n",
                "print(f\"False positive rate: {p_positive_given_healthy:.0%}\")\n",
                "print(f\"\\nâ†’ P(Disease | Positive Test) = {p_disease_given_positive:.1%}\")\n",
                "print(\"\\nðŸ’¡ Key Insight: Even with 99% accurate test, only ~2% chance!\")\n",
                "print(\"   This is why understanding priors matters in ML!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Maximum Likelihood Estimation (MLE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mle_bernoulli(data):\n",
                "    \"\"\"MLE for Bernoulli distribution parameter p.\"\"\"\n",
                "    return sum(data) / len(data)\n",
                "\n",
                "\n",
                "def mle_normal(data):\n",
                "    \"\"\"MLE for Normal distribution parameters (mean, variance).\"\"\"\n",
                "    mu = sum(data) / len(data)\n",
                "    var = sum((x - mu)**2 for x in data) / len(data)\n",
                "    return mu, var"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Estimate coin bias from flips\n",
                "biased_coin = [1, 1, 1, 0, 1, 1, 0, 1, 1, 1]  # Biased towards heads\n",
                "\n",
                "p_hat = mle_bernoulli(biased_coin)\n",
                "print(f\"MLE estimate of P(Heads) = {p_hat:.2f}\")\n",
                "\n",
                "# Example: Estimate normal distribution parameters\n",
                "normal_data = np.random.normal(loc=5, scale=2, size=1000)\n",
                "mu_hat, var_hat = mle_normal(normal_data)\n",
                "\n",
                "print(f\"\\nMLE for Normal Distribution:\")\n",
                "print(f\"  Î¼Ì‚ = {mu_hat:.4f} (true: 5)\")\n",
                "print(f\"  ÏƒÌ‚Â² = {var_hat:.4f} (true: 4)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Visualizations\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Common Probability Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "\n",
                "# 1. Normal Distribution\n",
                "x = np.linspace(-4, 4, 100)\n",
                "for mu, sigma in [(0, 1), (0, 0.5), (0, 2)]:\n",
                "    axes[0, 0].plot(x, stats.norm.pdf(x, mu, sigma), \n",
                "                    label=f'Î¼={mu}, Ïƒ={sigma}')\n",
                "axes[0, 0].set_title('Normal Distribution')\n",
                "axes[0, 0].legend()\n",
                "\n",
                "# 2. Uniform Distribution\n",
                "x = np.linspace(-1, 2, 100)\n",
                "axes[0, 1].plot(x, stats.uniform.pdf(x, 0, 1), linewidth=2)\n",
                "axes[0, 1].fill_between(x, stats.uniform.pdf(x, 0, 1), alpha=0.3)\n",
                "axes[0, 1].set_title('Uniform Distribution [0, 1]')\n",
                "\n",
                "# 3. Exponential Distribution\n",
                "x = np.linspace(0, 5, 100)\n",
                "for lam in [0.5, 1, 2]:\n",
                "    axes[0, 2].plot(x, stats.expon.pdf(x, scale=1/lam), label=f'Î»={lam}')\n",
                "axes[0, 2].set_title('Exponential Distribution')\n",
                "axes[0, 2].legend()\n",
                "\n",
                "# 4. Binomial Distribution\n",
                "n = 20\n",
                "x = np.arange(0, n+1)\n",
                "for p in [0.3, 0.5, 0.7]:\n",
                "    axes[1, 0].bar(x + p*0.2, stats.binom.pmf(x, n, p), \n",
                "                   width=0.2, alpha=0.7, label=f'p={p}')\n",
                "axes[1, 0].set_title(f'Binomial Distribution (n={n})')\n",
                "axes[1, 0].legend()\n",
                "\n",
                "# 5. Poisson Distribution\n",
                "x = np.arange(0, 20)\n",
                "for lam in [1, 4, 8]:\n",
                "    axes[1, 1].bar(x + lam*0.1, stats.poisson.pmf(x, lam), \n",
                "                   width=0.3, alpha=0.7, label=f'Î»={lam}')\n",
                "axes[1, 1].set_title('Poisson Distribution')\n",
                "axes[1, 1].legend()\n",
                "\n",
                "# 6. Beta Distribution (Important for Bayesian!)\n",
                "x = np.linspace(0, 1, 100)\n",
                "for a, b in [(0.5, 0.5), (2, 2), (2, 5), (5, 2)]:\n",
                "    axes[1, 2].plot(x, stats.beta.pdf(x, a, b), label=f'Î±={a}, Î²={b}')\n",
                "axes[1, 2].set_title('Beta Distribution')\n",
                "axes[1, 2].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Central Limit Theorem Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_clt(distribution, n_samples_list, n_experiments=1000):\n",
                "    \"\"\"Visualize Central Limit Theorem.\"\"\"\n",
                "    fig, axes = plt.subplots(1, len(n_samples_list), figsize=(15, 4))\n",
                "    \n",
                "    for ax, n_samples in zip(axes, n_samples_list):\n",
                "        # Generate sample means\n",
                "        sample_means = []\n",
                "        for _ in range(n_experiments):\n",
                "            sample = distribution(size=n_samples)\n",
                "            sample_means.append(np.mean(sample))\n",
                "        \n",
                "        # Plot histogram\n",
                "        ax.hist(sample_means, bins=30, density=True, alpha=0.7)\n",
                "        \n",
                "        # Overlay normal distribution\n",
                "        mu, sigma = np.mean(sample_means), np.std(sample_means)\n",
                "        x = np.linspace(min(sample_means), max(sample_means), 100)\n",
                "        ax.plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2)\n",
                "        \n",
                "        ax.set_title(f'n = {n_samples}')\n",
                "        ax.set_xlabel('Sample Mean')\n",
                "    \n",
                "    plt.suptitle('Central Limit Theorem: Sample Means â†’ Normal Distribution', \n",
                "                 fontsize=14, y=1.02)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "# Demonstrate CLT with uniform distribution\n",
                "print(\"CLT with Uniform Distribution:\")\n",
                "visualize_clt(np.random.uniform, [1, 5, 30, 100])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Bayes Theorem Visual"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_bayes_update(prior, likelihood, n_steps=5):\n",
                "    \"\"\"Visualize Bayesian updating process.\"\"\"\n",
                "    x = np.linspace(0, 1, 100)\n",
                "    \n",
                "    fig, axes = plt.subplots(1, n_steps, figsize=(15, 3))\n",
                "    \n",
                "    # Start with uniform prior\n",
                "    current_alpha, current_beta = 1, 1\n",
                "    \n",
                "    # Simulate coin flips (true p = 0.7)\n",
                "    np.random.seed(42)\n",
                "    data = np.random.binomial(1, 0.7, n_steps * 10)\n",
                "    \n",
                "    for i, ax in enumerate(axes):\n",
                "        # Update with 10 new observations\n",
                "        new_data = data[i*10:(i+1)*10]\n",
                "        successes = sum(new_data)\n",
                "        failures = len(new_data) - successes\n",
                "        \n",
                "        current_alpha += successes\n",
                "        current_beta += failures\n",
                "        \n",
                "        # Plot posterior\n",
                "        posterior = stats.beta.pdf(x, current_alpha, current_beta)\n",
                "        ax.plot(x, posterior, 'b-', linewidth=2)\n",
                "        ax.fill_between(x, posterior, alpha=0.3)\n",
                "        ax.axvline(x=0.7, color='r', linestyle='--', label='True p')\n",
                "        ax.set_title(f'After {(i+1)*10} flips\\nÎ±={current_alpha}, Î²={current_beta}')\n",
                "        ax.set_xlim(0, 1)\n",
                "    \n",
                "    plt.suptitle('Bayesian Updating: Estimating Coin Bias', fontsize=14, y=1.05)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "visualize_bayes_update(prior=(1, 1), likelihood=0.7)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    \"\"\"Run all unit tests.\"\"\"\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test 1: Probability\n",
                "    assert probability([0, 1, 1, 1], 1) == 0.75\n",
                "    print(\"âœ“ Probability test passed\")\n",
                "    \n",
                "    # Test 2: Expectation\n",
                "    assert expectation([1, 2, 3, 4, 5]) == 3.0\n",
                "    print(\"âœ“ Expectation test passed\")\n",
                "    \n",
                "    # Test 3: Variance\n",
                "    assert variance([1, 1, 1, 1]) == 0.0\n",
                "    print(\"âœ“ Variance (identical values) test passed\")\n",
                "    \n",
                "    # Test 4: Standard deviation\n",
                "    assert abs(std_dev([2, 4, 4, 4, 5, 5, 7, 9]) - 2.0) < 0.01\n",
                "    print(\"âœ“ Standard deviation test passed\")\n",
                "    \n",
                "    # Test 5: Bayes theorem\n",
                "    result = bayes_theorem(0.5, 1.0, 0.0)  # If P(B|A)=1 and P(B|~A)=0\n",
                "    assert abs(result - 1.0) < 1e-10\n",
                "    print(\"âœ“ Bayes theorem test passed\")\n",
                "    \n",
                "    # Test 6: MLE Bernoulli\n",
                "    assert mle_bernoulli([1, 1, 1, 0, 0]) == 0.6\n",
                "    print(\"âœ“ MLE Bernoulli test passed\")\n",
                "    \n",
                "    # Test 7: MLE Normal\n",
                "    mu, var = mle_normal([0, 0, 0])\n",
                "    assert mu == 0 and var == 0\n",
                "    print(\"âœ“ MLE Normal test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: Interview Prep\n",
                "---\n",
                "\n",
                "## Key Questions\n",
                "\n",
                "### Q1: What is Bayes' theorem and why is it important in ML?\n",
                "\n",
                "**Answer:**\n",
                "- Updates prior beliefs with new evidence\n",
                "- Foundation of Naive Bayes classifier\n",
                "- Essential for probabilistic models and uncertainty quantification\n",
                "- Formula: $P(H|E) = \\frac{P(E|H) \\cdot P(H)}{P(E)}$\n",
                "\n",
                "### Q2: Explain the difference between MLE and MAP.\n",
                "\n",
                "**Answer:**\n",
                "- **MLE**: Maximize $P(Data|\\theta)$ - find parameters that make data most likely\n",
                "- **MAP**: Maximize $P(\\theta|Data) \\propto P(Data|\\theta) \\cdot P(\\theta)$ - includes prior\n",
                "- MAP with uniform prior equals MLE\n",
                "- MAP can prevent overfitting (regularization)\n",
                "\n",
                "### Q3: What is the Central Limit Theorem?\n",
                "\n",
                "**Answer:**\n",
                "- Sample means of ANY distribution approach normal distribution\n",
                "- As sample size increases, distribution becomes more normal\n",
                "- Justifies using normal distribution in many ML contexts\n",
                "\n",
                "### Q4: How do you handle low confidence predictions?\n",
                "\n",
                "**Answer:**\n",
                "- Set confidence thresholds (e.g., abstain if P < 0.7)\n",
                "- Use calibration to make probabilities meaningful\n",
                "- Apply ensemble methods for uncertainty estimation\n",
                "- Consider Bayesian approaches for full posterior"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 6: Exercises\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Implement Naive Bayes Classifier\n",
                "def naive_bayes_train(X, y):\n",
                "    \"\"\"\n",
                "    Train a simple Naive Bayes classifier.\n",
                "    \n",
                "    Args:\n",
                "        X: Feature matrix (n_samples, n_features)\n",
                "        y: Labels (n_samples,)\n",
                "    \n",
                "    Returns:\n",
                "        Dictionary with class priors and feature likelihoods\n",
                "    \"\"\"\n",
                "    # TODO: Your implementation here\n",
                "    pass\n",
                "\n",
                "\n",
                "# Exercise 2: Implement Log-Likelihood\n",
                "def log_likelihood(data, mu, sigma):\n",
                "    \"\"\"\n",
                "    Calculate log-likelihood of data under normal distribution.\n",
                "    \n",
                "    Args:\n",
                "        data: Observed values\n",
                "        mu: Mean of normal distribution\n",
                "        sigma: Standard deviation\n",
                "    \n",
                "    Returns:\n",
                "        Log-likelihood value\n",
                "    \"\"\"\n",
                "    # TODO: Your implementation here\n",
                "    pass\n",
                "\n",
                "\n",
                "# Exercise 3: Implement Monte Carlo Estimation\n",
                "def monte_carlo_pi(n_samples=10000):\n",
                "    \"\"\"\n",
                "    Estimate Ï€ using Monte Carlo simulation.\n",
                "    \n",
                "    Hint: Sample uniform points in [0,1]x[0,1]\n",
                "          Count points inside unit circle\n",
                "    \n",
                "    Returns:\n",
                "        Estimated value of Ï€\n",
                "    \"\"\"\n",
                "    # TODO: Your implementation here\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 7: Deliverable\n",
                "---\n",
                "\n",
                "## What You Built This Week:\n",
                "\n",
                "1. **`probability_utils.py`** - Probability calculation functions\n",
                "2. **Bayes' Theorem Calculator** - With real-world examples\n",
                "3. **Distribution Visualizations** - Understanding common distributions\n",
                "4. **MLE Estimators** - Parameter estimation from data\n",
                "\n",
                "## Key Takeaways:\n",
                "\n",
                "- ML is fundamentally about modeling uncertainty\n",
                "- Bayes' theorem: Prior + Evidence = Posterior\n",
                "- MLE finds parameters that maximize data likelihood\n",
                "- CLT justifies normal distribution assumptions\n",
                "\n",
                "## Next Week: ML Core (Week 3)\n",
                "- Multi-Layer Perceptrons\n",
                "- Backpropagation from scratch\n",
                "- Gradient-based optimization"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}