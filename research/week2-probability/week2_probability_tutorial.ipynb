{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìä Week 2: Probability & Statistics for AI\n",
                "\n",
                "This notebook covers the essential probability and statistics concepts needed for AI/ML.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Probability Fundamentals](#1-probability-fundamentals)\n",
                "2. [Probability Distributions](#2-probability-distributions)\n",
                "3. [Bayes' Theorem](#3-bayes-theorem)\n",
                "4. [Statistical Concepts](#4-statistical-concepts)\n",
                "5. [Maximum Likelihood Estimation](#5-maximum-likelihood-estimation)\n",
                "6. [Sampling Methods](#6-sampling-methods)\n",
                "7. [Applications in ML](#7-applications-in-ml)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Probability Fundamentals\n",
                "\n",
                "### 1.1 Basic Definitions\n",
                "\n",
                "**Probability** measures the likelihood of an event occurring.\n",
                "\n",
                "| Concept | Definition | Example |\n",
                "|---------|------------|---------|\n",
                "| **Sample Space (Œ©)** | Set of all possible outcomes | Coin flip: {H, T} |\n",
                "| **Event (E)** | Subset of sample space | Getting heads |\n",
                "| **P(E)** | Probability of event E | P(H) = 0.5 |\n",
                "\n",
                "### 1.2 Probability Axioms (Kolmogorov)\n",
                "\n",
                "1. **Non-negativity**: $P(E) \\geq 0$ for all events E\n",
                "2. **Normalization**: $P(\\Omega) = 1$\n",
                "3. **Additivity**: $P(A \\cup B) = P(A) + P(B)$ if A and B are disjoint\n",
                "\n",
                "### 1.3 Key Formulas\n",
                "\n",
                "| Formula | Description |\n",
                "|---------|-------------|\n",
                "| $P(A \\cup B) = P(A) + P(B) - P(A \\cap B)$ | Union (OR) |\n",
                "| $P(A \\cap B) = P(A) \\cdot P(B|A)$ | Intersection (AND) |\n",
                "| $P(A') = 1 - P(A)$ | Complement (NOT) |\n",
                "| $P(A|B) = \\frac{P(A \\cap B)}{P(B)}$ | Conditional probability |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy import stats\n",
                "\n",
                "# Example: Calculating probabilities\n",
                "def calculate_probability(favorable, total):\n",
                "    \"\"\"Calculate probability of an event.\"\"\"\n",
                "    return favorable / total\n",
                "\n",
                "# Example: Card probability\n",
                "# P(drawing a heart) = 13/52 = 0.25\n",
                "p_heart = calculate_probability(13, 52)\n",
                "print(f\"P(Heart) = {p_heart:.4f}\")\n",
                "\n",
                "# P(drawing a face card) = 12/52\n",
                "p_face = calculate_probability(12, 52)\n",
                "print(f\"P(Face Card) = {p_face:.4f}\")\n",
                "\n",
                "# P(Heart AND Face) = 3/52\n",
                "p_heart_and_face = calculate_probability(3, 52)\n",
                "print(f\"P(Heart AND Face) = {p_heart_and_face:.4f}\")\n",
                "\n",
                "# P(Heart OR Face) = P(Heart) + P(Face) - P(Heart AND Face)\n",
                "p_heart_or_face = p_heart + p_face - p_heart_and_face\n",
                "print(f\"P(Heart OR Face) = {p_heart_or_face:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Probability Distributions\n",
                "\n",
                "### 2.1 Discrete Distributions\n",
                "\n",
                "#### Bernoulli Distribution\n",
                "Single trial with probability p of success.\n",
                "\n",
                "$$P(X = k) = p^k (1-p)^{1-k}, \\quad k \\in \\{0, 1\\}$$\n",
                "\n",
                "#### Binomial Distribution\n",
                "Number of successes in n independent Bernoulli trials.\n",
                "\n",
                "$$P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}$$\n",
                "\n",
                "- Mean: $\\mu = np$\n",
                "- Variance: $\\sigma^2 = np(1-p)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Binomial Distribution Example\n",
                "n, p = 10, 0.5  # 10 coin flips, fair coin\n",
                "\n",
                "# Create distribution\n",
                "x = np.arange(0, n+1)\n",
                "binomial_pmf = stats.binom.pmf(x, n, p)\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.bar(x, binomial_pmf, color='steelblue', alpha=0.7)\n",
                "plt.xlabel('Number of Heads')\n",
                "plt.ylabel('Probability')\n",
                "plt.title(f'Binomial Distribution (n={n}, p={p})')\n",
                "plt.xticks(x)\n",
                "plt.grid(axis='y', alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(f\"Mean: {n*p}, Variance: {n*p*(1-p)}\")\n",
                "print(f\"P(X = 5) = {stats.binom.pmf(5, n, p):.4f}\")\n",
                "print(f\"P(X >= 7) = {1 - stats.binom.cdf(6, n, p):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Continuous Distributions\n",
                "\n",
                "#### Normal (Gaussian) Distribution\n",
                "The most important distribution in statistics!\n",
                "\n",
                "$$f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$\n",
                "\n",
                "**Properties:**\n",
                "- Symmetric around mean Œº\n",
                "- 68-95-99.7 rule (within 1, 2, 3 standard deviations)\n",
                "- Central Limit Theorem: sums of random variables ‚Üí Normal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Normal Distribution\n",
                "mu, sigma = 0, 1  # Standard normal\n",
                "\n",
                "x = np.linspace(-4, 4, 1000)\n",
                "normal_pdf = stats.norm.pdf(x, mu, sigma)\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Plot PDF\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x, normal_pdf, 'b-', linewidth=2)\n",
                "plt.fill_between(x, normal_pdf, alpha=0.3)\n",
                "\n",
                "# Shade 1 standard deviation\n",
                "x_fill = np.linspace(-1, 1, 100)\n",
                "plt.fill_between(x_fill, stats.norm.pdf(x_fill), color='green', alpha=0.3, label='68%')\n",
                "\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('Probability Density')\n",
                "plt.title('Standard Normal Distribution')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "# Plot different normal distributions\n",
                "plt.subplot(1, 2, 2)\n",
                "for mu, sigma in [(0, 1), (0, 2), (2, 1)]:\n",
                "    plt.plot(x, stats.norm.pdf(x, mu, sigma), label=f'Œº={mu}, œÉ={sigma}')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('Probability Density')\n",
                "plt.title('Different Normal Distributions')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Probability calculations\n",
                "print(f\"P(X < 0) = {stats.norm.cdf(0):.4f}\")\n",
                "print(f\"P(-1 < X < 1) = {stats.norm.cdf(1) - stats.norm.cdf(-1):.4f}\")\n",
                "print(f\"P(X > 1.96) = {1 - stats.norm.cdf(1.96):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Bayes' Theorem\n",
                "\n",
                "### 3.1 The Formula\n",
                "\n",
                "$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$\n",
                "\n",
                "Where:\n",
                "- $P(A|B)$ = **Posterior** (probability of A given B)\n",
                "- $P(B|A)$ = **Likelihood** (probability of B given A)\n",
                "- $P(A)$ = **Prior** (initial probability of A)\n",
                "- $P(B)$ = **Evidence** (probability of B)\n",
                "\n",
                "### 3.2 Why Bayes Matters in ML\n",
                "\n",
                "- **Naive Bayes Classifier**: Fast, effective for text classification\n",
                "- **Bayesian Inference**: Update beliefs with new data\n",
                "- **Probabilistic Models**: Quantify uncertainty"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Bayes' Theorem Example: Medical Testing\n",
                "# Disease prevalence: 1% of population has the disease\n",
                "# Test sensitivity: 99% (true positive rate)\n",
                "# Test specificity: 95% (true negative rate)\n",
                "\n",
                "# Question: If you test positive, what's the probability you have the disease?\n",
                "\n",
                "p_disease = 0.01       # P(D) - Prior\n",
                "p_pos_given_disease = 0.99  # P(+|D) - Sensitivity\n",
                "p_neg_given_healthy = 0.95  # P(-|¬¨D) - Specificity\n",
                "p_pos_given_healthy = 1 - p_neg_given_healthy  # P(+|¬¨D) - False positive rate\n",
                "\n",
                "# Calculate P(+) using law of total probability\n",
                "p_positive = (p_pos_given_disease * p_disease + \n",
                "              p_pos_given_healthy * (1 - p_disease))\n",
                "\n",
                "# Apply Bayes' Theorem\n",
                "p_disease_given_pos = (p_pos_given_disease * p_disease) / p_positive\n",
                "\n",
                "print(\"Medical Test Bayes Analysis\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Prior P(Disease) = {p_disease:.2%}\")\n",
                "print(f\"P(Positive) = {p_positive:.4f}\")\n",
                "print(f\"\\n‚ö†Ô∏è P(Disease | Positive) = {p_disease_given_pos:.2%}\")\n",
                "print(f\"\\nSurprisingly, even with a positive test, there's only a {p_disease_given_pos:.1%}\")\n",
                "print(\"chance you actually have the disease! This is the 'base rate fallacy'.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Naive Bayes Classifier from Scratch\n",
                "class NaiveBayesClassifier:\n",
                "    \"\"\"\n",
                "    Simple Naive Bayes classifier for text.\n",
                "    \n",
                "    Assumes features are conditionally independent given the class.\n",
                "    P(C|X) ‚àù P(C) * Œ† P(xi|C)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.class_priors = {}  # P(C)\n",
                "        self.word_probs = {}    # P(word|C)\n",
                "        self.vocab = set()\n",
                "    \n",
                "    def fit(self, documents, labels):\n",
                "        \"\"\"Train the classifier.\"\"\"\n",
                "        # Count classes\n",
                "        class_counts = {}\n",
                "        word_counts = {}  # word_counts[class][word] = count\n",
                "        \n",
                "        for doc, label in zip(documents, labels):\n",
                "            class_counts[label] = class_counts.get(label, 0) + 1\n",
                "            \n",
                "            if label not in word_counts:\n",
                "                word_counts[label] = {}\n",
                "            \n",
                "            words = doc.lower().split()\n",
                "            self.vocab.update(words)\n",
                "            \n",
                "            for word in words:\n",
                "                word_counts[label][word] = word_counts[label].get(word, 0) + 1\n",
                "        \n",
                "        # Calculate priors\n",
                "        total = sum(class_counts.values())\n",
                "        for label, count in class_counts.items():\n",
                "            self.class_priors[label] = count / total\n",
                "        \n",
                "        # Calculate word probabilities with Laplace smoothing\n",
                "        for label in class_counts:\n",
                "            self.word_probs[label] = {}\n",
                "            total_words = sum(word_counts[label].values())\n",
                "            \n",
                "            for word in self.vocab:\n",
                "                count = word_counts[label].get(word, 0)\n",
                "                # Laplace smoothing: (count + 1) / (total + vocab_size)\n",
                "                self.word_probs[label][word] = (count + 1) / (total_words + len(self.vocab))\n",
                "    \n",
                "    def predict(self, document):\n",
                "        \"\"\"Predict class for a document.\"\"\"\n",
                "        words = document.lower().split()\n",
                "        \n",
                "        scores = {}\n",
                "        for label in self.class_priors:\n",
                "            # Start with log prior\n",
                "            score = np.log(self.class_priors[label])\n",
                "            \n",
                "            # Add log likelihoods\n",
                "            for word in words:\n",
                "                if word in self.vocab:\n",
                "                    score += np.log(self.word_probs[label][word])\n",
                "            \n",
                "            scores[label] = score\n",
                "        \n",
                "        return max(scores, key=scores.get)\n",
                "\n",
                "# Example: Sentiment classification\n",
                "documents = [\n",
                "    \"I love this movie it is great\",\n",
                "    \"Amazing film wonderful acting\",\n",
                "    \"This movie is terrible waste of time\",\n",
                "    \"Horrible film bad acting\",\n",
                "    \"Great movie loved every minute\",\n",
                "    \"Awful terrible waste\"\n",
                "]\n",
                "labels = [\"positive\", \"positive\", \"negative\", \"negative\", \"positive\", \"negative\"]\n",
                "\n",
                "# Train\n",
                "nb = NaiveBayesClassifier()\n",
                "nb.fit(documents, labels)\n",
                "\n",
                "# Test\n",
                "test_docs = [\n",
                "    \"I love this amazing film\",\n",
                "    \"terrible movie really bad\",\n",
                "    \"great acting wonderful story\"\n",
                "]\n",
                "\n",
                "print(\"Naive Bayes Predictions:\")\n",
                "for doc in test_docs:\n",
                "    pred = nb.predict(doc)\n",
                "    print(f\"  '{doc}' ‚Üí {pred}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Statistical Concepts\n",
                "\n",
                "### 4.1 Expectation and Variance\n",
                "\n",
                "**Expectation (Mean)**:\n",
                "$$E[X] = \\sum_x x \\cdot P(X = x) \\quad \\text{or} \\quad \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx$$\n",
                "\n",
                "**Variance**:\n",
                "$$Var(X) = E[(X - \\mu)^2] = E[X^2] - (E[X])^2$$\n",
                "\n",
                "**Standard Deviation**: $\\sigma = \\sqrt{Var(X)}$\n",
                "\n",
                "### 4.2 Covariance and Correlation\n",
                "\n",
                "**Covariance**: How two variables vary together\n",
                "$$Cov(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)]$$\n",
                "\n",
                "**Correlation**: Normalized covariance (-1 to 1)\n",
                "$$\\rho_{XY} = \\frac{Cov(X, Y)}{\\sigma_X \\sigma_Y}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation visualization\n",
                "np.random.seed(42)\n",
                "\n",
                "# Generate correlated data\n",
                "n = 200\n",
                "correlations = [-0.9, 0, 0.9]\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "\n",
                "for ax, target_corr in zip(axes, correlations):\n",
                "    # Generate correlated data\n",
                "    x = np.random.randn(n)\n",
                "    noise = np.random.randn(n)\n",
                "    y = target_corr * x + np.sqrt(1 - target_corr**2) * noise\n",
                "    \n",
                "    actual_corr = np.corrcoef(x, y)[0, 1]\n",
                "    \n",
                "    ax.scatter(x, y, alpha=0.5)\n",
                "    ax.set_xlabel('X')\n",
                "    ax.set_ylabel('Y')\n",
                "    ax.set_title(f'Correlation = {actual_corr:.2f}')\n",
                "    ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Maximum Likelihood Estimation\n",
                "\n",
                "### 5.1 The Concept\n",
                "\n",
                "Find parameters Œ∏ that maximize the probability of observing the data:\n",
                "\n",
                "$$\\hat{\\theta}_{MLE} = \\arg\\max_\\theta P(X|\\theta) = \\arg\\max_\\theta \\mathcal{L}(\\theta)$$\n",
                "\n",
                "In practice, we maximize log-likelihood:\n",
                "$$\\hat{\\theta}_{MLE} = \\arg\\max_\\theta \\log \\mathcal{L}(\\theta)$$\n",
                "\n",
                "### 5.2 Example: Estimating Mean of Normal Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MLE for Normal Distribution\n",
                "np.random.seed(42)\n",
                "\n",
                "# Generate data from N(5, 2)\n",
                "true_mu, true_sigma = 5, 2\n",
                "data = np.random.normal(true_mu, true_sigma, 100)\n",
                "\n",
                "# Log-likelihood function\n",
                "def log_likelihood(mu, sigma, data):\n",
                "    \"\"\"Log-likelihood of normal distribution.\"\"\"\n",
                "    n = len(data)\n",
                "    ll = -n/2 * np.log(2*np.pi) - n*np.log(sigma) - np.sum((data - mu)**2) / (2*sigma**2)\n",
                "    return ll\n",
                "\n",
                "# MLE estimates (closed form)\n",
                "mle_mu = np.mean(data)\n",
                "mle_sigma = np.std(data, ddof=0)  # MLE uses n, not n-1\n",
                "\n",
                "print(\"Maximum Likelihood Estimation\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"True parameters: Œº = {true_mu}, œÉ = {true_sigma}\")\n",
                "print(f\"MLE estimates:   Œº = {mle_mu:.3f}, œÉ = {mle_sigma:.3f}\")\n",
                "print(f\"Log-likelihood: {log_likelihood(mle_mu, mle_sigma, data):.2f}\")\n",
                "\n",
                "# Visualize likelihood surface for Œº\n",
                "mu_range = np.linspace(3, 7, 100)\n",
                "lls = [log_likelihood(mu, mle_sigma, data) for mu in mu_range]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(mu_range, lls, 'b-', linewidth=2)\n",
                "plt.axvline(mle_mu, color='r', linestyle='--', label=f'MLE Œº = {mle_mu:.3f}')\n",
                "plt.axvline(true_mu, color='g', linestyle=':', label=f'True Œº = {true_mu}')\n",
                "plt.xlabel('Œº')\n",
                "plt.ylabel('Log-Likelihood')\n",
                "plt.title('Log-Likelihood Function')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Sampling Methods\n",
                "\n",
                "### 6.1 Monte Carlo Sampling\n",
                "\n",
                "Use random sampling to estimate quantities that are difficult to compute analytically."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Monte Carlo estimation of œÄ\n",
                "def estimate_pi(n_samples):\n",
                "    \"\"\"Estimate œÄ using Monte Carlo sampling.\"\"\"\n",
                "    # Generate random points in [0,1] x [0,1]\n",
                "    x = np.random.uniform(0, 1, n_samples)\n",
                "    y = np.random.uniform(0, 1, n_samples)\n",
                "    \n",
                "    # Check if inside quarter circle\n",
                "    inside = (x**2 + y**2) <= 1\n",
                "    \n",
                "    # œÄ/4 = area of quarter circle / area of square\n",
                "    pi_estimate = 4 * np.sum(inside) / n_samples\n",
                "    return pi_estimate\n",
                "\n",
                "# Estimate with different sample sizes\n",
                "sample_sizes = [100, 1000, 10000, 100000]\n",
                "\n",
                "print(\"Monte Carlo Estimation of œÄ\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"True œÄ = {np.pi:.6f}\")\n",
                "print()\n",
                "\n",
                "for n in sample_sizes:\n",
                "    estimate = estimate_pi(n)\n",
                "    error = abs(estimate - np.pi)\n",
                "    print(f\"n = {n:>6}: œÄ ‚âà {estimate:.6f}, error = {error:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Applications in ML\n",
                "\n",
                "### 7.1 Cross-Entropy Loss\n",
                "\n",
                "Used for classification, derived from information theory:\n",
                "\n",
                "$$H(p, q) = -\\sum_x p(x) \\log q(x)$$\n",
                "\n",
                "For binary classification:\n",
                "$$L = -[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})]$$\n",
                "\n",
                "### 7.2 Regularization as Prior\n",
                "\n",
                "- **L2 Regularization** = Gaussian prior on weights\n",
                "- **L1 Regularization** = Laplace prior on weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cross-entropy loss\n",
                "def cross_entropy_loss(y_true, y_pred):\n",
                "    \"\"\"Binary cross-entropy loss.\"\"\"\n",
                "    epsilon = 1e-15  # Avoid log(0)\n",
                "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
                "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
                "\n",
                "# Example\n",
                "y_true = np.array([1, 0, 1, 1, 0])\n",
                "y_pred_good = np.array([0.9, 0.1, 0.8, 0.95, 0.2])\n",
                "y_pred_bad = np.array([0.5, 0.5, 0.5, 0.5, 0.5])\n",
                "\n",
                "print(f\"Good predictions loss: {cross_entropy_loss(y_true, y_pred_good):.4f}\")\n",
                "print(f\"Bad predictions loss: {cross_entropy_loss(y_true, y_pred_bad):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "Key concepts covered:\n",
                "\n",
                "1. **Probability fundamentals** - Sample space, events, axioms\n",
                "2. **Distributions** - Binomial, Normal, and their properties\n",
                "3. **Bayes' Theorem** - Updating beliefs with evidence\n",
                "4. **Statistics** - Mean, variance, correlation\n",
                "5. **MLE** - Finding best parameters from data\n",
                "6. **Sampling** - Monte Carlo methods\n",
                "7. **ML applications** - Cross-entropy, regularization\n",
                "\n",
                "### Key Takeaways for AI/ML\n",
                "\n",
                "- Understanding probability enables reasoning about uncertainty\n",
                "- Bayes' theorem is the foundation of probabilistic ML\n",
                "- MLE connects to loss functions and optimization\n",
                "- Distributions help model real-world phenomena"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}