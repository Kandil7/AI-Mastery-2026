{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ’° Week 10: Guardrails & Cost Metrics\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Implement evaluation frameworks (RAGAS-style)\n",
                "2. Track token usage and costs\n",
                "3. Measure faithfulness and relevance\n",
                "4. Build monitoring dashboards\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import Dict, List\n",
                "from dataclasses import dataclass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## RAG Evaluation Metrics\n",
                "\n",
                "| Metric | Measures | Range |\n",
                "|--------|----------|-------|\n",
                "| Faithfulness | Is answer grounded in context? | 0-1 |\n",
                "| Relevance | Does answer address question? | 0-1 |\n",
                "| Context Precision | Are retrieved docs relevant? | 0-1 |\n",
                "| Answer Correctness | Is answer factually correct? | 0-1 |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TokenUsage:\n",
                "    prompt_tokens: int\n",
                "    completion_tokens: int\n",
                "    \n",
                "    @property\n",
                "    def total_tokens(self) -> int:\n",
                "        return self.prompt_tokens + self.completion_tokens\n",
                "    \n",
                "    def cost(self, prompt_price=0.01, completion_price=0.03) -> float:\n",
                "        \"\"\"Calculate cost per 1K tokens.\"\"\"\n",
                "        return (self.prompt_tokens * prompt_price + \n",
                "                self.completion_tokens * completion_price) / 1000\n",
                "\n",
                "class CostTracker:\n",
                "    \"\"\"Track API costs.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.usages: List[TokenUsage] = []\n",
                "    \n",
                "    def add(self, usage: TokenUsage):\n",
                "        self.usages.append(usage)\n",
                "    \n",
                "    def total_cost(self) -> float:\n",
                "        return sum(u.cost() for u in self.usages)\n",
                "    \n",
                "    def summary(self) -> Dict:\n",
                "        return {\n",
                "            \"total_requests\": len(self.usages),\n",
                "            \"total_tokens\": sum(u.total_tokens for u in self.usages),\n",
                "            \"total_cost\": f\"${self.total_cost():.4f}\"\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RAGEvaluator:\n",
                "    \"\"\"Evaluate RAG responses.\"\"\"\n",
                "    \n",
                "    def faithfulness(self, answer: str, context: str) -> float:\n",
                "        \"\"\"Check if answer is grounded in context.\"\"\"\n",
                "        answer_words = set(answer.lower().split())\n",
                "        context_words = set(context.lower().split())\n",
                "        if not answer_words:\n",
                "            return 0\n",
                "        overlap = len(answer_words & context_words)\n",
                "        return min(overlap / len(answer_words), 1.0)\n",
                "    \n",
                "    def relevance(self, answer: str, question: str) -> float:\n",
                "        \"\"\"Check if answer addresses the question.\"\"\"\n",
                "        q_words = set(question.lower().split())\n",
                "        a_words = set(answer.lower().split())\n",
                "        if not q_words:\n",
                "            return 0\n",
                "        return len(q_words & a_words) / len(q_words)\n",
                "    \n",
                "    def evaluate(self, question: str, answer: str, context: str) -> Dict:\n",
                "        return {\n",
                "            \"faithfulness\": self.faithfulness(answer, context),\n",
                "            \"relevance\": self.relevance(answer, question)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test cost tracking\n",
                "tracker = CostTracker()\n",
                "tracker.add(TokenUsage(500, 100))\n",
                "tracker.add(TokenUsage(800, 200))\n",
                "\n",
                "print(\"Cost Summary:\", tracker.summary())\n",
                "\n",
                "# Test evaluation\n",
                "evaluator = RAGEvaluator()\n",
                "metrics = evaluator.evaluate(\n",
                "    question=\"What is machine learning?\",\n",
                "    answer=\"Machine learning is a subset of AI that learns from data.\",\n",
                "    context=\"Machine learning is a subset of artificial intelligence.\"\n",
                ")\n",
                "print(f\"\\nEvaluation: {metrics}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test cost tracking\n",
                "    usage = TokenUsage(1000, 500)\n",
                "    assert usage.total_tokens == 1500\n",
                "    print(\"âœ“ Token usage test passed\")\n",
                "    \n",
                "    # Test faithfulness\n",
                "    ev = RAGEvaluator()\n",
                "    assert ev.faithfulness(\"hello world\", \"hello world!\") > 0.5\n",
                "    print(\"âœ“ Faithfulness test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Interview Prep\n",
                "---\n",
                "\n",
                "### Q1: How do you reduce LLM costs?\n",
                "**Answer:** Prompt caching, smaller models, truncation, batching.\n",
                "\n",
                "### Q2: What is faithfulness in RAG?\n",
                "**Answer:** Whether the answer is grounded in retrieved context vs hallucinated.\n",
                "\n",
                "---\n",
                "# Section 5: Deliverable\n",
                "---\n",
                "\n",
                "**Created:** `orchestrator_v2.py` with cost tracking and evaluation\n",
                "\n",
                "**Next Week:** Flutter Chat UI"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}