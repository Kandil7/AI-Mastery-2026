{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Week 10: Evaluation Metrics for ML and RAG Systems\n",
                "\n",
                "This notebook covers comprehensive evaluation strategies for ML models and RAG systems.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Classification Metrics](#1-classification-metrics)\n",
                "2. [Regression Metrics](#2-regression-metrics)\n",
                "3. [Ranking Metrics](#3-ranking-metrics)\n",
                "4. [RAG Evaluation](#4-rag-evaluation)\n",
                "5. [A/B Testing](#5-ab-testing)\n",
                "6. [Building an Evaluation Pipeline](#6-building-an-evaluation-pipeline)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '../..')\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import List, Dict\n",
                "\n",
                "from src.evaluation import (\n",
                "    RAGEvaluator,\n",
                "    MLEvaluator,\n",
                "    LLMEvaluator,\n",
                "    BenchmarkRunner,\n",
                "    EvaluationReport,\n",
                ")\n",
                "\n",
                "from src.evaluation.evaluation import (\n",
                "    accuracy, precision, recall, f1_score, confusion_matrix,\n",
                "    mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score,\n",
                "    mean_reciprocal_rank, precision_at_k, recall_at_k, ndcg_at_k,\n",
                ")\n",
                "\n",
                "print(\"âœ… Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Classification Metrics\n",
                "\n",
                "### 1.1 The Confusion Matrix\n",
                "\n",
                "```\n",
                "                    Predicted\n",
                "                  Pos     Neg\n",
                "Actual  Pos       TP      FN\n",
                "        Neg       FP      TN\n",
                "```\n",
                "\n",
                "| Metric | Formula | When to Use |\n",
                "|--------|---------|-------------|\n",
                "| **Accuracy** | (TP+TN)/All | Balanced classes |\n",
                "| **Precision** | TP/(TP+FP) | Cost of false positives high |\n",
                "| **Recall** | TP/(TP+FN) | Cost of false negatives high |\n",
                "| **F1** | 2Â·PÂ·R/(P+R) | Need balance |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Spam Detection\n",
                "np.random.seed(42)\n",
                "\n",
                "# Simulate predictions\n",
                "y_true = np.array([1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1])\n",
                "y_pred = np.array([1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0])\n",
                "\n",
                "# Calculate metrics\n",
                "acc = accuracy(y_true, y_pred)\n",
                "prec = precision(y_true, y_pred)\n",
                "rec = recall(y_true, y_pred)\n",
                "f1 = f1_score(y_true, y_pred)\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "print(\"Spam Detection Evaluation\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Accuracy:  {acc:.3f}\")\n",
                "print(f\"Precision: {prec:.3f}\")\n",
                "print(f\"Recall:    {rec:.3f}\")\n",
                "print(f\"F1 Score:  {f1:.3f}\")\n",
                "print(f\"\\nConfusion Matrix:\")\n",
                "print(cm)\n",
                "\n",
                "# Visualize\n",
                "fig, ax = plt.subplots(figsize=(6, 5))\n",
                "im = ax.imshow(cm, cmap='Blues')\n",
                "ax.set_xticks([0, 1])\n",
                "ax.set_yticks([0, 1])\n",
                "ax.set_xticklabels(['Not Spam', 'Spam'])\n",
                "ax.set_yticklabels(['Not Spam', 'Spam'])\n",
                "ax.set_xlabel('Predicted')\n",
                "ax.set_ylabel('Actual')\n",
                "ax.set_title('Confusion Matrix')\n",
                "\n",
                "for i in range(2):\n",
                "    for j in range(2):\n",
                "        ax.text(j, i, cm[i, j], ha='center', va='center', fontsize=20)\n",
                "\n",
                "plt.colorbar(im)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Precision-Recall Trade-off"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate predicted probabilities\n",
                "y_prob = np.array([0.9, 0.2, 0.7, 0.85, 0.3, 0.95, 0.55, 0.1, 0.8, 0.25, 0.88, 0.92, 0.15, 0.05, 0.65])\n",
                "\n",
                "# Calculate metrics at different thresholds\n",
                "thresholds = np.arange(0.1, 1.0, 0.1)\n",
                "precisions = []\n",
                "recalls = []\n",
                "\n",
                "for threshold in thresholds:\n",
                "    y_pred_at_threshold = (y_prob >= threshold).astype(int)\n",
                "    precisions.append(precision(y_true, y_pred_at_threshold))\n",
                "    recalls.append(recall(y_true, y_pred_at_threshold))\n",
                "\n",
                "# Plot\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(thresholds, precisions, 'b-o', label='Precision')\n",
                "plt.plot(thresholds, recalls, 'r-s', label='Recall')\n",
                "plt.xlabel('Threshold')\n",
                "plt.ylabel('Score')\n",
                "plt.title('Precision-Recall vs Threshold')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(recalls, precisions, 'g-o')\n",
                "plt.xlabel('Recall')\n",
                "plt.ylabel('Precision')\n",
                "plt.title('Precision-Recall Curve')\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ Key Insight: Higher threshold â†’ Higher precision, Lower recall\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Regression Metrics\n",
                "\n",
                "| Metric | Formula | Interpretation |\n",
                "|--------|---------|----------------|\n",
                "| **MSE** | mean((y - Å·)Â²) | Penalizes large errors |\n",
                "| **RMSE** | âˆšMSE | Same units as target |\n",
                "| **MAE** | mean(\\|y - Å·\\|) | Robust to outliers |\n",
                "| **RÂ²** | 1 - SS_res/SS_tot | Variance explained |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate regression predictions\n",
                "np.random.seed(42)\n",
                "y_true_reg = np.array([100, 150, 200, 180, 220, 175, 190, 210, 140, 160])\n",
                "y_pred_reg = y_true_reg + np.random.normal(0, 15, len(y_true_reg))  # Add noise\n",
                "\n",
                "# Calculate metrics\n",
                "mse = mean_squared_error(y_true_reg, y_pred_reg)\n",
                "rmse = root_mean_squared_error(y_true_reg, y_pred_reg)\n",
                "mae = mean_absolute_error(y_true_reg, y_pred_reg)\n",
                "r2 = r2_score(y_true_reg, y_pred_reg)\n",
                "\n",
                "print(\"Regression Evaluation (e.g., House Price Prediction)\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"MSE:  {mse:.2f}\")\n",
                "print(f\"RMSE: {rmse:.2f} (Average prediction off by ${rmse:.0f}K)\")\n",
                "print(f\"MAE:  {mae:.2f}\")\n",
                "print(f\"RÂ²:   {r2:.3f} ({r2*100:.1f}% variance explained)\")\n",
                "\n",
                "# Visualize predictions\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.scatter(y_true_reg, y_pred_reg, alpha=0.7)\n",
                "plt.plot([100, 250], [100, 250], 'r--', label='Perfect')\n",
                "plt.xlabel('Actual')\n",
                "plt.ylabel('Predicted')\n",
                "plt.title('Actual vs Predicted')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "residuals = y_true_reg - y_pred_reg\n",
                "plt.hist(residuals, bins=10, edgecolor='black', alpha=0.7)\n",
                "plt.axvline(0, color='r', linestyle='--')\n",
                "plt.xlabel('Residual (Actual - Predicted)')\n",
                "plt.ylabel('Frequency')\n",
                "plt.title('Residual Distribution')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Ranking Metrics\n",
                "\n",
                "For search and recommendation systems."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Search Result Evaluation\n",
                "\n",
                "# Query 1: Relevant docs at positions 1, 3\n",
                "# Query 2: Relevant doc at position 2\n",
                "y_true_rank = [[0, 2], [1]]  # Indices of relevant docs\n",
                "y_pred_rank = [[0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]  # Ranked results\n",
                "\n",
                "# Calculate MRR\n",
                "mrr = mean_reciprocal_rank(y_true_rank, y_pred_rank)\n",
                "\n",
                "print(\"Search Ranking Evaluation\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Mean Reciprocal Rank (MRR): {mrr:.3f}\")\n",
                "print()\n",
                "\n",
                "# Calculate P@K and R@K for single query\n",
                "y_true_single = [0, 2, 4]  # Relevant\n",
                "y_pred_single = [0, 1, 2, 3, 4]  # Ranked\n",
                "\n",
                "for k in [1, 3, 5]:\n",
                "    p_k = precision_at_k(y_true_single, y_pred_single, k)\n",
                "    r_k = recall_at_k(y_true_single, y_pred_single, k)\n",
                "    print(f\"P@{k}: {p_k:.3f}, R@{k}: {r_k:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# nDCG Example\n",
                "y_true_relevance = [3, 2, 1, 0, 0]  # Relevance scores\n",
                "y_pred_perfect = [0, 1, 2, 3, 4]     # Perfect ranking\n",
                "y_pred_imperfect = [2, 0, 1, 3, 4]   # Imperfect ranking\n",
                "\n",
                "ndcg_perfect = ndcg_at_k(y_true_relevance, y_pred_perfect, k=5)\n",
                "ndcg_imperfect = ndcg_at_k(y_true_relevance, y_pred_imperfect, k=5)\n",
                "\n",
                "print(f\"nDCG@5 (Perfect ranking):   {ndcg_perfect:.3f}\")\n",
                "print(f\"nDCG@5 (Imperfect ranking): {ndcg_imperfect:.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. RAG Evaluation\n",
                "\n",
                "### 4.1 RAG Quality Dimensions\n",
                "\n",
                "| Dimension | Question | How to Measure |\n",
                "|-----------|----------|----------------|\n",
                "| **Faithfulness** | Is answer grounded in context? | Check claims vs context |\n",
                "| **Relevance** | Does answer address the question? | Semantic similarity |\n",
                "| **Context Precision** | Was retrieved context useful? | % context actually used |\n",
                "| **Context Recall** | Did we find all needed info? | Coverage of ground truth |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RAG Evaluation\n",
                "rag_evaluator = RAGEvaluator()\n",
                "\n",
                "# Test data\n",
                "questions = [\n",
                "    \"What is machine learning?\",\n",
                "    \"How does gradient descent work?\"\n",
                "]\n",
                "\n",
                "answers = [\n",
                "    \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
                "    \"Gradient descent optimizes by moving in the direction of steepest descent.\"\n",
                "]\n",
                "\n",
                "contexts = [\n",
                "    [\"Machine learning is a branch of artificial intelligence that uses data to learn.\",\n",
                "     \"ML algorithms improve through experience.\"],\n",
                "    [\"Gradient descent is an optimization algorithm.\",\n",
                "     \"It iteratively adjusts parameters to minimize loss.\"]\n",
                "]\n",
                "\n",
                "ground_truths = [\n",
                "    \"Machine learning is AI that learns patterns from data.\",\n",
                "    \"Gradient descent minimizes loss by following the negative gradient.\"\n",
                "]\n",
                "\n",
                "# Evaluate\n",
                "results = rag_evaluator.evaluate(questions, answers, contexts, ground_truths)\n",
                "\n",
                "print(\"RAG System Evaluation\")\n",
                "print(\"=\" * 40)\n",
                "for metric, value in results.items():\n",
                "    if isinstance(value, float):\n",
                "        print(f\"{metric:20s}: {value:.3f}\")\n",
                "    else:\n",
                "        print(f\"{metric:20s}: {value}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Individual Metric Deep Dive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Examine individual metrics\n",
                "question = \"What is deep learning?\"\n",
                "answer = \"Deep learning uses neural networks with many layers to learn patterns.\"\n",
                "context = \"Deep learning is a subset of ML using multi-layer neural networks.\"\n",
                "\n",
                "# Faithfulness: Is answer grounded in context?\n",
                "faithfulness = rag_evaluator.evaluate_faithfulness(answer, [context])\n",
                "print(f\"Faithfulness: {faithfulness:.3f}\")\n",
                "\n",
                "# Relevance: Does answer address the question?\n",
                "relevance = rag_evaluator.evaluate_relevance(question, answer)\n",
                "print(f\"Relevance: {relevance:.3f}\")\n",
                "\n",
                "print(\"\\nðŸ’¡ Interpretation:\")\n",
                "print(f\"  - Faithfulness {faithfulness:.0%}: Answer is {'well' if faithfulness > 0.7 else 'partially'} grounded\")\n",
                "print(f\"  - Relevance {relevance:.0%}: Answer {'addresses' if relevance > 0.7 else 'partially addresses'} the question\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. A/B Testing\n",
                "\n",
                "### 5.1 Statistical Significance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import stats\n",
                "\n",
                "# Simulate A/B test results\n",
                "np.random.seed(42)\n",
                "\n",
                "# Model A: baseline CTR = 2%\n",
                "# Model B: improved CTR = 2.2%\n",
                "n_samples = 10000\n",
                "ctr_a = 0.020\n",
                "ctr_b = 0.022\n",
                "\n",
                "clicks_a = np.random.binomial(1, ctr_a, n_samples)\n",
                "clicks_b = np.random.binomial(1, ctr_b, n_samples)\n",
                "\n",
                "observed_ctr_a = clicks_a.mean()\n",
                "observed_ctr_b = clicks_b.mean()\n",
                "\n",
                "# Two-proportion z-test\n",
                "pooled_ctr = (clicks_a.sum() + clicks_b.sum()) / (2 * n_samples)\n",
                "se = np.sqrt(pooled_ctr * (1 - pooled_ctr) * (2 / n_samples))\n",
                "z_score = (observed_ctr_b - observed_ctr_a) / se\n",
                "p_value = 1 - stats.norm.cdf(z_score)  # One-sided test\n",
                "\n",
                "print(\"A/B Test Analysis\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Model A CTR: {observed_ctr_a:.4f}\")\n",
                "print(f\"Model B CTR: {observed_ctr_b:.4f}\")\n",
                "print(f\"Lift: {(observed_ctr_b - observed_ctr_a) / observed_ctr_a * 100:.1f}%\")\n",
                "print(f\"\\nZ-score: {z_score:.3f}\")\n",
                "print(f\"P-value: {p_value:.4f}\")\n",
                "print(f\"\\nResult: {'Significant' if p_value < 0.05 else 'Not significant'} at Î±=0.05\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Building an Evaluation Pipeline\n",
                "\n",
                "### 6.1 Complete Evaluation Workflow"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Complete evaluation pipeline\n",
                "ml_evaluator = MLEvaluator()\n",
                "\n",
                "# Classification evaluation\n",
                "clf_results = ml_evaluator.evaluate_classification(\n",
                "    y_true=y_true,\n",
                "    y_pred=y_pred,\n",
                "    average=\"macro\"\n",
                ")\n",
                "\n",
                "# Regression evaluation\n",
                "reg_results = ml_evaluator.evaluate_regression(\n",
                "    y_true=y_true_reg,\n",
                "    y_pred=y_pred_reg\n",
                ")\n",
                "\n",
                "# Create report\n",
                "report = EvaluationReport(title=\"Model Evaluation Report\")\n",
                "report.add_metrics(\"Classification\", clf_results)\n",
                "report.add_metrics(\"Regression\", reg_results)\n",
                "report.add_metrics(\"RAG\", results)\n",
                "\n",
                "# Display report\n",
                "print(report.to_markdown())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ðŸ“ Summary\n",
                "\n",
                "### Choosing the Right Metrics\n",
                "\n",
                "| Task | Primary Metrics | Consider |\n",
                "|------|----------------|----------|\n",
                "| Binary Classification | F1, AUC | Class imbalance |\n",
                "| Multi-class | Macro F1 | Per-class performance |\n",
                "| Regression | RMSE, RÂ² | Outlier sensitivity |\n",
                "| Ranking | nDCG, MRR | Position importance |\n",
                "| RAG | Faithfulness, Relevance | Context quality |\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. **Use multiple metrics** - No single metric tells whole story\n",
                "2. **Match metrics to business goals** - Choose based on costs of errors\n",
                "3. **Evaluate on held-out data** - Avoid data leakage\n",
                "4. **Monitor over time** - Watch for drift\n",
                "5. **A/B test in production** - Validate real-world impact"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}