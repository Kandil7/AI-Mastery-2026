{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üîç Week 6: Dense Retrieval Systems\n",
                "\n",
                "This notebook covers building production-ready retrieval systems.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Retrieval Fundamentals](#1-retrieval-fundamentals)\n",
                "2. [BM25 (Sparse Retrieval)](#2-bm25-sparse-retrieval)\n",
                "3. [Dense Retrieval](#3-dense-retrieval)\n",
                "4. [Hybrid Retrieval](#4-hybrid-retrieval)\n",
                "5. [Vector Databases](#5-vector-databases)\n",
                "6. [Building a Search System](#6-building-a-search-system)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Setup complete!\n"
                    ]
                }
            ],
            "source": [
                "# Setup\n",
                "import sys\n",
                "sys.path.insert(0, '../..')\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "from src.retrieval import (\n",
                "    BM25Retriever,\n",
                "    DenseRetriever,\n",
                "    HybridRetriever,\n",
                "    RetrievalPipeline,\n",
                ")\n",
                "from src.retrieval.retrieval import Document, TextPreprocessor\n",
                "\n",
                "print(\"‚úÖ Setup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Retrieval Fundamentals\n",
                "\n",
                "### 1.1 The Information Retrieval Problem\n",
                "\n",
                "**Goal:** Given a query, find the most relevant documents from a corpus.\n",
                "\n",
                "```\n",
                "Query: \"How do neural networks learn?\"\n",
                "         ‚Üì\n",
                "    [Retrieval System]\n",
                "         ‚Üì\n",
                "Ranked Results:\n",
                "  1. \"Neural networks learn through backpropagation...\"\n",
                "  2. \"Learning in deep networks involves...\"\n",
                "  3. \"Training neural models requires...\"\n",
                "```\n",
                "\n",
                "### 1.2 Retrieval Methods Comparison\n",
                "\n",
                "| Method | Representation | Pros | Cons |\n",
                "|--------|---------------|------|------|\n",
                "| **BM25** | Sparse (term freq) | Fast, interpretable | No semantics |\n",
                "| **Dense** | Dense vectors | Captures meaning | Needs embeddings |\n",
                "| **Hybrid** | Both | Best of both | More complex |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. BM25 (Sparse Retrieval)\n",
                "\n",
                "### 2.1 BM25 Formula\n",
                "\n",
                "$$score(D, Q) = \\sum_{i=1}^{n} IDF(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot (1 - b + b \\cdot \\frac{|D|}{avgdl})}$$\n",
                "\n",
                "Where:\n",
                "- $f(q_i, D)$ = frequency of term $q_i$ in document $D$\n",
                "- $|D|$ = document length\n",
                "- $avgdl$ = average document length\n",
                "- $k_1, b$ = tuning parameters (typically 1.5, 0.75)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.retrieval.retrieval:Initialized BM25Retriever with k1=1.5, b=0.75\n",
                        "INFO:src.retrieval.retrieval:Indexed 6 documents with 51 unique terms\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Query: 'How do neural networks learn from data?'\n",
                        "\n",
                        "BM25 Results:\n",
                        "============================================================\n",
                        "\n",
                        "[Score: 3.778] Doc 2\n",
                        "  Deep learning uses neural networks with many layers to learn representations of ...\n",
                        "\n",
                        "[Score: 3.212] Doc 6\n",
                        "  Neural networks are inspired by biological neural networks in the human brain....\n",
                        "\n",
                        "[Score: 1.889] Doc 1\n",
                        "  Machine learning is a subset of artificial intelligence that enables computers t...\n"
                    ]
                }
            ],
            "source": [
                "# Create sample documents\n",
                "documents = [\n",
                "    Document(id=\"1\", content=\"Machine learning is a subset of artificial intelligence that enables computers to learn from data without being explicitly programmed.\"),\n",
                "    Document(id=\"2\", content=\"Deep learning uses neural networks with many layers to learn representations of data with multiple levels of abstraction.\"),\n",
                "    Document(id=\"3\", content=\"Natural language processing helps computers understand, interpret, and generate human language in useful ways.\"),\n",
                "    Document(id=\"4\", content=\"Computer vision enables machines to interpret and understand visual information from the world.\"),\n",
                "    Document(id=\"5\", content=\"Reinforcement learning trains agents to make sequences of decisions by rewarding desired behaviors.\"),\n",
                "    Document(id=\"6\", content=\"Neural networks are inspired by biological neural networks in the human brain.\"),\n",
                "]\n",
                "\n",
                "# Initialize BM25\n",
                "bm25 = BM25Retriever(k1=1.5, b=0.75)\n",
                "bm25.index(documents)\n",
                "\n",
                "# Search\n",
                "query = \"How do neural networks learn from data?\"\n",
                "results = bm25.retrieve(query, top_k=3)\n",
                "\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"BM25 Results:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for r in results:\n",
                "    print(f\"\\n[Score: {r.score:.3f}] Doc {r.document.id}\")\n",
                "    print(f\"  {r.document.content[:80]}...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Score Breakdown for Doc 1:\n",
                        "----------------------------------------\n",
                        "  learn          : 0.9446\n",
                        "  data           : 0.9446\n"
                    ]
                }
            ],
            "source": [
                "# Explain BM25 Score\n",
                "contributions = bm25.explain_score(query, 0)\n",
                "\n",
                "print(\"Score Breakdown for Doc 1:\")\n",
                "print(\"-\" * 40)\n",
                "for term, score in sorted(contributions.items(), key=lambda x: -x[1]):\n",
                "    if score > 0:\n",
                "        print(f\"  {term:15s}: {score:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Dense Retrieval\n",
                "\n",
                "### 3.1 How Dense Retrieval Works\n",
                "\n",
                "```\n",
                "Query: \"How do neural networks learn?\"\n",
                "         ‚Üì\n",
                "   [Encoder Model]\n",
                "         ‚Üì\n",
                "   Query Vector: [0.2, -0.1, 0.5, ...]\n",
                "         ‚Üì\n",
                "   Compare with Document Vectors (cosine similarity)\n",
                "         ‚Üì\n",
                "   Ranked Results\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "INFO:src.retrieval.retrieval:Initialized DenseRetriever with model: all-MiniLM-L6-v2\n",
                        "WARNING:src.retrieval.retrieval:sentence-transformers not installed, using TF-IDF fallback\n",
                        "INFO:src.retrieval.retrieval:Indexed 6 documents with dense embeddings\n"
                    ]
                },
                {
                    "ename": "ValueError",
                    "evalue": "shapes (6,62) and (7,) not aligned: 62 (dim 1) != 7 (dim 0)",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m dense.index(documents)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Search\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results = \u001b[43mdense\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDense Retrieval Results:\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32mk:\\learning\\technical\\ai-ml\\AI-Mastery-2026\\research\\week6-retrieval\\../..\\src\\retrieval\\retrieval.py:585\u001b[39m, in \u001b[36mDenseRetriever.retrieve\u001b[39m\u001b[34m(self, query, top_k)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    583\u001b[39m     \u001b[38;5;66;03m# Brute-force search\u001b[39;00m\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.similarity == \u001b[33m\"\u001b[39m\u001b[33mcosine\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m585\u001b[39m         scores = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    586\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    587\u001b[39m         \u001b[38;5;66;03m# Negative L2 distance (higher is better)\u001b[39;00m\n\u001b[32m    588\u001b[39m         scores = -np.linalg.norm(\u001b[38;5;28mself\u001b[39m.embeddings - query_embedding, axis=\u001b[32m1\u001b[39m)\n",
                        "\u001b[31mValueError\u001b[39m: shapes (6,62) and (7,) not aligned: 62 (dim 1) != 7 (dim 0)"
                    ]
                },
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
                        "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
                        "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
                        "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
                    ]
                }
            ],
            "source": [
                "# Initialize Dense Retriever\n",
                "dense = DenseRetriever(model_name=\"all-MiniLM-L6-v2\", use_faiss=False)\n",
                "dense.index(documents)\n",
                "\n",
                "# Search\n",
                "results = dense.retrieve(query, top_k=3)\n",
                "\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Dense Retrieval Results:\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for r in results:\n",
                "    print(f\"\\n[Score: {r.score:.3f}] Doc {r.document.id}\")\n",
                "    print(f\"  {r.document.content[:80]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 BM25 vs Dense: When to Use What\n",
                "\n",
                "| Scenario | BM25 | Dense |\n",
                "|----------|------|-------|\n",
                "| **Exact keyword match** | ‚úÖ Better | May miss |\n",
                "| **Semantic similarity** | ‚ùå Limited | ‚úÖ Better |\n",
                "| **Speed (no GPU)** | ‚úÖ Faster | Slower |\n",
                "| **Out-of-vocabulary** | ‚ùå Fails | ‚úÖ Can handle |\n",
                "| **Interpretability** | ‚úÖ Clear | ‚ùå Black box |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare on semantic query\n",
                "semantic_query = \"AI that understands text and language\"\n",
                "\n",
                "print(f\"Query: '{semantic_query}'\\n\")\n",
                "\n",
                "bm25_results = bm25.retrieve(semantic_query, top_k=2)\n",
                "dense_results = dense.retrieve(semantic_query, top_k=2)\n",
                "\n",
                "print(\"BM25 Top Result:\")\n",
                "print(f\"  {bm25_results[0].document.content[:60]}...\")\n",
                "\n",
                "print(\"\\nDense Top Result:\")\n",
                "print(f\"  {dense_results[0].document.content[:60]}...\")\n",
                "\n",
                "print(\"\\nüí° Dense retrieval better captures 'NLP' as relevant to 'understands text'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Hybrid Retrieval\n",
                "\n",
                "### 4.1 Combining Sparse and Dense\n",
                "\n",
                "Hybrid retrieval combines both methods:\n",
                "1. Get candidates from BM25 and Dense\n",
                "2. Combine rankings using RRF or weighted fusion"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hybrid Retriever\n",
                "hybrid = HybridRetriever(alpha=0.5, fusion=\"rrf\")\n",
                "hybrid.index(documents)\n",
                "\n",
                "# Compare all three\n",
                "test_queries = [\n",
                "    \"machine learning algorithms\",\n",
                "    \"understanding human language\",\n",
                "    \"neural network brain\"\n",
                "]\n",
                "\n",
                "print(\"Retrieval Comparison\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "for q in test_queries:\n",
                "    bm25_top = bm25.retrieve(q, top_k=1)[0].document.id\n",
                "    dense_top = dense.retrieve(q, top_k=1)[0].document.id\n",
                "    hybrid_top = hybrid.retrieve(q, top_k=1)[0].document.id\n",
                "    \n",
                "    print(f\"\\nQuery: '{q}'\")\n",
                "    print(f\"  BM25: Doc {bm25_top} | Dense: Doc {dense_top} | Hybrid: Doc {hybrid_top}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Vector Databases\n",
                "\n",
                "### 5.1 Why Vector Databases?\n",
                "\n",
                "For production, you need efficient similarity search at scale:\n",
                "\n",
                "| Database | Type | Key Features |\n",
                "|----------|------|-------------|\n",
                "| **FAISS** | Library | Facebook, very fast, GPU support |\n",
                "| **Pinecone** | Managed | Serverless, easy to use |\n",
                "| **Milvus** | Open source | Distributed, scalable |\n",
                "| **Qdrant** | Open source | Modern, filtering support |\n",
                "| **ChromaDB** | Open source | Simple, good for prototyping |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Simple Vector Store\n",
                "class SimpleVectorStore:\n",
                "    \"\"\"Basic in-memory vector store for demonstration.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.documents = []\n",
                "        self.embeddings = None\n",
                "    \n",
                "    def add(self, documents, embeddings):\n",
                "        \"\"\"Add documents with embeddings.\"\"\"\n",
                "        self.documents.extend(documents)\n",
                "        if self.embeddings is None:\n",
                "            self.embeddings = embeddings\n",
                "        else:\n",
                "            self.embeddings = np.vstack([self.embeddings, embeddings])\n",
                "    \n",
                "    def search(self, query_embedding, top_k=5):\n",
                "        \"\"\"Search by embedding similarity.\"\"\"\n",
                "        # Cosine similarity\n",
                "        similarities = np.dot(self.embeddings, query_embedding)\n",
                "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
                "        \n",
                "        return [\n",
                "            (self.documents[i], similarities[i])\n",
                "            for i in top_indices\n",
                "        ]\n",
                "\n",
                "print(\"‚úÖ Vector store pattern demonstrated!\")\n",
                "print(\"\\nIn production, use FAISS, Pinecone, or similar for efficient search.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Building a Search System\n",
                "\n",
                "### 6.1 Complete Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SearchSystem:\n",
                "    \"\"\"Complete search system with hybrid retrieval.\"\"\"\n",
                "    \n",
                "    def __init__(self, use_hybrid=True):\n",
                "        if use_hybrid:\n",
                "            self.retriever = HybridRetriever(alpha=0.5, fusion=\"rrf\")\n",
                "        else:\n",
                "            self.retriever = DenseRetriever()\n",
                "        \n",
                "        self.documents = []\n",
                "    \n",
                "    def index(self, documents):\n",
                "        \"\"\"Index documents for search.\"\"\"\n",
                "        self.documents = documents\n",
                "        self.retriever.index(documents)\n",
                "        print(f\"Indexed {len(documents)} documents\")\n",
                "    \n",
                "    def search(self, query, top_k=5):\n",
                "        \"\"\"Search for relevant documents.\"\"\"\n",
                "        results = self.retriever.retrieve(query, top_k=top_k)\n",
                "        \n",
                "        return [\n",
                "            {\n",
                "                \"id\": r.document.id,\n",
                "                \"content\": r.document.content,\n",
                "                \"score\": r.score,\n",
                "                \"rank\": r.rank\n",
                "            }\n",
                "            for r in results\n",
                "        ]\n",
                "\n",
                "# Build and use\n",
                "search = SearchSystem(use_hybrid=True)\n",
                "search.index(documents)\n",
                "\n",
                "# Search\n",
                "results = search.search(\"How do machines learn from experience?\", top_k=3)\n",
                "\n",
                "print(\"\\nSearch Results:\")\n",
                "for r in results:\n",
                "    print(f\"  [{r['score']:.3f}] {r['content'][:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "### Key Takeaways\n",
                "\n",
                "1. **BM25** - Great for keyword matching, fast, interpretable\n",
                "2. **Dense Retrieval** - Captures semantics, needs embeddings\n",
                "3. **Hybrid** - Best of both worlds, recommended for production\n",
                "4. **Vector DBs** - Essential for scale (FAISS, Pinecone, etc.)\n",
                "\n",
                "### Production Checklist\n",
                "\n",
                "- [ ] Choose retrieval method based on use case\n",
                "- [ ] Index documents with appropriate chunking\n",
                "- [ ] Use vector database for scale\n",
                "- [ ] Implement caching for efficiency\n",
                "- [ ] Add reranking for improved quality\n",
                "- [ ] Monitor latency and relevance metrics"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
