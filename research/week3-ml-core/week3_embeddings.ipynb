{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Week 3: ML & Embeddings\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Understand Multi-Layer Perceptrons (MLP) from scratch\n",
                "2. Master embeddings: what they are and why they matter\n",
                "3. Generate and visualize text embeddings\n",
                "4. Build an embedding pipeline for downstream tasks\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.manifold import TSNE\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## What is an Embedding?\n",
                "\n",
                "An **embedding** is a learned dense vector representation that captures semantic meaning:\n",
                "\n",
                "- Words â†’ Word2Vec, GloVe (300-dim)\n",
                "- Sentences â†’ Sentence-BERT (384-768 dim)\n",
                "- Images â†’ CNN features (2048 dim)\n",
                "\n",
                "**Key Property**: Similar concepts have similar vectors!\n",
                "\n",
                "## Multi-Layer Perceptron (MLP)\n",
                "\n",
                "$$\\text{Layer}: h = \\sigma(Wx + b)$$\n",
                "\n",
                "Where:\n",
                "- $W$ = weight matrix\n",
                "- $b$ = bias vector\n",
                "- $\\sigma$ = activation function (ReLU, Sigmoid, etc.)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Build MLP from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Layer:\n",
                "    \"\"\"Single dense layer.\"\"\"\n",
                "    def __init__(self, input_dim, output_dim):\n",
                "        # Xavier initialization\n",
                "        self.W = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / input_dim)\n",
                "        self.b = np.zeros(output_dim)\n",
                "        self.input = None\n",
                "        self.output = None\n",
                "    \n",
                "    def forward(self, x):\n",
                "        self.input = x\n",
                "        self.output = x @ self.W + self.b\n",
                "        return self.output\n",
                "    \n",
                "    def backward(self, grad_output, lr=0.01):\n",
                "        # Gradients\n",
                "        grad_W = self.input.T @ grad_output\n",
                "        grad_b = np.sum(grad_output, axis=0)\n",
                "        grad_input = grad_output @ self.W.T\n",
                "        \n",
                "        # Update weights\n",
                "        self.W -= lr * grad_W\n",
                "        self.b -= lr * grad_b\n",
                "        \n",
                "        return grad_input"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ReLU:\n",
                "    \"\"\"ReLU activation function.\"\"\"\n",
                "    def forward(self, x):\n",
                "        self.input = x\n",
                "        return np.maximum(0, x)\n",
                "    \n",
                "    def backward(self, grad_output, lr=None):\n",
                "        return grad_output * (self.input > 0)\n",
                "\n",
                "\n",
                "class Sigmoid:\n",
                "    \"\"\"Sigmoid activation function.\"\"\"\n",
                "    def forward(self, x):\n",
                "        self.output = 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "        return self.output\n",
                "    \n",
                "    def backward(self, grad_output, lr=None):\n",
                "        return grad_output * self.output * (1 - self.output)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MLP:\n",
                "    \"\"\"Multi-Layer Perceptron.\"\"\"\n",
                "    def __init__(self, layer_sizes):\n",
                "        self.layers = []\n",
                "        for i in range(len(layer_sizes) - 1):\n",
                "            self.layers.append(Layer(layer_sizes[i], layer_sizes[i+1]))\n",
                "            if i < len(layer_sizes) - 2:  # No activation on last layer\n",
                "                self.layers.append(ReLU())\n",
                "    \n",
                "    def forward(self, x):\n",
                "        for layer in self.layers:\n",
                "            x = layer.forward(x)\n",
                "        return x\n",
                "    \n",
                "    def backward(self, grad, lr=0.01):\n",
                "        for layer in reversed(self.layers):\n",
                "            grad = layer.backward(grad, lr)\n",
                "    \n",
                "    def get_embeddings(self, x, layer_idx=-2):\n",
                "        \"\"\"Get intermediate layer activations as embeddings.\"\"\"\n",
                "        for i, layer in enumerate(self.layers[:layer_idx]):\n",
                "            x = layer.forward(x)\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train MLP on XOR problem\n",
                "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y = np.array([[0], [1], [1], [0]])\n",
                "\n",
                "# Create MLP: 2 -> 8 -> 4 -> 1\n",
                "mlp = MLP([2, 8, 4, 1])\n",
                "\n",
                "# Training loop\n",
                "losses = []\n",
                "for epoch in range(1000):\n",
                "    # Forward\n",
                "    output = mlp.forward(X)\n",
                "    pred = 1 / (1 + np.exp(-output))  # Sigmoid output\n",
                "    \n",
                "    # Loss (BCE)\n",
                "    loss = -np.mean(y * np.log(pred + 1e-8) + (1 - y) * np.log(1 - pred + 1e-8))\n",
                "    losses.append(loss)\n",
                "    \n",
                "    # Backward\n",
                "    grad = (pred - y) / len(y)\n",
                "    mlp.backward(grad, lr=0.5)\n",
                "\n",
                "print(f\"Final loss: {losses[-1]:.4f}\")\n",
                "print(f\"Predictions: {(pred > 0.5).astype(int).flatten()}\")\n",
                "print(f\"Actual:      {y.flatten()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Understanding Embeddings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple word embeddings (simulated)\n",
                "word_embeddings = {\n",
                "    # Animals\n",
                "    \"cat\": np.array([0.9, 0.1, 0.2, 0.8]),\n",
                "    \"dog\": np.array([0.85, 0.15, 0.25, 0.75]),\n",
                "    \"lion\": np.array([0.8, 0.2, 0.1, 0.9]),\n",
                "    \n",
                "    # Vehicles\n",
                "    \"car\": np.array([0.1, 0.9, 0.8, 0.1]),\n",
                "    \"truck\": np.array([0.15, 0.85, 0.75, 0.15]),\n",
                "    \"bus\": np.array([0.2, 0.8, 0.7, 0.2]),\n",
                "    \n",
                "    # Food\n",
                "    \"apple\": np.array([0.3, 0.3, 0.9, 0.4]),\n",
                "    \"banana\": np.array([0.35, 0.25, 0.85, 0.45]),\n",
                "    \"orange\": np.array([0.4, 0.2, 0.8, 0.5]),\n",
                "}\n",
                "\n",
                "def cosine_sim(a, b):\n",
                "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find similar words\n",
                "def find_similar(word, embeddings, top_k=3):\n",
                "    query = embeddings[word]\n",
                "    similarities = []\n",
                "    for w, vec in embeddings.items():\n",
                "        if w != word:\n",
                "            similarities.append((w, cosine_sim(query, vec)))\n",
                "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
                "    return similarities[:top_k]\n",
                "\n",
                "print(\"Most similar to 'cat':\")\n",
                "for word, sim in find_similar(\"cat\", word_embeddings):\n",
                "    print(f\"  {word}: {sim:.4f}\")\n",
                "\n",
                "print(\"\\nMost similar to 'car':\")\n",
                "for word, sim in find_similar(\"car\", word_embeddings):\n",
                "    print(f\"  {word}: {sim:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Using Real Embeddings (Sentence Transformers)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Note: Uncomment to use real embeddings (requires: pip install sentence-transformers)\n",
                "\n",
                "# from sentence_transformers import SentenceTransformer\n",
                "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "# sentences = [\n",
                "#     \"The cat sat on the mat\",\n",
                "#     \"A dog is playing in the garden\",\n",
                "#     \"Machine learning is fascinating\",\n",
                "#     \"Deep neural networks transform AI\",\n",
                "#     \"The weather is sunny today\"\n",
                "# ]\n",
                "\n",
                "# embeddings = model.encode(sentences)\n",
                "# print(f\"Embedding shape: {embeddings.shape}\")\n",
                "\n",
                "# Simulated embeddings for demo\n",
                "sentences = [\n",
                "    \"The cat sat on the mat\",\n",
                "    \"A dog is playing in the garden\",\n",
                "    \"Machine learning is fascinating\",\n",
                "    \"Deep neural networks transform AI\",\n",
                "    \"The weather is sunny today\"\n",
                "]\n",
                "\n",
                "# Simulated 384-dim embeddings (clustered by topic)\n",
                "np.random.seed(42)\n",
                "embeddings = np.vstack([\n",
                "    np.random.randn(2, 384) + np.array([1, 0, 0] * 128),  # Animals\n",
                "    np.random.randn(2, 384) + np.array([0, 1, 0] * 128),  # Tech\n",
                "    np.random.randn(1, 384) + np.array([0, 0, 1] * 128),  # Weather\n",
                "])\n",
                "\n",
                "print(f\"Sentences: {len(sentences)}\")\n",
                "print(f\"Embedding shape: {embeddings.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Visualizations\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 PCA Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reduce embeddings to 2D using PCA\n",
                "pca = PCA(n_components=2)\n",
                "embeddings_2d = pca.fit_transform(embeddings)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=100, c=['blue', 'blue', 'green', 'green', 'orange'])\n",
                "\n",
                "for i, sentence in enumerate(sentences):\n",
                "    plt.annotate(sentence[:30] + \"...\", \n",
                "                 (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
                "                 fontsize=9, xytext=(5, 5), textcoords='offset points')\n",
                "\n",
                "plt.title('Sentence Embeddings (PCA)')\n",
                "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
                "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 t-SNE Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# t-SNE for non-linear visualization\n",
                "tsne = TSNE(n_components=2, perplexity=2, random_state=42)\n",
                "embeddings_tsne = tsne.fit_transform(embeddings)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "colors = ['blue', 'blue', 'green', 'green', 'orange']\n",
                "plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1], s=100, c=colors)\n",
                "\n",
                "for i, sentence in enumerate(sentences):\n",
                "    plt.annotate(sentence[:25] + \"...\", \n",
                "                 (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
                "                 fontsize=9, xytext=(5, 5), textcoords='offset points')\n",
                "\n",
                "plt.title('Sentence Embeddings (t-SNE)')\n",
                "plt.xlabel('t-SNE 1')\n",
                "plt.ylabel('t-SNE 2')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Training Loss Curve"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(losses)\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "plt.title('MLP Training Loss (XOR Problem)')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test 1: Layer forward pass\n",
                "    layer = Layer(3, 2)\n",
                "    x = np.array([[1, 2, 3]])\n",
                "    output = layer.forward(x)\n",
                "    assert output.shape == (1, 2)\n",
                "    print(\"âœ“ Layer forward pass test passed\")\n",
                "    \n",
                "    # Test 2: ReLU\n",
                "    relu = ReLU()\n",
                "    x = np.array([-1, 0, 1])\n",
                "    output = relu.forward(x)\n",
                "    assert np.array_equal(output, [0, 0, 1])\n",
                "    print(\"âœ“ ReLU test passed\")\n",
                "    \n",
                "    # Test 3: Sigmoid\n",
                "    sigmoid = Sigmoid()\n",
                "    output = sigmoid.forward(np.array([0]))\n",
                "    assert abs(output[0] - 0.5) < 1e-6\n",
                "    print(\"âœ“ Sigmoid test passed\")\n",
                "    \n",
                "    # Test 4: MLP forward\n",
                "    mlp = MLP([2, 4, 1])\n",
                "    x = np.array([[1, 2]])\n",
                "    output = mlp.forward(x)\n",
                "    assert output.shape == (1, 1)\n",
                "    print(\"âœ“ MLP forward test passed\")\n",
                "    \n",
                "    # Test 5: Cosine similarity\n",
                "    a = np.array([1, 0])\n",
                "    b = np.array([1, 0])\n",
                "    assert abs(cosine_sim(a, b) - 1.0) < 1e-6\n",
                "    print(\"âœ“ Cosine similarity test passed\")\n",
                "    \n",
                "    # Test 6: Find similar\n",
                "    results = find_similar(\"cat\", word_embeddings, top_k=1)\n",
                "    assert len(results) == 1\n",
                "    print(\"âœ“ Find similar test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: Interview Prep\n",
                "---\n",
                "\n",
                "## Key Questions\n",
                "\n",
                "### Q1: What are embeddings and why are they useful?\n",
                "\n",
                "**Answer:**\n",
                "- Dense vector representations that capture semantic meaning\n",
                "- Enable similarity comparison between items\n",
                "- Reduce dimensionality (sparse â†’ dense)\n",
                "- Transfer learning: pre-trained embeddings contain knowledge\n",
                "\n",
                "### Q2: Explain backpropagation in simple terms.\n",
                "\n",
                "**Answer:**\n",
                "1. Forward pass: compute predictions\n",
                "2. Calculate loss (error)\n",
                "3. Backward pass: use chain rule to compute gradients\n",
                "4. Update weights: $w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}$\n",
                "\n",
                "### Q3: What is the vanishing gradient problem?\n",
                "\n",
                "**Answer:**\n",
                "- Gradients become very small in deep networks\n",
                "- Early layers learn very slowly\n",
                "- Solutions: ReLU, residual connections, batch normalization\n",
                "\n",
                "### Q4: PCA vs t-SNE for visualization?\n",
                "\n",
                "**Answer:**\n",
                "- **PCA**: Linear, fast, preserves global structure\n",
                "- **t-SNE**: Non-linear, slow, preserves local clusters\n",
                "- Use PCA for initial exploration, t-SNE for clustering visualization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 6: Exercises\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Add Dropout to MLP\n",
                "class Dropout:\n",
                "    \"\"\"Dropout layer for regularization.\"\"\"\n",
                "    def __init__(self, rate=0.5):\n",
                "        self.rate = rate\n",
                "        self.mask = None\n",
                "    \n",
                "    def forward(self, x, training=True):\n",
                "        # TODO: Implement dropout\n",
                "        pass\n",
                "    \n",
                "    def backward(self, grad_output, lr=None):\n",
                "        # TODO: Implement backward pass\n",
                "        pass\n",
                "\n",
                "\n",
                "# Exercise 2: Implement Batch Normalization\n",
                "class BatchNorm:\n",
                "    \"\"\"Batch normalization layer.\"\"\"\n",
                "    def __init__(self, dim):\n",
                "        # TODO: Initialize parameters\n",
                "        pass\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # TODO: Implement forward pass\n",
                "        pass\n",
                "\n",
                "\n",
                "# Exercise 3: Create an embedding lookup table\n",
                "class EmbeddingLayer:\n",
                "    \"\"\"Learnable embedding lookup table.\"\"\"\n",
                "    def __init__(self, vocab_size, embed_dim):\n",
                "        # TODO: Initialize embedding matrix\n",
                "        pass\n",
                "    \n",
                "    def forward(self, indices):\n",
                "        # TODO: Look up embeddings\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 7: Deliverable\n",
                "---\n",
                "\n",
                "## What You Built:\n",
                "\n",
                "1. **MLP from scratch** - Forward/backward propagation\n",
                "2. **Embedding utilities** - Similarity search\n",
                "3. **Visualization tools** - PCA/t-SNE plots\n",
                "\n",
                "## Key Takeaways:\n",
                "\n",
                "- Neural networks learn through gradient descent\n",
                "- Embeddings capture semantic relationships\n",
                "- Intermediate layers are feature extractors\n",
                "\n",
                "## Next Week: Transformers & Tokenization\n",
                "- Attention mechanism\n",
                "- Tokenization strategies\n",
                "- BERT, GPT architecture overview"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}