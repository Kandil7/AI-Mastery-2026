{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 3: Mathematical Foundations for Machine Learning\n",
                "\n",
                "From PCA to Gradient Descent with real-world examples from Amazon, Netflix, and Google.\n",
                "\n",
                "## Topics Covered\n",
                "1. Linear Algebra: Eigendecomposition, SVD\n",
                "2. PCA: Theory and Implementation\n",
                "3. Gradient Descent: Optimization\n",
                "4. Naive Bayes: Probabilistic Classification\n",
                "5. EM Algorithm: Gaussian Mixture Models\n",
                "6. SVM: Kernel Trick"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Eigendecomposition\n",
                "\n",
                "For a square matrix A, eigenvector **v** and eigenvalue **λ** satisfy:\n",
                "\n",
                "$$Av = \\lambda v$$\n",
                "\n",
                "**Intuition**: Eigenvectors are directions that only get scaled, not rotated."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Eigendecomposition from scratch\n",
                "def power_iteration(A, num_iters=100):\n",
                "    \"\"\"Find dominant eigenvector using power iteration.\"\"\"\n",
                "    n = A.shape[0]\n",
                "    v = np.random.randn(n)\n",
                "    v = v / np.linalg.norm(v)\n",
                "    \n",
                "    for _ in range(num_iters):\n",
                "        Av = A @ v\n",
                "        v = Av / np.linalg.norm(Av)\n",
                "    \n",
                "    eigenvalue = (v @ A @ v) / (v @ v)\n",
                "    return eigenvalue, v\n",
                "\n",
                "A = np.array([[2., 1.], [1., 2.]])\n",
                "val, vec = power_iteration(A)\n",
                "print(f'Dominant eigenvalue: {val:.4f}')\n",
                "print(f'Eigenvector: {vec}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Singular Value Decomposition (SVD)\n",
                "\n",
                "Any matrix A can be decomposed:\n",
                "\n",
                "$$A = U \\Sigma V^T$$\n",
                "\n",
                "**Netflix Application**: Decompose user-movie ratings into latent factors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SVD for Movie Recommendations (Netflix-style)\n",
                "# Ratings matrix: users x movies (? = missing)\n",
                "R = np.array([\n",
                "    [5, 3, 0, 1],\n",
                "    [4, 0, 0, 1],\n",
                "    [1, 1, 0, 5],\n",
                "    [1, 0, 0, 4],\n",
                "    [0, 1, 5, 4]\n",
                "], dtype=float)\n",
                "\n",
                "# Fill missing with mean for SVD\n",
                "R_filled = R.copy()\n",
                "R_filled[R_filled == 0] = R[R != 0].mean()\n",
                "\n",
                "# SVD decomposition\n",
                "U, S, Vt = np.linalg.svd(R_filled, full_matrices=False)\n",
                "\n",
                "# Low-rank approximation (k=2 latent factors)\n",
                "k = 2\n",
                "R_approx = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]\n",
                "\n",
                "print('Original ratings (0=missing):')\n",
                "print(R.astype(int))\n",
                "print('\\nPredicted ratings:')\n",
                "print(np.round(R_approx, 1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Principal Component Analysis (PCA)\n",
                "\n",
                "**Goal**: Find orthogonal directions of maximum variance.\n",
                "\n",
                "**Steps**:\n",
                "1. Center data: $X_c = X - \\mu$\n",
                "2. Covariance: $C = \\frac{1}{n} X_c^T X_c$\n",
                "3. Eigendecompose C → principal components"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PCAFromScratch:\n",
                "    \"\"\"PCA implementation from scratch.\"\"\"\n",
                "    \n",
                "    def __init__(self, n_components):\n",
                "        self.n_components = n_components\n",
                "        self.components_ = None\n",
                "        self.explained_variance_ = None\n",
                "    \n",
                "    def fit(self, X):\n",
                "        # Center data\n",
                "        self.mean_ = X.mean(axis=0)\n",
                "        X_centered = X - self.mean_\n",
                "        \n",
                "        # Covariance matrix\n",
                "        cov = X_centered.T @ X_centered / (len(X) - 1)\n",
                "        \n",
                "        # Eigendecomposition\n",
                "        eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
                "        \n",
                "        # Sort by eigenvalue (descending)\n",
                "        idx = eigenvalues.argsort()[::-1]\n",
                "        eigenvalues = eigenvalues[idx]\n",
                "        eigenvectors = eigenvectors[:, idx]\n",
                "        \n",
                "        self.components_ = eigenvectors[:, :self.n_components].T\n",
                "        self.explained_variance_ = eigenvalues[:self.n_components]\n",
                "        return self\n",
                "    \n",
                "    def transform(self, X):\n",
                "        X_centered = X - self.mean_\n",
                "        return X_centered @ self.components_.T\n",
                "\n",
                "# Example: Reduce 4D to 2D\n",
                "X = np.random.randn(100, 4)\n",
                "pca = PCAFromScratch(n_components=2).fit(X)\n",
                "X_reduced = pca.transform(X)\n",
                "print(f'Original shape: {X.shape} → Reduced: {X_reduced.shape}')\n",
                "print(f'Explained variance: {pca.explained_variance_}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Gradient Descent\n",
                "\n",
                "**Update Rule**:\n",
                "$$\\theta_{new} = \\theta_{old} - \\alpha \\nabla L(\\theta)$$\n",
                "\n",
                "**Intuition**: Walk downhill on the loss surface."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradient_descent(f, grad_f, x0, lr=0.1, n_iters=50):\n",
                "    \"\"\"General gradient descent.\n",
                "    \n",
                "    Args:\n",
                "        f: Function to minimize\n",
                "        grad_f: Gradient of f\n",
                "        x0: Starting point\n",
                "        lr: Learning rate (α)\n",
                "        n_iters: Number of iterations\n",
                "    \"\"\"\n",
                "    x = x0.copy()\n",
                "    history = [x.copy()]\n",
                "    \n",
                "    for _ in range(n_iters):\n",
                "        grad = grad_f(x)\n",
                "        x = x - lr * grad  # THE UPDATE RULE\n",
                "        history.append(x.copy())\n",
                "    \n",
                "    return x, history\n",
                "\n",
                "# Example: Minimize f(x,y) = x² + y²\n",
                "f = lambda x: x[0]**2 + x[1]**2\n",
                "grad_f = lambda x: np.array([2*x[0], 2*x[1]])\n",
                "\n",
                "x_min, hist = gradient_descent(f, grad_f, np.array([3., 4.]))\n",
                "print(f'Minimum found at: {x_min}')\n",
                "print(f'f(minimum) = {f(x_min):.6f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Naive Bayes Classifier (Google Spam Filter)\n",
                "\n",
                "**Bayes' Theorem**:\n",
                "$$P(spam|words) = \\frac{P(words|spam) \\cdot P(spam)}{P(words)}$$\n",
                "\n",
                "**Naive Assumption**: Words are independent given class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NaiveBayesSpam:\n",
                "    \"\"\"Naive Bayes spam classifier (simplified).\"\"\"\n",
                "    \n",
                "    def __init__(self, alpha=1.0):\n",
                "        self.alpha = alpha  # Laplace smoothing\n",
                "        self.word_probs = {'spam': {}, 'ham': {}}\n",
                "        self.class_probs = {}\n",
                "    \n",
                "    def fit(self, emails, labels):\n",
                "        spam_count = sum(labels)\n",
                "        ham_count = len(labels) - spam_count\n",
                "        \n",
                "        self.class_probs = {\n",
                "            'spam': spam_count / len(labels),\n",
                "            'ham': ham_count / len(labels)\n",
                "        }\n",
                "        \n",
                "        # Count words\n",
                "        vocab = set()\n",
                "        for email in emails:\n",
                "            vocab.update(email.lower().split())\n",
                "        \n",
                "        spam_words = []\n",
                "        ham_words = []\n",
                "        for email, label in zip(emails, labels):\n",
                "            words = email.lower().split()\n",
                "            if label:\n",
                "                spam_words.extend(words)\n",
                "            else:\n",
                "                ham_words.extend(words)\n",
                "        \n",
                "        # Laplace-smoothed probabilities\n",
                "        for word in vocab:\n",
                "            self.word_probs['spam'][word] = (\n",
                "                spam_words.count(word) + self.alpha\n",
                "            ) / (len(spam_words) + self.alpha * len(vocab))\n",
                "            self.word_probs['ham'][word] = (\n",
                "                ham_words.count(word) + self.alpha\n",
                "            ) / (len(ham_words) + self.alpha * len(vocab))\n",
                "    \n",
                "    def predict(self, email):\n",
                "        words = email.lower().split()\n",
                "        log_spam = np.log(self.class_probs['spam'])\n",
                "        log_ham = np.log(self.class_probs['ham'])\n",
                "        \n",
                "        for word in words:\n",
                "            if word in self.word_probs['spam']:\n",
                "                log_spam += np.log(self.word_probs['spam'][word])\n",
                "                log_ham += np.log(self.word_probs['ham'][word])\n",
                "        \n",
                "        return 'spam' if log_spam > log_ham else 'ham'\n",
                "\n",
                "# Training data\n",
                "emails = [\n",
                "    'free money click now', 'winner lottery claim prize',\n",
                "    'meeting tomorrow morning', 'project report attached',\n",
                "    'urgent claim your free gift', 'lunch plans today'\n",
                "]\n",
                "labels = [1, 1, 0, 0, 1, 0]  # 1=spam, 0=ham\n",
                "\n",
                "clf = NaiveBayesSpam()\n",
                "clf.fit(emails, labels)\n",
                "\n",
                "print(clf.predict('free prize winner'))       # spam\n",
                "print(clf.predict('meeting scheduled today'))  # ham"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. EM Algorithm for GMMs\n",
                "\n",
                "**E-Step**: Compute responsibilities (soft assignments)\n",
                "$$r_{nk} = \\frac{\\pi_k \\mathcal{N}(x_n|\\mu_k, \\Sigma_k)}{\\sum_j \\pi_j \\mathcal{N}(x_n|\\mu_j, \\Sigma_j)}$$\n",
                "\n",
                "**M-Step**: Update parameters using responsibilities"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gaussian_pdf(x, mu, sigma):\n",
                "    \"\"\"Univariate Gaussian PDF.\"\"\"\n",
                "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2) / (sigma * np.sqrt(2 * np.pi))\n",
                "\n",
                "def em_gmm_1d(X, K=2, n_iters=20):\n",
                "    \"\"\"EM for 1D Gaussian Mixture Model.\"\"\"\n",
                "    n = len(X)\n",
                "    \n",
                "    # Initialize\n",
                "    mus = np.random.choice(X, K)\n",
                "    sigmas = np.ones(K)\n",
                "    pis = np.ones(K) / K\n",
                "    \n",
                "    for _ in range(n_iters):\n",
                "        # E-STEP: Compute responsibilities\n",
                "        resp = np.zeros((n, K))\n",
                "        for k in range(K):\n",
                "            resp[:, k] = pis[k] * gaussian_pdf(X, mus[k], sigmas[k])\n",
                "        resp /= resp.sum(axis=1, keepdims=True)\n",
                "        \n",
                "        # M-STEP: Update parameters\n",
                "        Nk = resp.sum(axis=0)\n",
                "        for k in range(K):\n",
                "            mus[k] = (resp[:, k] @ X) / Nk[k]\n",
                "            sigmas[k] = np.sqrt((resp[:, k] @ (X - mus[k])**2) / Nk[k])\n",
                "            pis[k] = Nk[k] / n\n",
                "    \n",
                "    return mus, sigmas, pis\n",
                "\n",
                "# Generate bimodal data\n",
                "X = np.concatenate([np.random.normal(-2, 0.5, 50),\n",
                "                    np.random.normal(2, 0.8, 50)])\n",
                "\n",
                "mus, sigmas, pis = em_gmm_1d(X, K=2)\n",
                "print(f'Found means: {mus}')\n",
                "print(f'Found stds: {sigmas}')\n",
                "print(f'Mixture weights: {pis}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. SVM with Kernel Trick\n",
                "\n",
                "**Kernel**: Implicit inner product in high-dimensional space\n",
                "\n",
                "$$k(x, z) = \\phi(x)^T \\phi(z)$$\n",
                "\n",
                "**RBF Kernel**: $k(x, z) = \\exp(-\\gamma ||x-z||^2)$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def rbf_kernel(X1, X2, gamma=1.0):\n",
                "    \"\"\"RBF (Gaussian) kernel.\n",
                "    \n",
                "    The kernel trick allows linear algorithms to\n",
                "    work in infinite-dimensional feature spaces!\n",
                "    \"\"\"\n",
                "    # Compute pairwise squared distances\n",
                "    sq_dist = np.sum(X1**2, axis=1).reshape(-1, 1) + \\\n",
                "              np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n",
                "    return np.exp(-gamma * sq_dist)\n",
                "\n",
                "# Example: XOR problem (not linearly separable)\n",
                "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y = np.array([0, 1, 1, 0])  # XOR\n",
                "\n",
                "# Kernel matrix\n",
                "K = rbf_kernel(X, X, gamma=1.0)\n",
                "print('RBF Kernel Matrix (allows non-linear separation):')\n",
                "print(np.round(K, 3))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary: Real-World Applications\n",
                "\n",
                "| Algorithm | Company | Application |\n",
                "|-----------|---------|-------------|\n",
                "| SVD | Netflix | Movie recommendations |\n",
                "| PCA | Amazon | Feature reduction for forecasting |\n",
                "| Naive Bayes | Google | Spam filtering |\n",
                "| Gradient Descent | All | Training neural networks |\n",
                "| EM | Many | Customer segmentation |\n",
                "| SVM | Various | Text classification |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}