{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ—ï¸ Week 0: Full Fundamentals & ML Foundations\n",
                "\n",
                "**Objectives:**\n",
                "1. Mathematical Foundations (Linear Algebra, Calculus, Probability)\n",
                "2. Programming Foundations (Python, NumPy, Pandas)\n",
                "3. ML Preliminaries (Regression, Loss Functions, Training Loops)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "# For reproducibility\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Python & Programming\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 List Comprehensions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic list comprehension\n",
                "squares = [x**2 for x in range(10)]\n",
                "print(f\"Squares: {squares}\")\n",
                "\n",
                "# With condition\n",
                "even_squares = [x**2 for x in range(10) if x % 2 == 0]\n",
                "print(f\"Even squares: {even_squares}\")\n",
                "\n",
                "# Nested comprehension (matrix creation)\n",
                "matrix = [[i * j for j in range(1, 4)] for i in range(1, 4)]\n",
                "print(f\"Matrix:\\n{np.array(matrix)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Generators"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generator function - memory efficient for large datasets\n",
                "def batch_generator(data, batch_size):\n",
                "    \"\"\"Yield batches of data - crucial for ML training.\"\"\"\n",
                "    for i in range(0, len(data), batch_size):\n",
                "        yield data[i:i + batch_size]\n",
                "\n",
                "# Example usage\n",
                "data = list(range(100))\n",
                "for i, batch in enumerate(batch_generator(data, 20)):\n",
                "    print(f\"Batch {i}: {batch[:5]}...\")\n",
                "    if i >= 2:\n",
                "        break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 NumPy Essentials"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Array creation\n",
                "a = np.array([1, 2, 3])\n",
                "b = np.zeros((3, 3))\n",
                "c = np.ones((2, 4))\n",
                "d = np.eye(3)  # Identity matrix\n",
                "e = np.random.randn(3, 3)  # Random normal\n",
                "\n",
                "print(f\"Random matrix:\\n{e}\")\n",
                "print(f\"Shape: {e.shape}, Dtype: {e.dtype}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Broadcasting - key NumPy concept\n",
                "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
                "b = np.array([10, 20, 30])\n",
                "\n",
                "# b is broadcasted across rows\n",
                "result = A + b\n",
                "print(f\"A + b (broadcast):\\n{result}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Pandas Basics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create sample dataframe\n",
                "df = pd.DataFrame({\n",
                "    'feature_1': np.random.randn(100),\n",
                "    'feature_2': np.random.randn(100),\n",
                "    'label': np.random.choice([0, 1], 100)\n",
                "})\n",
                "\n",
                "print(\"DataFrame Info:\")\n",
                "print(df.describe())\n",
                "print(f\"\\nCorrelation:\\n{df.corr()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Linear Algebra\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Vectors & Basic Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Vectors\n",
                "v1 = np.array([1, 2, 3])\n",
                "v2 = np.array([4, 5, 6])\n",
                "\n",
                "# Operations\n",
                "print(f\"v1 + v2 = {v1 + v2}\")\n",
                "print(f\"v1 * 2 = {v1 * 2}\")\n",
                "print(f\"Element-wise: v1 * v2 = {v1 * v2}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Dot Product\n",
                "\n",
                "$\\vec{a} \\cdot \\vec{b} = \\sum_{i=1}^{n} a_i \\cdot b_i = |\\vec{a}| |\\vec{b}| \\cos\\theta$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual dot product\n",
                "def dot_product(a, b):\n",
                "    return sum(x * y for x, y in zip(a, b))\n",
                "\n",
                "# Compare with NumPy\n",
                "manual = dot_product(v1, v2)\n",
                "numpy_dot = np.dot(v1, v2)\n",
                "\n",
                "print(f\"Manual: {manual}\")\n",
                "print(f\"NumPy:  {numpy_dot}\")\n",
                "assert manual == numpy_dot"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Norms (Vector Length)\n",
                "\n",
                "- **L1 Norm:** $||v||_1 = \\sum_i |v_i|$\n",
                "- **L2 Norm:** $||v||_2 = \\sqrt{\\sum_i v_i^2}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "v = np.array([3, 4])\n",
                "\n",
                "# L1 norm (Manhattan distance)\n",
                "l1 = np.linalg.norm(v, ord=1)\n",
                "print(f\"L1 norm: {l1}\")\n",
                "\n",
                "# L2 norm (Euclidean distance)\n",
                "l2 = np.linalg.norm(v, ord=2)\n",
                "print(f\"L2 norm: {l2}\")\n",
                "\n",
                "# Manual L2\n",
                "l2_manual = np.sqrt(sum(x**2 for x in v))\n",
                "print(f\"L2 manual: {l2_manual}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 Cosine Similarity\n",
                "\n",
                "$\\text{sim}(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{||\\vec{a}|| \\cdot ||\\vec{b}||}$\n",
                "\n",
                "**Key for embeddings and semantic search!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def cosine_similarity(a, b):\n",
                "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
                "\n",
                "# Same direction vectors\n",
                "a = np.array([1, 0])\n",
                "b = np.array([1, 0])\n",
                "print(f\"Same direction: {cosine_similarity(a, b):.4f}\")\n",
                "\n",
                "# Perpendicular vectors\n",
                "c = np.array([0, 1])\n",
                "print(f\"Perpendicular: {cosine_similarity(a, c):.4f}\")\n",
                "\n",
                "# Opposite direction\n",
                "d = np.array([-1, 0])\n",
                "print(f\"Opposite: {cosine_similarity(a, d):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.5 Matrix Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4]])\n",
                "B = np.array([[5, 6], [7, 8]])\n",
                "\n",
                "# Matrix multiplication\n",
                "C = A @ B  # or np.matmul(A, B)\n",
                "print(f\"A @ B =\\n{C}\")\n",
                "\n",
                "# Transpose\n",
                "print(f\"\\nA^T =\\n{A.T}\")\n",
                "\n",
                "# Inverse\n",
                "A_inv = np.linalg.inv(A)\n",
                "print(f\"\\nA^-1 =\\n{A_inv}\")\n",
                "\n",
                "# Verify: A @ A^-1 = I\n",
                "print(f\"\\nA @ A^-1 =\\n{A @ A_inv}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.6 Eigenvalues & Eigenvectors (Intuition)\n",
                "\n",
                "$A \\vec{v} = \\lambda \\vec{v}$\n",
                "\n",
                "- **Eigenvector**: Direction that doesn't change under transformation\n",
                "- **Eigenvalue**: How much the vector is scaled"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[4, 2], [1, 3]])\n",
                "\n",
                "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
                "\n",
                "print(f\"Eigenvalues: {eigenvalues}\")\n",
                "print(f\"Eigenvectors:\\n{eigenvectors}\")\n",
                "\n",
                "# Verify: A @ v = lambda * v\n",
                "v = eigenvectors[:, 0]\n",
                "lam = eigenvalues[0]\n",
                "print(f\"\\nVerification:\")\n",
                "print(f\"A @ v = {A @ v}\")\n",
                "print(f\"Î» * v = {lam * v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Calculus\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Derivatives\n",
                "\n",
                "$f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def numerical_derivative(f, x, h=1e-5):\n",
                "    \"\"\"Compute derivative numerically.\"\"\"\n",
                "    return (f(x + h) - f(x - h)) / (2 * h)\n",
                "\n",
                "# Example: f(x) = x^2, f'(x) = 2x\n",
                "f = lambda x: x**2\n",
                "f_prime = lambda x: 2 * x\n",
                "\n",
                "x = 3\n",
                "numerical = numerical_derivative(f, x)\n",
                "analytical = f_prime(x)\n",
                "\n",
                "print(f\"At x={x}:\")\n",
                "print(f\"Numerical derivative:  {numerical:.6f}\")\n",
                "print(f\"Analytical derivative: {analytical}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Partial Derivatives\n",
                "\n",
                "For $f(x, y) = x^2 + xy + y^2$:\n",
                "- $\\frac{\\partial f}{\\partial x} = 2x + y$\n",
                "- $\\frac{\\partial f}{\\partial y} = x + 2y$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f(x, y):\n",
                "    return x**2 + x*y + y**2\n",
                "\n",
                "def partial_x(x, y):\n",
                "    return 2*x + y\n",
                "\n",
                "def partial_y(x, y):\n",
                "    return x + 2*y\n",
                "\n",
                "x, y = 2, 3\n",
                "print(f\"f({x},{y}) = {f(x,y)}\")\n",
                "print(f\"âˆ‚f/âˆ‚x = {partial_x(x,y)}\")\n",
                "print(f\"âˆ‚f/âˆ‚y = {partial_y(x,y)}\")\n",
                "print(f\"Gradient: [{partial_x(x,y)}, {partial_y(x,y)}]\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Gradient Descent\n",
                "\n",
                "$\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta} J(\\theta)$\n",
                "\n",
                "**The core algorithm behind all neural network training!**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradient_descent_1d(f, f_prime, lr=0.1, start=5.0, steps=50):\n",
                "    \"\"\"1D Gradient Descent with history.\"\"\"\n",
                "    x = start\n",
                "    history = [(x, f(x))]\n",
                "    \n",
                "    for _ in range(steps):\n",
                "        x = x - lr * f_prime(x)\n",
                "        history.append((x, f(x)))\n",
                "    \n",
                "    return x, history\n",
                "\n",
                "# Minimize f(x) = x^2 (minimum at x=0)\n",
                "f = lambda x: x**2\n",
                "f_prime = lambda x: 2*x\n",
                "\n",
                "final_x, history = gradient_descent_1d(f, f_prime, lr=0.1, start=5.0)\n",
                "print(f\"Started at x=5.0, converged to x={final_x:.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize gradient descent\n",
                "x_vals = np.linspace(-6, 6, 100)\n",
                "y_vals = x_vals**2\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(x_vals, y_vals, 'b-', label='f(x) = xÂ²')\n",
                "\n",
                "# Plot gradient descent steps\n",
                "hist_x = [h[0] for h in history]\n",
                "hist_y = [h[1] for h in history]\n",
                "plt.scatter(hist_x, hist_y, c=range(len(history)), cmap='Reds', s=50, zorder=5)\n",
                "plt.plot(hist_x, hist_y, 'r--', alpha=0.5, label='GD path')\n",
                "\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('f(x)')\n",
                "plt.title('Gradient Descent Visualization')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Probability & Statistics\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.1 Probability Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "\n",
                "# Normal Distribution\n",
                "x = np.linspace(-4, 4, 100)\n",
                "axes[0, 0].plot(x, stats.norm.pdf(x), 'b-', lw=2)\n",
                "axes[0, 0].fill_between(x, stats.norm.pdf(x), alpha=0.3)\n",
                "axes[0, 0].set_title('Normal Distribution (Î¼=0, Ïƒ=1)')\n",
                "\n",
                "# Uniform Distribution\n",
                "x = np.linspace(-0.5, 1.5, 100)\n",
                "axes[0, 1].plot(x, stats.uniform.pdf(x), 'g-', lw=2)\n",
                "axes[0, 1].fill_between(x, stats.uniform.pdf(x), alpha=0.3)\n",
                "axes[0, 1].set_title('Uniform Distribution [0, 1]')\n",
                "\n",
                "# Binomial Distribution\n",
                "n, p = 10, 0.5\n",
                "x = np.arange(0, 11)\n",
                "axes[1, 0].bar(x, stats.binom.pmf(x, n, p), color='orange', alpha=0.7)\n",
                "axes[1, 0].set_title(f'Binomial Distribution (n={n}, p={p})')\n",
                "\n",
                "# Exponential Distribution\n",
                "x = np.linspace(0, 5, 100)\n",
                "axes[1, 1].plot(x, stats.expon.pdf(x), 'r-', lw=2)\n",
                "axes[1, 1].fill_between(x, stats.expon.pdf(x), alpha=0.3)\n",
                "axes[1, 1].set_title('Exponential Distribution')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 Expectation & Variance\n",
                "\n",
                "- **Expectation:** $E[X] = \\sum_i x_i \\cdot P(x_i)$\n",
                "- **Variance:** $Var(X) = E[(X - \\mu)^2]$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample data\n",
                "data = np.random.normal(loc=5, scale=2, size=1000)\n",
                "\n",
                "# Manual calculations\n",
                "mean_manual = sum(data) / len(data)\n",
                "var_manual = sum((x - mean_manual)**2 for x in data) / len(data)\n",
                "\n",
                "print(f\"Mean (manual):     {mean_manual:.4f}\")\n",
                "print(f\"Mean (numpy):      {np.mean(data):.4f}\")\n",
                "print(f\"Variance (manual): {var_manual:.4f}\")\n",
                "print(f\"Variance (numpy):  {np.var(data):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.3 Bayes' Theorem\n",
                "\n",
                "$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$\n",
                "\n",
                "**Example:** Medical test accuracy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Medical test example\n",
                "# P(Disease) = 1%\n",
                "# P(Positive | Disease) = 99% (sensitivity)\n",
                "# P(Positive | No Disease) = 5% (false positive)\n",
                "\n",
                "p_disease = 0.01\n",
                "p_positive_given_disease = 0.99\n",
                "p_positive_given_healthy = 0.05\n",
                "\n",
                "# P(Positive) = P(Pos|D)*P(D) + P(Pos|H)*P(H)\n",
                "p_positive = (p_positive_given_disease * p_disease + \n",
                "              p_positive_given_healthy * (1 - p_disease))\n",
                "\n",
                "# P(Disease | Positive) using Bayes\n",
                "p_disease_given_positive = (p_positive_given_disease * p_disease) / p_positive\n",
                "\n",
                "print(\"Medical Test Bayes Analysis:\")\n",
                "print(f\"P(Disease | Positive Test) = {p_disease_given_positive:.2%}\")\n",
                "print(\"\\nSurprising! Even with 99% accurate test, only ~17% chance of disease!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: ML Preliminaries\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.1 Linear Regression from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data\n",
                "np.random.seed(42)\n",
                "n_samples = 100\n",
                "\n",
                "X = 2 * np.random.rand(n_samples, 1)\n",
                "y = 4 + 3 * X + np.random.randn(n_samples, 1) * 0.5\n",
                "\n",
                "# Add bias term (column of ones)\n",
                "X_b = np.c_[np.ones((n_samples, 1)), X]\n",
                "\n",
                "print(f\"X shape: {X_b.shape}\")\n",
                "print(f\"y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Closed-form solution: w = (X^T X)^{-1} X^T y\n",
                "w_closed = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
                "print(f\"Closed-form weights: {w_closed.flatten()}\")\n",
                "print(f\"True weights: [4, 3]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gradient Descent solution\n",
                "def linear_regression_gd(X, y, lr=0.1, epochs=100):\n",
                "    n_samples, n_features = X.shape\n",
                "    w = np.zeros((n_features, 1))\n",
                "    losses = []\n",
                "    \n",
                "    for _ in range(epochs):\n",
                "        # Predictions\n",
                "        y_pred = X @ w\n",
                "        \n",
                "        # MSE Loss\n",
                "        loss = np.mean((y_pred - y)**2)\n",
                "        losses.append(loss)\n",
                "        \n",
                "        # Gradient\n",
                "        gradient = (2/n_samples) * X.T @ (y_pred - y)\n",
                "        \n",
                "        # Update\n",
                "        w -= lr * gradient\n",
                "    \n",
                "    return w, losses\n",
                "\n",
                "w_gd, losses = linear_regression_gd(X_b, y, lr=0.1, epochs=100)\n",
                "print(f\"GD weights: {w_gd.flatten()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot data and fit\n",
                "axes[0].scatter(X, y, alpha=0.5, label='Data')\n",
                "X_line = np.array([[0], [2]])\n",
                "X_line_b = np.c_[np.ones((2, 1)), X_line]\n",
                "y_line = X_line_b @ w_gd\n",
                "axes[0].plot(X_line, y_line, 'r-', linewidth=2, label='Prediction')\n",
                "axes[0].set_xlabel('X')\n",
                "axes[0].set_ylabel('y')\n",
                "axes[0].set_title('Linear Regression Fit')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot loss curve\n",
                "axes[1].plot(losses)\n",
                "axes[1].set_xlabel('Epoch')\n",
                "axes[1].set_ylabel('MSE Loss')\n",
                "axes[1].set_title('Training Loss')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 Loss Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mse_loss(y_true, y_pred):\n",
                "    \"\"\"Mean Squared Error - for regression.\"\"\"\n",
                "    return np.mean((y_true - y_pred)**2)\n",
                "\n",
                "def cross_entropy_loss(y_true, y_pred, eps=1e-15):\n",
                "    \"\"\"Binary Cross-Entropy - for classification.\"\"\"\n",
                "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
                "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
                "\n",
                "# Example\n",
                "y_true = np.array([0, 0, 1, 1])\n",
                "y_pred_good = np.array([0.1, 0.2, 0.8, 0.9])\n",
                "y_pred_bad = np.array([0.9, 0.8, 0.2, 0.1])\n",
                "\n",
                "print(f\"Cross-Entropy (good predictions): {cross_entropy_loss(y_true, y_pred_good):.4f}\")\n",
                "print(f\"Cross-Entropy (bad predictions):  {cross_entropy_loss(y_true, y_pred_bad):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.3 Overfitting vs Underfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.pipeline import make_pipeline\n",
                "\n",
                "# Generate non-linear data\n",
                "np.random.seed(42)\n",
                "X = np.linspace(0, 1, 30).reshape(-1, 1)\n",
                "y = np.sin(2 * np.pi * X) + np.random.randn(30, 1) * 0.3\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "X_plot = np.linspace(0, 1, 100).reshape(-1, 1)\n",
                "\n",
                "for ax, degree, title in zip(axes, [1, 4, 15], \n",
                "                               ['Underfitting (d=1)', 'Good Fit (d=4)', 'Overfitting (d=15)']):\n",
                "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
                "    model.fit(X, y)\n",
                "    y_plot = model.predict(X_plot)\n",
                "    \n",
                "    ax.scatter(X, y, alpha=0.6)\n",
                "    ax.plot(X_plot, y_plot, 'r-', linewidth=2)\n",
                "    ax.set_title(title)\n",
                "    ax.set_ylim(-2, 2)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 6: Unit Tests & Exercises\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Unit tests for your implementations\n",
                "\n",
                "def test_dot_product():\n",
                "    a = [1, 2, 3]\n",
                "    b = [4, 5, 6]\n",
                "    assert dot_product(a, b) == 32, \"Dot product failed!\"\n",
                "    print(\"âœ“ Dot product test passed\")\n",
                "\n",
                "def test_cosine_similarity():\n",
                "    a = np.array([1, 0])\n",
                "    b = np.array([1, 0])\n",
                "    assert cosine_similarity(a, b) == 1.0, \"Cosine similarity failed!\"\n",
                "    print(\"âœ“ Cosine similarity test passed\")\n",
                "\n",
                "def test_mse():\n",
                "    y_true = np.array([1, 2, 3])\n",
                "    y_pred = np.array([1, 2, 3])\n",
                "    assert mse_loss(y_true, y_pred) == 0.0, \"MSE failed!\"\n",
                "    print(\"âœ“ MSE test passed\")\n",
                "\n",
                "# Run tests\n",
                "test_dot_product()\n",
                "test_cosine_similarity()\n",
                "test_mse()\n",
                "print(\"\\nðŸŽ‰ All tests passed!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise: Implement These Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# TODO: Implement these functions\n",
                "\n",
                "def euclidean_distance(a, b):\n",
                "    \"\"\"Implement Euclidean distance between two vectors.\"\"\"\n",
                "    # Your code here\n",
                "    pass\n",
                "\n",
                "def softmax(x):\n",
                "    \"\"\"Implement softmax function.\"\"\"\n",
                "    # Your code here\n",
                "    pass\n",
                "\n",
                "def sigmoid(x):\n",
                "    \"\"\"Implement sigmoid activation.\"\"\"\n",
                "    # Your code here\n",
                "    pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 7: Interview Prep\n",
                "---\n",
                "\n",
                "## Key Questions to Answer:\n",
                "\n",
                "**Linear Algebra:**\n",
                "1. What is the dot product and when is it used?\n",
                "2. Explain eigenvalues/eigenvectors intuitively\n",
                "3. What is cosine similarity and why is it used in NLP?\n",
                "\n",
                "**Calculus:**\n",
                "1. What is gradient descent and why does it work?\n",
                "2. Explain the chain rule and its role in backpropagation\n",
                "\n",
                "**Probability:**\n",
                "1. Explain Bayes' theorem with an example\n",
                "2. What's the difference between variance and standard deviation?\n",
                "\n",
                "**ML Basics:**\n",
                "1. What is overfitting and how do you prevent it?\n",
                "2. Explain the bias-variance tradeoff\n",
                "3. What's the difference between MSE and Cross-Entropy?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 8: Summary & Next Steps\n",
                "---\n",
                "\n",
                "## What You Learned:\n",
                "- âœ… Python advanced features (comprehensions, generators)\n",
                "- âœ… NumPy operations and broadcasting\n",
                "- âœ… Vector operations (dot product, cosine similarity, norms)\n",
                "- âœ… Matrix operations and eigenvalues\n",
                "- âœ… Derivatives and gradient descent\n",
                "- âœ… Probability distributions and Bayes' theorem\n",
                "- âœ… Linear regression from scratch\n",
                "- âœ… Loss functions and overfitting\n",
                "\n",
                "## Next: Week 1 - Vector Similarity\n",
                "- Deep dive into embedding spaces\n",
                "- Similarity search algorithms\n",
                "- Introduction to vector databases"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}