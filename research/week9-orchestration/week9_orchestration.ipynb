{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”— Week 9: LLM Orchestration & Streaming\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Connect retrieval pipeline to LLM\n",
                "2. Implement Server-Sent Events (SSE) streaming\n",
                "3. Build structured output parsing\n",
                "4. Create prompt templates and chains\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import time\n",
                "from typing import Generator, Dict, Any"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## RAG Pipeline\n",
                "```\n",
                "Query â†’ Embed â†’ Retrieve â†’ [Re-rank] â†’ Context + Query â†’ LLM â†’ Response\n",
                "```\n",
                "\n",
                "## Why Streaming?\n",
                "- Better UX: Users see response immediately\n",
                "- Lower perceived latency\n",
                "- Can stop early if answer found"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PromptTemplate:\n",
                "    \"\"\"Simple prompt template.\"\"\"\n",
                "    \n",
                "    def __init__(self, template: str):\n",
                "        self.template = template\n",
                "    \n",
                "    def format(self, **kwargs) -> str:\n",
                "        return self.template.format(**kwargs)\n",
                "\n",
                "# RAG prompt\n",
                "RAG_PROMPT = PromptTemplate(\"\"\"\n",
                "You are a helpful assistant. Answer based only on the context provided.\n",
                "\n",
                "Context:\n",
                "{context}\n",
                "\n",
                "Question: {question}\n",
                "\n",
                "Answer:\"\"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MockLLM:\n",
                "    \"\"\"Simulated LLM for demo.\"\"\"\n",
                "    \n",
                "    def generate(self, prompt: str) -> str:\n",
                "        \"\"\"Generate response.\"\"\"\n",
                "        return f\"Based on the context, the answer is: [simulated response]\"\n",
                "    \n",
                "    def stream(self, prompt: str) -> Generator[str, None, None]:\n",
                "        \"\"\"Stream response token by token.\"\"\"\n",
                "        response = self.generate(prompt)\n",
                "        for word in response.split():\n",
                "            time.sleep(0.1)  # Simulate token generation delay\n",
                "            yield word + \" \""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RAGOrchestrator:\n",
                "    \"\"\"Orchestrates retrieval and LLM.\"\"\"\n",
                "    \n",
                "    def __init__(self, retriever, llm, prompt_template):\n",
                "        self.retriever = retriever\n",
                "        self.llm = llm\n",
                "        self.prompt = prompt_template\n",
                "    \n",
                "    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
                "        \"\"\"Run full RAG pipeline.\"\"\"\n",
                "        # Retrieve\n",
                "        docs = self.retriever.search(question, top_k=top_k)\n",
                "        context = \"\\n\".join([d[2] for d in docs])\n",
                "        \n",
                "        # Generate\n",
                "        prompt = self.prompt.format(context=context, question=question)\n",
                "        response = self.llm.generate(prompt)\n",
                "        \n",
                "        return {\n",
                "            \"answer\": response,\n",
                "            \"sources\": [d[2] for d in docs],\n",
                "            \"prompt\": prompt\n",
                "        }\n",
                "    \n",
                "    def stream_query(self, question: str, top_k: int = 3) -> Generator[str, None, None]:\n",
                "        \"\"\"Stream RAG response.\"\"\"\n",
                "        docs = self.retriever.search(question, top_k=top_k)\n",
                "        context = \"\\n\".join([d[2] for d in docs])\n",
                "        prompt = self.prompt.format(context=context, question=question)\n",
                "        \n",
                "        for token in self.llm.stream(prompt):\n",
                "            yield token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mock retriever\n",
                "class MockRetriever:\n",
                "    def search(self, query, top_k=3):\n",
                "        return [\n",
                "            (0, 0.9, \"Machine learning is AI subset.\"),\n",
                "            (1, 0.8, \"Deep learning uses neural networks.\"),\n",
                "            (2, 0.7, \"Python is popular for ML.\")\n",
                "        ]\n",
                "\n",
                "# Test streaming\n",
                "rag = RAGOrchestrator(MockRetriever(), MockLLM(), RAG_PROMPT)\n",
                "\n",
                "print(\"Streaming response:\")\n",
                "for token in rag.stream_query(\"What is ML?\"):\n",
                "    print(token, end=\"\", flush=True)\n",
                "print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 SSE for Web Streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sse_example = \"\"\"\n",
                "# FastAPI SSE Endpoint\n",
                "\n",
                "from fastapi import FastAPI\n",
                "from fastapi.responses import StreamingResponse\n",
                "\n",
                "app = FastAPI()\n",
                "\n",
                "@app.post(\"/chat/stream\")\n",
                "async def stream_chat(query: str):\n",
                "    async def generate():\n",
                "        for token in rag.stream_query(query):\n",
                "            yield f\"data: {json.dumps({'token': token})}\\\\n\\\\n\"\n",
                "        yield \"data: [DONE]\\\\n\\\\n\"\n",
                "    \n",
                "    return StreamingResponse(\n",
                "        generate(),\n",
                "        media_type=\"text/event-stream\"\n",
                "    )\n",
                "\"\"\"\n",
                "print(sse_example)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test prompt template\n",
                "    pt = PromptTemplate(\"Hello {name}!\")\n",
                "    assert pt.format(name=\"World\") == \"Hello World!\"\n",
                "    print(\"âœ“ Prompt template test passed\")\n",
                "    \n",
                "    # Test RAG pipeline\n",
                "    rag = RAGOrchestrator(MockRetriever(), MockLLM(), RAG_PROMPT)\n",
                "    result = rag.query(\"test\")\n",
                "    assert \"answer\" in result\n",
                "    assert \"sources\" in result\n",
                "    print(\"âœ“ RAG pipeline test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Interview Prep\n",
                "---\n",
                "\n",
                "### Q1: How do you handle hallucinations in RAG?\n",
                "**Answer:** Grounding in retrieved docs, citation verification, guardrails, user feedback.\n",
                "\n",
                "### Q2: Streaming vs batch for chat UX?\n",
                "**Answer:** Streaming for chat (lower latency feel), batch for batch processing.\n",
                "\n",
                "---\n",
                "# Section 5: Deliverable\n",
                "---\n",
                "\n",
                "**Created:** `orchestrator.py` with RAG pipeline and streaming\n",
                "\n",
                "**Next Week:** Guardrails & Cost Metrics"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}