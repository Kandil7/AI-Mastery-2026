{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¤– Week 4: Transformers & Tokenization\n",
                "\n",
                "**Learning Objectives:**\n",
                "1. Understand the Attention mechanism\n",
                "2. Master tokenization strategies (BPE, WordPiece, SentencePiece)\n",
                "3. Explore Transformer architecture (Encoder/Decoder)\n",
                "4. Analyze how tokenization affects model performance\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from collections import Counter\n",
                "import re\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 1: Theory\n",
                "---\n",
                "\n",
                "## What is Attention?\n",
                "\n",
                "**Core Idea**: Not all inputs are equally important for a given output.\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
                "\n",
                "Where:\n",
                "- **Q** (Query): What am I looking for?\n",
                "- **K** (Key): What do I contain?\n",
                "- **V** (Value): What do I return?\n",
                "\n",
                "## Why Tokenization Matters\n",
                "\n",
                "| Tokenizer | Vocab Size | Handling Unknown Words |\n",
                "|-----------|------------|------------------------|\n",
                "| Word-level | Large (100K+) | OOV tokens |\n",
                "| Character | Small (100) | Very long sequences |\n",
                "| BPE/WordPiece | Medium (30-50K) | Subword decomposition |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 2: Hands-On Implementation\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Simple Attention Mechanism"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(x, axis=-1):\n",
                "    \"\"\"Numerically stable softmax.\"\"\"\n",
                "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
                "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
                "\n",
                "\n",
                "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
                "    \"\"\"\n",
                "    Scaled Dot-Product Attention.\n",
                "    \n",
                "    Args:\n",
                "        Q: Query matrix (seq_len, d_k)\n",
                "        K: Key matrix (seq_len, d_k)\n",
                "        V: Value matrix (seq_len, d_v)\n",
                "        mask: Optional attention mask\n",
                "    \n",
                "    Returns:\n",
                "        output: Attention output\n",
                "        weights: Attention weights\n",
                "    \"\"\"\n",
                "    d_k = K.shape[-1]\n",
                "    \n",
                "    # Compute attention scores\n",
                "    scores = Q @ K.T / np.sqrt(d_k)\n",
                "    \n",
                "    # Apply mask if provided\n",
                "    if mask is not None:\n",
                "        scores = scores + (mask * -1e9)\n",
                "    \n",
                "    # Softmax to get attention weights\n",
                "    weights = softmax(scores)\n",
                "    \n",
                "    # Weighted sum of values\n",
                "    output = weights @ V\n",
                "    \n",
                "    return output, weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Simple attention\n",
                "seq_len = 4\n",
                "d_k = 8\n",
                "d_v = 8\n",
                "\n",
                "# Random Q, K, V matrices\n",
                "Q = np.random.randn(seq_len, d_k)\n",
                "K = np.random.randn(seq_len, d_k)\n",
                "V = np.random.randn(seq_len, d_v)\n",
                "\n",
                "output, weights = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(f\"Input shape: ({seq_len}, {d_k})\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {weights.shape}\")\n",
                "print(f\"\\nAttention weights (rows sum to 1):\\n{weights}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention:\n",
                "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model, num_heads):\n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        \n",
                "        # Weight matrices for each head\n",
                "        self.W_Q = np.random.randn(d_model, d_model) * 0.1\n",
                "        self.W_K = np.random.randn(d_model, d_model) * 0.1\n",
                "        self.W_V = np.random.randn(d_model, d_model) * 0.1\n",
                "        self.W_O = np.random.randn(d_model, d_model) * 0.1\n",
                "    \n",
                "    def split_heads(self, x):\n",
                "        \"\"\"Split into multiple heads.\"\"\"\n",
                "        batch_size, seq_len, _ = x.shape\n",
                "        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
                "    \n",
                "    def forward(self, Q, K, V, mask=None):\n",
                "        batch_size = Q.shape[0]\n",
                "        \n",
                "        # Linear projections\n",
                "        Q = Q @ self.W_Q\n",
                "        K = K @ self.W_K\n",
                "        V = V @ self.W_V\n",
                "        \n",
                "        # Split heads\n",
                "        Q = self.split_heads(Q)\n",
                "        K = self.split_heads(K)\n",
                "        V = self.split_heads(V)\n",
                "        \n",
                "        # Attention for each head\n",
                "        all_outputs = []\n",
                "        all_weights = []\n",
                "        for h in range(self.num_heads):\n",
                "            output, weights = scaled_dot_product_attention(\n",
                "                Q[0, h], K[0, h], V[0, h], mask\n",
                "            )\n",
                "            all_outputs.append(output)\n",
                "            all_weights.append(weights)\n",
                "        \n",
                "        # Concatenate heads\n",
                "        concat = np.concatenate(all_outputs, axis=-1)\n",
                "        \n",
                "        # Final linear projection\n",
                "        output = concat @ self.W_O\n",
                "        \n",
                "        return output, all_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Multi-Head Attention\n",
                "d_model = 64\n",
                "num_heads = 8\n",
                "seq_len = 10\n",
                "\n",
                "mha = MultiHeadAttention(d_model, num_heads)\n",
                "\n",
                "# Input: (batch_size, seq_len, d_model)\n",
                "x = np.random.randn(1, seq_len, d_model)\n",
                "\n",
                "output, weights = mha.forward(x, x, x)  # Self-attention\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Number of attention heads: {len(weights)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Tokenization from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleTokenizer:\n",
                "    \"\"\"Simple word-level tokenizer.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.word_to_id = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
                "        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
                "        self.vocab_size = 4\n",
                "    \n",
                "    def fit(self, texts):\n",
                "        \"\"\"Build vocabulary from texts.\"\"\"\n",
                "        for text in texts:\n",
                "            words = text.lower().split()\n",
                "            for word in words:\n",
                "                if word not in self.word_to_id:\n",
                "                    self.word_to_id[word] = self.vocab_size\n",
                "                    self.id_to_word[self.vocab_size] = word\n",
                "                    self.vocab_size += 1\n",
                "    \n",
                "    def encode(self, text):\n",
                "        \"\"\"Convert text to token IDs.\"\"\"\n",
                "        words = text.lower().split()\n",
                "        return [self.word_to_id.get(w, 1) for w in words]  # 1 = <UNK>\n",
                "    \n",
                "    def decode(self, ids):\n",
                "        \"\"\"Convert token IDs back to text.\"\"\"\n",
                "        return \" \".join([self.id_to_word.get(i, \"<UNK>\") for i in ids])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test simple tokenizer\n",
                "texts = [\n",
                "    \"The cat sat on the mat\",\n",
                "    \"The dog played in the garden\",\n",
                "    \"Machine learning is amazing\"\n",
                "]\n",
                "\n",
                "tokenizer = SimpleTokenizer()\n",
                "tokenizer.fit(texts)\n",
                "\n",
                "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
                "print(f\"\\nVocabulary: {tokenizer.word_to_id}\")\n",
                "\n",
                "# Encode and decode\n",
                "test_text = \"The cat is amazing\"\n",
                "encoded = tokenizer.encode(test_text)\n",
                "decoded = tokenizer.decode(encoded)\n",
                "\n",
                "print(f\"\\nOriginal: '{test_text}'\")\n",
                "print(f\"Encoded: {encoded}\")\n",
                "print(f\"Decoded: '{decoded}'\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.4 BPE Tokenizer (Simplified)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleBPE:\n",
                "    \"\"\"Simplified Byte-Pair Encoding tokenizer.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size=100):\n",
                "        self.vocab_size = vocab_size\n",
                "        self.merges = {}\n",
                "        self.vocab = {}\n",
                "    \n",
                "    def get_pair_counts(self, word_freqs):\n",
                "        \"\"\"Count adjacent pairs.\"\"\"\n",
                "        pairs = Counter()\n",
                "        for word, freq in word_freqs.items():\n",
                "            symbols = word.split()\n",
                "            for i in range(len(symbols) - 1):\n",
                "                pairs[symbols[i], symbols[i+1]] += freq\n",
                "        return pairs\n",
                "    \n",
                "    def merge_pair(self, word_freqs, pair):\n",
                "        \"\"\"Merge most frequent pair.\"\"\"\n",
                "        new_word_freqs = {}\n",
                "        bigram = \" \".join(pair)\n",
                "        replacement = \"\".join(pair)\n",
                "        \n",
                "        for word, freq in word_freqs.items():\n",
                "            new_word = word.replace(bigram, replacement)\n",
                "            new_word_freqs[new_word] = freq\n",
                "        \n",
                "        return new_word_freqs\n",
                "    \n",
                "    def fit(self, texts, num_merges=10):\n",
                "        \"\"\"Learn BPE merges from texts.\"\"\"\n",
                "        # Count word frequencies\n",
                "        word_freqs = Counter()\n",
                "        for text in texts:\n",
                "            for word in text.lower().split():\n",
                "                # Add space between characters + end token\n",
                "                word_freqs[\" \".join(list(word)) + \" </w>\"] += 1\n",
                "        \n",
                "        # Learn merges\n",
                "        for i in range(num_merges):\n",
                "            pairs = self.get_pair_counts(word_freqs)\n",
                "            if not pairs:\n",
                "                break\n",
                "            best_pair = max(pairs, key=pairs.get)\n",
                "            word_freqs = self.merge_pair(word_freqs, best_pair)\n",
                "            self.merges[best_pair] = \"\".join(best_pair)\n",
                "            print(f\"Merge {i+1}: {best_pair} -> {''.join(best_pair)}\")\n",
                "        \n",
                "        # Build vocabulary\n",
                "        self.vocab = set()\n",
                "        for word in word_freqs.keys():\n",
                "            for token in word.split():\n",
                "                self.vocab.add(token)\n",
                "        \n",
                "        return self"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train BPE\n",
                "corpus = [\n",
                "    \"low lower lowest\",\n",
                "    \"new newer newest\",\n",
                "    \"show shower\"\n",
                "]\n",
                "\n",
                "bpe = SimpleBPE()\n",
                "bpe.fit(corpus, num_merges=10)\n",
                "\n",
                "print(f\"\\nFinal vocabulary ({len(bpe.vocab)} tokens):\")\n",
                "print(sorted(bpe.vocab))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 3: Visualizations\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Attention Weights Heatmap"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize attention patterns\n",
                "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
                "\n",
                "# Simulated attention weights\n",
                "np.random.seed(42)\n",
                "attention = softmax(np.random.randn(len(sentence), len(sentence)))\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(attention, xticklabels=sentence, yticklabels=sentence,\n",
                "            annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
                "plt.title(\"Attention Weights (Self-Attention)\")\n",
                "plt.xlabel(\"Key Position\")\n",
                "plt.ylabel(\"Query Position\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Multi-Head Attention Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize multiple attention heads\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
                "\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    # Simulated attention for each head\n",
                "    np.random.seed(i)\n",
                "    head_attention = softmax(np.random.randn(len(sentence), len(sentence)))\n",
                "    \n",
                "    sns.heatmap(head_attention, ax=ax, cmap=\"Blues\", cbar=False,\n",
                "                xticklabels=sentence if i >= 4 else [],\n",
                "                yticklabels=sentence if i % 4 == 0 else [])\n",
                "    ax.set_title(f\"Head {i+1}\")\n",
                "\n",
                "plt.suptitle(\"Multi-Head Attention: Different Heads Learn Different Patterns\", fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 Token Distribution Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare tokenization strategies\n",
                "sample_text = \"Natural language processing enables machines to understand human communication\"\n",
                "\n",
                "# Word-level tokenization\n",
                "word_tokens = sample_text.split()\n",
                "\n",
                "# Character-level tokenization\n",
                "char_tokens = list(sample_text.replace(\" \", \"_\"))\n",
                "\n",
                "# Simulated BPE-like (subword)\n",
                "subword_tokens = [\"Nat\", \"ural\", \"_lang\", \"uage\", \"_process\", \"ing\", \n",
                "                  \"_enables\", \"_machines\", \"_to\", \"_understand\", \n",
                "                  \"_human\", \"_commun\", \"ication\"]\n",
                "\n",
                "print(f\"Original: {sample_text}\")\n",
                "print(f\"\\nWord-level ({len(word_tokens)} tokens): {word_tokens}\")\n",
                "print(f\"\\nCharacter-level ({len(char_tokens)} tokens): {char_tokens[:20]}...\")\n",
                "print(f\"\\nSubword/BPE ({len(subword_tokens)} tokens): {subword_tokens}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize token counts\n",
                "methods = ['Word-level', 'Character-level', 'Subword (BPE)']\n",
                "token_counts = [len(word_tokens), len(char_tokens), len(subword_tokens)]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "bars = plt.bar(methods, token_counts, color=['steelblue', 'coral', 'seagreen'])\n",
                "plt.ylabel('Number of Tokens')\n",
                "plt.title('Tokenization Methods Comparison')\n",
                "\n",
                "for bar, count in zip(bars, token_counts):\n",
                "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
                "             str(count), ha='center', fontsize=12)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 4: Unit Tests\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_tests():\n",
                "    print(\"Running Unit Tests...\\n\")\n",
                "    \n",
                "    # Test 1: Softmax sums to 1\n",
                "    x = np.array([1, 2, 3])\n",
                "    assert abs(softmax(x).sum() - 1.0) < 1e-6\n",
                "    print(\"âœ“ Softmax sum test passed\")\n",
                "    \n",
                "    # Test 2: Attention output shape\n",
                "    Q = np.random.randn(5, 8)\n",
                "    K = np.random.randn(5, 8)\n",
                "    V = np.random.randn(5, 16)\n",
                "    output, weights = scaled_dot_product_attention(Q, K, V)\n",
                "    assert output.shape == (5, 16)\n",
                "    assert weights.shape == (5, 5)\n",
                "    print(\"âœ“ Attention shape test passed\")\n",
                "    \n",
                "    # Test 3: Attention weights sum to 1\n",
                "    assert np.allclose(weights.sum(axis=-1), 1.0)\n",
                "    print(\"âœ“ Attention weights normalization test passed\")\n",
                "    \n",
                "    # Test 4: Tokenizer encode/decode\n",
                "    tok = SimpleTokenizer()\n",
                "    tok.fit([\"hello world\"])\n",
                "    encoded = tok.encode(\"hello world\")\n",
                "    decoded = tok.decode(encoded)\n",
                "    assert decoded == \"hello world\"\n",
                "    print(\"âœ“ Tokenizer encode/decode test passed\")\n",
                "    \n",
                "    # Test 5: Unknown token handling\n",
                "    encoded = tok.encode(\"hello unknown\")\n",
                "    assert 1 in encoded  # 1 = <UNK>\n",
                "    print(\"âœ“ Unknown token handling test passed\")\n",
                "    \n",
                "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
                "\n",
                "run_tests()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 5: Interview Prep\n",
                "---\n",
                "\n",
                "## Key Questions\n",
                "\n",
                "### Q1: Explain the Attention mechanism in simple terms.\n",
                "\n",
                "**Answer:**\n",
                "- Attention allows the model to focus on relevant parts of the input\n",
                "- Uses Query-Key-Value: Query asks \"what's relevant?\", Keys answer, Values provide content\n",
                "- Softmax creates importance weights that sum to 1\n",
                "- Enables long-range dependencies without recurrence\n",
                "\n",
                "### Q2: What's the difference between encoder and decoder in Transformers?\n",
                "\n",
                "**Answer:**\n",
                "- **Encoder**: Bidirectional, sees entire input (BERT)\n",
                "- **Decoder**: Autoregressive, can only see past tokens (GPT)\n",
                "- **Encoder-Decoder**: Full Transformer for seq2seq (T5, BART)\n",
                "\n",
                "### Q3: How does tokenization affect model performance?\n",
                "\n",
                "**Answer:**\n",
                "- Too many tokens = longer sequences, slower, limited context\n",
                "- Too few tokens = large vocabulary, memory issues\n",
                "- Subword (BPE) balances both: handles OOV, reasonable sequence length\n",
                "- Domain-specific tokenizers can improve performance\n",
                "\n",
                "### Q4: Why do we scale by sqrt(d_k) in attention?\n",
                "\n",
                "**Answer:**\n",
                "- Without scaling, dot products grow large for high dimensions\n",
                "- Large values push softmax into extreme regions (gradients vanish)\n",
                "- Scaling keeps values in a reasonable range for stable training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 6: Exercises\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Implement Positional Encoding\n",
                "def positional_encoding(seq_len, d_model):\n",
                "    \"\"\"\n",
                "    Create sinusoidal positional encodings.\n",
                "    \n",
                "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
                "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
                "    \"\"\"\n",
                "    # TODO: Your implementation here\n",
                "    pass\n",
                "\n",
                "\n",
                "# Exercise 2: Implement Masked Self-Attention\n",
                "def create_causal_mask(seq_len):\n",
                "    \"\"\"\n",
                "    Create mask for decoder self-attention.\n",
                "    Position i can only attend to positions <= i.\n",
                "    \"\"\"\n",
                "    # TODO: Your implementation here\n",
                "    pass\n",
                "\n",
                "\n",
                "# Exercise 3: Implement a simple Transformer block\n",
                "class TransformerBlock:\n",
                "    \"\"\"Single Transformer encoder block.\"\"\"\n",
                "    def __init__(self, d_model, num_heads, d_ff):\n",
                "        # TODO: Initialize MHA, FFN, Layer Norms\n",
                "        pass\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # TODO: Implement forward pass with residual connections\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "# Section 7: Deliverable\n",
                "---\n",
                "\n",
                "## What You Built:\n",
                "\n",
                "1. **Scaled Dot-Product Attention** - Core attention mechanism\n",
                "2. **Multi-Head Attention** - Parallel attention heads\n",
                "3. **Simple Tokenizer** - Word-level tokenization\n",
                "4. **BPE Tokenizer** - Subword tokenization\n",
                "\n",
                "## Key Takeaways:\n",
                "\n",
                "- Attention enables modeling long-range dependencies\n",
                "- Multi-head attention learns different relationship patterns\n",
                "- Tokenization choice significantly impacts model input\n",
                "- BPE/WordPiece balance vocabulary size and sequence length\n",
                "\n",
                "## Next Week: FastAPI Backend\n",
                "- Building REST APIs\n",
                "- Async endpoints\n",
                "- JWT authentication"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}