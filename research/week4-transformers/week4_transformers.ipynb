{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¤– Week 4: Transformers & Tokenization\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand the Attention mechanism\n",
    "2. Master tokenization strategies (BPE, WordPiece, SentencePiece)\n",
    "3. Explore Transformer architecture (Encoder/Decoder)\n",
    "4. Analyze how tokenization affects model performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Theory\n",
    "---\n",
    "\n",
    "## What is Attention?\n",
    "\n",
    "**Core Idea**: Not all inputs are equally important for a given output.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "Where:\n",
    "- **Q** (Query): What am I looking for?\n",
    "- **K** (Key): What do I contain?\n",
    "- **V** (Value): What do I return?\n",
    "\n",
    "## Why Tokenization Matters\n",
    "\n",
    "| Tokenizer | Vocab Size | Handling Unknown Words |\n",
    "|-----------|------------|------------------------|\n",
    "| Word-level | Large (100K+) | OOV tokens |\n",
    "| Character | Small (100) | Very long sequences |\n",
    "| BPE/WordPiece | Medium (30-50K) | Subword decomposition |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: Hands-On Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Simple Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "        mask: Optional attention mask\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        weights: Attention weights\n",
    "    \"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = Q @ K.T / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    weights = softmax(scores)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = weights @ V\n",
    "    \n",
    "    return output, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple attention\n",
    "seq_len = 4\n",
    "d_k = 8\n",
    "d_v = 8\n",
    "\n",
    "# Random Q, K, V matrices\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Input shape: ({seq_len}, {d_k})\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape}\")\n",
    "print(f\"\\nAttention weights (rows sum to 1):\\n{weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, num_heads):\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Weight matrices for each head\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_O = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads.\"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch_size = Q.shape[0]\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = Q @ self.W_Q\n",
    "        K = K @ self.W_K\n",
    "        V = V @ self.W_V\n",
    "        \n",
    "        # Split heads\n",
    "        Q = self.split_heads(Q)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Attention for each head\n",
    "        all_outputs = []\n",
    "        all_weights = []\n",
    "        for h in range(self.num_heads):\n",
    "            output, weights = scaled_dot_product_attention(\n",
    "                Q[0, h], K[0, h], V[0, h], mask\n",
    "            )\n",
    "            all_outputs.append(output)\n",
    "            all_weights.append(weights)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        concat = np.concatenate(all_outputs, axis=-1)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = concat @ self.W_O\n",
    "        \n",
    "        return output, all_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Multi-Head Attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "# Input: (batch_size, seq_len, d_model)\n",
    "x = np.random.randn(1, seq_len, d_model)\n",
    "\n",
    "output, weights = mha.forward(x, x, x)  # Self-attention\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention heads: {len(weights)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Tokenization from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word-level tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.word_to_id = {\"<PAD>\": 0, \"<UNK>\": 1, \"<BOS>\": 2, \"<EOS>\": 3}\n",
    "        self.id_to_word = {v: k for k, v in self.word_to_id.items()}\n",
    "        self.vocab_size = 4\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"Build vocabulary from texts.\"\"\"\n",
    "        for text in texts:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in self.word_to_id:\n",
    "                    self.word_to_id[word] = self.vocab_size\n",
    "                    self.id_to_word[self.vocab_size] = word\n",
    "                    self.vocab_size += 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        \"\"\"Convert text to token IDs.\"\"\"\n",
    "        words = text.lower().split()\n",
    "        return [self.word_to_id.get(w, 1) for w in words]  # 1 = <UNK>\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        \"\"\"Convert token IDs back to text.\"\"\"\n",
    "        return \" \".join([self.id_to_word.get(i, \"<UNK>\") for i in ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test simple tokenizer\n",
    "texts = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"The dog played in the garden\",\n",
    "    \"Machine learning is amazing\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "tokenizer.fit(texts)\n",
    "\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"\\nVocabulary: {tokenizer.word_to_id}\")\n",
    "\n",
    "# Encode and decode\n",
    "test_text = \"The cat is amazing\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nOriginal: '{test_text}'\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 BPE Tokenizer (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBPE:\n",
    "    \"\"\"Simplified Byte-Pair Encoding tokenizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.merges = {}\n",
    "        self.vocab = {}\n",
    "    \n",
    "    def get_pair_counts(self, word_freqs):\n",
    "        \"\"\"Count adjacent pairs.\"\"\"\n",
    "        pairs = Counter()\n",
    "        for word, freq in word_freqs.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[symbols[i], symbols[i+1]] += freq\n",
    "        return pairs\n",
    "    \n",
    "    def merge_pair(self, word_freqs, pair):\n",
    "        \"\"\"Merge most frequent pair.\"\"\"\n",
    "        new_word_freqs = {}\n",
    "        bigram = \" \".join(pair)\n",
    "        replacement = \"\".join(pair)\n",
    "        \n",
    "        for word, freq in word_freqs.items():\n",
    "            new_word = word.replace(bigram, replacement)\n",
    "            new_word_freqs[new_word] = freq\n",
    "        \n",
    "        return new_word_freqs\n",
    "    \n",
    "    def fit(self, texts, num_merges=10):\n",
    "        \"\"\"Learn BPE merges from texts.\"\"\"\n",
    "        # Count word frequencies\n",
    "        word_freqs = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.lower().split():\n",
    "                # Add space between characters + end token\n",
    "                word_freqs[\" \".join(list(word)) + \" </w>\"] += 1\n",
    "        \n",
    "        # Learn merges\n",
    "        for i in range(num_merges):\n",
    "            pairs = self.get_pair_counts(word_freqs)\n",
    "            if not pairs:\n",
    "                break\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            word_freqs = self.merge_pair(word_freqs, best_pair)\n",
    "            self.merges[best_pair] = \"\".join(best_pair)\n",
    "            print(f\"Merge {i+1}: {best_pair} -> {''.join(best_pair)}\")\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.vocab = set()\n",
    "        for word in word_freqs.keys():\n",
    "            for token in word.split():\n",
    "                self.vocab.add(token)\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE\n",
    "corpus = [\n",
    "    \"low lower lowest\",\n",
    "    \"new newer newest\",\n",
    "    \"show shower\"\n",
    "]\n",
    "\n",
    "bpe = SimpleBPE()\n",
    "bpe.fit(corpus, num_merges=10)\n",
    "\n",
    "print(f\"\\nFinal vocabulary ({len(bpe.vocab)} tokens):\")\n",
    "print(sorted(bpe.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Attention Weights Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "sentence = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "\n",
    "# Simulated attention weights\n",
    "np.random.seed(42)\n",
    "attention = softmax(np.random.randn(len(sentence), len(sentence)))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention, xticklabels=sentence, yticklabels=sentence,\n",
    "            annot=True, fmt=\".2f\", cmap=\"Blues\")\n",
    "plt.title(\"Attention Weights (Self-Attention)\")\n",
    "plt.xlabel(\"Key Position\")\n",
    "plt.ylabel(\"Query Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Multi-Head Attention Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize multiple attention heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    # Simulated attention for each head\n",
    "    np.random.seed(i)\n",
    "    head_attention = softmax(np.random.randn(len(sentence), len(sentence)))\n",
    "    \n",
    "    sns.heatmap(head_attention, ax=ax, cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=sentence if i >= 4 else [],\n",
    "                yticklabels=sentence if i % 4 == 0 else [])\n",
    "    ax.set_title(f\"Head {i+1}\")\n",
    "\n",
    "plt.suptitle(\"Multi-Head Attention: Different Heads Learn Different Patterns\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Token Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization strategies\n",
    "sample_text = \"Natural language processing enables machines to understand human communication\"\n",
    "\n",
    "# Word-level tokenization\n",
    "word_tokens = sample_text.split()\n",
    "\n",
    "# Character-level tokenization\n",
    "char_tokens = list(sample_text.replace(\" \", \"_\"))\n",
    "\n",
    "# Simulated BPE-like (subword)\n",
    "subword_tokens = [\"Nat\", \"ural\", \"_lang\", \"uage\", \"_process\", \"ing\", \n",
    "                  \"_enables\", \"_machines\", \"_to\", \"_understand\", \n",
    "                  \"_human\", \"_commun\", \"ication\"]\n",
    "\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"\\nWord-level ({len(word_tokens)} tokens): {word_tokens}\")\n",
    "print(f\"\\nCharacter-level ({len(char_tokens)} tokens): {char_tokens[:20]}...\")\n",
    "print(f\"\\nSubword/BPE ({len(subword_tokens)} tokens): {subword_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize token counts\n",
    "methods = ['Word-level', 'Character-level', 'Subword (BPE)']\n",
    "token_counts = [len(word_tokens), len(char_tokens), len(subword_tokens)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "bars = plt.bar(methods, token_counts, color=['steelblue', 'coral', 'seagreen'])\n",
    "plt.ylabel('Number of Tokens')\n",
    "plt.title('Tokenization Methods Comparison')\n",
    "\n",
    "for bar, count in zip(bars, token_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "             str(count), ha='center', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Unit Tests\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    print(\"Running Unit Tests...\\n\")\n",
    "    \n",
    "    # Test 1: Softmax sums to 1\n",
    "    x = np.array([1, 2, 3])\n",
    "    assert abs(softmax(x).sum() - 1.0) < 1e-6\n",
    "    print(\"âœ“ Softmax sum test passed\")\n",
    "    \n",
    "    # Test 2: Attention output shape\n",
    "    Q = np.random.randn(5, 8)\n",
    "    K = np.random.randn(5, 8)\n",
    "    V = np.random.randn(5, 16)\n",
    "    output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "    assert output.shape == (5, 16)\n",
    "    assert weights.shape == (5, 5)\n",
    "    print(\"âœ“ Attention shape test passed\")\n",
    "    \n",
    "    # Test 3: Attention weights sum to 1\n",
    "    assert np.allclose(weights.sum(axis=-1), 1.0)\n",
    "    print(\"âœ“ Attention weights normalization test passed\")\n",
    "    \n",
    "    # Test 4: Tokenizer encode/decode\n",
    "    tok = SimpleTokenizer()\n",
    "    tok.fit([\"hello world\"])\n",
    "    encoded = tok.encode(\"hello world\")\n",
    "    decoded = tok.decode(encoded)\n",
    "    assert decoded == \"hello world\"\n",
    "    print(\"âœ“ Tokenizer encode/decode test passed\")\n",
    "    \n",
    "    # Test 5: Unknown token handling\n",
    "    encoded = tok.encode(\"hello unknown\")\n",
    "    assert 1 in encoded  # 1 = <UNK>\n",
    "    print(\"âœ“ Unknown token handling test passed\")\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ All tests passed!\")\n",
    "\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Interview Prep\n",
    "---\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "### Q1: Explain the Attention mechanism in simple terms.\n",
    "\n",
    "**Answer:**\n",
    "- Attention allows the model to focus on relevant parts of the input\n",
    "- Uses Query-Key-Value: Query asks \"what's relevant?\", Keys answer, Values provide content\n",
    "- Softmax creates importance weights that sum to 1\n",
    "- Enables long-range dependencies without recurrence\n",
    "\n",
    "### Q2: What's the difference between encoder and decoder in Transformers?\n",
    "\n",
    "**Answer:**\n",
    "- **Encoder**: Bidirectional, sees entire input (BERT)\n",
    "- **Decoder**: Autoregressive, can only see past tokens (GPT)\n",
    "- **Encoder-Decoder**: Full Transformer for seq2seq (T5, BART)\n",
    "\n",
    "### Q3: How does tokenization affect model performance?\n",
    "\n",
    "**Answer:**\n",
    "- Too many tokens = longer sequences, slower, limited context\n",
    "- Too few tokens = large vocabulary, memory issues\n",
    "- Subword (BPE) balances both: handles OOV, reasonable sequence length\n",
    "- Domain-specific tokenizers can improve performance\n",
    "\n",
    "### Q4: Why do we scale by sqrt(d_k) in attention?\n",
    "\n",
    "**Answer:**\n",
    "- Without scaling, dot products grow large for high dimensions\n",
    "- Large values push softmax into extreme regions (gradients vanish)\n",
    "- Scaling keeps values in a reasonable range for stable training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 6: Exercises\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement Positional Encoding\n",
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encodings.\n",
    "    \n",
    "    PE(pos, 2i) = sin(pos / 10000^(2i/d_model))\n",
    "    PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "\n",
    "# Exercise 2: Implement Masked Self-Attention\n",
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create mask for decoder self-attention.\n",
    "    Position i can only attend to positions <= i.\n",
    "    \"\"\"\n",
    "    # TODO: Your implementation here\n",
    "    pass\n",
    "\n",
    "\n",
    "# Exercise 3: Implement a simple Transformer block\n",
    "class TransformerBlock:\n",
    "    \"\"\"Single Transformer encoder block.\"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        # TODO: Initialize MHA, FFN, Layer Norms\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Implement forward pass with residual connections\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 7: Deliverable\n",
    "---\n",
    "\n",
    "## What You Built:\n",
    "\n",
    "1. **Scaled Dot-Product Attention** - Core attention mechanism\n",
    "2. **Multi-Head Attention** - Parallel attention heads\n",
    "3. **Simple Tokenizer** - Word-level tokenization\n",
    "4. **BPE Tokenizer** - Subword tokenization\n",
    "\n",
    "## Key Takeaways:\n",
    "\n",
    "- Attention enables modeling long-range dependencies\n",
    "- Multi-head attention learns different relationship patterns\n",
    "- Tokenization choice significantly impacts model input\n",
    "- BPE/WordPiece balance vocabulary size and sequence length\n",
    "\n",
    "## Next Week: FastAPI Backend\n",
    "- Building REST APIs\n",
    "- Async endpoints\n",
    "- JWT authentication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 3: Advanced Attention Mechanisms\n",
    "---\n",
    "\n",
    "Modern LLMs like GPT-4, Llama 2, and Mistral use sophisticated attention variants.\n",
    "This section covers the cutting-edge mechanisms implemented in `src/llm/attention.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Causal (Autoregressive) Attention\n",
    "\n",
    "In GPT-style models, tokens can only attend to **previous** tokens.\n",
    "This is achieved using a **causal mask** (lower triangular matrix).\n",
    "\n",
    "```\n",
    "Causal Mask:\n",
    "[1, 0, 0, 0]    Token 1 sees: only itself\n",
    "[1, 1, 0, 0]    Token 2 sees: tokens 1-2\n",
    "[1, 1, 1, 0]    Token 3 sees: tokens 1-3\n",
    "[1, 1, 1, 1]    Token 4 sees: all tokens\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.llm.attention import CausalSelfAttention\n",
    "\n",
    "# Create causal attention\n",
    "d_model, num_heads, block_size = 64, 4, 32\n",
    "causal_attn = CausalSelfAttention(d_model, num_heads, block_size)\n",
    "\n",
    "# Input sequence\n",
    "batch_size, seq_len = 2, 8\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output = causal_attn(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# Visualize the causal mask\n",
    "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.imshow(causal_mask.numpy(), cmap='Blues')\n",
    "plt.title('Causal Attention Mask')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.colorbar(label='Attention Allowed')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, int(causal_mask[i, j].item()), ha='center', va='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Rotary Position Embeddings (RoPE)\n",
    "\n",
    "RoPE encodes position through **rotation in complex space**, enabling:\n",
    "- Relative position awareness\n",
    "- Better extrapolation to longer sequences\n",
    "- No learnable position parameters\n",
    "\n",
    "**Used in:** Llama, Mistral, Falcon, CodeLlama\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a query $q$ at position $m$ and key $k$ at position $n$:\n",
    "\n",
    "$$q_m \\cdot k_n = \\text{Re}[(q e^{im\\theta}) \\cdot (k e^{in\\theta})^*] = \\text{Re}[q \\cdot k^* \\cdot e^{i(m-n)\\theta}]$$\n",
    "\n",
    "The attention score depends on **relative position** $(m-n)$, not absolute positions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.llm.attention import AttentionWithRoPE\n",
    "\n",
    "# Create RoPE attention\n",
    "d_model, num_heads = 64, 4\n",
    "rope_attn = AttentionWithRoPE(d_model, num_heads, max_len=256)\n",
    "\n",
    "# Test forward pass\n",
    "x = torch.randn(2, 16, d_model)\n",
    "output = rope_attn(x)\n",
    "print(f\"RoPE Attention output shape: {output.shape}\")\n",
    "\n",
    "# Visualize RoPE frequencies\n",
    "dim = d_model // num_heads\n",
    "positions = np.arange(64)\n",
    "freqs = 1.0 / (10000 ** (np.arange(0, dim, 2) / dim))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Frequency spectrum\n",
    "ax = axes[0]\n",
    "ax.plot(np.arange(len(freqs)), freqs, 'b-o')\n",
    "ax.set_xlabel('Dimension pair index')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('RoPE Frequency Spectrum')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Rotation angles for different positions\n",
    "ax = axes[1]\n",
    "for i, freq in enumerate(freqs[:4]):\n",
    "    angles = positions * freq\n",
    "    ax.plot(positions, np.sin(angles), label=f'Dim {i}: freq={freq:.4f}')\n",
    "ax.set_xlabel('Position')\n",
    "ax.set_ylabel('sin(position Ã— frequency)')\n",
    "ax.set_title('Rotation Patterns by Dimension')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Grouped Query Attention (GQA)\n",
    "\n",
    "GQA reduces memory by sharing Key-Value heads across Query heads.\n",
    "\n",
    "**Used in:** Llama 2 (70B uses 8 KV heads for 64 Q heads = 8x reduction)\n",
    "\n",
    "| Variant | Q Heads | KV Heads | Memory Savings |\n",
    "|---------|---------|----------|----------------|\n",
    "| MHA | 32 | 32 | Baseline |\n",
    "| GQA | 32 | 8 | 4x for KV cache |\n",
    "| MQA | 32 | 1 | 32x for KV cache |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from src.llm.attention import GroupedQueryAttention, MultiHeadAttention\n",
    "\n",
    "# Compare MHA vs GQA memory\n",
    "d_model = 512\n",
    "seq_len = 1024\n",
    "batch_size = 8\n",
    "\n",
    "# Standard Multi-Head Attention\n",
    "mha = MultiHeadAttention(d_model, num_heads=32)\n",
    "mha_params = sum(p.numel() for p in mha.parameters())\n",
    "\n",
    "# Grouped Query Attention (8 KV heads instead of 32)\n",
    "gqa = GroupedQueryAttention(d_model, num_heads=32, num_kv_heads=8)\n",
    "gqa_params = sum(p.numel() for p in gqa.parameters())\n",
    "\n",
    "print(f\"MHA parameters: {mha_params:,}\")\n",
    "print(f\"GQA parameters: {gqa_params:,}\")\n",
    "print(f\"Parameter reduction: {(1 - gqa_params/mha_params)*100:.1f}%\")\n",
    "\n",
    "# KV Cache size comparison\n",
    "kv_cache_mha = 2 * batch_size * seq_len * d_model  # K and V\n",
    "kv_cache_gqa = 2 * batch_size * seq_len * (d_model * 8 // 32)  # Reduced KV\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "methods = ['MHA\\n(32 KV heads)', 'GQA\\n(8 KV heads)', 'MQA\\n(1 KV head)']\n",
    "kv_sizes = [32, 8, 1]  # Relative KV cache size\n",
    "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(methods, kv_sizes, color=colors)\n",
    "ax.set_ylabel('Relative KV Cache Size')\n",
    "ax.set_title('Key-Value Cache Memory Comparison')\n",
    "\n",
    "for bar, size in zip(bars, kv_sizes):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "            f'{size}x', ha='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Flash Attention (Conceptual)\n",
    "\n",
    "Flash Attention achieves **2-4x speedup** by:\n",
    "1. **Tiling**: Process attention in blocks that fit in SRAM\n",
    "2. **Recomputation**: Recompute during backward pass instead of storing\n",
    "3. **Kernel fusion**: Combine ops to reduce memory transfers\n",
    "\n",
    "```\n",
    "Standard Attention:           Flash Attention:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Full QK^T     â”‚ O(nÂ²)     â”‚   Block 1  â”‚ â”‚ â”‚ O(nÂ²/B)\n",
    "â”‚   in HBM        â”‚ memory   â”‚   in SRAM  â”‚ â”‚ â”‚ per tile\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Impact:** Enables training with 16x longer sequences!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.llm.attention import FlashAttention\n",
    "\n",
    "# Create Flash Attention (simplified implementation)\n",
    "flash_attn = FlashAttention(d_model=128, num_heads=8)\n",
    "\n",
    "# Test with longer sequence\n",
    "x = torch.randn(4, 256, 128)\n",
    "output = flash_attn(x)\n",
    "print(f\"Flash Attention output: {output.shape}\")\n",
    "\n",
    "# Memory comparison visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "standard_mem = [s**2 for s in seq_lengths]  # O(nÂ²)\n",
    "flash_mem = [s * 256 for s in seq_lengths]  # O(n Ã— block_size)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(seq_lengths, [m/1e6 for m in standard_mem], 'b-o', label='Standard Attention', lw=2)\n",
    "plt.plot(seq_lengths, [m/1e6 for m in flash_mem], 'g-s', label='Flash Attention', lw=2)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Memory (millions of elements)')\n",
    "plt.title('Memory Scaling: Standard vs Flash Attention')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAt seq_len=8192:\")\n",
    "print(f\"  Standard: {standard_mem[-1]/1e6:.1f}M elements\")\n",
    "print(f\"  Flash: {flash_mem[-1]/1e6:.1f}M elements\")\n",
    "print(f\"  Reduction: {standard_mem[-1]/flash_mem[-1]:.0f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 4: Building a Mini-GPT\n",
    "---\n",
    "\n",
    "Now let's assemble these components into a working **GPT-style decoder block**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.llm.attention import CausalSelfAttention\n",
    "\n",
    "class MiniGPTBlock(nn.Module):\n",
    "    \"\"\"A single GPT decoder block.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, num_heads: int, block_size: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, num_heads, block_size, dropout)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Pre-norm architecture (like GPT-2)\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"Minimal GPT model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, num_heads: int = 4,\n",
    "                 num_layers: int = 4, block_size: int = 128):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb = nn.Embedding(block_size, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MiniGPTBlock(d_model, num_heads, block_size)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.ln_f = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor:\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_emb(idx)  # (B, T, d_model)\n",
    "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))  # (T, d_model)\n",
    "        x = tok_emb + pos_emb\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)  # (B, T, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Test the model\n",
    "model = MiniGPT(vocab_size=1000, d_model=128, num_heads=4, num_layers=2, block_size=64)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Forward pass\n",
    "idx = torch.randint(0, 1000, (2, 32))  # Batch of 2, seq_len 32\n",
    "logits = model(idx)\n",
    "\n",
    "print(f\"MiniGPT Parameters: {total_params:,}\")\n",
    "print(f\"Input shape: {idx.shape}\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  - Token embedding: 1000 Ã— 128\")\n",
    "print(f\"  - Position embedding: 64 Ã— 128\")\n",
    "print(f\"  - Transformer blocks: 2\")\n",
    "print(f\"  - Each block: CausalSelfAttention + MLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 5: Interview Questions - Advanced Attention\n",
    "---\n",
    "\n",
    "**Q1: Why does Flash Attention achieve O(1) memory for attention weights?**\n",
    "\n",
    "A: Flash Attention never materializes the full NÃ—N attention matrix. It computes attention in tiles,\n",
    "keeping only one tile in SRAM at a time. The numerator and denominator for softmax are accumulated\n",
    "across tiles using the online softmax trick.\n",
    "\n",
    "---\n",
    "\n",
    "**Q2: How does RoPE enable extrapolation to longer sequences?**\n",
    "\n",
    "A: RoPE encodes position through rotation, so the attention score depends only on **relative position**\n",
    "(m-n), not absolute positions. The model can theoretically handle any position as long as it can\n",
    "represent the relative distance. (Though in practice, NTK-aware scaling or ALiBi may be needed.)\n",
    "\n",
    "---\n",
    "\n",
    "**Q3: What's the trade-off between MHA, GQA, and MQA?**\n",
    "\n",
    "| Method | KV Cache | Quality | Use Case |\n",
    "|--------|----------|---------|----------|\n",
    "| MHA | Largest | Best | Training, smaller models |\n",
    "| GQA | Medium | Good | Llama 2 70B, balance |\n",
    "| MQA | Smallest | Lower | Ultra-fast inference |\n",
    "\n",
    "---\n",
    "\n",
    "**Q4: Why is pre-norm (layernorm before attention) preferred in modern LLMs?**\n",
    "\n",
    "A: Pre-norm creates a more direct gradient path (like ResNet) and improves training stability,\n",
    "especially for deep models. Post-norm (original Transformer) can have gradient issues in very deep networks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
