{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üìà Week 14: Advanced Topics in AI Engineering\n",
                "\n",
                "This notebook covers advanced topics for senior AI engineers.\n",
                "\n",
                "## Table of Contents\n",
                "1. [Model Optimization](#1-model-optimization)\n",
                "2. [Inference Optimization](#2-inference-optimization)\n",
                "3. [Multi-Modal Systems](#3-multi-modal-systems)\n",
                "4. [Distributed Training](#4-distributed-training)\n",
                "5. [Emerging Architectures](#5-emerging-architectures)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model Optimization\n",
                "\n",
                "### 1.1 Quantization\n",
                "\n",
                "Reduce model size and increase speed by lowering precision:\n",
                "\n",
                "| Precision | Bits | Memory | Speed | Quality |\n",
                "|-----------|------|--------|-------|--------|\n",
                "| FP32 | 32 | 1x | 1x | Baseline |\n",
                "| FP16 | 16 | 0.5x | 1.5-2x | ~Same |\n",
                "| INT8 | 8 | 0.25x | 2-4x | 95-99% |\n",
                "| INT4 | 4 | 0.125x | 3-5x | 90-95% |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def quantize_to_int8(weights, scale=None):\n",
                "    \"\"\"\n",
                "    Symmetric INT8 quantization.\n",
                "    \n",
                "    q = round(w / scale)\n",
                "    w_reconstructed = q * scale\n",
                "    \"\"\"\n",
                "    if scale is None:\n",
                "        scale = np.max(np.abs(weights)) / 127\n",
                "    \n",
                "    quantized = np.clip(np.round(weights / scale), -128, 127).astype(np.int8)\n",
                "    return quantized, scale\n",
                "\n",
                "def dequantize_int8(quantized, scale):\n",
                "    \"\"\"Reconstruct FP32 from INT8.\"\"\"\n",
                "    return quantized.astype(np.float32) * scale\n",
                "\n",
                "# Example\n",
                "weights = np.random.randn(1000).astype(np.float32)\n",
                "quantized, scale = quantize_to_int8(weights)\n",
                "reconstructed = dequantize_int8(quantized, scale)\n",
                "\n",
                "error = np.mean(np.abs(weights - reconstructed))\n",
                "print(f\"Original size:   {weights.nbytes:,} bytes\")\n",
                "print(f\"Quantized size:  {quantized.nbytes:,} bytes\")\n",
                "print(f\"Compression:     {weights.nbytes / quantized.nbytes:.1f}x\")\n",
                "print(f\"Mean abs error:  {error:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Knowledge Distillation\n",
                "\n",
                "Train a smaller \"student\" model to mimic a larger \"teacher\":\n",
                "\n",
                "$$L = \\alpha \\cdot L_{CE}(y, \\hat{y}_{student}) + (1-\\alpha) \\cdot L_{KD}(\\hat{y}_{teacher}, \\hat{y}_{student})$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0, alpha=0.5):\n",
                "    \"\"\"\n",
                "    Knowledge distillation loss.\n",
                "    \n",
                "    Args:\n",
                "        student_logits: Student model outputs\n",
                "        teacher_logits: Teacher model outputs\n",
                "        labels: Ground truth labels\n",
                "        temperature: Softmax temperature for soft targets\n",
                "        alpha: Weight for hard vs soft targets\n",
                "    \"\"\"\n",
                "    # Soft targets (from teacher)\n",
                "    soft_student = np.exp(student_logits / temperature)\n",
                "    soft_student = soft_student / soft_student.sum(axis=-1, keepdims=True)\n",
                "    \n",
                "    soft_teacher = np.exp(teacher_logits / temperature)\n",
                "    soft_teacher = soft_teacher / soft_teacher.sum(axis=-1, keepdims=True)\n",
                "    \n",
                "    # KL divergence for soft targets\n",
                "    kl_loss = np.sum(soft_teacher * np.log(soft_teacher / soft_student)) * (temperature ** 2)\n",
                "    \n",
                "    # Hard target loss (standard cross-entropy)\n",
                "    hard_loss = 0  # Simplified\n",
                "    \n",
                "    return alpha * hard_loss + (1 - alpha) * kl_loss\n",
                "\n",
                "print(\"Distillation function defined!\")\n",
                "print(\"\\nBenefits of distillation:\")\n",
                "print(\"  - 2-10x smaller models\")\n",
                "print(\"  - Often 95%+ of teacher performance\")\n",
                "print(\"  - Works with any architecture\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Inference Optimization\n",
                "\n",
                "### 2.1 KV Cache\n",
                "\n",
                "For autoregressive generation, cache key-value pairs to avoid recomputation:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class KVCache:\n",
                "    \"\"\"\n",
                "    Key-Value cache for efficient autoregressive generation.\n",
                "    \n",
                "    Without cache: O(n¬≤) per token (recompute all)\n",
                "    With cache:    O(n) per token (only new position)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_layers: int, max_length: int, dim: int):\n",
                "        self.num_layers = num_layers\n",
                "        self.max_length = max_length\n",
                "        self.dim = dim\n",
                "        self.cache = {}\n",
                "        self.length = 0\n",
                "    \n",
                "    def update(self, layer_idx: int, key: np.ndarray, value: np.ndarray):\n",
                "        \"\"\"Add new K, V to cache.\"\"\"\n",
                "        if layer_idx not in self.cache:\n",
                "            self.cache[layer_idx] = {\"key\": [], \"value\": []}\n",
                "        \n",
                "        self.cache[layer_idx][\"key\"].append(key)\n",
                "        self.cache[layer_idx][\"value\"].append(value)\n",
                "    \n",
                "    def get(self, layer_idx: int):\n",
                "        \"\"\"Get cached K, V.\"\"\"\n",
                "        if layer_idx in self.cache:\n",
                "            return (\n",
                "                np.concatenate(self.cache[layer_idx][\"key\"]),\n",
                "                np.concatenate(self.cache[layer_idx][\"value\"])\n",
                "            )\n",
                "        return None, None\n",
                "    \n",
                "    def clear(self):\n",
                "        self.cache = {}\n",
                "\n",
                "cache = KVCache(num_layers=12, max_length=2048, dim=768)\n",
                "print(\"‚úÖ KV Cache implemented for efficient generation\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Speculative Decoding\n",
                "\n",
                "Use a small model to draft, then verify with the large model:\n",
                "\n",
                "```\n",
                "1. Draft model generates K tokens quickly\n",
                "2. Target model verifies all K tokens in parallel\n",
                "3. Accept matching tokens, reject and resample from target\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Multi-Modal Systems\n",
                "\n",
                "### 3.1 Vision-Language Models\n",
                "\n",
                "| Model | Architecture | Input | Use Case |\n",
                "|-------|-------------|-------|----------|\n",
                "| CLIP | Dual encoder | Image + Text | Retrieval |\n",
                "| BLIP | Encoder-Decoder | Image ‚Üí Text | Captioning |\n",
                "| LLaVA | LLM + Vision | Image + Text | Chat |\n",
                "| GPT-4V | Transformer | Image + Text | General |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleVisionLanguageModel:\n",
                "    \"\"\"\n",
                "    Conceptual VLM architecture.\n",
                "    \n",
                "    Image ‚Üí Vision Encoder ‚Üí Projection ‚Üí LLM\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, vision_dim: int = 768, llm_dim: int = 4096):\n",
                "        self.vision_dim = vision_dim\n",
                "        self.llm_dim = llm_dim\n",
                "        \n",
                "        # Projection layer to align vision with LLM\n",
                "        self.projection = np.random.randn(vision_dim, llm_dim) * 0.01\n",
                "    \n",
                "    def encode_image(self, image):\n",
                "        \"\"\"Encode image to vision features.\"\"\"\n",
                "        # Simulated vision encoder output\n",
                "        vision_features = np.random.randn(196, self.vision_dim)  # 14x14 patches\n",
                "        return vision_features\n",
                "    \n",
                "    def project_to_llm(self, vision_features):\n",
                "        \"\"\"Project vision features to LLM embedding space.\"\"\"\n",
                "        return vision_features @ self.projection\n",
                "    \n",
                "    def forward(self, image, text_embeddings):\n",
                "        \"\"\"Process image and text together.\"\"\"\n",
                "        vision_features = self.encode_image(image)\n",
                "        projected = self.project_to_llm(vision_features)\n",
                "        \n",
                "        # Concatenate vision tokens with text tokens\n",
                "        combined = np.concatenate([projected, text_embeddings], axis=0)\n",
                "        return combined\n",
                "\n",
                "vlm = SimpleVisionLanguageModel()\n",
                "print(f\"Vision dim:     {vlm.vision_dim}\")\n",
                "print(f\"LLM dim:        {vlm.llm_dim}\")\n",
                "print(f\"Projection:     {vlm.projection.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Distributed Training\n",
                "\n",
                "### 4.1 Parallelism Strategies\n",
                "\n",
                "| Strategy | Splits | Use Case |\n",
                "|----------|--------|----------|\n",
                "| **Data Parallel** | Data batches | Most common |\n",
                "| **Tensor Parallel** | Individual layers | Large layers |\n",
                "| **Pipeline Parallel** | Sequential layers | Deep models |\n",
                "| **FSDP** | Both | Memory efficient |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data Parallel pseudocode\n",
                "print(\"\"\"\n",
                "Data Parallel Training:\n",
                "=======================\n",
                "\n",
                "# Each GPU gets different data batch\n",
                "for batch in data_loader:\n",
                "    # 1. Split batch across GPUs\n",
                "    batch_gpu0, batch_gpu1, batch_gpu2, batch_gpu3 = split(batch)\n",
                "    \n",
                "    # 2. Forward pass (parallel)\n",
                "    loss_0 = model_gpu0(batch_gpu0)\n",
                "    loss_1 = model_gpu1(batch_gpu1)\n",
                "    loss_2 = model_gpu2(batch_gpu2)\n",
                "    loss_3 = model_gpu3(batch_gpu3)\n",
                "    \n",
                "    # 3. Backward pass (parallel)\n",
                "    grads_0 = backward(loss_0)\n",
                "    grads_1 = backward(loss_1)\n",
                "    grads_2 = backward(loss_2)\n",
                "    grads_3 = backward(loss_3)\n",
                "    \n",
                "    # 4. All-reduce gradients\n",
                "    avg_grads = all_reduce_avg([grads_0, grads_1, grads_2, grads_3])\n",
                "    \n",
                "    # 5. Update weights (same on all GPUs)\n",
                "    optimizer.step(avg_grads)\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Emerging Architectures\n",
                "\n",
                "### 5.1 Mixture of Experts (MoE)\n",
                "\n",
                "```\n",
                "Input ‚Üí Router ‚Üí [Expert 1] ‚Üê‚îÄ Selected\n",
                "            ‚ï≤         [Expert 2]\n",
                "             ‚ï≤        [Expert 3] ‚Üê‚îÄ Selected\n",
                "              ‚ï≤       [Expert 4]\n",
                "               ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Weighted Sum ‚Üí Output\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SimpleMoE:\n",
                "    \"\"\"\n",
                "    Simple Mixture of Experts layer.\n",
                "    \n",
                "    - Router selects top-k experts per token\n",
                "    - Only selected experts compute\n",
                "    - Enables massive models with sparse compute\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, num_experts: int = 8, top_k: int = 2, dim: int = 768):\n",
                "        self.num_experts = num_experts\n",
                "        self.top_k = top_k\n",
                "        \n",
                "        # Expert networks (simplified as linear)\n",
                "        self.experts = [np.random.randn(dim, dim) for _ in range(num_experts)]\n",
                "        \n",
                "        # Router\n",
                "        self.router = np.random.randn(dim, num_experts)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Forward pass with sparse computation.\"\"\"\n",
                "        # Compute router scores\n",
                "        scores = x @ self.router  # [batch, num_experts]\n",
                "        \n",
                "        # Select top-k experts\n",
                "        top_k_indices = np.argsort(scores)[:, -self.top_k:]\n",
                "        \n",
                "        # Softmax over selected experts\n",
                "        top_k_scores = np.take_along_axis(scores, top_k_indices, axis=1)\n",
                "        weights = np.exp(top_k_scores) / np.sum(np.exp(top_k_scores), axis=1, keepdims=True)\n",
                "        \n",
                "        # Compute weighted sum of expert outputs\n",
                "        output = np.zeros_like(x)\n",
                "        for i, (idx, w) in enumerate(zip(top_k_indices, weights)):\n",
                "            for j, (expert_idx, weight) in enumerate(zip(idx, w)):\n",
                "                expert_out = x[i:i+1] @ self.experts[expert_idx]\n",
                "                output[i] += weight * expert_out.flatten()\n",
                "        \n",
                "        return output\n",
                "\n",
                "moe = SimpleMoE(num_experts=8, top_k=2)\n",
                "print(f\"Total experts:   {moe.num_experts}\")\n",
                "print(f\"Active per token: {moe.top_k}\")\n",
                "print(f\"Compute ratio:    {moe.top_k / moe.num_experts:.1%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìù Summary\n",
                "\n",
                "### Key Advanced Topics\n",
                "\n",
                "| Topic | Key Technique | Benefit |\n",
                "|-------|--------------|--------|\n",
                "| Quantization | INT8/INT4 | 2-4x smaller, faster |\n",
                "| Distillation | Teacher-student | 10x smaller models |\n",
                "| KV Cache | Reuse computation | Faster generation |\n",
                "| MoE | Sparse experts | Scale to trillions |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}