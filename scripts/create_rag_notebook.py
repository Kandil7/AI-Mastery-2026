import json
import os

def create_notebook():
    notebook = {
        "cells": [],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                },
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.8.5"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

    cells = []

    # Cell 1: Header (Markdown)
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "# =====================\n",
            "# Advanced RAG: Pushing Boundaries Beyond Basic Retrieval\n",
            "# Mathematical Theory -> Code Implementation -> Production Considerations\n",
            "# ====================="
        ]
    })

    # Cell 2: Markdown
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 1. Limitations of Standard RAG\n",
            "Retrieval-Augmented Generation (RAG) has become a standard for integrating external knowledge with Large Language Models (LLMs). However, standard RAG systems face practical challenges including:\n",
            "\n",
            "1. **Context Drift**: When the user's current context diverges from the context in retrieved data.\n",
            "2. **Incomplete Coverage**: When required information is distributed across multiple documents.\n",
            "3. **Noisy Retrieval**: When irrelevant documents are retrieved, confusing the model.\n",
            "4. **Context Window Limitations**: When retrieved documents exceed the model's context capacity.\n",
            "\n",
            "Traditional solutions like simple chunking are often insufficient, necessitating Advanced RAG techniques."
        ]
    })

    # Cell 3: Markdown
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 2. GraphRAG Theory: Representing Knowledge as Graphs\n",
            "GraphRAG represents a paradigm shift in retrieval, relying on understanding relationships between entities rather than just textual similarity.\n",
            "\n",
            "### 2.1 Key Graph Theory Concepts\n",
            "- **Nodes**: Represent entities (people, places, concepts).\n",
            "- **Edges**: Represent relationships between entities.\n",
            "- **Properties**: Additional attributes of nodes and edges.\n",
            "- **Weight**: Strength of the relationship.\n",
            "\n",
            "### 2.2 Retrieval Mathematics in GraphRAG\n",
            "When a query is received, it is converted into a graph traversal path using:\n",
            "1. **Semantic Similarity**: To find initial seed nodes.\n",
            "2. **Graph Propagation**: To find related nodes.\n",
            "3. **Shortest Path**: To connect different concepts.\n",
            "\n",
            "Algorithms like **PageRank** and **Personalized PageRank** are used to evaluate node importance in the context of a query.\n",
            "\n",
            "### 2.3 GraphRAG Advantage: Handling Complex Queries\n",
            "For queries like \"How did trade relations between Company X and Company Y change after Z became the new CEO?\", GraphRAG can:\n",
            "1. Identify key entities (X, Y, Z).\n",
            "2. Explore relationships between these entities over time.\n",
            "3. Aggregate information from multiple paths in the graph."
        ]
    })

    # Cell 4: Code (GraphRAG)
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "import numpy as np\n",
            "import networkx as nx\n",
            "from typing import Dict, Any, List, Tuple, Optional\n",
            "from dataclasses import dataclass\n",
            "import matplotlib.pyplot as plt\n",
            "from sklearn.metrics.pairwise import cosine_similarity\n",
            "from sentence_transformers import SentenceTransformer\n",
            "\n",
            "@dataclass\n",
            "class GraphNode:\n",
            "    \"\"\"Graph Node\"\"\"\n",
            "    id: str\n",
            "    type: str  # \"person\", \"organization\", \"location\", \"concept\", etc.\n",
            "    name: str\n",
            "    properties: Dict[str, Any]\n",
            "    embedding: Optional[np.ndarray] = None\n",
            "\n",
            "@dataclass\n",
            "class GraphEdge:\n",
            "    \"\"\"Graph Edge\"\"\"\n",
            "    source_id: str\n",
            "    target_id: str\n",
            "    relation_type: str  # \"works_at\", \"located_in\", \"influenced_by\", etc.\n",
            "    weight: float = 1.0\n",
            "    properties: Dict[str, Any] = None\n",
            "\n",
            "class KnowledgeGraph:\n",
            "    \"\"\"Knowledge Graph Representation\"\"\"\n",
            "    \n",
            "    def __init__(self, embedding_model: str = \"all-mpnet-base-v2\"):\n",
            "        self.nodes: Dict[str, GraphNode] = {}\n",
            "        self.edges: List[GraphEdge] = []\n",
            "        self.graph = nx.Graph()\n",
            "        self.embedding_model = SentenceTransformer(embedding_model)\n",
            "    \n",
            "    def add_node(self, node: GraphNode):\n",
            "        \"\"\"Add node to graph\"\"\"\n",
            "        self.nodes[node.id] = node\n",
            "        self.graph.add_node(node.id, **{\n",
            "            'type': node.type,\n",
            "            'name': node.name,\n",
            "            'properties': node.properties\n",
            "        })\n",
            "    \n",
            "    def add_edge(self, edge: GraphEdge):\n",
            "        \"\"\"Add edge to graph\"\"\"\n",
            "        self.edges.append(edge)\n",
            "        self.graph.add_edge(edge.source_id, edge.target_id, **{\n",
            "            'relation_type': edge.relation_type,\n",
            "            'weight': edge.weight,\n",
            "            'properties': edge.properties\n",
            "        })\n",
            "    \n",
            "    def build_from_documents(self, documents: List[Dict[str, Any]]):\n",
            "        \"\"\"Build graph from documents (Stub for implementation)\"\"\"\n",
            "        pass\n",
            "    \n",
            "    def retrieve_context(self, query: str, max_hops: int = 2, top_k: int = 5) -> List[Dict[str, Any]]:\n",
            "        \"\"\"\n",
            "        Retrieve context using GraphRAG\n",
            "        \n",
            "        Args:\n",
            "            query: Text query\n",
            "            max_hops: Maximum hops in graph traversal\n",
            "            top_k: Number of top results\n",
            "        \n",
            "        Returns:\n",
            "            List of retrieved contexts\n",
            "        \"\"\"\n",
            "        # 1. Encode query\n",
            "        query_embedding = self.embedding_model.encode([query])[0]\n",
            "        \n",
            "        # 2. Find nearest nodes to query\n",
            "        candidate_nodes = []\n",
            "        for node_id, node in self.nodes.items():\n",
            "            if node.embedding is not None:\n",
            "                similarity = cosine_similarity([query_embedding], [node.embedding])[0][0]\n",
            "                candidate_nodes.append((node_id, similarity))\n",
            "        \n",
            "        # Sort and select top candidates\n",
            "        candidate_nodes.sort(key=lambda x: x[1], reverse=True)\n",
            "        seed_nodes = [node_id for node_id, _ in candidate_nodes[:top_k]]\n",
            "        \n",
            "        # 3. Explore graph from seed nodes\n",
            "        context_nodes = set()\n",
            "        for seed_node in seed_nodes:\n",
            "            # BFS with hop limit\n",
            "            neighbors = nx.single_source_shortest_path_length(self.graph, seed_node, cutoff=max_hops)\n",
            "            context_nodes.update(neighbors.keys())\n",
            "        \n",
            "        # 4. Aggregate context\n",
            "        context = []\n",
            "        for node_id in context_nodes:\n",
            "            node = self.nodes[node_id]\n",
            "            context.append({\n",
            "                'id': node.id,\n",
            "                'name': node.name,\n",
            "                'type': node.type,\n",
            "                'properties': node.properties,\n",
            "                'neighbors': list(self.graph.neighbors(node_id))\n",
            "            })\n",
            "        \n",
            "        return context\n",
            "    \n",
            "    def visualize_subgraph(self, node_ids: List[str], output_path: str = \"graph_viz.png\"):\n",
            "        \"\"\"Visualize subgraph\"\"\"\n",
            "        subgraph = self.graph.subgraph(node_ids)\n",
            "        \n",
            "        plt.figure(figsize=(12, 10))\n",
            "        pos = nx.spring_layout(subgraph, k=0.5, iterations=50)\n",
            "        \n",
            "        # Draw nodes by type\n",
            "        node_types = set(nx.get_node_attributes(subgraph, 'type').values())\n",
            "        colors = plt.cm.tab20(np.linspace(0, 1, len(node_types)))\n",
            "        type_to_color = {t: colors[i] for i, t in enumerate(node_types)}\n",
            "        \n",
            "        for node_type in node_types:\n",
            "            nodes_of_type = [n for n, attr in subgraph.nodes(data=True) if attr['type'] == node_type]\n",
            "            nx.draw_networkx_nodes(\n",
            "                subgraph, pos, nodelist=nodes_of_type,\n",
            "                node_color=[type_to_color[node_type]], node_size=500,\n",
            "                label=node_type\n",
            "            )\n",
            "        \n",
            "        # Draw edges\n",
            "        nx.draw_networkx_edges(subgraph, pos, alpha=0.5)\n",
            "        \n",
            "        # Draw labels\n",
            "        labels = {n: attr['name'][:10] + \"...\" if len(attr['name']) > 10 else attr['name'] \n",
            "                 for n, attr in subgraph.nodes(data=True)}\n",
            "        nx.draw_networkx_labels(subgraph, pos, labels, font_size=8)\n",
            "        \n",
            "        plt.legend()\n",
            "        plt.axis('off')\n",
            "        plt.tight_layout()\n",
            "        plt.savefig(output_path)\n",
            "        plt.close()\n",
            "        \n",
            "        print(f\"Graph visualization saved to {output_path}\")"
        ]
    })

    # Cell 5: Markdown (Hybrid Search)
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 3. Hybrid Search: Combining the Best of Both Worlds\n",
            "Hybrid Search integrates Dense Retrieval (Vector Search) and Sparse Retrieval (Keyword Search).\n",
            "\n",
            "### 3.1 Mathematical Theory\n",
            "Relevance is calculated using a weighted combination function:\n",
            "\n",
            "$$Relevance = \\alpha \\cdot DenseRelevance + (1 - \\alpha) \\cdot SparseRelevance$$\n",
            "\n",
            "Where:\n",
            "- **DenseRelevance**: Cosine similarity between query and document embeddings.\n",
            "- **SparseRelevance**: BM25 or TF-IDF score matching keywords.\n",
            "- **α**: Weight parameter determining the balance (typically 0.5).\n",
            "\n",
            "### 3.2 Re-ranking\n",
            "After initial retrieval, specialized re-ranking models refine results:\n",
            "\n",
            "1. **Cross-Encoders**: BERT-based models scoring pairs (Query, Document) for high precision.\n",
            "2. **ColBERT**: Computes similarity at the token level while maintaining efficiency.\n",
            "\n",
            "### 3.3 Trade-off Analysis\n",
            "| Technique | Pros | Cons | Best Use Case |\n",
            "|----------|---------|--------|--------------|\n",
            "| Vector Search (Dense) | Handles synonyms & semantic meaning | Fails with exact keywords/IDs | Complex, conceptual queries |\n",
            "| Keyword Search (Sparse) | Exact matches for terms/codes | Fails with synonyms/intent | Searching for specific terms |\n",
            "| Hybrid Search | Best of both worlds | Higher computational cost | Most production scenarios |"
        ]
    })

    # Cell 6: Code (Hybrid Retriever)
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class HybridRetriever:\n",
            "    \"\"\"Hybrid Retrieval System combining Vector and Keyword Search\"\"\"\n",
            "    \n",
            "    def __init__(self, vector_index, sparse_index, reranker=None, alpha=0.5):\n",
            "        \"\"\"\n",
            "        Args:\n",
            "            vector_index: Dense search index\n",
            "            sparse_index: Sparse/Keyword search index\n",
            "            reranker: Re-ranking model (optional)\n",
            "            alpha: Weight for dense search (0-1)\n",
            "        \"\"\"\n",
            "        self.vector_index = vector_index\n",
            "        self.sparse_index = sparse_index\n",
            "        self.reranker = reranker\n",
            "        self.alpha = alpha\n",
            "    \n",
            "    def retrieve(self, query: str, top_k: int = 10, rerank: bool = True) -> List[Dict[str, Any]]:\n",
            "        \"\"\"\n",
            "        Execute hybrid retrieval\n",
            "        \n",
            "        Args:\n",
            "            query: Search query\n",
            "            top_k: Number of expected results\n",
            "            rerank: Whether to apply re-ranking\n",
            "        \n",
            "        Returns:\n",
            "            List of ranked results\n",
            "        \"\"\"\n",
            "        # 1. Vector Search\n",
            "        vector_results = self.vector_index.search(query, top_k=top_k*2)\n",
            "        \n",
            "        # 2. Sparse Search\n",
            "        sparse_results = self.sparse_index.search(query, top_k=top_k*2)\n",
            "        \n",
            "        # 3. Fuse Results (Reciprocal Rank Fusion or Score Weighted)\n",
            "        combined_results = self._combine_results(vector_results, sparse_results)\n",
            "        \n",
            "        # 4. Cutoff\n",
            "        combined_results = combined_results[:top_k]\n",
            "        \n",
            "        # 5. Re-ranking\n",
            "        if rerank and self.reranker:\n",
            "            combined_results = self.reranker.rerank(query, combined_results)\n",
            "        \n",
            "        return combined_results\n",
            "    \n",
            "    def _combine_results(self, vector_results: List[Dict[str, Any]], \n",
            "                        sparse_results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
            "        \"\"\"Fuse dense and sparse results\"\"\"\n",
            "        score_map = {}\n",
            "        \n",
            "        # Process Vector Results\n",
            "        for i, result in enumerate(vector_results):\n",
            "            doc_id = result['id']\n",
            "            # Normalize score 0-1 (approx)\n",
            "            normalized_score = 1 - (i / len(vector_results))\n",
            "            score_map[doc_id] = {\n",
            "                'score': normalized_score * self.alpha,\n",
            "                'vector_rank': i,\n",
            "                'sparse_rank': None,\n",
            "                'content': result['content'],\n",
            "                'metadata': result['metadata']\n",
            "            }\n",
            "        \n",
            "        # Process Sparse Results\n",
            "        for i, result in enumerate(sparse_results):\n",
            "            doc_id = result['id']\n",
            "            normalized_score = 1 - (i / len(sparse_results))\n",
            "            \n",
            "            if doc_id in score_map:\n",
            "                # Update existing score\n",
            "                score_map[doc_id]['score'] += normalized_score * (1 - self.alpha)\n",
            "                score_map[doc_id]['sparse_rank'] = i\n",
            "            else:\n",
            "                score_map[doc_id] = {\n",
            "                    'score': normalized_score * (1 - self.alpha),\n",
            "                    'vector_rank': None,\n",
            "                    'sparse_rank': i,\n",
            "                    'content': result['content'],\n",
            "                    'metadata': result['metadata']\n",
            "                }\n",
            "        \n",
            "        # Sort by final score\n",
            "        sorted_results = sorted(score_map.items(), key=lambda x: x[1]['score'], reverse=True)\n",
            "        \n",
            "        # Format Output\n",
            "        results = []\n",
            "        for doc_id, data in sorted_results:\n",
            "            results.append({\n",
            "                'id': doc_id,\n",
            "                'score': data['score'],\n",
            "                'vector_rank': data['vector_rank'],\n",
            "                'sparse_rank': data['sparse_rank'],\n",
            "                'content': data['content'],\n",
            "                'metadata': data['metadata']\n",
            "            })\n",
            "        \n",
            "        return results"
        ]
    })

    # Cell 7: Markdown (Memory Augmentation)
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 4. Memory Augmented RAG: Maintaining Context\n",
            "A major challenge in RAG is maintaining context across multi-turn conversations.\n",
            "\n",
            "### 4.1 Memory Types\n",
            "1. **Short-term Memory**: Current conversation context.\n",
            "2. **Medium-term Memory**: Summaries of recent sessions.\n",
            "3. **Long-term Memory**: Persisted key facts and user preferences.\n",
            "\n",
            "### 4.2 Workflow\n",
            "```mermaid\n",
            "graph TD\n",
            "    A[User Query] --> B{Context in Memory?}\n",
            "    B -->|Yes| C[Use Memory Context]\n",
            "    B -->|No| D[Query Knowledge Base]\n",
            "    D --> E[Retrieve Context]\n",
            "    E --> F[Fuse with Memory]\n",
            "    C --> G[Generate Answer]\n",
            "    F --> G\n",
            "    G --> H[Update Memory]\n",
            "    H --> I[Final Response]\n",
            "```\n",
            "\n",
            "### 4.3 Challenges\n",
            "- **Memory Bloat**: Context window overflow.\n",
            "  * *Solution*: Automatic summarization and eviction policies.\n",
            "- **Context Conflict**: Contradictory information.\n",
            "  * *Solution*: Source trustworthiness weighting."
        ]
    })

    # Cell 8: Code (Memory)
    cells.append({
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": [
            "class MemoryAugmentedRAG:\n",
            "    \"\"\"RAG System with Memory Capabilities\"\"\"\n",
            "    \n",
            "    def __init__(self, retriever, llm, memory_window=5):\n",
            "        self.retriever = retriever\n",
            "        self.llm = llm\n",
            "        self.memory_window = memory_window\n",
            "        self.conversation_history = []\n",
            "        self.key_facts = {}\n",
            "    \n",
            "    def add_to_memory(self, query: str, response: str, facts: List[str] = None):\n",
            "        \"\"\"Add interaction to memory\"\"\"\n",
            "        # Add to history\n",
            "        self.conversation_history.append({\n",
            "            'query': query,\n",
            "            'response': response,\n",
            "            'timestamp': time.time()\n",
            "        })\n",
            "        \n",
            "        # Maintain window size\n",
            "        if len(self.conversation_history) > self.memory_window:\n",
            "            self.conversation_history.pop(0)\n",
            "        \n",
            "        # Persist key facts\n",
            "        if facts:\n",
            "            for fact in facts:\n",
            "                fact_hash = hash(fact)\n",
            "                self.key_facts[fact_hash] = {\n",
            "                    'fact': fact,\n",
            "                    'timestamp': time.time(),\n",
            "                    'source_query': query\n",
            "                }\n",
            "    \n",
            "    def retrieve_context_with_memory(self, query: str, top_k: int = 5) -> str:\n",
            "        \"\"\"Retrieve context blending KB and Memory\"\"\"\n",
            "        # 1. Check relevant facts\n",
            "        relevant_facts = []\n",
            "        for fact_hash, data in self.key_facts.items():\n",
            "            if self._is_fact_relevant(query, data['fact']):\n",
            "                relevant_facts.append(data['fact'])\n",
            "        \n",
            "        # 2. Retrieve from KB\n",
            "        retrieved_contexts = self.retriever.retrieve(query, top_k=top_k)\n",
            "        base_context = \"\\n\\n\".join([result['content'] for result in retrieved_contexts])\n",
            "        \n",
            "        # 3. Format Memory Context\n",
            "        memory_context = \"\"\n",
            "        if relevant_facts:\n",
            "            memory_context += \"Key Facts:\\n\" + \"\\n\".join(relevant_facts) + \"\\n\\n\"\n",
            "        \n",
            "        if self.conversation_history:\n",
            "            memory_context += \"Previous Conversation:\\n\"\n",
            "            for i, turn in enumerate(self.conversation_history[-2:]):  # Last 2 turns\n",
            "                memory_context += f\"User: {turn['query']}\\n\"\n",
            "                memory_context += f\"Assistant: {turn['response']}\\n\"\n",
            "        \n",
            "        # 4. Combine\n",
            "        final_context = \"\"\n",
            "        if memory_context:\n",
            "            final_context += \"=== Memory Context ===\\n\" + memory_context + \"\\n\"\n",
            "        if base_context:\n",
            "            final_context += \"=== Knowledge Base Context ===\\n\" + base_context\n",
            "        \n",
            "        return final_context\n",
            "    \n",
            "    def _is_fact_relevant(self, query: str, fact: str) -> bool:\n",
            "        \"\"\"Check fact relevance (Stub)\"\"\"\n",
            "        # Use small LLM or embedding similarity in production\n",
            "        return True  # Simplified for demo\n",
            "    \n",
            "    def generate_response(self, query: str) -> str:\n",
            "        \"\"\"Generate RAG response\"\"\"\n",
            "        # 1. Retrieve blended context\n",
            "        context = self.retrieve_context_with_memory(query)\n",
            "        \n",
            "        # 2. Generate\n",
            "        prompt = f\"\"\"\n",
            "        You are an intelligent assistant using context from multiple sources.\n",
            "        \n",
            "        {context}\n",
            "        \n",
            "        Question: {query}\n",
            "        Answer:\n",
            "        \"\"\"\n",
            "        response = self.llm.generate(prompt)\n",
            "        \n",
            "        # 3. Update Memory\n",
            "        self.add_to_memory(query, response)\n",
            "        \n",
            "        return response"
        ]
    })

    # Cell 9: Markdown (Production Engineering)
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 5. Production Engineering: Deploying Advanced RAG\n",
            "The real challenge involves operationalizing these systems securely and reliably.\n",
            "\n",
            "### 5.1 Architecture\n",
            "```\n",
            "┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
            "│   User      │───▶│  API Gateway│───▶│  RAG Router │\n",
            "└─────────────┘    └─────────────┘    └─────────────┘\n",
            "                                          │\n",
            "                      ┌───────────────────┼───────────────────┐\n",
            "                      ▼                   ▼                   ▼\n",
            "              ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
            "              │Hybrid Search│    │  GraphRAG   │    │Memory System│\n",
            "              └─────────────┘    └─────────────┘    └─────────────┘\n",
            "                      │                   │                   │\n",
            "                      └───────────────────┼───────────────────┘\n",
            "                                          ▼\n",
            "                                  ┌─────────────┐\n",
            "                                  │  LLM Router │\n",
            "                                  └─────────────┘\n",
            "                                          │\n",
            "                      ┌───────────────────┼───────────────────┐\n",
            "                      ▼                   ▼                   ▼\n",
            "              ┌─────────────┐    ┌─────────────┐    ┌─────────────┐\n",
            "              │ Small Model │    │Medium Model │    │Large Model  │\n",
            "              │ (Fast/cheap)│    │ (Balanced)  │    │(Slow/expensive)│\n",
            "              └─────────────┘    └─────────────┘    └─────────────┘\n",
            "```\n",
            "\n",
            "### 5.2 Observability & Metrics\n",
            "- **Retrieval Quality**: Precision@K, Recall@K.\n",
            "- **Generation Quality**: BLEU, ROUGE, or LLM-as-a-Judge evaluation.\n",
            "- **Latency**: End-to-end response time.\n",
            "- **Cost**: Token usage per query.\n",
            "\n",
            "### 5.3 Practical Challenges\n",
            "- **Consistency**: Ensuring deterministic outputs for identical inputs.\n",
            "- **Bias**: Detecting and mitigating KB bias.\n",
            "- **Security**: Preventing Prompt Injection and Data Exfiltration."
        ]
    })

    # Cell 10: Markdown (Advanced Challenges)
    cells.append({
        "cell_type": "markdown",
        "metadata": {},
        "source": [
            "## 6. Advanced Challenges\n",
            "### 6.1 Multi-Source Aggregation\n",
            "Build a system capable of:\n",
            "1. Retrieving from PDFs, SQL databases, and Knowledge Graphs simultaneously.\n",
            "2. Logically merging conflicting information.\n",
            "\n",
            "### 6.2 Ambiguity Resolution\n",
            "Develop a system allowing:\n",
            "1. Clarifying questions back to the user.\n",
            "2. Suggesting related queries.\n",
            "\n",
            "### 6.3 User Adaptation\n",
            "Create a system that:\n",
            "1. Learns from feedback (thumbs up/down).\n",
            "2. Adapts verbosity based on user expertise.\n",
            "\n",
            "## 7. Conclusion\n",
            "Advanced RAG is about bridging the gap between static knowledge and dynamic human reasoning. By leveraging GraphRAG, Hybrid Search, and Memory, we create AI systems that don't just 'search' but 'understand' and 'remember'. Success requires a tight integration of mathematical theory and robust software engineering."
        ]
    })

    notebook["cells"] = cells

    output_dir = "k:\\learning\\technical\\ai-ml\\AI-Mastery-2026\\notebooks\\week_11"
    os.makedirs(output_dir, exist_ok=True)
    
    output_path = os.path.join(output_dir, "03_rag_advanced_techniques.ipynb")
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(notebook, f, indent=1, ensure_ascii=False)
    
    print(f"Notebook created at: {output_path}")

if __name__ == "__main__":
    create_notebook()
