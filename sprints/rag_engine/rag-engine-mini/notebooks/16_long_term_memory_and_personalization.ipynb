{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Level 14: Long-Term Memory & Personalization\n",
                "### The Final Frontier: Building a Digital Partner\n",
                "\n",
                "In this absolute final notebook, we implement **Persistent Memory**. We will build an AI that doesn't just answer questions, but \"learns\" about you and adapts its behavior over time.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Memory Store\n",
                "\n",
                "We need a place to store \"User Facts\" that survives beyond the chat context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class UserMemory:\n",
                "    def __init__(self):\n",
                "        self.facts = {}\n",
                "\n",
                "    def learn_fact(self, key: str, value: str):\n",
                "        self.facts[key] = value\n",
                "\n",
                "    def get_context_string(self):\n",
                "        if not self.facts:\n",
                "            return \"No specific user preferences known.\"\n",
                "        return \"\\n\".join([f\"- {k}: {v}\" for k, v in self.facts.items()])\n",
                "\n",
                "memory = UserMemory()\n",
                "print(\"Initial Memory:\", memory.get_context_string())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Reflection Agent\n",
                "\n",
                "This agent \"reflects\" on the conversation after it happens to extract knowledge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def simulate_reflection(chat_history: str):\n",
                "    print(\"[Reflection Agent] Analyzing chat history...\")\n",
                "    # In a real system, an LLM would summarize this history\n",
                "    if \"python\" in chat_history.lower():\n",
                "        memory.learn_fact(\"preferred_language\", \"Python\")\n",
                "    if \"concise\" in chat_history.lower():\n",
                "        memory.learn_fact(\"response_style\", \"Very concise and technical\")\n",
                "    \n",
                "    print(\"[Reflection Agent] Memory Updated!\")\n",
                "\n",
                "history = \"User: Can you explain RAG in Python? I am a senior dev and I like concise answers.\"\n",
                "simulate_reflection(history)\n",
                "print(\"\\nCurrent Memory Context:\\n\", memory.get_context_string())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Adaptive RAG Response\n",
                "\n",
                "Now we use the memory to change how the RAG system behaves."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def adaptive_rag_agent(user_query: str):\n",
                "    user_profile = memory.get_context_string()\n",
                "    \n",
                "    print(f\"[System] Loading User Profile... \")\n",
                "    print(f\"[System] Context: {user_profile}\")\n",
                "    \n",
                "    # Logic influenced by memory\n",
                "    if \"Python\" in user_profile and \"concise\" in user_profile.lower():\n",
                "        return \"[Adaptive Response] RAG = Search + LLM Prompt. Use 'qdrant-client' and 'openai' libs. High speed, low fluff.\"\n",
                "    \n",
                "    return \"[Standard Response] RAG is a technique to retrieve documents and use them as context for an LLM...\"\n",
                "\n",
                "print(\"New Query: 'What is RAG?'\")\n",
                "print(\"Result:\", adaptive_rag_agent(\"What is RAG?\"))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Grand Finale: You are the Master ðŸŽ“\n",
                "\n",
                "You have completed all **14 Levels** of the AI-Mastery-2026 journey.\n",
                "\n",
                "You have built a system that:\n",
                "1.  **Reads** and **Stores** (RAG Stage 1 & 2).\n",
                "2.  **Reasons** and **Self-Corrects** (Stage 3).\n",
                "3.  **Acts** and **Searches** the web (Stage 5).\n",
                "4.  **Analyzes** its own vector space (Performance Level).\n",
                "5.  **Navigates** Knowledge Graphs (Legend Status).\n",
                "6.  **Collaborates** in Swarms (Architect Level).\n",
                "7.  **Defends** against attacks (Security Level).\n",
                "8.  **Remembers** and **Personalizes** (Final Level).\n",
                "\n",
                "### **THE END.**\n",
                "**Go out and build the next revolution.**\n",
                "\n",
                "**- Antigravity**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}