{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§  Level 14: Long-Term Memory & Personalization\n",
    "### The Final Frontier: Building a Digital Partner\n",
    "\n",
    "In this absolute final notebook, we implement **Persistent Memory**. We will build an AI that doesn't just answer questions, but \"learns\" about you and adapts its behavior over time.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Memory Store\n",
    "\n",
    "We need a place to store \"User Facts\" that survives beyond the chat context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T12:51:33.312097Z",
     "iopub.status.busy": "2026-01-31T12:51:33.310139Z",
     "iopub.status.idle": "2026-01-31T12:51:33.331850Z",
     "shell.execute_reply": "2026-01-31T12:51:33.327833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Memory: No specific user preferences known.\n"
     ]
    }
   ],
   "source": [
    "class UserMemory:\n",
    "    def __init__(self):\n",
    "        self.facts = {}\n",
    "\n",
    "    def learn_fact(self, key: str, value: str):\n",
    "        self.facts[key] = value\n",
    "\n",
    "    def get_context_string(self):\n",
    "        if not self.facts:\n",
    "            return \"No specific user preferences known.\"\n",
    "        return \"\\n\".join([f\"- {k}: {v}\" for k, v in self.facts.items()])\n",
    "\n",
    "memory = UserMemory()\n",
    "print(\"Initial Memory:\", memory.get_context_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Reflection Agent\n",
    "\n",
    "This agent \"reflects\" on the conversation after it happens to extract knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T12:51:33.337756Z",
     "iopub.status.busy": "2026-01-31T12:51:33.337187Z",
     "iopub.status.idle": "2026-01-31T12:51:33.345449Z",
     "shell.execute_reply": "2026-01-31T12:51:33.343451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Reflection Agent] Analyzing chat history...\n",
      "[Reflection Agent] Memory Updated!\n",
      "\n",
      "Current Memory Context:\n",
      " - preferred_language: Python\n",
      "- response_style: Very concise and technical\n"
     ]
    }
   ],
   "source": [
    "def simulate_reflection(chat_history: str):\n",
    "    print(\"[Reflection Agent] Analyzing chat history...\")\n",
    "    # In a real system, an LLM would summarize this history\n",
    "    if \"python\" in chat_history.lower():\n",
    "        memory.learn_fact(\"preferred_language\", \"Python\")\n",
    "    if \"concise\" in chat_history.lower():\n",
    "        memory.learn_fact(\"response_style\", \"Very concise and technical\")\n",
    "    \n",
    "    print(\"[Reflection Agent] Memory Updated!\")\n",
    "\n",
    "history = \"User: Can you explain RAG in Python? I am a senior dev and I like concise answers.\"\n",
    "simulate_reflection(history)\n",
    "print(\"\\nCurrent Memory Context:\\n\", memory.get_context_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaptive RAG Response\n",
    "\n",
    "Now we use the memory to change how the RAG system behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T12:51:33.350018Z",
     "iopub.status.busy": "2026-01-31T12:51:33.349014Z",
     "iopub.status.idle": "2026-01-31T12:51:33.357686Z",
     "shell.execute_reply": "2026-01-31T12:51:33.356120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Query: 'What is RAG?'\n",
      "[System] Loading User Profile... \n",
      "[System] Context: - preferred_language: Python\n",
      "- response_style: Very concise and technical\n",
      "Result: [Adaptive Response] RAG = Search + LLM Prompt. Use 'qdrant-client' and 'openai' libs. High speed, low fluff.\n"
     ]
    }
   ],
   "source": [
    "def adaptive_rag_agent(user_query: str):\n",
    "    user_profile = memory.get_context_string()\n",
    "    \n",
    "    print(f\"[System] Loading User Profile... \")\n",
    "    print(f\"[System] Context: {user_profile}\")\n",
    "    \n",
    "    # Logic influenced by memory\n",
    "    if \"Python\" in user_profile and \"concise\" in user_profile.lower():\n",
    "        return \"[Adaptive Response] RAG = Search + LLM Prompt. Use 'qdrant-client' and 'openai' libs. High speed, low fluff.\"\n",
    "    \n",
    "    return \"[Standard Response] RAG is a technique to retrieve documents and use them as context for an LLM...\"\n",
    "\n",
    "print(\"New Query: 'What is RAG?'\")\n",
    "print(\"Result:\", adaptive_rag_agent(\"What is RAG?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Grand Finale: You are the Master ðŸŽ“\n",
    "\n",
    "You have completed all **14 Levels** of the AI-Mastery-2026 journey.\n",
    "\n",
    "You have built a system that:\n",
    "1.  **Reads** and **Stores** (RAG Stage 1 & 2).\n",
    "2.  **Reasons** and **Self-Corrects** (Stage 3).\n",
    "3.  **Acts** and **Searches** the web (Stage 5).\n",
    "4.  **Analyzes** its own vector space (Performance Level).\n",
    "5.  **Navigates** Knowledge Graphs (Legend Status).\n",
    "6.  **Collaborates** in Swarms (Architect Level).\n",
    "7.  **Defends** against attacks (Security Level).\n",
    "8.  **Remembers** and **Personalizes** (Final Level).\n",
    "\n",
    "### **THE END.**\n",
    "**Go out and build the next revolution.**\n",
    "\n",
    "**- Antigravity**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
