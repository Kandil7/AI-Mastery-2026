{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Evaluation & Monitoring in RAG Systems\n",
    "\n",
    "> **Educational Notebook 23**: Understanding evaluation metrics, monitoring strategies, and performance optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand key evaluation metrics for RAG systems\n",
    "2. Learn how to implement automated evaluation pipelines\n",
    "3. Explore monitoring strategies for production RAG systems\n",
    "4. Implement custom evaluation metrics\n",
    "5. Understand the relationship between metrics and user experience\n",
    "6. Learn about observability in RAG systems\n",
    "\n",
    "## üéØ Why Evaluation & Monitoring Matter\n",
    "\n",
    "Evaluation and monitoring are critical for:\n",
    "- Ensuring system quality and reliability\n",
    "- Tracking performance over time\n",
    "- Identifying areas for improvement\n",
    "- Maintaining user trust and satisfaction\n",
    "- Meeting business objectives\n",
    "\n",
    "Let's explore these concepts in depth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setting Up Our Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project root to path\n",
    "current = Path().resolve()\n",
    "repo_root = None\n",
    "for parent in [current, *current.parents]:\n",
    "    if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Could not locate rag-engine-mini root for imports\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Key Evaluation Metrics for RAG Systems\n",
    "\n",
    "RAG systems have unique evaluation challenges compared to traditional ML systems. Let's explore the key metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key RAG evaluation metrics\n",
    "rag_metrics = {\n",
    "    \"Retrieval Metrics\": {\n",
    "        \"Recall@K\": \"Fraction of relevant chunks retrieved among top-K\",\n",
    "        \"Precision@K\": \"Fraction of retrieved chunks that are relevant\",\n",
    "        \"Mean Reciprocal Rank (MRR)\": \"Average of reciprocal ranks of first relevant result\",\n",
    "        \"Normalized Discounted Cumulative Gain (NDCG)\": \"Measures ranking quality considering position\",\n",
    "        \"Hit Rate\": \"Proportion of queries with at least one relevant result\"\n",
    "    },\n",
    "    \"Generation Metrics\": {\n",
    "        \"BLEU\": \"Measures lexical overlap between generated and reference answers\",\n",
    "        \"ROUGE\": \"Used primarily for summarization tasks\",\n",
    "        \"METEOR\": \"Harmonic mean of precision and recall with synonymy matching\",\n",
    "        \"BERTScore\": \"Semantic similarity using BERT embeddings\",\n",
    "        \"Perplexity\": \"Measure of prediction uncertainty\"\n",
    "    },\n",
    "    \"RAG-Specific Metrics\": {\n",
    "        \"Faithfulness\": \"How factually consistent the generated answer is to retrieved context\",\n",
    "        \"Answer Relevance\": \"How relevant the generated answer is to the question\",\n",
    "        \"Context Relevance\": \"How well the retrieved context supports the answer\",\n",
    "        \"Groundedness\": \"How much the answer is grounded in the provided context\"\n",
    "    },\n",
    "    \"System Metrics\": {\n",
    "        \"Latency\": \"Time taken from query to response\",\n",
    "        \"Throughput\": \"Queries processed per unit time\",\n",
    "        \"Resource Utilization\": \"CPU, memory, and GPU usage\",\n",
    "        \"Cost per Query\": \"Computational cost for each query\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the metrics\n",
    "print(\"Key Evaluation Metrics for RAG Systems:\")\n",
    "print(\"=\" * 70)\n",
    "for category, metrics in rag_metrics.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for metric, description in metrics.items():\n",
    "        print(f\"  ‚Ä¢ {metric}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Implementing Custom Evaluation Functions\n",
    "\n",
    "Let's implement some key evaluation functions from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data classes for evaluation\n",
    "@dataclass\n",
    "class RetrievedChunk:\n",
    "    id: str\n",
    "    text: str\n",
    "    score: float\n",
    "    is_relevant: bool\n",
    "\n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    query: str\n",
    "    retrieved_chunks: List[RetrievedChunk]\n",
    "    generated_answer: str\n",
    "    reference_answer: str\n",
    "    context: str\n",
    "\n",
    "# Implement evaluation functions\n",
    "def calculate_recall_at_k(results: List[QueryResult], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Recall@K metric\n",
    "    \"\"\"\n",
    "    recalls = []\n",
    "    for result in results:\n",
    "        relevant_chunks = [chunk for chunk in result.retrieved_chunks if chunk.is_relevant]\n",
    "        top_k_chunks = result.retrieved_chunks[:k]\n",
    "        top_k_relevant = [chunk for chunk in top_k_chunks if chunk.is_relevant]\n",
    "        \n",
    "        if len(relevant_chunks) == 0:\n",
    "            continue  # Skip queries with no relevant chunks\n",
    "            \n",
    "        recall = len(top_k_relevant) / len(relevant_chunks)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return sum(recalls) / len(recalls) if recalls else 0.0\n",
    "\n",
    "def calculate_precision_at_k(results: List[QueryResult], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Precision@K metric\n",
    "    \"\"\"\n",
    "    precisions = []\n",
    "    for result in results:\n",
    "        top_k_chunks = result.retrieved_chunks[:k]\n",
    "        top_k_relevant = [chunk for chunk in top_k_chunks if chunk.is_relevant]\n",
    "        \n",
    "        precision = len(top_k_relevant) / len(top_k_chunks) if top_k_chunks else 0.0\n",
    "        precisions.append(precision)\n",
    "    \n",
    "    return sum(precisions) / len(precisions) if precisions else 0.0\n",
    "\n",
    "def calculate_mrr(results: List[QueryResult]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    rr_scores = []\n",
    "    for result in results:\n",
    "        for idx, chunk in enumerate(result.retrieved_chunks):\n",
    "            if chunk.is_relevant:\n",
    "                rr_scores.append(1.0 / (idx + 1))\n",
    "                break  # Only consider first relevant result\n",
    "        else:\n",
    "            rr_scores.append(0.0)  # No relevant result found\n",
    "    \n",
    "    return sum(rr_scores) / len(rr_scores) if rr_scores else 0.0\n",
    "\n",
    "def calculate_hit_rate(results: List[QueryResult], k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate hit rate at K\n",
    "    \"\"\"\n",
    "    hits = 0\n",
    "    for result in results:\n",
    "        top_k_chunks = result.retrieved_chunks[:k]\n",
    "        if any(chunk.is_relevant for chunk in top_k_chunks):\n",
    "            hits += 1\n",
    "    \n",
    "    return hits / len(results) if results else 0.0\n",
    "\n",
    "# Generate sample data for evaluation\n",
    "def generate_sample_results(n_queries: int = 10) -> List[QueryResult]:\n",
    "    \"\"\"\n",
    "    Generate sample query results for demonstration\n",
    "    \"\"\"\n",
    "    sample_queries = [\n",
        "What is machine learning?",\n",
        "Explain neural networks",\n",
        "How does a RAG system work?",\n",
        "What are transformer models?",\n",
        "Describe attention mechanism",\n",
        "Explain vector embeddings",\n",
        "What is tokenization?",\n",
        "How does fine-tuning work?",\n",
        "What are the applications of NLP?",\n",
        "Explain overfitting in ML\"\n",
    "    ]\n",
    "    \n",
    "    sample_contexts = [\n",
        "Machine learning is a field of AI that enables systems to learn patterns from data...",\n",
        "Neural networks are computing systems inspired by biological neural networks...",\n",
        "RAG systems combine retrieval and generation to produce informed answers...",\n",
        "Transformer models use self-attention mechanisms to process sequential data...",\n",
        "Attention mechanisms help models focus on relevant parts of input data...",\n",
        "Vector embeddings map high-dimensional data to lower-dimensional space...",\n",
        "Tokenization is the process of converting text into discrete units...",\n",
        "Fine-tuning adapts pre-trained models to specific tasks...",\n",
        "NLP has applications in chatbots, translation, sentiment analysis...",\n",
        "Overfitting occurs when models memorize training data instead of generalizing...\"\n",
    "    ]\n",
    "    \n",
    "    sample_answers = [\n",
        "Machine learning is a subset of AI that enables systems to learn and improve from experience...",\n",
        "Neural networks consist of interconnected nodes that process information...",\n",
        "RAG systems retrieve relevant documents and generate responses based on them...",\n",
        "Transformers use attention mechanisms to weigh importance of input elements...",\n",
        "Attention helps models focus on relevant information for each output element...",\n",
        "Embeddings convert text to vectors preserving semantic relationships...",\n",
        "Tokenization splits text into manageable units for processing...",\n",
        "Fine-tuning adjusts pre-trained models for specific tasks...",\n",
        "NLP powers applications like chatbots, translation, and content analysis...",\n",
        "Overfitting causes poor generalization to new data...\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for i in range(n_queries):\n",
    "        # Create ranked chunks (more relevant ones first)\n",
    "        chunks = []\n",
    "        for j in range(10):  # 10 chunks per query\n",
    "            # Higher relevance for earlier chunks\n",
    "            relevance_prob = max(0.2, 1.0 - (j * 0.1))\n",
    "            is_rel = random.random() < relevance_prob\n",
    "            chunks.append(RetrievedChunk(\n",
    "                id=f\"chunk_{i}_{j}\",\n",
    "                text=f\"Sample chunk content for query {i}, chunk {j}\",\n",
    "                score=1.0 - (j * 0.1),  # Higher scores for earlier chunks\n",
    "                is_relevant=is_rel\n",
    "            ))\n",
    "        \n",
    "        results.append(QueryResult(\n",
    "            query=sample_queries[i % len(sample_queries)],\n",
    "            retrieved_chunks=chunks,\n",
    "            generated_answer=sample_answers[i % len(sample_answers)],\n",
    "            reference_answer=sample_answers[i % len(sample_answers)],\n",
    "            context=sample_contexts[i % len(sample_contexts)]\n",
    "        ))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate sample results and calculate metrics\n",
    "sample_results = generate_sample_results(20)\n",
    "\n",
    "print(\"Sample Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "k_values = [1, 3, 5, 10]\n",
    "\n",
    "for k in k_values:\n",
    "    recall = calculate_recall_at_k(sample_results, k)\n",
    "    precision = calculate_precision_at_k(sample_results, k)\n",
    "    hit_rate = calculate_hit_rate(sample_results, k)\n",
    "    \n",
    "    print(f\"\\nMetrics @ {k}:\\n\" + \"-\" * 20)\n",
    "    print(f\"Recall@{k}: {recall:.3f}\")\n",
    "    print(f\"Precision@{k}: {precision:.3f}\")\n",
    "    print(f\"Hit Rate@{k}: {hit_rate:.3f}\")\n",
    "\n",
    "mrr = calculate_mrr(sample_results)\n",
    "print(f\"\\nMRR: {mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualization of RAG Metrics\n",
    "\n",
    "Let's visualize how different metrics behave with varying parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate metrics for different K values\n",
    "k_range = range(1, 11)\n",
    "recall_values = []\n",
    "precision_values = []\n",
    "hit_rate_values = []\n",
    "\n",
    "for k in k_range:\n",
    "    recall_values.append(calculate_recall_at_k(sample_results, k))\n",
    "    precision_values.append(calculate_precision_at_k(sample_results, k))\n",
    "    hit_rate_values.append(calculate_hit_rate(sample_results, k))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Recall@K vs K\n",
    "axes[0, 0].plot(k_range, recall_values, marker='o', linewidth=2, markersize=8)\n",
    "axes[0, 0].set_title('Recall@K vs K Value', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('K Value')\n",
    "axes[0, 0].set_ylabel('Recall@K')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Precision@K vs K\n",
    "axes[0, 1].plot(k_range, precision_values, marker='s', color='orange', linewidth=2, markersize=8)\n",
    "axes[0, 1].set_title('Precision@K vs K Value', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('K Value')\n",
    "axes[0, 1].set_ylabel('Precision@K')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hit Rate vs K\n",
    "axes[1, 0].plot(k_range, hit_rate_values, marker='^', color='green', linewidth=2, markersize=8)\n",
    "axes[1, 0].set_title('Hit Rate vs K Value', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('K Value')\n",
    "axes[1, 0].set_ylabel('Hit Rate')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Combined metrics\n",
    "axes[1, 1].plot(k_range, recall_values, label='Recall@K', marker='o', linewidth=2)\n",
    "axes[1, 1].plot(k_range, precision_values, label='Precision@K', marker='s', linewidth=2)\n",
    "axes[1, 1].plot(k_range, hit_rate_values, label='Hit Rate', marker='^', linewidth=2)\n",
    "axes[1, 1].set_title('Combined Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('K Value')\n",
    "axes[1, 1].set_ylabel('Metric Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show insights\n",
    "print(\"\\nKey Insights from Metrics Visualization:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"‚Ä¢ Recall generally increases with higher K values\")\n",
    "print(\"‚Ä¢ Precision typically decreases with higher K values\")\n",
    "print(\"‚Ä¢ Hit rate approaches 1.0 as K increases\")\n",
    "print(\"‚Ä¢ There's a trade-off between recall and precision\")\n",
    "print(\"‚Ä¢ Optimal K depends on use case requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç RAG-Specific Evaluation Metrics\n",
    "\n",
    "Let's implement and explore RAG-specific metrics like Faithfulness and Context Relevance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RAG-specific evaluation metrics\n",
    "def calculate_faithfulness(generated_answer: str, retrieved_context: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate faithfulness score based on how much the answer is supported by context\n",
    "    This is a simplified implementation - in practice, use LLM-based evaluation\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and tokenize\n",
    "    answer_words = set(generated_answer.lower().split())\n",
    "    context_words = set(retrieved_context.lower().split())\n",
    "    \n",
    "    # Calculate overlap ratio\n",
    "    if not answer_words:\n",
    "        return 0.0\n",
    "        \n",
    "    overlap = answer_words.intersection(context_words)\n",
    "    faithfulness = len(overlap) / len(answer_words)\n",
    "    \n",
    "    # Cap at 1.0\n",
    "    return min(1.0, faithfulness)\n",
    "\n",
    "def calculate_context_relevance(question: str, retrieved_context: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate how relevant the context is to the question\n",
    "    \"\"\"\n",
    "    question_words = set(question.lower().split())\n",
    "    context_words = set(retrieved_context.lower().split())\n",
    "    \n",
    "    if not question_words:\n",
    "        return 0.0\n",
    "        \n",
    "    overlap = question_words.intersection(context_words)\n",
    "    relevance = len(overlap) / len(question_words)\n",
    "    \n",
    "    return min(1.0, relevance)\n",
    "\n",
    "def calculate_answer_relevance(question: str, generated_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate how relevant the answer is to the question\n",
    "    \"\"\"\n",
    "    question_words = set(question.lower().split())\n",
    "    answer_words = set(generated_answer.lower().split())\n",
    "    \n",
    "    if not question_words:\n",
    "        return 0.0\n",
    "        \n",
    "    overlap = question_words.intersection(answer_words)\n",
    "    relevance = len(overlap) / len(question_words)\n",
    "    \n",
    "    return min(1.0, relevance)\n",
    "\n",
    "# Test these metrics with our sample results\n",
    "print(\"RAG-Specific Evaluation Metrics:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "faithfulness_scores = []\n",
    "context_relevance_scores = []\n",
    "answer_relevance_scores = []\n",
    "\n",
    "for i, result in enumerate(sample_results[:5]):  # Just first 5 for brevity\n",
    "    faithfulness = calculate_faithfulness(result.generated_answer, result.context)\n",
    "    context_rel = calculate_context_relevance(result.query, result.context)\n",
    "    answer_rel = calculate_answer_relevance(result.query, result.generated_answer)\n",
    "    \n",
    "    faithfulness_scores.append(faithfulness)\n",
    "    context_relevance_scores.append(context_rel)\n",
    "    answer_relevance_scores.append(answer_rel)\n",
    "    \n",
    "    print(f\"\\nQuery {i+1}: {result.query[:50]}...\")\n",
    "    print(f\"  Faithfulness: {faithfulness:.3f}\")\n",
    "    print(f\"  Context Relevance: {context_rel:.3f}\")\n",
    "    print(f\"  Answer Relevance: {answer_rel:.3f}\")\n",
    "\n",
    "# Overall averages\n",
    "avg_faithfulness = np.mean(faithfulness_scores)\n",
    "avg_context_rel = np.mean(context_relevance_scores)\n",
    "avg_answer_rel = np.mean(answer_relevance_scores)\n",
    "\n",
    "print(f\"\\nOverall Averages:\")\n",
    "print(f\"  Faithfulness: {avg_faithfulness:.3f}\")\n",
    "print(f\"  Context Relevance: {avg_context_rel:.3f}\")\n",
    "print(f\"  Answer Relevance: {avg_answer_rel:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïµÔ∏è Evaluating with Synthetic Test Sets\n",
    "\n",
    "Let's create a synthetic evaluation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic evaluation dataset\n",
    "class SyntheticEvaluator:\n",
    "    def __init__(self):\n",
    "        self.questions = [\n",
    "            \"What is the capital of France?\",\n",
    "            \"Explain quantum computing briefly\",\n",
    "            \"What are the benefits of renewable energy?\",\n",
    "            \"How does photosynthesis work?\",\n",
    "            \"What is machine learning?\"\n",
    "        ]\n",
    "        \n",
    "        self.expected_contexts = [\n",
    "            \"France is a country in Europe. Paris is the capital and largest city.\",\n",
    "            \"Quantum computing uses quantum bits (qubits) that can exist in superposition.\",\n",
    "            \"Renewable energy comes from sources that naturally replenish like solar, wind, hydro.\",\n",
    "            \"Photosynthesis converts carbon dioxide and water to glucose using sunlight.\",\n",
    "            \"Machine learning enables systems to learn patterns from data without explicit programming.\"\n",
    "        ]\n",
    "        \n",
    "        self.expected_answers = [\n",
    "            \"The capital of France is Paris.\",\n",
    "            \"Quantum computing leverages quantum mechanical phenomena to process information.\",\n",
    "            \"Renewable energy is sustainable and environmentally friendly.\",\n",
    "            \"Plants convert light energy to chemical energy through photosynthesis.\",\n",
    "            \"Machine learning algorithms find patterns in data to make predictions.\"\n",
    "        ]\n",
    "    \n",
    "    def simulate_rag_response(self, question: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Simulate a RAG response based on question and context\n",
    "        \"\"\"\n",
    "        # This is a simplified simulation - in reality, this would involve\n",
    "        # the full RAG pipeline with retrieval, generation, etc.\n",
    "        \n",
    "        # For demonstration, we'll create variations of the expected answer\n",
    "        # with some potential errors to demonstrate evaluation\n",
    "        import random\n",
    "        \n",
    "        # Sometimes return correct answer\n",
    "        if random.random() > 0.3:\n",
    "            # Return answer with some variation\n",
    "            answer = self.expected_answers[self.questions.index(question)]\n",
    "            if random.random() > 0.7:\n",
    "                # Add some extra context\n",
    "                answer += \" This information is based on the provided context.\"\n",
    "            return answer\n",
    "        else:\n",
    "            # Return incorrect or unrelated answer\n",
    "            wrong_answers = [\n",
    "                \"I don't have information about that.\",\n",
    "                \"The answer is 42.\",\n",
    "                \"This topic is not covered in the context.\",\n",
    "                \"Information not available.\"\n",
    "            ]\n",
    "            return random.choice(wrong_answers)\n",
    "    \n",
    "    def run_evaluation(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the complete evaluation\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, q in enumerate(self.questions):\n",
    "            context = self.expected_contexts[i]\n",
    "            generated_answer = self.simulate_rag_response(q, context)\n",
    "            \n",
    "            # Create retrieved chunks (simulated)\n",
    "            retrieved_chunks = [\n",
    "                RetrievedChunk(id=f\"ctx_{i}_0\", text=context, score=0.9, is_relevant=True),\n",
    "                RetrievedChunk(id=f\"ctx_{i}_1\", text=\"Additional context\", score=0.7, is_relevant=False),\n",
    "                RetrievedChunk(id=f\"ctx_{i}_2\", text=\"Related info\", score=0.5, is_relevant=False)\n",
    "            ]\n",
    "            \n",
    "            result = QueryResult(\n",
    "                query=q,\n",
    "                retrieved_chunks=retrieved_chunks,\n",
    "                generated_answer=generated_answer,\n",
    "                reference_answer=self.expected_answers[i],\n",
    "                context=context\n",
    "            )\n",
    "            results.append(result)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        k = 3\n",
    "        metrics = {\n",
    "            \"recall_at_k\": calculate_recall_at_k(results, k),\n",
    "            \"precision_at_k\": calculate_precision_at_k(results, k),\n",
    "            \"mrr\": calculate_mrr(results),\n",
    "            \"hit_rate_at_k\": calculate_hit_rate(results, k),\n",
    "            \"faithfulness\": np.mean([calculate_faithfulness(r.generated_answer, r.context) for r in results]),\n",
    "            \"context_relevance\": np.mean([calculate_context_relevance(r.query, r.context) for r in results]),\n",
    "            \"answer_relevance\": np.mean([calculate_answer_relevance(r.query, r.generated_answer) for r in results])\n",
    "        }\n",
    "        \n",
    "        return metrics, results\n",
    "\n",
    "# Run the synthetic evaluation\n",
    "evaluator = SyntheticEvaluator()\n",
    "metrics, eval_results = evaluator.run_evaluation()\n",
    "\n",
    "print(\"Synthetic Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric.replace('_', ' ').title()}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Real-Time Monitoring Dashboard\n",
    "\n",
    "Let's create a simulated monitoring dashboard that tracks RAG performance in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a monitoring system for RAG performance\n",
    "class RagMonitoringSystem:\n",
    "    def __init__(self):\n",
    "        self.metrics_history = []\n",
    "        \n",
    "    def simulate_query(self, timestamp: datetime) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Simulate a RAG query and capture metrics\n",
    "        \"\"\"\n",
    "        # Simulate realistic metrics with some variance\n",
    "        import random\n",
    "        \n",
    "        # Base metrics with some random fluctuation\n",
    "        retrieval_latency = max(0.1, random.normalvariate(0.5, 0.2))  # seconds\n",
    "        generation_latency = max(0.2, random.normalvariate(1.2, 0.4))  # seconds\n",
    "        total_latency = retrieval_latency + generation_latency\n",
    "        \n",
    "        # Success rate (some queries might fail)\n",
    "        success = random.random() > 0.05  # 5% failure rate\n",
    "        \n",
    "        # Quality metrics\n",
    "        faithfulness = max(0.0, min(1.0, random.normalvariate(0.8, 0.1)))\n",
    "        answer_relevance = max(0.0, min(1.0, random.normalvariate(0.75, 0.12)))\n",
    "        \n",
    "        return {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"retrieval_latency\": retrieval_latency,\n",
    "            \"generation_latency\": generation_latency,\n",
    "            \"total_latency\": total_latency,\n",
    "            \"success\": success,\n",
    "            \"faithfulness\": faithfulness,\n",
    "            \"answer_relevance\": answer_relevance,\n",
    "            \"query_complexity\": random.randint(1, 5)  # 1-5 scale\n",
    "        }\n",
    "    \n",
    "    def simulate_time_series(self, duration_hours: int = 1, queries_per_hour: int = 50):\n",
    "        \"\"\"\n",
    "        Simulate metric collection over time\n",
    "        \"\"\"\n",
    "        start_time = datetime.now() - timedelta(hours=duration_hours)\n",
    "        current_time = start_time\n",
    "        \n",
    "        total_queries = duration_hours * queries_per_hour\n",
    "        for i in range(total_queries):\n",
    "            # Advance time by random interval\n",
    "            current_time += timedelta(minutes=random.uniform(0.5, 2.5))\n",
    "            \n",
    "            # Record metrics\n",
    "            metrics = self.simulate_query(current_time)\n",
    "            self.metrics_history.append(metrics)\n",
    "    \n",
    "    def get_current_status(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get current system status based on recent metrics\n",
    "        \"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {\"status\": \"No data\"}\n",
    "        \n",
    "        recent_metrics = self.metrics_history[-20:]  # Last 20 queries\n",
    "        \n",
    "        avg_latency = np.mean([m['total_latency'] for m in recent_metrics])\n",
    "        success_rate = np.mean([1 if m['success'] else 0 for m in recent_metrics])\n",
    "        avg_faithfulness = np.mean([m['faithfulness'] for m in recent_metrics])\n",
    "        avg_relevance = np.mean([m['answer_relevance'] for m in recent_metrics])\n",
    "        \n",
    "        # Determine status based on thresholds\n",
    "        status = \"OK\"\n",
    "        if avg_latency > 2.0:\n",
    "            status = \"WARNING: High Latency\"\n",
    "        elif success_rate < 0.95:\n",
    "            status = \"WARNING: High Failure Rate\"\n",
    "        elif avg_faithfulness < 0.7:\n",
    "            status = \"WARNING: Low Faithfulness\"\n",
    "        elif avg_relevance < 0.7:\n",
    "            status = \"WARNING: Low Answer Relevance\"\n",
    "        \n",
    "        return {\n",
    "            \"status\": status,\n",
    "            \"avg_latency\": avg_latency,\n",
    "            \"success_rate\": success_rate,\n",
    "            \"avg_faithfulness\": avg_faithfulness,\n",
    "            \"avg_relevance\": avg_relevance,\n",
    "            \"total_queries\": len(self.metrics_history)\n",
    "        }\n",
    "    \n",
    "    def plot_dashboard(self):\n",
    "        \"\"\"\n",
    "        Create a dashboard visualization\n",
    "        \"\"\"\n",
    "        if not self.metrics_history:\n",
    "            print(\"No metrics data to display\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.metrics_history)\n",
    "        df['time'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Latency over time\n",
    "        axes[0, 0].plot(df['time'], df['total_latency'], label='Total Latency', alpha=0.7)\n",
    "        axes[0, 0].plot(df['time'], df['retrieval_latency'], label='Retrieval Latency', alpha=0.7)\n",
    "        axes[0, 0].plot(df['time'], df['generation_latency'], label='Generation Latency', alpha=0.7)\n",
    "        axes[0, 0].set_title('Latency Over Time')\n",
    "        axes[0, 0].set_ylabel('Seconds')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Success rate over time (rolling average)\n",
    "        df['success_int'] = df['success'].astype(int)\n",
    "        rolling_success = df['success_int'].rolling(window=10, min_periods=1).mean()\n",
    "        axes[0, 1].plot(df['time'], rolling_success, color='green', linewidth=2)\n",
    "        axes[0, 1].set_title('Success Rate (Rolling Average)')\n",
    "        axes[0, 1].set_ylabel('Success Rate')\n",
    "        axes[0, 1].set_ylim(0, 1.05)\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Quality metrics\n",
    "        axes[1, 0].plot(df['time'], df['faithfulness'], label='Faithfulness', alpha=0.7)\n",
    "        axes[1, 0].plot(df['time'], df['answer_relevance'], label='Answer Relevance', alpha=0.7)\n",
    "        axes[1, 0].set_title('Quality Metrics Over Time')\n",
    "        axes[1, 0].set_ylabel('Score')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_ylim(0, 1.05)\n",
    "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Distribution of query complexity\n",
    "        complexity_counts = df['query_complexity'].value_counts().sort_index()\n",
    "        bars = axes[1, 1].bar(complexity_counts.index, complexity_counts.values)\n",
    "        axes[1, 1].set_title('Distribution of Query Complexity')\n",
    "        axes[1, 1].set_xlabel('Complexity Level (1-5)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                            f'{int(height)}',\n",
    "                            ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and run the monitoring system\n",
    "monitor = RagMonitoringSystem()\n",
    "monitor.simulate_time_series(duration_hours=2, queries_per_hour=30)  # 60 queries over 2 hours\n",
    "\n",
    "# Show current status\n",
    "status = monitor.get_current_status()\n",
    "print(\"RAG System Status Dashboard:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in status.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value:.3f}\")\n",
    "    else:\n",
    "        print(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "# Show dashboard\n",
    "monitor.plot_dashboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö® Alerting and Anomaly Detection\n",
    "\n",
    "Let's implement a simple alerting system for monitoring RAG performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a simple alerting system\n",
    "class RagAlertSystem:\n",
    "    def __init__(self):\n",
    "        self.alerts = []\n",
    "        \n",
    "    def check_thresholds(self, metrics: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Check if metrics exceed defined thresholds\n",
    "        \"\"\"\n",
    "        alerts = []\n",
    "        \n",
    "        # Define thresholds\n",
    "        thresholds = {\n",
    "            'latency': 2.0,  # seconds\n",
    "            'success_rate': 0.95,\n",
    "            'faithfulness': 0.7,\n",
    "            'answer_relevance': 0.7\n",
    "        }\n",
    "        \n",
    "        if metrics.get('avg_latency', 0) > thresholds['latency']:\n",
    "            alerts.append(f\"HIGH_LATENCY: {metrics['avg_latency']:.2f}s > {thresholds['latency']}s\")\n",
    "        \n",
    "        if metrics.get('success_rate', 1.0) < thresholds['success_rate']:\n",
    "            alerts.append(f\"HIGH_FAILURE_RATE: {metrics['success_rate']:.2%} < {thresholds['success_rate']:.0%}\")\n",
    "        \n",
    "        if metrics.get('avg_faithfulness', 1.0) < thresholds['faithfulness']:\n",
    "            alerts.append(f\"LOW_FAITHFULNESS: {metrics['avg_faithfulness']:.2%} < {thresholds['faithfulness']:.0%}\")\n",
    "        \n",
    "        if metrics.get('avg_relevance', 1.0) < thresholds['answer_relevance']:\n",
    "            alerts.append(f\"LOW_ANSWER_RELEVANCE: {metrics['avg_relevance']:.2%} < {thresholds['answer_relevance']:.0%}\")\n",
    "        \n",
    "        return alerts\n",
    "    \n",
    "    def detect_anomalies(self, historical_data: List[Dict[str, Any]]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Detect anomalies in performance metrics\n",
    "        \"\"\"\n",
    "        if len(historical_data) < 10:\n",
    "            return []  # Need sufficient data for anomaly detection\n",
    "        \n",
    "        recent_data = historical_data[-5:]  # Last 5 measurements\n",
    "        historical_data = historical_data[:-5]  # Previous measurements\n",
    "        \n",
    "        alerts = []\n",
    "        \n",
    "        # Calculate baseline metrics\n",
    "        baseline_latencies = [m['total_latency'] for m in historical_data]\n",
    "        baseline_latency_mean = np.mean(baseline_latencies)\n",
    "        baseline_latency_std = np.std(baseline_latencies)\n",
    "        \n",
    "        recent_latencies = [m['total_latency'] for m in recent_data]\n",
    "        recent_latency_mean = np.mean(recent_latencies)\n",
    "        \n",
    "        # Check for significant increase in latency\n",
    "        if baseline_latency_std > 0 and \\\n",
    "           (recent_latency_mean - baseline_latency_mean) / baseline_latency_std > 2.0:\n",
    "            alerts.append(f\"LATENCY_SPIKE: Recent avg latency {recent_latency_mean:.2f}s vs baseline {baseline_latency_mean:.2f}s\")\n",
    "        \n",
    "        return alerts\n",
    "\n",
    "# Test the alerting system\n",
    "alert_system = RagAlertSystem()\n",
    "\n",
    "# Get current status from our monitoring system\n",
    "current_status = monitor.get_current_status()\n",
    "\n",
    "# Check for threshold violations\n",
    "threshold_alerts = alert_system.check_thresholds(current_status)\n",
    "\n",
    "# Check for anomalies in historical data\n",
    "anomaly_alerts = alert_system.detect_anomalies(monitor.metrics_history)\n",
    "\n",
    "print(\"Alert System Report:\")\n",
    "print(\"=\" * 30)\n",
    "if threshold_alerts:\n",
    "    print(\"Threshold Violations:\")\n",
    "    for alert in threshold_alerts:\n",
    "        print(f\"  ‚ö†Ô∏è  {alert}\")\n",
    "else:\n",
    "    print(\"‚úì No threshold violations detected\")\n",
    "\n",
    "if anomaly_alerts:\n",
    "    print(\"\\nAnomalies Detected:\")\n",
    "    for alert in anomaly_alerts:\n",
    "        print(f\"  üîç {alert}\")\n",
    "else:\n",
    "    print(\"\\n‚úì No anomalies detected\")\n",
    "\n",
    "if not threshold_alerts and not anomaly_alerts:\n",
    "    print(\"\\nüéâ System is performing normally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Key Takeaways\n",
    "\n",
    "Evaluation and monitoring are essential for maintaining RAG system quality:\n",
    "\n",
    "1. **Multiple Metrics**: Use a combination of retrieval, generation, and RAG-specific metrics\n",
    "2. **Realistic Evaluation**: Combine automated metrics with human evaluation\n",
    "3. **Continuous Monitoring**: Track metrics in real-time to catch degradation\n",
    "4. **Threshold Setting**: Establish meaningful thresholds for your use case\n",
    "5. **Alerting System**: Implement automatic alerts for performance issues\n",
    "6. **Root Cause Analysis**: Have tools to investigate performance problems\n",
    "7. **Iterative Improvement**: Regularly review metrics to improve the system\n",
    "\n",
    "The RAG Engine Mini implements these concepts with production-grade monitoring using Prometheus, Grafana, and custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we've explored the critical aspects of evaluating and monitoring RAG systems:\n",
    "\n",
    "- We learned about key evaluation metrics for RAG systems\n",
    "- We implemented custom evaluation functions from scratch\n",
    "- We visualized how metrics behave with different parameters\n",
    "- We created a synthetic evaluation pipeline\n",
    "- We built a real-time monitoring dashboard\n",
    "- We implemented an alerting system for anomaly detection\n",
    "\n",
    "These capabilities are essential for maintaining production RAG systems that deliver consistent quality and performance. Proper evaluation and monitoring ensure that your RAG system continues to meet user expectations over time.\n",
    "\n",
    "Continue to the next notebooks to learn about advanced RAG techniques and optimization strategies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}