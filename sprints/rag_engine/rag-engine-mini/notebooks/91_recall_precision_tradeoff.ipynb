{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš–ï¸ RAG Evaluation: Recall-Precision Trade-offs\n",
    "\n",
    "This notebook explores the critical trade-offs between recall and precision in RAG systems, helping you understand how to optimize for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Understanding Recall vs Precision in RAG\n",
    "\n",
    "In RAG systems, we face fundamental trade-offs between:\n",
    "\n",
    "- **Recall**: The fraction of relevant documents that are retrieved\n",
    "- **Precision**: The fraction of retrieved documents that are relevant\n",
    "\n",
    "These trade-offs manifest at multiple levels in the RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall_precision_curve(recall_values: List[float], precision_values: List[float], labels: List[str]):\n",
    "    \"\"\"\n",
    "    Plot recall-precision curve\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_values, precision_values, marker='o', linewidth=2, markersize=8)\n",
    "    \n",
    "    # Annotate points\n",
    "    for i, label in enumerate(labels):\n",
    "        plt.annotate(label, (recall_values[i], precision_values[i]), \n",
    "                    textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "    \n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Recall-Precision Trade-off in RAG Configurations')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "\n",
    "# Example recall-precision values for different RAG configurations\n",
    "configs = [\n",
    "    \"Vector-only (strict)\",\n",
    "    \"Vector-only (loose)\",\n",
    "    \"Hybrid search\",\n",
    "    \"Hybrid + Re-ranking\",\n",
    "    \"Hybrid + Re-ranking + Rerank-depth\"\n",
    "]\n",
    "\n",
    "recall_vals = [0.45, 0.75, 0.78, 0.77, 0.82]\n",
    "precision_vals = [0.85, 0.60, 0.70, 0.78, 0.75]\n",
    "\n",
    "plot_recall_precision_curve(recall_vals, precision_vals, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Experimenting with Retrieval Parameters\n",
    "\n",
    "Let's examine how different retrieval parameters affect the recall-precision trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_retrieval_performance(top_k: int, threshold: float = None) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Simulate retrieval performance based on top_k parameter\n",
    "    In a real scenario, this would call your RAG system\n",
    "    \"\"\"\n",
    "    # Simulated relationship: increasing top_k generally increases recall but decreases precision\n",
    "    recall = min(0.3 + 0.05 * top_k, 0.95)  # Cap at 0.95\n",
    "    precision = max(0.9 - 0.03 * top_k, 0.2)  # Floor at 0.2\n",
    "    \n",
    "    # Add some randomness to make it more realistic\n",
    "    recall += np.random.normal(0, 0.02)\n",
    "    precision += np.random.normal(0, 0.02)\n",
    "    \n",
    "    # Ensure values stay within bounds\n",
    "    recall = max(0, min(1, recall))\n",
    "    precision = max(0, min(1, precision))\n",
    "    \n",
    "    return recall, precision\n",
    "\n",
    "# Test different top-k values\n",
    "top_k_values = range(5, 51, 5)\n",
    "recalls = []\n",
    "precisions = []\n",
    "\n",
    "for k in top_k_values:\n",
    "    recall, precision = simulate_retrieval_performance(k)\n",
    "    recalls.append(recall)\n",
    "    precisions.append(precision)\n",
    "\n",
    "# Plot the results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Recall vs Top-K\n",
    "ax1.plot(top_k_values, recalls, marker='o', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Top-K Value')\n",
    "ax1.set_ylabel('Recall')\n",
    "ax1.set_title('Recall vs Top-K Retrieval')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Precision vs Top-K\n",
    "ax2.plot(top_k_values, precisions, marker='s', linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel('Top-K Value')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision vs Top-K Retrieval')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the trade-off values\n",
    "results_df = pd.DataFrame({\n",
    "    'Top-K': top_k_values,\n",
    "    'Recall': recalls,\n",
    "    'Precision': precisions,\n",
    "    'F1-Score': [2*(r*p)/(r+p) for r, p in zip(recalls, precisions)]\n",
    "})\n",
    "\n",
    "print(\"Recall-Precision Trade-off for Different Top-K Values:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Before/After: Hybrid Search Impact\n",
    "\n",
    "Let's compare retrieval performance before and after implementing hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_search_methods():\n",
    "    \"\"\"\n",
    "    Compare vector-only vs hybrid search performance\n",
    "    \"\"\"\n",
    "    # Simulated performance data\n",
    "    methods = ['Vector-only', 'Hybrid Search']\n",
    "    \n",
    "    # Performance metrics for different query types\n",
    "    query_types = ['Factual', 'Conceptual', 'Multi-hop', 'Negation']\n",
    "    \n",
    "    vector_only = {\n",
    "        'Factual': (0.65, 0.78),\n",
    "        'Conceptual': (0.52, 0.71),\n",
    "        'Multi-hop': (0.38, 0.82),\n",
    "        'Negation': (0.41, 0.69)\n",
    "    }\n",
    "    \n",    \n",
    "    hybrid_search = {\n",
    "        'Factual': (0.78, 0.75),\n",
    "        'Conceptual': (0.71, 0.68),\n",
    "        'Multi-hop': (0.62, 0.71),\n",
    "        'Negation': (0.65, 0.67)\n",
    "    }\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = []\n",
    "    for qtype in query_types:\n",
    "        vo_recall, vo_precision = vector_only[qtype]\n",
    "        hs_recall, hs_precision = hybrid_search[qtype]\n",
    "        \n",
    "        comparison_data.extend([\n",
    "            {'Method': 'Vector-only', 'Query_Type': qtype, 'Recall': vo_recall, 'Precision': vo_precision},\n",
    "            {'Method': 'Hybrid Search', 'Query_Type': qtype, 'Recall': hs_recall, 'Precision': hs_precision}\n",
    "        ])\n",
    "    \n",
    "    df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Pivot for easier visualization\n",
    "    recall_pivot = df.pivot(index='Query_Type', columns='Method', values='Recall')\n",
    "    precision_pivot = df.pivot(index='Query_Type', columns='Method', values='Precision')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    recall_pivot.plot(kind='bar', ax=ax1)\n",
    "    ax1.set_title('Recall Comparison: Vector-only vs Hybrid Search')\n",
    "    ax1.set_ylabel('Recall')\n",
    "    ax1.legend()\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    precision_pivot.plot(kind='bar', ax=ax2)\n",
    "    ax2.set_title('Precision Comparison: Vector-only vs Hybrid Search')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Detailed Comparison:\")\n",
    "    print(df.round(3))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    print(\"\\nImprovement Summary:\")\n",
    "    for qtype in query_types:\n",
    "        vo_recall, vo_precision = vector_only[qtype]\n",
    "        hs_recall, hs_precision = hybrid_search[qtype]\n",
    "        \n",
    "        recall_imp = hs_recall - vo_recall\n",
    "        precision_imp = hs_precision - vo_precision\n",
    "        \n",
    "        print(f\"{qtype:12s}: Recall +{recall_imp:+.3f}, Precision {precision_imp:+.3f}\")\n",
    "\n",
    "compare_search_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Re-ranking Impact Analysis\n",
    "\n",
    "Let's analyze how re-ranking affects the position of relevant documents in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rerank_impact():\n",
    "    \"\"\"\n",
    "    Analyze how re-ranking affects document positioning\n",
    "    \"\"\"\n",
    "    # Simulated data: positions of relevant documents before and after re-ranking\n",
    "    queries = [f\"Q{i}\" for i in range(1, 11)]\n",
    "    \n",
    "    # Positions of relevant documents (lower is better)\n",
    "    before_rerank = [np.random.choice(range(1, 21), size=3, replace=False) for _ in queries]  # Top 20\n",
    "    after_rerank = []\n",
    "    \n",
    "    for positions in before_rerank:\n",
    "        # Simulate re-ranking improvement: relevant docs move up in rank\n",
    "        new_positions = []\n",
    "        for pos in positions:\n",
    "            # On average, docs move up 2 positions, with some variance\n",
    "            new_pos = max(1, pos - np.random.uniform(0.5, 3.5))\n",
    "            new_positions.append(new_pos)\n",
    "        after_rerank.append(sorted(new_positions))  # Keep in order\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mrr_before = [1/np.mean(positions) for positions in before_rerank]\n",
    "    mrr_after = [1/np.mean(positions) for positions in after_rerank]\n",
    "    \n",
    "    # Calculate reciprocal rank for the highest-ranked relevant doc\n",
    "    rr_before = [1/min(positions) for positions in before_rerank]\n",
    "    rr_after = [1/min(positions) for positions in after_rerank]\n",
    "    \n",
    "    # Plot comparison\n",
    "    x = np.arange(len(queries))\n",
    "    width = 0.35\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # MRR comparison\n",
    "    ax1.bar(x - width/2, mrr_before, width, label='Before Re-ranking', alpha=0.8)\n",
    "    ax1.bar(x + width/2, mrr_after, width, label='After Re-ranking', alpha=0.8)\n",
    "    ax1.set_xlabel('Query')\n",
    "    ax1.set_ylabel('MRR')\n",
    "    ax1.set_title('Mean Reciprocal Rank: Before vs After Re-ranking')\n",
    "    ax1.legend()\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(queries)\n",
    "    \n",
    "    # RR comparison\n",
    "    ax2.bar(x - width/2, rr_before, width, label='Before Re-ranking', alpha=0.8)\n",
    "    ax2.bar(x + width/2, rr_after, width, label='After Re-ranking', alpha=0.8)\n",
    "    ax2.set_xlabel('Query')\n",
    "    ax2.set_ylabel('Reciprocal Rank')\n",
    "    ax2.set_title('Reciprocal Rank of Best Doc: Before vs After Re-ranking')\n",
    "    ax2.legend()\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(queries)\n",
    "    \n",
    "    # Average position comparison\n",
    "    avg_before = [np.mean(positions) for positions in before_rerank]\n",
    "    avg_after = [np.mean(positions) for positions in after_rerank]\n",
    "    \n",
    "    ax3.bar(x - width/2, avg_before, width, label='Before Re-ranking', alpha=0.8)\n",
    "    ax3.bar(x + width/2, avg_after, width, label='After Re-ranking', alpha=0.8)\n",
    "    ax3.set_xlabel('Query')\n",
    "    ax3.set_ylabel('Avg Position of Relevant Docs')\n",
    "    ax3.set_title('Average Position: Before vs After Re-ranking')\n",
    "    ax3.legend()\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(queries)\n",
    "    \n",
    "    # Improvement in positions\n",
    "    improvement = [b - a for b, a in zip(avg_before, avg_after)]\n",
    "    ax4.bar(x, improvement, color=['green' if imp > 0 else 'red' for imp in improvement])\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_xlabel('Query')\n",
    "    ax4.set_ylabel('Position Improvement (+ve = better)')\n",
    "    ax4.set_title('Position Improvement Due to Re-ranking')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(queries)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"Re-ranking Impact Summary:\")\n",
    "    print(f\"Average MRR before: {np.mean(mrr_before):.3f}\")\n",
    "    print(f\"Average MRR after:  {np.mean(mrr_after):.3f}\")\n",
    "    print(f\"MRR improvement:   {(np.mean(mrr_after) - np.mean(mrr_before))*100:+.2f}%\")\n",
    "    \n",
    "    print(f\"\\nAverage position before: {np.mean(avg_before):.2f}\")\n",
    "    print(f\"Average position after:  {np.mean(avg_after):.2f}\")\n",
    "    print(f\"Position improvement:   {np.mean(avg_before) - np.mean(avg_after):+.2f} positions\")\n",
    "\n",
    "analyze_rerank_impact()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Choosing the Right Balance\n",
    "\n",
    "Different applications require different recall-precision balances. Let's explore how to choose the right configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_configuration(use_case: str):\n",
    "    \"\"\"\n",
    "    Recommend RAG configuration based on use case\n",
    "    \"\"\"\n",
    "    configs = {\n",
    "        \"High Recall (e.g., Research Assistant)\": {\n",
    "            \"approach\": \"Hybrid search with high top-k and re-ranking\",\n",
    "            \"focus\": \"Minimize false negatives - better to retrieve too much than miss important info\",\n",
    "            \"metrics\": \"Prioritize context_recall and MRR\",\n",
    "            \"trade-offs\": \"Higher latency, more irrelevant results\"\n",
    "        },\n",
    "        \"High Precision (e.g., Customer Support)\": {\n",
    "            \"approach\": \"Strict vector search with re-ranking and validation\",\n",
    "            \"focus\": \"Minimize false positives - only return highly confident answers\",\n",
    "            \"metrics\": \"Prioritize faithfulness and precision\",\n",
    "            \"trade-offs\": \"May miss some relevant info, higher chance of 'I don't know'\"\n",
    "        },\n",
    "        \"Balanced (e.g., General QA)\": {\n",
    "            \"approach\": \"Hybrid search with moderate parameters\",\n",
    "            \"focus\": \"Good balance of coverage and accuracy\",\n",
    "            \"metrics\": \"F1-score, overall user satisfaction\",\n",
    "            \"trade-offs\": \"Compromise on both extremes\"\n",
    "        },\n",
    "        \"Low Latency (e.g., Real-time Chatbot)\": {\n",
    "            \"approach\": \"Optimized vector search with caching\",\n",
    "            \"focus\": \"Fast response times with acceptable accuracy\",\n",
    "            \"metrics\": \"Latency, throughput, minimal accuracy threshold\",\n",
    "            \"trade-offs\": \"Reduced accuracy for speed\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if use_case in configs:\n",
    "        config = configs[use_case]\n",
    "        print(f\"Recommended Configuration for {use_case}:\")\n",
    "        print(f\"Approach: {config['approach']}\")\n",
    "        print(f\"Focus: {config['focus']}\")\n",
    "        print(f\"Metrics: {config['metrics']}\")\n",
    "        print(f\"Trade-offs: {config['trade-offs']}\")\n",
    "    else:\n",
    "        print(f\"Unknown use case: {use_case}\")\n",
    "        print(\"Available use cases:\", list(configs.keys()))\n",
    "\n",
    "# Demonstrate recommendations for different use cases\n",
    "use_cases = [\n",
    "    \"High Recall (e.g., Research Assistant)\",\n",
    "    \"High Precision (e.g., Customer Support)\",\n",
    "    \"Balanced (e.g., General QA)\",\n",
    "    \"Low Latency (e.g., Real-time Chatbot)\"\n",
    "]\n",
    "\n",
    "for uc in use_cases:\n",
    "    recommend_configuration(uc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Key Insights\n",
    "\n",
    "1. **Recall-Precision Trade-off**: Increasing one typically decreases the other\n",
    "2. **Use Case Drives Configuration**: Different applications require different balances\n",
    "3. **Hybrid Search Benefits**: Generally improves recall with minimal precision loss\n",
    "4. **Re-ranking Value**: Improves precision by reordering results based on relevance\n",
    "5. **Parameter Tuning**: Top-K and threshold values significantly impact performance\n",
    "6. **Evaluation is Iterative**: Continuously measure and adjust based on results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}