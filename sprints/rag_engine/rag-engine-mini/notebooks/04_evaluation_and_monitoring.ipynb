{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Notebook 04: Evaluation & Monitoring\n",
                "\n",
                "In this notebook, we explore how to measure the performance of our RAG engine and how to monitor it in production.\n",
                "\n",
                "## ðŸŽ¯ Objectives\n",
                "1. **Retrieval Evaluation**: Compare Vector vs Hybrid search precision.\n",
                "2. **Guardrails**: Test prompt grounding and hallucination prevention.\n",
                "3. **Monitoring**: Introduction to Prometheus metrics in RAG Engine Mini.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "from typing import List\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath(\"../\"))\n",
                "\n",
                "from src.core.bootstrap import get_container\n",
                "from src.domain.entities import TenantId, Answer\n",
                "from src.application.use_cases.ask_question_hybrid import AskHybridRequest\n",
                "\n",
                "container = get_container()\n",
                "log = f\"Container services loaded: {list(container.keys())}\"\n",
                "print(log)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Retrieval Evaluation\n",
                "\n",
                "We use a \"Golden Set\" of questions and expected answers or keywords to evaluate retrieval quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "GOLDEN_SET = [\n",
                "    {\n",
                "        \"question\": \"What is the main architecture of this project?\",\n",
                "        \"expected\": [\"Clean Architecture\", \"Ports and Adapters\", \"Hexagonal\"]\n",
                "    },\n",
                "    {\n",
                "        \"question\": \"How do we handle hybrid search?\",\n",
                "        \"expected\": [\"Reciprocal Rank Fusion\", \"RRF\", \"Vector\", \"Keyword\"]\n",
                "    }\n",
                "]\n",
                "\n",
                "def evaluate_recall(question, expected_keywords, retrieved_chunks):\n",
                "    combined_text = \" \".join([c.text.lower() for c in retrieved_chunks])\n",
                "    found = [kw for kw in expected_keywords if kw.lower() in combined_text]\n",
                "    return len(found) / len(expected_keywords)\n",
                "\n",
                "eval_use_case = container[\"ask_hybrid_use_case\"]\n",
                "tenant = TenantId(\"notebook_eval\")\n",
                "\n",
                "for entry in GOLDEN_SET:\n",
                "    # Execute retrieval only\n",
                "    chunks = eval_use_case.execute_retrieval_only(\n",
                "        tenant_id=tenant, \n",
                "        question=entry[\"question\"],\n",
                "        expand_query=True  # Power of expansion!\n",
                "    )\n",
                "    \n",
                "    score = evaluate_recall(entry[\"question\"], entry[\"expected\"], chunks)\n",
                "    print(f\"Question: {entry['question']}\")\n",
                "    print(f\"Recall Score: {score:.2f} ({len(chunks)} chunks retrieved)\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Guardrail Testing\n",
                "\n",
                "Let's try to \"trick\" the model into hallucinating something not in the context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def test_guardrails(question, context):\n",
                "    # We'll use the prompt builder directly to see how it grounds the model\n",
                "    from src.application.services.prompt_builder import build_rag_prompt\n",
                "    from src.domain.entities import Chunk, DocumentId\n",
                "    \n",
                "    mock_chunks = [Chunk(id=\"1\", tenant_id=tenant, document_id=DocumentId(\"doc\"), text=context)]\n",
                "    prompt = build_rag_prompt(question, mock_chunks)\n",
                "    \n",
                "    llm = container[\"llm\"]\n",
                "    answer = llm.generate(prompt)\n",
                "    return answer\n",
                "\n",
                "irrelevant_question = \"What is the capital of Mars?\"\n",
                "context = \"This project is about RAG Engine and Artificial Intelligence.\"\n",
                "\n",
                "answer = test_guardrails(irrelevant_question, context)\n",
                "print(f\"Question: {irrelevant_question}\")\n",
                "print(f\"Context: {context}\")\n",
                "print(f\"Model Response: {answer}\")\n",
                "print(\"\\nIs Hallucination Prevented?\", \"Yes\" if \"don't have\" in answer.lower() or \"not mentioned\" in answer.lower() else \"No\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Production Monitoring\n",
                "\n",
                "RAG Engine Mini exposes its internal state via **Prometheus**. \n",
                "\n",
                "You can monitor:\n",
                "- **rag_api_request_duration_seconds**: How long requests take.\n",
                "- **rag_llm_tokens_total**: Current token costs.\n",
                "- **rag_embedding_cache_total**: Efficiency of Redis storage.\n",
                "\n",
                "To see metrics in action, run `make run` and visit `http://localhost:8000/metrics`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "**Congratulations!** You've completed the educational journey for RAG Engine Mini Stage 1.\n",
                "\n",
                "Check out the [White-Box Documentation](file:///../docs/README.md) for more details."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}