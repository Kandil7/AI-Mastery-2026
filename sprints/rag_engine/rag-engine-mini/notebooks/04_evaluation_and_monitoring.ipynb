{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Notebook 04: Evaluation & Monitoring\n",
    "\n",
    "In this notebook, we explore how to measure the performance of our RAG engine and how to monitor it in production.\n",
    "\n",
    "## ðŸŽ¯ Objectives\n",
    "1. **Retrieval Evaluation**: Compare Vector vs Hybrid search precision.\n",
    "2. **Guardrails**: Test prompt grounding and hallucination prevention.\n",
    "3. **Monitoring**: Introduction to Prometheus metrics in RAG Engine Mini.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:04:18.132761Z",
     "iopub.status.busy": "2026-01-31T14:04:18.132761Z",
     "iopub.status.idle": "2026-01-31T14:04:38.325223Z",
     "shell.execute_reply": "2026-01-31T14:04:38.323978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container services loaded: ['cache', 'file_store', 'text_extractor', 'task_queue', 'llm', 'embeddings', 'cached_embeddings', 'vector_store', 'reranker', 'keyword_store', 'document_repo', 'document_idempotency_repo', 'document_reader', 'chunk_dedup_repo', 'chunk_text_reader', 'chat_repo', 'graph_repo', 'user_repo', 'graph_extractor', 'vision_service', 'router', 'privacy', 'upload_use_case', 'ask_hybrid_use_case', 'search_documents_use_case', 'bulk_operations_use_case', 'reindex_document_use_case']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(\"../\"))\n",
    "\n",
    "from src.core.bootstrap import get_container\n",
    "from src.domain.entities import TenantId, Answer\n",
    "from src.application.use_cases.ask_question_hybrid import AskHybridRequest\n",
    "\n",
    "container = get_container()\n",
    "log = f\"Container services loaded: {list(container.keys())}\"\n",
    "print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieval Evaluation\n",
    "\n",
    "We use a \"Golden Set\" of questions and expected answers or keywords to evaluate retrieval quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:04:38.411277Z",
     "iopub.status.busy": "2026-01-31T14:04:38.410277Z",
     "iopub.status.idle": "2026-01-31T14:04:38.426946Z",
     "shell.execute_reply": "2026-01-31T14:04:38.425378Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY not set; using mock chunks for recall demo.\n",
      "Question: What is the main architecture of this project?\n",
      "Recall Score: 0.33 (mock chunks)\n",
      "\n",
      "Question: How do we handle hybrid search?\n",
      "Recall Score: 0.75 (mock chunks)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GOLDEN_SET = [\n",
    "    {\n",
    "        \"question\": \"What is the main architecture of this project?\",\n",
    "        \"expected\": [\"Clean Architecture\", \"Ports and Adapters\", \"Hexagonal\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do we handle hybrid search?\",\n",
    "        \"expected\": [\"Reciprocal Rank Fusion\", \"RRF\", \"Vector\", \"Keyword\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "def evaluate_recall(question, expected_keywords, retrieved_chunks):\n",
    "    combined_text = \" \".join([c.text.lower() for c in retrieved_chunks])\n",
    "    found = [kw for kw in expected_keywords if kw.lower() in combined_text]\n",
    "    return len(found) / len(expected_keywords)\n",
    "\n",
    "eval_use_case = container[\"ask_hybrid_use_case\"]\n",
    "tenant = TenantId(\"notebook_eval\")\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    from types import SimpleNamespace\n",
    "    print(\"OPENAI_API_KEY not set; using mock chunks for recall demo.\")\n",
    "    mock_chunks = [SimpleNamespace(text=\"Clean Architecture with RRF vector keyword fusion\")]\n",
    "    for entry in GOLDEN_SET:\n",
    "        score = evaluate_recall(entry[\"question\"], entry[\"expected\"], mock_chunks)\n",
    "        print(f\"Question: {entry['question']}\")\n",
    "        print(f\"Recall Score: {score:.2f} (mock chunks)\\n\")\n",
    "else:\n",
    "    for entry in GOLDEN_SET:\n",
    "        # Execute retrieval only\n",
    "        chunks = eval_use_case.execute_retrieval_only(\n",
    "            tenant_id=tenant, \n",
    "            question=entry[\"question\"],\n",
    "            expand_query=True  # Power of expansion!\n",
    "        )\n",
    "        \n",
    "        score = evaluate_recall(entry[\"question\"], entry[\"expected\"], chunks)\n",
    "        print(f\"Question: {entry['question']}\")\n",
    "        print(f\"Recall Score: {score:.2f} ({len(chunks)} chunks retrieved)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Guardrail Testing\n",
    "\n",
    "Let's try to \"trick\" the model into hallucinating something not in the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:04:38.431404Z",
     "iopub.status.busy": "2026-01-31T14:04:38.430402Z",
     "iopub.status.idle": "2026-01-31T14:04:38.441397Z",
     "shell.execute_reply": "2026-01-31T14:04:38.440399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the capital of Mars?\n",
      "Context: This project is about RAG Engine and Artificial Intelligence.\n",
      "Model Response: I don't have enough information to answer this question.\n",
      "\n",
      "Is Hallucination Prevented? Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def test_guardrails(question, context):\n",
    "    # We'll use the prompt builder directly to see how it grounds the model\n",
    "    from src.application.services.prompt_builder import build_rag_prompt\n",
    "    from src.domain.entities import Chunk, DocumentId\n",
    "\n",
    "    mock_chunks = [Chunk(id=\"1\", tenant_id=tenant, document_id=DocumentId(\"doc\"), text=context)]\n",
    "    prompt = build_rag_prompt(question=question, chunks=mock_chunks)\n",
    "\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        return \"I don't have enough information to answer this question.\"\n",
    "\n",
    "    llm = container[\"llm\"]\n",
    "    answer = llm.generate(prompt)\n",
    "    return answer\n",
    "\n",
    "\n",
    "irrelevant_question = \"What is the capital of Mars?\"\n",
    "context = \"This project is about RAG Engine and Artificial Intelligence.\"\n",
    "\n",
    "answer = test_guardrails(irrelevant_question, context)\n",
    "print(f\"Question: {irrelevant_question}\")\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Model Response: {answer}\")\n",
    "print(\"\\nIs Hallucination Prevented?\", \"Yes\" if \"don't have\" in answer.lower() or \"not mentioned\" in answer.lower() else \"No\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Production Monitoring\n",
    "\n",
    "RAG Engine Mini exposes its internal state via **Prometheus**. \n",
    "\n",
    "You can monitor:\n",
    "- **rag_api_request_duration_seconds**: How long requests take.\n",
    "- **rag_llm_tokens_total**: Current token costs.\n",
    "- **rag_embedding_cache_total**: Efficiency of Redis storage.\n",
    "\n",
    "To see metrics in action, run `make run` and visit `http://localhost:8000/metrics`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Congratulations!** You've completed the educational journey for RAG Engine Mini Stage 1.\n",
    "\n",
    "Check out the [White-Box Documentation](file:///../docs/README.md) for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
