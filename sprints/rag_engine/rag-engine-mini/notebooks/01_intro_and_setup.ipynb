{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” RAG Engine Mini - Introduction & Setup\n",
    "\n",
    "> **Educational Notebook 01**: Project overview, architecture, and environment setup.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "\n",
    "1. **Retrieving** relevant documents based on the user's query\n",
    "2. **Augmenting** the prompt with this context\n",
    "3. **Generating** a grounded answer using the LLM\n",
    "\n",
    "```\n",
    "Query â†’ Retrieval â†’ Context + Query â†’ LLM â†’ Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Project Architecture\n",
    "\n",
    "RAG Engine Mini follows **Clean Architecture** with these layers:\n",
    "\n",
    "| Layer | Purpose | Example |\n",
    "|-------|---------|--------|\n",
    "| **Domain** | Pure business entities | `Chunk`, `Answer`, `TenantId` |\n",
    "| **Application** | Use cases + interfaces | `AskQuestionHybridUseCase`, `LLMPort` |\n",
    "| **Adapters** | External implementations | `OpenAILLM`, `QdrantVectorStore` |\n",
    "| **API** | HTTP endpoints | FastAPI routes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:09:12.828536Z",
     "iopub.status.busy": "2026-01-31T13:09:12.828536Z",
     "iopub.status.idle": "2026-01-31T13:09:14.982893Z",
     "shell.execute_reply": "2026-01-31T13:09:14.982893Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import from src/ instead of reimplementing\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.core.config import settings\n",
    "from src.domain.entities import Chunk, TenantId, DocumentId, ChunkSpec\n",
    "from src.application.services.chunking import chunk_text_token_aware"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration\n",
    "\n",
    "All settings are loaded from environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:09:14.986668Z",
     "iopub.status.busy": "2026-01-31T13:09:14.985953Z",
     "iopub.status.idle": "2026-01-31T13:09:14.992660Z",
     "shell.execute_reply": "2026-01-31T13:09:14.992660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: dev\n",
      "LLM Backend: openai\n",
      "Embedding Model: text-embedding-3-small\n",
      "Chunk Max Tokens: 512\n",
      "Rerank Backend: cross_encoder\n"
     ]
    }
   ],
   "source": [
    "print(f\"Environment: {settings.env}\")\n",
    "print(f\"LLM Backend: {settings.llm_backend}\")\n",
    "print(f\"Embedding Model: {settings.openai_embed_model}\")\n",
    "print(f\"Chunk Max Tokens: {settings.chunk_max_tokens}\")\n",
    "print(f\"Rerank Backend: {settings.rerank_backend}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ Token-Aware Chunking\n",
    "\n",
    "Let's explore the chunking service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:09:15.043010Z",
     "iopub.status.busy": "2026-01-31T13:09:15.043010Z",
     "iopub.status.idle": "2026-01-31T13:09:15.306656Z",
     "shell.execute_reply": "2026-01-31T13:09:15.305367Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 chunks:\n",
      "\n",
      "--- Chunk 1 (541 chars) ---\n",
      "Artificial Intelligence (AI) has transformed numerous industries in recent years.\n",
      "Machine learning, a subset of AI, enables computers to learn from data without\n",
      "being explicitly programmed. Deep learn...\n",
      "\n",
      "--- Chunk 2 (204 chars) ---\n",
      "massive text corpora, can generate human-like text, answer questions,\n",
      "and assist with various tasks. RAG (Retrieval-Augmented Generation) enhances LLMs\n",
      "by providing relevant context from a knowledge b...\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "Artificial Intelligence (AI) has transformed numerous industries in recent years.\n",
    "Machine learning, a subset of AI, enables computers to learn from data without\n",
    "being explicitly programmed. Deep learning, using neural networks with many layers,\n",
    "has achieved remarkable results in image recognition, natural language processing,\n",
    "and game playing.\n",
    "\n",
    "Large Language Models (LLMs) represent the latest advancement in AI. These models,\n",
    "trained on massive text corpora, can generate human-like text, answer questions,\n",
    "and assist with various tasks. RAG (Retrieval-Augmented Generation) enhances LLMs\n",
    "by providing relevant context from a knowledge base.\n",
    "\"\"\"\n",
    "\n",
    "# Chunk with small max_tokens for demo\n",
    "chunks = chunk_text_token_aware(\n",
    "    sample_text,\n",
    "    spec=ChunkSpec(max_tokens=100, overlap_tokens=20)\n",
    ")\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\n--- Chunk {i} ({len(chunk)} chars) ---\")\n",
    "    print(chunk[:200] + \"...\" if len(chunk) > 200 else chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Domain Entities\n",
    "\n",
    "Let's explore the core domain objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T13:09:15.309694Z",
     "iopub.status.busy": "2026-01-31T13:09:15.308690Z",
     "iopub.status.idle": "2026-01-31T13:09:15.313723Z",
     "shell.execute_reply": "2026-01-31T13:09:15.313723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk ID: chunk_001\n",
      "Tenant: user_123\n",
      "Document: doc_abc\n",
      "Text: This is a sample chunk of text from a document.\n"
     ]
    }
   ],
   "source": [
    "# Create domain entities\n",
    "tenant = TenantId(\"user_123\")\n",
    "doc_id = DocumentId(\"doc_abc\")\n",
    "\n",
    "chunk = Chunk(\n",
    "    id=\"chunk_001\",\n",
    "    tenant_id=tenant,\n",
    "    document_id=doc_id,\n",
    "    text=\"This is a sample chunk of text from a document.\"\n",
    ")\n",
    "\n",
    "print(f\"Chunk ID: {chunk.id}\")\n",
    "print(f\"Tenant: {chunk.tenant_id}\")\n",
    "print(f\"Document: {chunk.document_id}\")\n",
    "print(f\"Text: {chunk.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Next Steps\n",
    "\n",
    "Continue with:\n",
    "\n",
    "1. **02_end_to_end_rag.ipynb** - Complete RAG pipeline walkthrough\n",
    "2. **03_hybrid_search_and_rerank.ipynb** - Deep dive into hybrid retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
