{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Testing Functionality Demo\n",
    "\n",
    "This notebook demonstrates the A/B testing functionality of the RAG Engine Mini.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How the A/B testing functionality works in the RAG Engine\n",
    "2. How to set up and run A/B tests\n",
    "3. How to interpret A/B test results\n",
    "4. The architecture of the A/B testing service\n",
    "5. How A/B testing fits into RAG optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path(\"../\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Environment set up successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the A/B Testing Architecture\n",
    "\n",
    "The A/B testing functionality follows the same architectural patterns as the rest of the RAG Engine:\n",
    "\n",
    "1. **Port/Adapter Pattern**: The `ABTestingServicePort` defines the interface\n",
    "2. **Dependency Injection**: Services are injected through the container\n",
    "3. **Separation of Concerns**: A/B logic is separate from API logic\n",
    "4. **Statistical Analysis**: Built-in statistical significance calculations\n",
    "5. **Experiment Lifecycle**: Full management from creation to analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the A/B testing service definition\n",
    "from src.application.services.ab_testing_service import ABTestingService, ABExperiment, ExperimentVariant, ExperimentStatus, VariantType\n",
    "\n",
    "print(\"A/B Testing Service Components:\")\n",
    "print(f\"- A/B Testing Service: {ABTestingService.__name__}\")\n",
    "print(f\"- A/B Experiment: {ABExperiment.__name__}\")\n",
    "print(f\"- Experiment Variant: {ExperimentVariant.__name__}\")\n",
    "\n",
    "print(f\"\\nExperiment statuses available:\")\n",
    "for status in ExperimentStatus:\n",
    "    print(f\"- {status.value}\")\n",
    "\n",
    "print(f\"\\nVariant types available:\")\n",
    "for variant_type in VariantType:\n",
    "    print(f\"- {variant_type.value}\")\n",
    "\n",
    "print(f\"\\nA/B testing service methods: {[method for method in dir(ABTestingService) if not method.startswith('_') and callable(getattr(ABTestingService, method, None))]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the A/B Testing Service\n",
    "\n",
    "Let's see how to use the A/B testing service to create and manage experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required classes\n",
    "from src.application.services.ab_testing_service import ABTestingService, ABExperiment, ExperimentVariant, ExperimentStatus, VariantType\n",
    "\n",
    "# Create the A/B testing service\n",
    "ab_service = ABTestingService()\n",
    "\n",
    "print(\"A/B testing service initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an A/B Test Experiment\n",
    "\n",
    "Let's create an experiment to compare two different LLM models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our experiment comparing two LLM models\n",
    "experiment = ABExperiment(\n",
    "    experiment_id=\"llm-comparison-001\",\n",
    "    name=\"GPT-3.5 vs GPT-4 Response Quality\",\n",
    "    description=\"Comparing response quality between GPT-3.5 and GPT-4 for RAG queries\",\n",
    "    status=ExperimentStatus.DRAFT,\n",
    "    variants=[\n",
    "        ExperimentVariant(\n",
    "            name=\"gpt-3.5-control\",\n",
    "            description=\"Using GPT-3.5 as the base model\",\n",
    "            traffic_split=0.5,\n",
    "            config={\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.7},\n",
    "            variant_type=VariantType.CONTROL\n",
    "        ),\n",
    "        ExperimentVariant(\n",
    "            name=\"gpt-4-treatment\",\n",
    "            description=\"Using GPT-4 as the improved model\",\n",
    "            traffic_split=0.5,\n",
    "            config={\"model\": \"gpt-4\", \"temperature\": 0.7},\n",
    "            variant_type=VariantType.TREATMENT\n",
    "        )\n",
    "    ],\n",
    "    metrics=[\"response_time\", \"user_satisfaction_score\", \"answer_relevance\"],\n",
    "    created_by=\"rag-engine-admin\",\n",
    "    hypothesis=\"GPT-4 will produce higher quality answers with similar response times\",\n",
    "    owner=\"ai-team\"\n",
    ")\n",
    "\n",
    "# Create the experiment\n",
    "try:\n",
    "    created_experiment = asyncio.run(ab_service.create_experiment(experiment))\n",
    "    print(\"✅ A/B test experiment created successfully\")\n",
    "    print(f\"   Experiment ID: {created_experiment.experiment_id}\")\n",
    "    print(f\"   Name: {created_experiment.name}\")\n",
    "    print(f\"   Status: {created_experiment.status}\")\n",
    "    print(f\"   Variants: {len(created_experiment.variants)}\")\n",
    "    print(f\"   Metrics: {created_experiment.metrics}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to create experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activating the Experiment\n",
    "\n",
    "Now let's activate the experiment to start assigning users to variants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    activated_experiment = asyncio.run(\n",
    "        ab_service.update_experiment_status(\n",
    "            experiment_id=created_experiment.experiment_id,\n",
    "            status=ExperimentStatus.ACTIVE\n",
    "        )\n",
    "    )\n",
    "    print(\"✅ A/B test experiment activated successfully\")\n",
    "    print(f\"   Experiment ID: {activated_experiment.experiment_id}\")\n",
    "    print(f\"   New Status: {activated_experiment.status}\")\n",
    "    print(f\"   Updated at: {activated_experiment.updated_at}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to activate experiment: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Users to Variants\n",
    "\n",
    "Let's simulate assigning users to different variants in the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate assigning several users to variants\n",
    "users = [f\"user-{i}\" for i in range(10)]\n",
    "assignments = []\n",
    "\n",
    "print(\"Assigning users to experiment variants:\\n\")\n",
    "\n",
    "for user_id in users:\n",
    "    try:\n",
    "        assignment = asyncio.run(\n",
    "            ab_service.assign_variant(\n",
    "                experiment_id=created_experiment.experiment_id,\n",
    "                user_id=user_id,\n",
    "                context={\"user_type\": \"premium\" if i % 2 == 0 else \"standard\"}\n",
    "            )\n",
    "        )\n",
    "        assignments.append(assignment)\n",
    "        print(f\"✅ User {user_id} assigned to variant: {assignment.variant_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to assign user {user_id}: {e}\")\n",
    "\n",
    "print(f\"\\nTotal assignments made: {len(assignments)}\")\n",
    "\n",
    "# Count assignments by variant\n",
    "variant_counts = {}\n",
    "for assignment in assignments:\n",
    "    variant = assignment.variant_name\n",
    "    variant_counts[variant] = variant_counts.get(variant, 0) + 1\n",
    "\n",
    "print(\"\\nDistribution by variant:\")\n",
    "for variant, count in variant_counts.items():\n",
    "    print(f\"  {variant}: {count} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Events for Analysis\n",
    "\n",
    "Now let's simulate tracking events during the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate tracking events for each assignment\n",
    "print(\"Tracking events for analysis:\\n\")\n",
    "\n",
    "for i, assignment in enumerate(assignments):\n",
    "    # Generate simulated metrics based on the variant\n",
    "    if \"gpt-4\" in assignment.variant_name:\n",
    "        # GPT-4 variant performs slightly better\n",
    "        satisfaction_score = 4.2 + (i % 3) * 0.1  # Slightly higher scores\n",
    "        response_time = 1.2 + (i % 5) * 0.1\n",
    "        relevance = 0.85 + (i % 4) * 0.02\n",
    "    else:\n",
    "        # GPT-3.5 variant baseline performance\n",
    "        satisfaction_score = 3.8 + (i % 3) * 0.1\n",
    "        response_time = 1.1 + (i % 5) * 0.1\n",
    "        relevance = 0.78 + (i % 4) * 0.02\n",
    "\n",
    "    # Track different types of events\n",
    "    events_to_track = [\n",
    "        {\"type\": \"user_satisfaction_score\", \"value\": satisfaction_score},\n",
    "        {\"type\": \"response_time\", \"value\": response_time},\n",
    "        {\"type\": \"answer_relevance\", \"value\": relevance}\n",
    "    ]\n",
    "    \n",
    "    for event in events_to_track:\n",
    "        try:\n",
    "            asyncio.run(\n",
    "                ab_service.track_event(\n",
    "                    experiment_id=created_experiment.experiment_id,\n",
    "                    user_id=assignment.user_id,\n",
    "                    variant_name=assignment.variant_name,\n",
    "                    event_type=event[\"type\"],\n",
    "                    value=event[\"value\"],\n",
    "                    metadata={\"session_id\": f\"session-{i}\", \"timestamp\": assignment.assigned_at.isoformat()}\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to track event for user {assignment.user_id}: {e}\")\n",
    "\n",
    "print(f\"✅ Tracked events for {len(assignments)} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Experiment Results\n",
    "\n",
    "Now let's get the results of our A/B test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    results = asyncio.run(\n",
    "        ab_service.get_experiment_results(created_experiment.experiment_id)\n",
    "    )\n",
    "    \n",
    "    print(\"A/B Test Results:\")\n",
    "    print(f\"- Experiment ID: {results.experiment_id}\")\n",
    "    print(f\"- Winner: {results.winner or 'No clear winner'}\")\n",
    "    print(f\"- Statistically Significant: {results.is_significant}\")\n",
    "    print(f\"- Conclusion: {results.conclusion}\")\n",
    "    \n",
    "    print(f\"\\nVariant Results:\")\n",
    "    for variant_name, metrics in results.variant_results.items():\n",
    "        print(f\"  \\033[1m{variant_name}\\033[0m:\")\n",
    "        print(f\"    - Total Events: {metrics.get('total_events', 0)}\")\n",
    "        print(f\"    - Conversion Rate: {metrics.get('conversion_rate', 0):.3f}\")\n",
    "        print(f\"    - Average Value: {metrics.get('average_value', 0):.3f}\")\n",
    "        \n",
    "    print(f\"\\nStatistical Significance:\")\n",
    "    for metric, comparisons in results.statistical_significance.items():\n",
    "        print(f\"  \\033[1m{metric}\\033[0m:\")\n",
    "        for variant, stats in comparisons.items():\n",
    "            print(f\"    - vs {variant}: p-value = {stats.get('p_value', 'N/A'):.3f}, \")\n",
    "            print(f\"               significant = {stats.get('significant', 'N/A')}, \")\n",
    "            print(f\"               effect_size = {stats.get('effect_size', 'N/A'):.3f}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to get experiment results: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Sample Size\n",
    "\n",
    "Let's see how to calculate the required sample size for an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Calculate sample size needed for an experiment\n",
    "    # Assuming a baseline conversion rate of 10%, wanting to detect a 3% absolute improvement\n",
    "    sample_size = asyncio.run(\n",
    "        ab_service.calculate_sample_size(\n",
    "            baseline_conversion_rate=0.10,  # 10% baseline\n",
    "            minimum_detectable_effect=0.03,  # Want to detect 3% absolute improvement\n",
    "            significance_level=0.05,  # 95% confidence\n",
    "            power=0.8  # 80% power\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Required Sample Size Calculation:\")\n",
    "    print(f\"- Baseline conversion rate: 10%\")\n",
    "    print(f\"- Minimum detectable effect: 3% absolute\")\n",
    "    print(f\"- Confidence level: 95%\")\n",
    "    print(f\"- Statistical power: 80%\")\n",
    "    print(f\"- Required sample size: {sample_size:,} total participants\")\n",
    "    print(f\"- Recommended: {sample_size//2:,} per variant\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to calculate sample size: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Endpoints\n",
    "\n",
    "The A/B testing functionality is also available through API endpoints. Let's examine the routes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the API router to see available endpoints\n",
    "from src.api.v1.routes_ab_testing import router\n",
    "\n",
    "print(\"A/B Testing API routes:\")\n",
    "for route in router.routes:\n",
    "    if hasattr(route, 'methods') and hasattr(route, 'path'):\n",
    "        print(f\"- {list(route.methods)}: {route.path}\")\n",
    "\n",
    "print(f\"\\nTotal A/B testing API routes: {len([r for r in router.routes if hasattr(r, 'methods')])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How A/B Testing Benefits RAG Systems\n",
    "\n",
    "A/B testing is especially valuable in RAG systems for optimizing various components:\n",
    "\n",
    "1. **Model Comparison**: Compare different LLMs for response quality\n",
    "2. **Prompt Engineering**: Test different prompt strategies\n",
    "3. **Retrieval Methods**: Compare vector vs keyword search effectiveness\n",
    "4. **Chunking Strategies**: Evaluate different document segmentation approaches\n",
    "5. **Reranking Algorithms**: Test different re-ranking methods\n",
    "6. **System Parameters**: Optimize temperature, top_p, and other settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the A/B testing functionality of the RAG Engine Mini:\n",
    "\n",
    "1. **Architecture**: The A/B testing service follows the same architectural patterns as the rest of the system\n",
    "2. **Experiment Management**: Full lifecycle from creation to analysis\n",
    "3. **Statistical Analysis**: Built-in significance testing and result interpretation\n",
    "4. **API Access**: Multiple endpoints for programmatic access\n",
    "5. **RAG Optimization**: Specific value for improving RAG system components\n",
    "\n",
    "A/B testing is essential for continuously improving RAG systems, allowing teams to make data-driven decisions about which models, prompts, and algorithms perform best in production environments. The RAG Engine's A/B testing implementation provides comprehensive tools for conducting rigorous experiments with proper statistical analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}