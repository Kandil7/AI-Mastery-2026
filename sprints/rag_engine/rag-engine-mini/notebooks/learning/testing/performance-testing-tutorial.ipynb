{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Testing Tutorial\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this tutorial, you will:\n",
    "1. Understand different types of performance testing\n",
    "2. Set up Locust for load testing\n",
    "3. Create realistic user scenarios\n",
    "4. Interpret performance metrics\n",
    "5. Identify and fix performance bottlenecks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+\n",
    "- RAG Engine Mini running locally\n",
    "- Basic understanding of HTTP APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Performance Testing\n",
    "\n",
    "### What is Performance Testing?\n",
    "\n",
    "Performance testing evaluates how a system performs under various workloads. It helps answer questions like:\n",
    "\n",
    "- How many users can the system handle?\n",
    "- What's the response time under load?\n",
    "- Where are the bottlenecks?\n",
    "- Will the system crash under stress?\n",
    "\n",
    "### Types of Performance Testing\n",
    "\n",
    "```\n",
    "1. Load Testing      → Normal expected traffic\n",
    "2. Stress Testing    → Beyond normal capacity\n",
    "3. Spike Testing     → Sudden traffic increases\n",
    "4. Endurance Testing → Long duration stability\n",
    "5. Benchmark Testing → Baseline metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up Locust\n",
    "\n",
    "### Installation\n",
    "\n",
    "Let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install locust pytest-benchmark aiohttp psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your First Locustfile\n",
    "\n",
    "A Locustfile defines user behavior using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Locust example\n",
    "locust_code = '''\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class SimpleUser(HttpUser):\n",
    "    \"\"\"A simple user that makes requests.\"\"\"\n",
    "    \n",
    "    # Wait 1-3 seconds between tasks\n",
    "    wait_time = between(1, 3)\n",
    "    \n",
    "    @task\n",
    "    def visit_homepage(self):\n",
    "        \"\"\"Simulate visiting the homepage.\"\"\"\n",
    "        self.client.get(\"/health\")\n",
    "    \n",
    "    @task(3)  # 3x more likely than other tasks\n",
    "    def ask_question(self):\n",
    "        \"\"\"Simulate asking a question.\"\"\"\n",
    "        self.client.post(\n",
    "            \"/api/v1/ask\",\n",
    "            json={\"question\": \"What is RAG?\", \"k\": 5}\n",
    "        )\n",
    "'''\n",
    "\n",
    "print(locust_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts\n",
    "\n",
    "1. **HttpUser**: Represents a user making HTTP requests\n",
    "2. **@task**: Decorator marking a method as a task to perform\n",
    "3. **wait_time**: Time between consecutive tasks\n",
    "4. **self.client**: HTTP client for making requests\n",
    "\n",
    "### Task Weights\n",
    "\n",
    "The number in `@task(n)` determines probability:\n",
    "- `@task` = weight 1\n",
    "- `@task(3)` = 3x more likely\n",
    "\n",
    "Example with multiple tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate task weights\n",
    "import random\n",
    "\n",
    "# Simulate task selection\n",
    "tasks = [\n",
    "    (\"health_check\", 2),    # Weight 2\n",
    "    (\"ask_question\", 5),    # Weight 5\n",
    "    (\"search\", 3),          # Weight 3\n",
    "]\n",
    "\n",
    "total_weight = sum(w for _, w in tasks)\n",
    "\n",
    "print(\"Task Distribution (out of 1000 requests):\")\n",
    "for task_name, weight in tasks:\n",
    "    percentage = (weight / total_weight) * 100\n",
    "    count = int((weight / total_weight) * 1000)\n",
    "    print(f\"  {task_name}: {percentage:.1f}% (~{count} requests)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Creating Realistic User Scenarios\n",
    "\n",
    "### Authenticated Users\n",
    "\n",
    "Most API endpoints require authentication. Let's create an authenticated user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticated user example\n",
    "auth_user_code = '''\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class AuthenticatedUser(HttpUser):\n",
    "    wait_time = between(1, 5)\n",
    "    \n",
    "    def on_start(self):\n",
    "        \"\"\"Called when user starts - use for authentication.\"\"\"\n",
    "        response = self.client.post(\n",
    "            \"/api/v1/auth/login\",\n",
    "            json={\n",
    "                \"email\": \"test@example.com\",\n",
    "                \"password\": \"TestPass123!\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            self.token = response.json()[\"access_token\"]\n",
    "        else:\n",
    "            print(f\"Failed to login: {response.status_code}\")\n",
    "    \n",
    "    def get_headers(self):\n",
    "        \"\"\"Get request headers with authentication.\"\"\"\n",
    "        return {\"Authorization\": f\"Bearer {self.token}\"}\n",
    "    \n",
    "    @task(3)\n",
    "    def ask_question(self):\n",
    "        self.client.post(\n",
    "            \"/api/v1/ask\",\n",
    "            headers=self.get_headers(),\n",
    "            json={\"question\": \"What is RAG?\", \"k\": 5}\n",
    "        )\n",
    "'''\n",
    "\n",
    "print(auth_user_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple User Types\n",
    "\n",
    "Real systems have different user types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple user types\n",
    "print(\"\"\"\n",
    "Different User Types in Load Testing:\n",
    "\n",
    "1. RegularUser (80% of traffic)\n",
    "   - Browses documents\n",
    "   - Asks occasional questions\n",
    "   - Low resource usage\n",
    "\n",
    "2. PowerUser (15% of traffic)\n",
    "   - Uploads many documents\n",
    "   - Complex queries\n",
    "   - High resource usage\n",
    "\n",
    "3. MonitoringBot (5% of traffic)\n",
    "   - Only health checks\n",
    "   - Very frequent\n",
    "   - Minimal payload\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Running Load Tests\n",
    "\n",
    "### Web Interface Mode\n",
    "\n",
    "Best for development and debugging:\n",
    "\n",
    "```bash\n",
    "locust -f tests/performance/locustfile.py --host=http://localhost:8000\n",
    "```\n",
    "\n",
    "Then open http://localhost:8089 in your browser.\n",
    "\n",
    "### Headless Mode\n",
    "\n",
    "Best for CI/CD and automation:\n",
    "\n",
    "```bash\n",
    "locust -f tests/performance/locustfile.py \\\\\n",
    "    --host=http://localhost:8000 \\\\\n",
    "    --headless \\\\\n",
    "    -u 100 \\\\\n",
    "    -r 10 \\\\\n",
    "    --run-time 5m \\\\\n",
    "    --csv=results\n",
    "```\n",
    "\n",
    "Parameters explained:\n",
    "- `-u 100`: 100 concurrent users\n",
    "- `-r 10`: Spawn 10 users per second\n",
    "- `--run-time 5m`: Run for 5 minutes\n",
    "- `--csv=results`: Save results to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding Performance Metrics\n",
    "\n",
    "### Key Metrics Explained\n",
    "\n",
    "Let's simulate and visualize performance metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simulate response times (in milliseconds)\n",
    "# Most requests are fast, some are slow (realistic distribution)\n",
    "np.random.seed(42)\n",
    "response_times = np.concatenate([\n",
    "    np.random.normal(200, 50, 900),    # 90% around 200ms\n",
    "    np.random.normal(1000, 200, 80),   # 8% around 1s\n",
    "    np.random.normal(3000, 500, 20),   # 2% around 3s (slow)\n",
    "])\n",
    "\n",
    "# Calculate percentiles\n",
    "p50 = np.percentile(response_times, 50)\n",
    "p95 = np.percentile(response_times, 95)\n",
    "p99 = np.percentile(response_times, 99)\n",
    "\n",
    "print(f\"Response Time Metrics:\")\n",
    "print(f\"  P50 (Median): {p50:.0f}ms\")\n",
    "print(f\"  P95: {p95:.0f}ms\")\n",
    "print(f\"  P99: {p99:.0f}ms\")\n",
    "print(f\"  Mean: {np.mean(response_times):.0f}ms\")\n",
    "print(f\"  Max: {np.max(response_times):.0f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(response_times, bins=50, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(p50, color='green', linestyle='--', label=f'P50: {p50:.0f}ms')\n",
    "ax1.axvline(p95, color='orange', linestyle='--', label=f'P95: {p95:.0f}ms')\n",
    "ax1.axvline(p99, color='red', linestyle='--', label=f'P99: {p99:.0f}ms')\n",
    "ax1.set_xlabel('Response Time (ms)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Response Time Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(response_times, vert=True)\n",
    "ax2.set_ylabel('Response Time (ms)')\n",
    "ax2.set_title('Response Time Box Plot')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Percentiles\n",
    "\n",
    "- **P50 (Median)**: Half of users experience response times faster than this\n",
    "- **P95**: 95% of users experience response times faster than this\n",
    "- **P99**: 99% of users experience response times faster than this\n",
    "\n",
    "**Why percentiles matter more than averages:**\n",
    "\n",
    "Averages hide outliers. A system with:\n",
    "- Average: 500ms\n",
    "- P95: 5 seconds\n",
    "\n",
    "is worse than:\n",
    "- Average: 700ms\n",
    "- P95: 1 second\n",
    "\n",
    "Even though the average is higher, users have a more consistent experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Load Patterns\n",
    "\n",
    "### Custom Load Shapes\n",
    "\n",
    "Sometimes you need non-linear load patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom load shapes example\n",
    "print(\"\"\"\n",
    "Custom Load Shape Examples:\n",
    "\n",
    "1. Spike Test\n",
    "   Normal: 50 users\n",
    "   Spike (every 60s): 200 users for 10s\n",
    "   Tests: Auto-scaling, circuit breakers\n",
    "\n",
    "2. Ramp Up\n",
    "   Start: 10 users\n",
    "   Increase: +10 users every 30s\n",
    "   Max: 300 users\n",
    "   Tests: Capacity limits, gradual scaling\n",
    "\n",
    "3. Daily Pattern\n",
    "   Low: 20 users (night)\n",
    "   Medium: 100 users (morning)\n",
    "   High: 500 users (peak hours)\n",
    "   Tests: Variable capacity planning\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different load patterns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "time_minutes = np.arange(0, 30, 0.5)\n",
    "\n",
    "# Spike pattern\n",
    "spike_users = []\n",
    "for t in time_minutes:\n",
    "    if int(t) % 6 < 1:  # Spike every 6 minutes\n",
    "        spike_users.append(200)\n",
    "    else:\n",
    "        spike_users.append(50)\n",
    "\n",
    "# Ramp up pattern\n",
    "ramp_users = np.minimum(10 + time_minutes * 5, 300)\n",
    "\n",
    "# Steady pattern\n",
    "steady_users = [100] * len(time_minutes)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time_minutes, spike_users, label='Spike Pattern', linewidth=2)\n",
    "plt.plot(time_minutes, ramp_users, label='Ramp Up', linewidth=2)\n",
    "plt.plot(time_minutes, steady_users, label='Steady Load', linewidth=2)\n",
    "plt.xlabel('Time (minutes)')\n",
    "plt.ylabel('Number of Users')\n",
    "plt.title('Different Load Patterns')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Identifying Bottlenecks\n",
    "\n",
    "### Common Performance Bottlenecks\n",
    "\n",
    "1. **Database Connections**\n",
    "   - Symptom: Increasing response times under load\n",
    "   - Solution: Connection pooling, query optimization\n",
    "\n",
    "2. **LLM API Rate Limits**\n",
    "   - Symptom: Timeouts on /ask endpoint\n",
    "   - Solution: Request queuing, caching\n",
    "\n",
    "3. **Vector Search Latency**\n",
    "   - Symptom: Slow RAG queries\n",
    "   - Solution: Index optimization, approximate search\n",
    "\n",
    "4. **Memory Leaks**\n",
    "   - Symptom: Memory grows over time\n",
    "   - Solution: Proper cleanup, monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate bottleneck detection\n",
    "print(\"Bottleneck Detection Example:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simulate metrics\n",
    "metrics = {\n",
    "    \"Response Time\": {\"current\": \"1.2s\", \"threshold\": \"< 2s\", \"status\": \"✓ OK\"},\n",
    "    \"DB Connections\": {\"current\": \"45/50\", \"threshold\": \"< 80%\", \"status\": \"⚠ WARNING\"},\n",
    "    \"Memory Usage\": {\"current\": \"512MB\", \"threshold\": \"< 1GB\", \"status\": \"✓ OK\"},\n",
    "    \"Error Rate\": {\"current\": \"0.5%\", \"threshold\": \"< 1%\", \"status\": \"✓ OK\"},\n",
    "    \"LLM Latency\": {\"current\": \"3.5s\", \"threshold\": \"< 2s\", \"status\": \"✗ CRITICAL\"},\n",
    "}\n",
    "\n",
    "for metric, data in metrics.items():\n",
    "    print(f\"{metric:20} {data['current']:>10} (target: {data['threshold']}) {data['status']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ALERT: LLM latency is above threshold!\")\n",
    "print(\"ACTION: Consider implementing request queuing or caching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Exercise\n",
    "\n",
    "### Exercise 1: Baseline Performance Test\n",
    "\n",
    "1. Start your RAG Engine locally\n",
    "2. Run a baseline load test:\n",
    "   ```bash\n",
    "   locust -f tests/performance/locustfile.py --host=http://localhost:8000\n",
    "   ```\n",
    "3. Configure:\n",
    "   - Users: 50\n",
    "   - Spawn rate: 5\n",
    "   - Duration: 5 minutes\n",
    "4. Record the results\n",
    "\n",
    "### Exercise 2: Identify Breaking Point\n",
    "\n",
    "1. Gradually increase users from 50 to 500\n",
    "2. Monitor:\n",
    "   - Response times\n",
    "   - Error rates\n",
    "   - Resource usage\n",
    "3. Note when performance degrades\n",
    "4. Document the breaking point\n",
    "\n",
    "### Exercise 3: Optimize and Retest\n",
    "\n",
    "1. Implement one optimization:\n",
    "   - Add caching\n",
    "   - Optimize database queries\n",
    "   - Increase connection pool\n",
    "2. Rerun the same test\n",
    "3. Compare before/after results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: CI/CD Integration\n",
    "\n",
    "### Running Performance Tests in CI\n",
    "\n",
    "```yaml\n",
    "name: Performance Tests\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 2 * * *'  # Daily at 2 AM\n",
    "\n",
    "jobs:\n",
    "  performance:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Start services\n",
    "        run: docker-compose up -d\n",
    "      \n",
    "      - name: Run performance tests\n",
    "        run: |\n",
    "          locust -f tests/performance/locustfile.py \\\n",
    "            --headless \\\n",
    "            --host=http://localhost:8000 \\\n",
    "            -u 100 \\\n",
    "            -r 10 \\\n",
    "            --run-time 5m \\\n",
    "            --csv=results\n",
    "      \n",
    "      - name: Upload results\n",
    "        uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: performance-results\n",
    "          path: results_*.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Performance testing is continuous** - Not a one-time activity\n",
    "2. **Use realistic scenarios** - Match production usage patterns\n",
    "3. **Monitor percentiles** - Not just averages\n",
    "4. **Test different patterns** - Steady, spike, ramp up\n",
    "5. **Integrate into CI/CD** - Catch regressions early\n",
    "\n",
    "### Performance Checklist\n",
    "\n",
    "- [ ] Define performance targets (RPS, latency)\n",
    "- [ ] Create realistic user scenarios\n",
    "- [ ] Set up automated load testing\n",
    "- [ ] Monitor resource utilization\n",
    "- [ ] Document bottlenecks and optimizations\n",
    "- [ ] Track performance over time\n",
    "- [ ] Set up alerts for regressions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Run the practical exercises in this notebook\n",
    "2. Set up daily automated performance tests\n",
    "3. Create a performance dashboard\n",
    "4. Define SLOs (Service Level Objectives)\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- [Locust Documentation](https://docs.locust.io/)\n",
    "- [Performance Testing Best Practices](https://www.guru99.com/performance-testing.html)\n",
    "- [Google SRE Book](https://sre.google/sre-book/table-of-contents/)\n",
    "- [RAG Engine Performance Guide](../../docs/learning/testing/03-performance-testing.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
