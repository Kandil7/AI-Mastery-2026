{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics & Observability - Interactive Tutorial\n",
    "# ==========================================\n",
    "In this notebook, you'll learn about metrics and observability.\n",
    "\n",
    "## ðŸ“š Learning Objectives\n",
    "\n",
    "By the end of you will:\n",
    "- Understand Prometheus metrics\n",
    "- Implement counters, histograms, gauges\n",
    "- Learn about OpenTelemetry tracing\n",
    "- Track request latency\n",
    "- Monitor error rates\n",
    "\n",
    "## ðŸ”§ Prerequisites\n",
    "\n",
    "Ensure you have installed:\n",
    "- prometheus-client\n",
    "- opentelemetry-api\n",
    "- python 3.11+\n",
    "\n",
    "## ðŸ“¦ Setup\n",
    "\n",
    "Let's start by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server, exposition\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Print setup confirmation\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"   - Prometheus client: {Counter.__module__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prometheus Metrics\n",
    "\n",
    "### 1.1 Counters\n",
    "\n",
    "Counters track cumulative values (e.g., total requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define counters\n",
    "REQUEST_COUNT = Counter(\n",
    "    'api_requests_total',\n",
    "    'API Total Requests',\n",
    "    ['endpoint', 'method', 'status']\n",
    ")\n",
    "\n",
    "ERROR_COUNT = Counter(\n",
    "    'api_errors_total',\n",
    "    'API Total Errors',\n",
    "    ['error_type']\n",
    ")\n",
    "\n",
    "# Increment counters\n",
    "REQUEST_COUNT.labels(endpoint='graphql', method='POST').inc()\n",
    "ERROR_COUNT.labels(error_type='validation').inc()\n",
    "\n",
    "print(\"âœ… Counters defined!\")\n",
    "print(f\"   - api_requests_total: {REQUEST_COUNT}\")\n",
    "print(f\"   - api_errors_total: {ERROR_COUNT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Histograms\n",
    "\n",
    "Histograms track distributions of values (e.g., request latency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define histogram\n",
    "REQUEST_LATENCY = Histogram(\n",
    "    'api_request_duration_seconds',\n",
    "    'API Request Duration',\n",
    "    [0.005, 0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0, 2.5, 5.0, 10.0],\n",
    ")\n",
    "\n",
    "# Observe latency\n",
    "start = time.time()\n",
    "time.sleep(0.1)\n",
    "latency = time.time() - start\n",
    "REQUEST_LATENCY.observe(latency)\n",
    "\n",
    "print(f\"âœ… Latency observed: {latency:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Gauges\n",
    "\n",
    "Gauges track current values (e.g., active connections, queue size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gauges\n",
    "ACTIVE_WEBHOOKS = Gauge(\n",
    "    'webhooks_active',\n",
    "    'Active Webhooks',\n",
    ")\n",
    "\n",
    "CACHE_HIT_RATE = Gauge(\n",
    "    'cache_hit_rate',\n",
    "    'Cache Hit Rate',\n",
    ")\n",
    "\n",
    "# Set gauge values\n",
    "ACTIVE_WEBHOOKS.set(5)\n",
    "CACHE_HIT_RATE.set(0.75)\n",
    "\n",
    "print(\"âœ… Gauges defined and set!\")\n",
    "print(f\"   - webhooks_active: {ACTIVE_WEBHOOKS}\")\n",
    "print(f\"   - cache_hit_rate: {CACHE_HIT_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exposing Metrics\n",
    "\n",
    "### 2.1 Metrics Endpoint\n",
    "\n",
    "Prometheus provides a /metrics endpoint for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prometheus_client import make_asgi_app\n",
    "from fastapi import FastAPI\n",
    "\n",
    "# Create sample app\n",
    "app = FastAPI()\n",
    "\n",
    "# Create metrics app\n",
    "metrics_app = make_asgi_app()\n",
    "\n",
    "# Mount metrics endpoint\n",
    "app.mount(\"/metrics\", metrics_app)\n",
    "\n",
    "print(\"âœ… Metrics endpoint mounted at /metrics\")\n",
    "print(f\"   Access at: http://localhost:8000/metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Practice Exercise\n",
    "\n",
    "### Task: Create custom metrics\n",
    "\n",
    "Create metrics for:\n",
    "1. Document retrieval time\n",
    "2. Embedding cache hit rate\n",
    "3. LLM token usage\n",
    "4. Reranking latency\n",
    "5. Webhook delivery success rate\n",
    "\n",
    "Implement these metrics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE: Create custom metrics\n",
    "\n",
    "# TODO: Create metrics for:\n",
    "# - Document retrieval time\n",
    "# - Embedding cache hit rate\n",
    "# - LLM token usage\n",
    "# - Reranking latency\n",
    "# - Webhook delivery success rate\n",
    "\n",
    "# Example:\n",
    "# RETRIEVAL_TIME = Histogram(...)\n",
    "# CACHE_HIT_RATE = Gauge(...)\n",
    "# LLM_TOKENS = Counter(...)\n",
    "\n",
    "print(\"âœ… Custom metrics defined (TODO: implement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Prometheus Metrics** - Counters, histograms, gauges\n",
    "2. **Metric Types** - Counters (cumulative), Histograms (distribution), Gauges (current)\n",
    "3. **Metrics Endpoint** - /metrics for Prometheus scraping\n",
    "4. **Labeling** - Use labels for filtering (endpoint, method, status)\n",
    "5. **Best Practices** - Track latency, errors, resource usage\n",
    "\n",
    "### ðŸŽ¯ Key Takeaways\n",
    "\n",
    "- Use counters for cumulative values (total requests)\n",
    "- Use histograms for distributions (latency, response times)\n",
    "- Use gauges for current values (active connections, queue size)\n",
    "- Add labels for multi-dimensional metrics\n",
    "- Expose /metrics endpoint for Prometheus\n",
    "- Track business metrics (retrieval time, cache hit rate)\n",
    "\n",
    "### ðŸš€ Next Steps\n",
    "\n",
    "1. Implement custom metrics in production\n",
    "2. Add Grafana dashboard configuration\n",
    "3. Set up alerting rules\n",
    "4. Monitor metrics in production\n",
    "\n",
    "### ðŸ“š Further Reading\n",
    "\n",
    "- [Prometheus Best Practices](https://prometheus.io/docs/practices/)\n",
    "- [OpenTelemetry](https://opentelemetry.io/)\n",
    "- [Metrics Types](https://prometheus.io/docs/concepts/metric_types/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
