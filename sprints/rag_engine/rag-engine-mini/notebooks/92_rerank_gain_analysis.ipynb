{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ RAG Evaluation: Re-ranking Gain Analysis\n",
    "\n",
    "This notebook analyzes the effectiveness of re-ranking in RAG systems, measuring the improvement in result relevance and overall system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Understanding Re-ranking in RAG\n",
    "\n",
    "Re-ranking is a crucial step in RAG pipelines that re-orders initially retrieved documents based on a more sophisticated relevance model. This notebook will help you understand:\n",
    "\n",
    "1. How re-ranking improves result relevance\n",
    "2. Different re-ranking approaches and their effectiveness\n",
    "3. How to measure re-ranking gain\n",
    "4. When re-ranking is most beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_retrieval_results(n_docs: int = 20, n_relevant: int = 5) -> List[Tuple[int, bool, float]]:\n",
    "    \"\"\"\n",
    "    Simulate initial retrieval results with relevance labels and scores\n",
    "    Returns list of (rank, is_relevant, initial_score)\n",
    "    \"\"\"\n",
    "    # Generate random relevance labels (first n_relevant are relevant)\n",
    "    is_relevant = [True]*n_relevant + [False]*(n_docs-n_relevant)\n",
    "    np.random.shuffle(is_relevant)\n",
    "    \n",
    "    # Generate initial scores (higher for relevant docs, with noise)\n",
    "    initial_scores = []\n",
    "    for rel in is_relevant:\n",
    "        if rel:\n",
    "            score = np.random.normal(0.8, 0.15)  # Relevant docs have higher scores\n",
    "        else:\n",
    "            score = np.random.normal(0.4, 0.15)  # Non-relevant docs have lower scores\n",
    "        initial_scores.append(max(0, min(1, score)))  # Clamp to [0, 1]\n",
    "    \n",
    "    # Sort by initial scores (descending) to get initial ranking\n",
    "    combined = list(zip(range(n_docs), is_relevant, initial_scores))\n",
    "    combined.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Assign ranks (0-indexed)\n",
    "    ranked_results = [(i, item[1], item[2]) for i, item in enumerate(combined)]\n",
    "    \n",
    "    return ranked_results\n",
    "\n",
    "# Example simulation\n",
    "initial_results = simulate_retrieval_results(n_docs=20, n_relevant=5)\n",
    "\n",
    "print(\"Initial Retrieval Results (Rank, Relevant, Score):\")\n",
    "for rank, rel, score in initial_results[:10]:  # Show top 10\n",
    "    print(f\"{rank+1:2d}. Relevant: {rel!s:5s} Score: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Measuring Re-ranking Effectiveness\n",
    "\n",
    "Let's implement metrics to measure how much re-ranking improves our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ranking_metrics(results: List[Tuple[int, bool, float]]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate ranking metrics for a list of results\n",
    "    Each result is (rank, is_relevant, score)\n",
    "    \"\"\"\n",
    "    relevant_ranks = [rank+1 for rank, is_rel, _ in results if is_rel]  # 1-indexed\n",
    "    n_relevant = len(relevant_ranks)\n",
    "    n_total = len(results)\n",
    "    \n",
    "    # Precision at K values\n",
    "    prec_at_k = {}\n",
    "    for k in [1, 3, 5, 10]:\n",
    "        if k > n_total:\n",
    "            k = n_total\n",
    "        relevant_in_top_k = sum(1 for rank, is_rel, _ in results[:k] if is_rel)\n",
    "        prec_at_k[f'P@{k}'] = relevant_in_top_k / k\n",
    "    \n",
    "    # Recall at K values\n",
    "    rec_at_k = {}\n",
    "    for k in [5, 10, 20]:\n",
    "        if k > n_total:\n",
    "            k = n_total\n",
    "        relevant_in_top_k = sum(1 for rank, is_rel, _ in results[:k] if is_rel)\n",
    "        rec_at_k[f'R@{k}'] = relevant_in_top_k / n_relevant if n_relevant > 0 else 0\n",
    "    \n",
    "    # Mean Reciprocal Rank\n",
    "    if relevant_ranks:\n",
    "        mrr = sum(1/rank for rank in relevant_ranks) / n_relevant\n",
    "    else:\n",
    "        mrr = 0.0\n",
    "    \n",
    "    # Normalized Discounted Cumulative Gain\n",
    "    def dcg(ranks, n):\n",
    "        return sum(1/np.log2(rank + 1) for rank in ranks)\n",
    "    \n",
    "    # Ideal DCG (relevant docs at top)\n",
    "    ideal_ranks = list(range(1, n_relevant + 1))\n",
    "    ideal_dcg = dcg(ideal_ranks, n_relevant)\n",
    "    \n",
    "    # Actual DCG\n",
    "    actual_dcg = dcg(relevant_ranks, n_relevant)\n",
    "    \n",
    "    ndcg = actual_dcg / ideal_dcg if ideal_dcg > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        **prec_at_k,\n",
    "        **rec_at_k,\n",
    "        'MRR': mrr,\n",
    "        'NDCG': ndcg\n",
    "    }\n",
    "\n",
    "# Calculate metrics for initial results\n",
    "initial_metrics = calculate_ranking_metrics(initial_results)\n",
    "print(\"Initial Retrieval Metrics:\")\n",
    "for metric, value in initial_metrics.items():\n",
    "    print(f\"{metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Simulating Re-ranking Process\n",
    "\n",
    "Now let's simulate how re-ranking might improve these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reranking(initial_results: List[Tuple[int, bool, float]], \n",
    "                      effectiveness: float = 0.7) -> List[Tuple[int, bool, float]]:\n",
    "    \"\"\"\n",
    "    Simulate re-ranking process\n",
    "    effectiveness: How effective the re-ranking is (0-1)\n",
    "    \"\"\"\n",
    "    # Create new scores based on relevance and some re-ranking effectiveness\n",
    "    reranked_items = []\n",
    "    for orig_rank, is_rel, orig_score in initial_results:\n",
    "        # Base new score on relevance\n",
    "        base_score = 0.9 if is_rel else 0.3\n",
    "        \n",
    "        # Add some randomness based on effectiveness\n",
    "        random_factor = np.random.random() * (1 - effectiveness)\n",
    "        new_score = base_score * effectiveness + orig_score * (1 - effectiveness) + random_factor * 0.1\n",
    "        \n",
    "        # Clamp to [0, 1]\n",
    "        new_score = max(0, min(1, new_score))\n",
    "        \n",
    "        reranked_items.append((orig_rank, is_rel, new_score))\n",
    "    \n",
    "    # Sort by new scores (descending) to get reranked results\n",
    "    reranked_items.sort(key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Assign new ranks\n",
    "    reranked_results = [(i, item[1], item[2]) for i, item in enumerate(reranked_items)]\n",
    "    \n",
    "    return reranked_results\n",
    "\n",
    "# Simulate re-ranking with different effectiveness levels\n",
    "effectiveness_levels = [0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "print(\"Comparing Re-ranking Effectiveness Levels:\\n\")\n",
    "comparison_data = []\n",
    "\n",
    "for eff in effectiveness_levels:\n",
    "    reranked_results = simulate_reranking(initial_results, effectiveness=eff)\n",
    "    reranked_metrics = calculate_ranking_metrics(reranked_results)\n",
    "    \n",
    "    print(f\"Effectiveness Level: {eff}\")\n",
    "    print(f\"  MRR: {reranked_metrics['MRR']:.3f} (vs initial {initial_metrics['MRR']:.3f})\")\n",
    "    print(f\"  NDCG: {reranked_metrics['NDCG']:.3f} (vs initial {initial_metrics['NDCG']:.3f})\")\n",
    "    print(f\"  P@5: {reranked_metrics['P@5']:.3f} (vs initial {initial_metrics['P@5']:.3f})\")\n",
    "    print()\n",
    "    \n",
    "    # Store for comparison plot\n",
    "    comparison_data.append({\n",
    "        'Effectiveness': eff,\n",
    "        'MRR_Initial': initial_metrics['MRR'],\n",
    "        'MRR_Reranked': reranked_metrics['MRR'],\n",
    "        'NDCG_Initial': initial_metrics['NDCG'],\n",
    "        'NDCG_Reranked': reranked_metrics['NDCG'],\n",
    "        'P5_Initial': initial_metrics['P@5'],\n",
    "        'P5_Reranked': reranked_metrics['P@5']\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Summary Table:\")\n",
    "print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Visualizing Re-ranking Gains\n",
    "\n",
    "Let's visualize how re-ranking effectiveness impacts different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rerank_gains(comparison_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot re-ranking gains across different effectiveness levels\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # MRR comparison\n",
    "    axes[0, 0].plot(comparison_df['Effectiveness'], comparison_df['MRR_Initial'], \n",
    "                    label='Initial', marker='o', linewidth=2)\n",
    "    axes[0, 0].plot(comparison_df['Effectiveness'], comparison_df['MRR_Reranked'], \n",
    "                    label='Re-ranked', marker='s', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Re-ranking Effectiveness')\n",
    "    axes[0, 0].set_ylabel('MRR')\n",
    "    axes[0, 0].set_title('Mean Reciprocal Rank: Before vs After Re-ranking')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # NDCG comparison\n",
    "    axes[0, 1].plot(comparison_df['Effectiveness'], comparison_df['NDCG_Initial'], \n",
    "                    label='Initial', marker='o', linewidth=2)\n",
    "    axes[0, 1].plot(comparison_df['Effectiveness'], comparison_df['NDCG_Reranked'], \n",
    "                    label='Re-ranked', marker='s', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Re-ranking Effectiveness')\n",
    "    axes[0, 1].set_ylabel('NDCG')\n",
    "    axes[0, 1].set_title('Normalized Discounted Cumulative Gain: Before vs After Re-ranking')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # P@5 comparison\n",
    "    axes[1, 0].plot(comparison_df['Effectiveness'], comparison_df['P5_Initial'], \n",
    "                    label='Initial', marker='o', linewidth=2)\n",
    "    axes[1, 0].plot(comparison_df['Effectiveness'], comparison_df['P5_Reranked'], \n",
    "                    label='Re-ranked', marker='s', linewidth=2)\n",
    "    axes[1, 0].set_xlabel('Re-ranking Effectiveness')\n",
    "    axes[1, 0].set_ylabel('Precision@5')\n",
    "    axes[1, 0].set_title('Precision@5: Before vs After Re-ranking')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gain calculation\n",
    "    mrr_gain = comparison_df['MRR_Reranked'] - comparison_df['MRR_Initial']\n",
    "    ndcg_gain = comparison_df['NDCG_Reranked'] - comparison_df['NDCG_Initial']\n",
    "    p5_gain = comparison_df['P5_Reranked'] - comparison_df['P5_Initial']\n",
    "    \n",
    "    axes[1, 1].plot(comparison_df['Effectiveness'], mrr_gain, \n",
    "                    label='MRR Gain', marker='o', linewidth=2)\n",
    "    axes[1, 1].plot(comparison_df['Effectiveness'], ndcg_gain, \n",
    "                    label='NDCG Gain', marker='s', linewidth=2)\n",
    "    axes[1, 1].plot(comparison_df['Effectiveness'], p5_gain, \n",
    "                    label='P@5 Gain', marker='^', linewidth=2)\n",
    "    axes[1, 1].set_xlabel('Re-ranking Effectiveness')\n",
    "    axes[1, 1].set_ylabel('Improvement')\n",
    "    axes[1, 1].set_title('Improvement from Re-ranking')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rerank_gains(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Comparing Re-ranking Approaches\n",
    "\n",
    "Let's compare different re-ranking approaches: Cross-Encoder vs LLM-based vs No Re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_different_rerank_approaches(n_queries: int = 50):\n",
    "    \"\"\"\n",
    "    Simulate performance of different re-ranking approaches\n",
    "    \"\"\"\n",
    "    approaches = {\n",
    "        'No Re-ranking': {'speed': 1.0, 'effectiveness': 0.0, 'cost': 0.0},\n",
    "        'Cross-Encoder': {'speed': 0.7, 'effectiveness': 0.7, 'cost': 0.3},\n",
    "        'LLM Re-ranking': {'speed': 0.2, 'effectiveness': 0.85, 'cost': 0.8}\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for approach_name, params in approaches.items():\n",
    "        approach_results = []\n",
    "        \n",
    "        for i in range(n_queries):\n",
    "            # Simulate initial retrieval\n",
    "            initial = simulate_retrieval_results(n_docs=20, n_relevant=5)\n",
    "            initial_metrics = calculate_ranking_metrics(initial)\n",
    "            \n",
    "            # Apply re-ranking based on approach effectiveness\n",
    "            if params['effectiveness'] > 0:\n",
    "                reranked = simulate_reranking(initial, effectiveness=params['effectiveness'])\n",
    "                reranked_metrics = calculate_ranking_metrics(reranked)\n",
    "            else:\n",
    "                # No re-ranking\n",
    "                reranked_metrics = initial_metrics\n",
    "            \n",
    "            approach_results.append({\n",
    "                'Query_ID': i,\n",
    "                'MRR': reranked_metrics['MRR'],\n",
    "                'NDCG': reranked_metrics['NDCG'],\n",
    "                'P@5': reranked_metrics['P@5'],\n",
    "                'R@10': reranked_metrics['R@10']\n",
    "            })\n",
    "        \n",
    "        # Aggregate results for this approach\n",
    "        df = pd.DataFrame(approach_results)\n",
    "        avg_results = df.mean(numeric_only=True)\n",
    "        \n",
    "        results.append({\n",
    "            'Approach': approach_name,\n",
    "            'Speed_Score': params['speed'],\n",
    "            'Cost_Score': params['cost'],\n",
    "            'Avg_MRR': avg_results['MRR'],\n",
    "            'Avg_NDCG': avg_results['NDCG'],\n",
    "            'Avg_P@5': avg_results['P@5'],\n",
    "            'Avg_R@10': avg_results['R@10']\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run simulation\n",
    "approach_comparison = simulate_different_rerank_approaches(n_queries=50)\n",
    "print(\"Re-ranking Approach Comparison:\")\n",
    "print(approach_comparison.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Cost-Benefit Analysis\n",
    "\n",
    "Let's analyze the cost-benefit trade-offs of different re-ranking approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_benefit_analysis(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Plot cost-benefit analysis of different re-ranking approaches\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "    \n",
    "    # Use Avg_P@5 as benefit metric and Cost_Score as cost metric\n",
    "    scatter = ax.scatter(df['Cost_Score'], df['Avg_P@5'], \n",
    "                        s=200, c=df['Avg_MRR'], cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Annotate points\n",
    "    for _, row in df.iterrows():\n",
    "        ax.annotate(row['Approach'], \n",
    "                   (row['Cost_Score'], row['Avg_P@5']),\n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=12)\n",
    "    \n",
    "    ax.set_xlabel('Cost Score (Lower is Better)')\n",
    "    ax.set_ylabel('Benefit: P@5 (Higher is Better)')\n",
    "    ax.set_title('Cost-Benefit Analysis of Re-ranking Approaches\\n(Color represents MRR)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('MRR Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_cost_benefit_analysis(approach_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª When is Re-ranking Most Beneficial?\n",
    "\n",
    "Let's analyze scenarios where re-ranking provides the most value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rerank_scenarios():\n",
    "    \"\"\"\n",
    "    Analyze when re-ranking is most beneficial\n",
    "    \"\"\"\n",
    "    scenarios = [\n",
    "        {\n",
    "            'name': 'High Noise in Initial Retrieval',\n",
    "            'initial_quality': 0.3,  # Low initial quality\n",
    "            'n_docs': 50,\n",
    "            'n_relevant': 5\n",
    "        },\n",
    "        {\n",
    "            'name': 'Medium Noise in Initial Retrieval',\n",
    "            'initial_quality': 0.5,  # Medium initial quality\n",
    "            'n_docs': 50,\n",
    "            'n_relevant': 5\n",
    "        },\n",
    "        {\n",
    "            'name': 'Low Noise in Initial Retrieval',\n",
    "            'initial_quality': 0.8,  # High initial quality\n",
    "            'n_docs': 50,\n",
    "            'n_relevant': 5\n",
    "        },\n",
    "        {\n",
    "            'name': 'Many Relevant Documents',\n",
    "            'initial_quality': 0.5,\n",
    "            'n_docs': 50,\n",
    "            'n_relevant': 15  # More relevant docs\n",
    "        },\n",
    "        {\n",
    "            'name': 'Few Relevant Documents',\n",
    "            'initial_quality': 0.5,\n",
    "            'n_docs': 50,\n",
    "            'n_relevant': 2   # Fewer relevant docs\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        # Simulate initial retrieval with varying quality\n",
    "        initial_results = []\n",
    "        n_rel = scenario['n_relevant']\n",
    "        n_irrel = scenario['n_docs'] - n_rel\n",
    "        \n",
    "        # Generate relevance labels\n",
    "        is_relevant = [True]*n_rel + [False]*n_irrel\n",
    "        \n",
    "        # Generate scores based on quality level\n",
    "        scores = []\n",
    "        for rel in is_relevant:\n",
    "            if rel:\n",
    "                # Relevant docs get higher scores with some noise based on quality\n",
    "                score = np.random.normal(0.7 + scenario['initial_quality']*0.2, \n",
    "                                       (1-scenario['initial_quality'])*0.2)\n",
    "            else:\n",
    "                # Irrelevant docs get lower scores\n",
    "                score = np.random.normal(0.3 - scenario['initial_quality']*0.1, \n",
    "                                       (1-scenario['initial_quality'])*0.2)\n",
    "            scores.append(max(0, min(1, score)))\n",
    "        \n",
    "        # Combine and sort by score\n",
    "        combined = list(zip(range(len(is_relevant)), is_relevant, scores))\n",
    "        combined.sort(key=lambda x: x[2], reverse=True)\n",
    "        initial_results = [(i, item[1], item[2]) for i, item in enumerate(combined)]\n",
    "        \n",
    "        # Calculate initial metrics\n",
    "        initial_metrics = calculate_ranking_metrics(initial_results)\n",
    "        \n",
    "        # Apply re-ranking\n",
    "        reranked_results = simulate_reranking(initial_results, effectiveness=0.7)\n",
    "        reranked_metrics = calculate_ranking_metrics(reranked_results)\n",
    "        \n",
    "        # Calculate gains\n",
    "        mrr_gain = reranked_metrics['MRR'] - initial_metrics['MRR']\n",
    "        ndcg_gain = reranked_metrics['NDCG'] - initial_metrics['NDCG']\n",
    "        p5_gain = reranked_metrics['P@5'] - initial_metrics['P@5']\n",
    "        \n",
    "        results.append({\n",
    "            'Scenario': scenario['name'],\n",
    "            'Initial_Quality': scenario['initial_quality'],\n",
    "            'N_Docs': scenario['n_docs'],\n",
    "            'N_Relevant': scenario['n_relevant'],\n",
    "            'Initial_MRR': initial_metrics['MRR'],\n",
    "            'Reranked_MRR': reranked_metrics['MRR'],\n",
    "            'MRR_Gain': mrr_gain,\n",
    "            'NDCG_Gain': ndcg_gain,\n",
    "            'P5_Gain': p5_gain\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Analyze scenarios\n",
    "scenario_analysis = analyze_rerank_scenarios()\n",
    "print(\"When is Re-ranking Most Beneficial?\")\n",
    "print(scenario_analysis.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Findings & Recommendations\n",
    "\n",
    "Based on our analysis, here are the key findings about re-ranking in RAG systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_findings(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Summarize key findings from the analysis\n",
    "    \"\"\"\n",
    "    print(\"KEY FINDINGS FROM RE-RANKING ANALYSIS\")\n",
    "    print(\"====================================\")\n",
    "    \n",
    "    # Finding 1: Effectiveness correlation\n",
    "    print(\"1. Re-ranking Effectiveness Correlates with Initial Retrieval Quality:\")\n",
    "    low_noise_row = df[df['Initial_Quality'] == 0.8].iloc[0]\n",
    "    high_noise_row = df[df['Initial_Quality'] == 0.3].iloc[0]\n",
    "    print(f\"   High initial quality (0.8) -> MRR gain: {low_noise_row['MRR_Gain']:.3f}\")\n",
    "    print(f\"   Low initial quality (0.3)  -> MRR gain: {high_noise_row['MRR_Gain']:.3f}\")\n",
    "    print(\"   â†’ Re-ranking is more beneficial when initial retrieval is noisy\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Finding 2: Approach comparison\n",
    "    print(\"2. Approach Trade-offs:\")\n",
    "    cross_enc = approach_comparison[approach_comparison['Approach'] == 'Cross-Encoder'].iloc[0]\n",
    "    llm_rerank = approach_comparison[approach_comparison['Approach'] == 'LLM Re-ranking'].iloc[0]\n",
    "    print(f\"   Cross-Encoder: P@5={cross_enc['Avg_P@5']:.3f}, Speed={cross_enc['Speed_Score']:.1f}, Cost={cross_enc['Cost_Score']:.1f}\")\n",
    "    print(f\"   LLM Re-ranking: P@5={llm_rerank['Avg_P@5']:.3f}, Speed={llm_rerank['Speed_Score']:.1f}, Cost={llm_rerank['Cost_Score']:.1f}\")\n",
    "    print(\"   â†’ Cross-encoder offers best balance of effectiveness, speed, and cost\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Finding 3: Scenario analysis\n",
    "    print(\"3. Re-ranking is Most Beneficial When:\")\n",
    "    max_gain_idx = df['MRR_Gain'].idxmax()\n",
    "    max_gain_row = df.loc[max_gain_idx]\n",
    "    print(f\"   - Initial retrieval has high noise (low quality): {max_gain_row['Scenario']}\")\n",
    "    print(\"   â†’ Use re-ranking when initial retriever is unreliable\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    print(\"PRACTICAL RECOMMENDATIONS\")\n",
    "    print(\"=========================\")\n",
    "    print(\"1. Start with Cross-Encoder re-ranking for best cost-effectiveness\")\n",
    "    print(\"2. Use re-ranking when initial retrieval MRR < 0.3\")\n",
    "    print(\"3. Monitor re-ranking effectiveness and disable if gains < 5%\")\n",
    "    print(\"4. Consider LLM re-ranking only for complex semantic matching needs\")\n",
    "    print(\"5. Implement fallback to skip re-ranking for latency-sensitive applications\")\n",
    "\n",
    "summarize_findings(scenario_analysis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}