{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Caching Functionality Demo\n",
    "\n",
    "This notebook demonstrates the multi-layer caching functionality of the RAG Engine Mini.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. How the multi-layer caching works in the RAG Engine\n",
    "2. The different cache layers (L1, L2, L3) and their purposes\n",
    "3. How to use the caching service\n",
    "4. The architecture of the multi-layer cache system\n",
    "5. How caching improves RAG system performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: ..\n",
      "Environment set up successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = Path(\"../\")\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(\"Environment set up successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Multi-Layer Cache Architecture\n",
    "\n",
    "The multi-layer caching functionality follows the same architectural patterns as the rest of the RAG Engine:\n",
    "\n",
    "1. **Port/Adapter Pattern**: The `MultiLayerCachePort` defines the interface\n",
    "2. **Dependency Injection**: Services are injected through the container\n",
    "3. **Separation of Concerns**: Caching logic is separate from API logic\n",
    "4. **Hierarchical Storage**: Three-tier cache hierarchy (L1, L2, L3)\n",
    "5. **Performance Optimization**: Fast access to frequently used data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Cache Service Components:\n",
      "- MultiLayerCacheService: MultiLayerCacheService\n",
      "- Cache Layers: 3 layers\n",
      "\n",
      "Cache layers:\n",
      "- l1_memory: L1_MEMORY\n",
      "- l2_redis: L2_REDIS\n",
      "- l3_persistent: L3_PERSISTENT\n",
      "\n",
      "Multi-layer cache service methods: ['clear_layer', 'delete', 'get', 'get_stats', 'invalidate_by_prefix', 'set', 'warm_up']\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the multi-layer cache service definition\n",
    "from src.application.services.multi_layer_cache_service import MultiLayerCacheService, CacheLayer\n",
    "\n",
    "print(\"Multi-Layer Cache Service Components:\")\n",
    "print(f\"- MultiLayerCacheService: {MultiLayerCacheService.__name__}\")\n",
    "print(f\"- Cache Layers: {len(list(CacheLayer))} layers\")\n",
    "\n",
    "print(f\"\\nCache layers:\")\n",
    "for layer in CacheLayer:\n",
    "    print(f\"- {layer.value}: {layer.name}\")\n",
    "\n",
    "print(f\"\\nMulti-layer cache service methods: {[method for method in dir(MultiLayerCacheService) if not method.startswith('_') and callable(getattr(MultiLayerCacheService, method, None))]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Multi-Layer Cache Service\n",
    "\n",
    "Let's see how to use the multi-layer cache service to improve performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-layer cache service initialized successfully\n",
      "L1 Memory Cache: Max items = 1000, Max bytes = 10485760\n"
     ]
    }
   ],
   "source": [
    "# Import required classes\n",
    "from src.application.services.multi_layer_cache_service import MultiLayerCacheService, CacheLayer\n",
    "from unittest.mock import AsyncMock\n",
    "\n",
    "# Create a mock Redis client for demonstration\n",
    "mock_redis_client = AsyncMock()\n",
    "mock_redis_client.get = AsyncMock(return_value=None)\n",
    "mock_redis_client.set = AsyncMock(return_value=True)\n",
    "mock_redis_client.delete = AsyncMock(return_value=1)\n",
    "mock_redis_client.keys = AsyncMock(return_value=[])\n",
    "mock_redis_client.flushdb = AsyncMock(return_value=True)\n",
    "mock_redis_client.info = AsyncMock(return_value={\"used_memory\": 1000, \"db0\": {\"keys\": 5}})\n",
    "\n",
    "# Create the multi-layer cache service\n",
    "cache_service = MultiLayerCacheService(redis_client=mock_redis_client)\n",
    "\n",
    "print(\"Multi-layer cache service initialized successfully\")\n",
    "print(f\"L1 Memory Cache: Max items = {cache_service._l1_max_size}, Max bytes = {cache_service._l1_max_bytes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Different Cache Layers\n",
    "\n",
    "Let's interact with different cache layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting values in different cache layers:\n",
      "\n",
      "L1 Only - Set and retrieved: L1 Only Value\n",
      "L2 Only - Set and retrieved: L2 Only Value\n",
      "All Layers - Set and retrieved: All Layers Value\n",
      "With TTL - Set and retrieved: TTL Value\n"
     ]
    }
   ],
   "source": [
    "# Set values in different cache layers\n",
    "print(\"Setting values in different cache layers:\\n\")\n",
    "\n",
    "# Set in L1 only\n",
    "await cache_service.set(\"l1_only_key\", \"L1 Only Value\", target_layers=[CacheLayer.L1_MEMORY])\n",
    "result = await cache_service.get(\"l1_only_key\", layer=CacheLayer.L1_MEMORY)\n",
    "print(f\"L1 Only - Set and retrieved: {result}\")\n",
    "\n",
    "# Set in L2 only (simulated via our mock)\n",
    "await cache_service.set(\"l2_only_key\", \"L2 Only Value\", target_layers=[CacheLayer.L2_REDIS])\n",
    "result = await cache_service.get(\"l2_only_key\", layer=CacheLayer.L2_REDIS)\n",
    "print(f\"L2 Only - Set and retrieved: {result}\")\n",
    "\n",
    "# Set in all layers\n",
    "await cache_service.set(\"all_layers_key\", \"All Layers Value\")\n",
    "result = await cache_service.get(\"all_layers_key\")\n",
    "print(f\"All Layers - Set and retrieved: {result}\")\n",
    "\n",
    "# Set with TTL\n",
    "await cache_service.set(\"ttl_key\", \"TTL Value\", ttl=timedelta(minutes=10))\n",
    "result = await cache_service.get(\"ttl_key\")\n",
    "print(f\"With TTL - Set and retrieved: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Statistics\n",
    "\n",
    "Let's examine cache statistics to understand performance characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache Statistics:\n",
      "\n",
      "L1_MEMORY (L1_MEMORY):\n",
      "  size: 4\n",
      "  memory_usage_bytes: 116\n",
      "  hit_count: 6\n",
      "  miss_count: 1\n",
      "  hit_rate: 0.8571428571428571\n",
      "  max_size_items: 1000\n",
      "  max_size_bytes: 10485760\n",
      "\n",
      "L2_REDIS (L2_REDIS):\n",
      "  size: 5\n",
      "  memory_usage_bytes: 1000\n",
      "  hit_count: 0\n",
      "  miss_count: 0\n",
      "  hit_rate: 0.0\n",
      "  connected: True\n",
      "\n",
      "L3_PERSISTENT (L3_PERSISTENT):\n",
      "  hit_count: 0\n",
      "  miss_count: 0\n",
      "  hit_rate: 0.0\n",
      "  available: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get cache statistics\n",
    "stats = await cache_service.get_stats()\n",
    "\n",
    "print(\"Cache Statistics:\\n\")\n",
    "for layer, layer_stats in stats.items():\n",
    "    print(f\"{layer.value.upper()} ({layer.name}):\")\n",
    "    for stat_name, stat_value in layer_stats.items():\n",
    "        print(f\"  {stat_name}: {stat_value}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Operations\n",
    "\n",
    "Let's perform various cache operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before deletion: To be deleted\n",
      "After deletion: None, Deletion successful: True\n",
      "\n",
      "Before invalidation:\n",
      "user:profile:123 exists: True\n",
      "user:settings:123 exists: True\n",
      "post:content:456 exists: True\n",
      "\n",
      "Invalidated 0 keys with prefix 'user:'\n",
      "\n",
      "After invalidation:\n",
      "user:profile:123 exists: True\n",
      "user:settings:123 exists: True\n",
      "post:content:456 exists: True\n"
     ]
    }
   ],
   "source": [
    "# Delete a key\n",
    "await cache_service.set(\"delete_me\", \"To be deleted\")\n",
    "result = await cache_service.get(\"delete_me\")\n",
    "print(f\"Before deletion: {result}\")\n",
    "\n",
    "deleted = await cache_service.delete(\"delete_me\")\n",
    "result = await cache_service.get(\"delete_me\")\n",
    "print(f\"After deletion: {result}, Deletion successful: {deleted}\")\n",
    "\n",
    "# Invalidate by prefix\n",
    "await cache_service.set(\"user:profile:123\", \"User profile data\")\n",
    "await cache_service.set(\"user:settings:123\", \"User settings data\")\n",
    "await cache_service.set(\"post:content:456\", \"Post content data\")\n",
    "\n",
    "print(f\"\\nBefore invalidation:\")\n",
    "print(f\"user:profile:123 exists: {await cache_service.get('user:profile:123', layer=CacheLayer.L1_MEMORY) is not None}\")\n",
    "print(f\"user:settings:123 exists: {await cache_service.get('user:settings:123', layer=CacheLayer.L1_MEMORY) is not None}\")\n",
    "print(f\"post:content:456 exists: {await cache_service.get('post:content:456', layer=CacheLayer.L1_MEMORY) is not None}\")\n",
    "\n",
    "# Invalidate all user keys\n",
    "invalidated_count = await cache_service.invalidate_by_prefix(\"user:\")\n",
    "print(f\"\\nInvalidated {invalidated_count} keys with prefix 'user:'\")\n",
    "\n",
    "print(f\"\\nAfter invalidation:\")\n",
    "print(f\"user:profile:123 exists: {await cache_service.get('user:profile:123', layer=CacheLayer.L1_MEMORY) is not None}\")\n",
    "print(f\"user:settings:123 exists: {await cache_service.get('user:settings:123', layer=CacheLayer.L1_MEMORY) is not None}\")\n",
    "print(f\"post:content:456 exists: {await cache_service.get('post:content:456', layer=CacheLayer.L1_MEMORY) is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Warming\n",
    "\n",
    "Let's see how to warm up the cache with initial data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache warming successful: True\n",
      "\n",
      "Cached data verification:\n",
      "embedding:gpt4:doc1: ✓\n",
      "embedding:gpt4:doc2: ✓\n",
      "prompt:qa:template: ✓\n",
      "chunk:summary:doc1: ✓\n",
      "chunk:entities:doc1: ✓\n"
     ]
    }
   ],
   "source": [
    "# Warm up the cache with sample data\n",
    "sample_data = {\n",
    "    \"embedding:gpt4:doc1\": [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    \"embedding:gpt4:doc2\": [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "    \"prompt:qa:template\": \"Answer the question based on the context: {context}. Question: {question}\",\n",
    "    \"chunk:summary:doc1\": \"This document discusses RAG architectures\",\n",
    "    \"chunk:entities:doc1\": [\"RAG\", \"Architecture\", \"LLM\"]\n",
    "}\n",
    "\n",
    "warm_success = await cache_service.warm_up(sample_data, ttl=timedelta(hours=1))\n",
    "print(f\"Cache warming successful: {warm_success}\")\n",
    "\n",
    "# Verify the data was cached\n",
    "print(f\"\\nCached data verification:\")\n",
    "for key in sample_data.keys():\n",
    "    cached_val = await cache_service.get(key)\n",
    "    exists = cached_val is not None\n",
    "    print(f\"{key}: {'✓' if exists else '✗'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer-Specific Operations\n",
    "\n",
    "Let's perform operations on specific cache layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 cache size before clearing: 9\n",
      "L1 layer cleared successfully: True\n",
      "L1 cache size after clearing: 0\n"
     ]
    }
   ],
   "source": [
    "# Fill L1 with some data\n",
    "for i in range(5):\n",
    "    await cache_service.set(f\"l1_item_{i}\", f\"Value {i}\", target_layers=[CacheLayer.L1_MEMORY])\n",
    "\n",
    "# Check L1 size before clearing\n",
    "stats_before = await cache_service.get_stats()\n",
    "l1_size_before = stats_before[CacheLayer.L1_MEMORY]['size']\n",
    "print(f\"L1 cache size before clearing: {l1_size_before}\")\n",
    "\n",
    "# Clear L1 layer\n",
    "cleared = await cache_service.clear_layer(CacheLayer.L1_MEMORY)\n",
    "print(f\"L1 layer cleared successfully: {cleared}\")\n",
    "\n",
    "# Check L1 size after clearing\n",
    "stats_after = await cache_service.get_stats()\n",
    "l1_size_after = stats_after[CacheLayer.L1_MEMORY]['size']\n",
    "print(f\"L1 cache size after clearing: {l1_size_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benefits in RAG Systems\n",
    "\n",
    "Multi-layer caching provides significant performance benefits in RAG systems:\n",
    "\n",
    "1. **L1 (Memory)**: Ultra-fast access to frequently requested embeddings and prompts\n",
    "2. **L2 (Redis)**: Shared caching across multiple application instances\n",
    "3. **L3 (Persistent)**: Long-term storage of computed results\n",
    "\n",
    "This hierarchy ensures optimal performance while minimizing redundant computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored the multi-layer caching functionality of the RAG Engine Mini:\n",
    "\n",
    "1. **Architecture**: The multi-layer cache follows the same architectural patterns as the rest of the system\n",
    "2. **Cache Hierarchy**: Three-tier cache (L1, L2, L3) for optimal performance\n",
    "3. **Management Operations**: Comprehensive tools for cache management\n",
    "4. **Statistics**: Detailed metrics for cache performance analysis\n",
    "5. **RAG Applications**: Essential for optimizing RAG system performance\n",
    "\n",
    "Multi-layer caching is critical for RAG systems, dramatically reducing latency and computational overhead by storing frequently accessed data at progressively faster storage tiers. The RAG Engine's implementation provides comprehensive tools for managing cache hierarchies in production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}