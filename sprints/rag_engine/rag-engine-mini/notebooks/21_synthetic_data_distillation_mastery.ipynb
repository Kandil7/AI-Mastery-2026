{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§ª Level 19: Synthetic Data & Distillation Mastery\n",
                "### The Data Flywheel: Generating Your Own Training Set\n",
                "\n",
                "In this notebook, we build a **Synthetic Data Generator**. We will take a raw text chunk and use a strong LLM to \"hallucinate\" high-quality Q&A pairs for training or evaluation.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Generator Agent\n",
                "\n",
                "We define a prompt that encourages the model to be a \"Teacher\" creating an exam."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_synthetic_qa(context_chunk: str, difficulty: str = \"hard\"):\n",
                "    print(f\"[Generator] Creating {difficulty} questions for chunk...\")\n",
                "    # Simulation of LLM call\n",
                "    prompt = f\"Given this text: '{context_chunk[:50]}...', generate a complex {difficulty} question and answer.\"\n",
                "    \n",
                "    if difficulty == \"hard\":\n",
                "        return {\n",
                "            \"question\": \"How does RAG compare to Fine-Tuning when handling dynamic knowledge?\",\n",
                "            \"answer\": \"RAG excels at dynamic knowledge because it retrieves real-time data, whereas Fine-Tuning locks knowledge at training time.\",\n",
                "            \"reasoning\": \"The key distinction is the 'knowledge cutoff'. RAG bypasses this.\"\n",
                "        }\n",
                "    return {\"question\": \"What is RAG?\", \"answer\": \"Retrieval Augmented Generation.\"}\n",
                "\n",
                "sample_text = \"RAG is a technique that connects LLMs to external data...\"\n",
                "dataset_row = generate_synthetic_qa(sample_text, \"hard\")\n",
                "print(\"Generated Row:\", dataset_row)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Formatting for Distillation (JSONL)\n",
                "\n",
                "To train a small model (e.g., Phi-3), we need this specific format."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "\n",
                "def format_for_finetuning(row):\n",
                "    # ChatML format is common for modern fine-tuning\n",
                "    chat_entry = {\n",
                "        \"messages\": [\n",
                "            {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
                "            {\"role\": \"user\", \"content\": row['question']},\n",
                "            {\"role\": \"assistant\", \"content\": row['answer']}\n",
                "        ]\n",
                "    }\n",
                "    return json.dumps(chat_entry)\n",
                "\n",
                "print(format_for_finetuning(dataset_row))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Alchemist's Certificate\n",
                "\n",
                "You have unlocked the ability to **Scale Intelligence**.\n",
                "\n",
                "### **This is Level 19.**\n",
                "We are one step away from the absolute theoretical limit of this course.\n",
                "\n",
                "**- Antigravity**"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}