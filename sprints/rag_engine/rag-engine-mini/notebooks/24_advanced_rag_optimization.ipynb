{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ Advanced RAG Techniques & Optimization\n",
    "\n",
    "> **Educational Notebook 24**: Deep dive into advanced RAG techniques, performance optimization, and system tuning\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand advanced RAG techniques like Self-RAG, CRAG, and ReAct\n",
    "2. Learn optimization strategies for performance and cost\n",
    "3. Explore query routing and adaptive retrieval\n",
    "4. Implement advanced evaluation techniques\n",
    "5. Master system tuning for different use cases\n",
    "\n",
    "## ðŸŽ¯ Advanced RAG Paradigms\n",
    "\n",
    "Beyond basic RAG, there are several advanced paradigms that can significantly improve performance:\n",
    "\n",
    "- **Self-RAG**: Includes reasoning about retrieved content\n",
    "- **CRAG**: Corrective RAG that validates and corrects retrieved information\n",
    "- **ReAct**: Reasoning and Acting with thought chains\n",
    "- **Adaptive RAG**: Dynamically adjusts retrieval strategy based on query complexity\n",
    "\n",
    "Let's explore these concepts in detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setting Up Our Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import time\n",
    "import random\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from functools import wraps\n",
    "\n",
    "# Add project root to path\n",
    "current = Path().resolve()\n",
    "repo_root = None\n",
    "for parent in [current, *current.parents]:\n",
    "    if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Could not locate rag-engine-mini root for imports\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Advanced RAG Techniques\n",
    "\n",
    "Let's explore the different advanced RAG techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RAG techniques\n",
    "rag_techniques = {\n",
    "    \"Basic RAG\": {\n",
    "        \"description\": \"Standard RAG pipeline: retrieve â†’ augment â†’ generate\",\n",
    "        \"advantages\": [\"Simple to implement\", \"Good baseline performance\"],\n",
    "        \"limitations\": [\"No validation of retrieved content\", \"Static retrieval strategy\"],\n",
    "        \"use_case\": \"General purpose QA with well-structured documents\"\n",
    "    },\n",
    "    \"Self-RAG\": {\n",
    "        \"description\": \"Includes reasoning about retrieved content with special tokens\",\n",
    "        \"advantages\": [\"Validates retrieved information\", \"Can decide to not retrieve\", \"Self-reflection\"],\n",
    "        \"limitations\": [\"Higher computational cost\", \"Requires special training\"],\n",
    "        \"use_case\": \"Fact-checking applications, critical domains\"\n",
    "    },\n",
    "    \"CRAG (Corrective RAG)\": {\n",
    "        \"description\": \"Uses knowledge graphs to validate and correct retrieved content\",\n",
    "        \"advantages\": [\"Corrects factual errors\", \"Improves reliability\"],\n",
    "        \"limitations\": [\"Requires knowledge graphs\", \"Complex implementation\"],\n",
    "        \"use_case\": \"Medical, legal, scientific domains\"\n",
    "    },\n",
    "    \"ReAct (Reasoning + Acting)\": {\n",
    "        \"description\": \"Combines reasoning steps with actions (like retrieval)\",\n",
    "        \"advantages\": [\"Transparent reasoning\", \"Modular approach\"],\n",
    "        \"limitations\": [\"More complex prompting\", \"Longer response times\"],\n",
    "        \"use_case\": \"Multi-step reasoning tasks\"\n",
    "    },\n",
    "    \"Adaptive RAG\": {\n",
    "        \"description\": \"Dynamically selects retrieval strategy based on query complexity\",\n",
    "        \"advantages\": [\"Optimizes performance per query\", \"Balances cost and quality\"],\n",
    "        \"limitations\": [\"Requires routing classifier\", \"More complex architecture\"],\n",
    "        \"use_case\": \"Mixed query workloads\"\n",
    "    },\n",
    "    \"Active RAG\": {\n",
    "        \"description\": \"Interactively refines retrieval based on intermediate results\",\n",
    "        \"advantages\": [\"Iteratively improves results\", \"Handles ambiguous queries\"],\n",
    "        \"limitations\": [\"Higher latency\", \"Risk of infinite loops\"],\n",
    "        \"use_case\": \"Research assistance, complex queries\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the techniques\n",
    "print(\"Advanced RAG Techniques:\")\n",
    "print(\"=\" * 70)\n",
    "for name, details in rag_techniques.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Description: {details['description']}\")\n",
    "    print(f\"  Advantages: {', '.join(details['advantages'])}\")\n",
    "    print(f\"  Limitations: {', '.join(details['limitations'])}\")\n",
    "    print(f\"  Use Case: {details['use_case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Implementing Adaptive RAG\n",
    "\n",
    "Let's implement a simplified version of Adaptive RAG that dynamically chooses retrieval strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query complexity classifier\n",
    "class QueryComplexityClassifier:\n",
    "    \"\"\"\n",
    "    Classifies query complexity to determine the appropriate retrieval strategy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.simple_keywords = [\n",
    "            \"what is\", \"define\", \"who is\", \"when\", \"where\", \"list\", \"name\"\n",
    "        ]\n",
    "        \n",
    "        self.complex_keywords = [\n",
    "            \"compare\", \"contrast\", \"analyze\", \"evaluate\", \"how does\", \"why\", \n",
    "            \"relationship\", \"impact\", \"effect\", \"should\", \"could\", \"would\"\n",
    "        ]\n",
    "    \n",
    "    def classify_complexity(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Classify query complexity as 'simple', 'medium', or 'complex'\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Count complex vs simple indicators\n",
    "        simple_count = sum(1 for kw in self.simple_keywords if kw in query_lower)\n",
    "        complex_count = sum(1 for kw in self.complex_keywords if kw in query_lower)\n",
    "        \n",
    "        # Consider query length as well\n",
    "        length_score = min(len(query.split()), 20) / 20  # Normalize to 0-1\n",
    "        \n",
    "        if complex_count > simple_count:\n",
    "            if length_score > 0.5:\n",
    "                return \"complex\"\n",
    "            else:\n",
    "                return \"medium\"\n",
    "        elif simple_count > 0:\n",
    "            return \"simple\"\n",
    "        else:\n",
    "            # If no clear indicators, use length as heuristic\n",
    "            if length_score < 0.3:\n",
    "                return \"simple\"\n",
    "            elif length_score < 0.6:\n",
    "                return \"medium\"\n",
    "            else:\n",
    "                return \"complex\"\n",
    "\n",
    "# Define retrieval strategies\n",
    "class RetrievalStrategy(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    HYBRID = \"hybrid\"\n",
    "    RERANK = \"rerank\"\n",
    "    KNOWLEDGE_GRAPH = \"knowledge_graph\"\n",
    "\n",
    "# Adaptive RAG implementation\n",
    "class AdaptiveRAG:\n",
    "    \"\"\"\n",
    "    Adaptive RAG system that selects retrieval strategy based on query complexity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classifier = QueryComplexityClassifier()\n",
    "        \n",
    "        # Map complexity to strategy\n",
    "        self.complexity_to_strategy = {\n",
    "            \"simple\": RetrievalStrategy.SIMPLE,\n",
    "            \"medium\": RetrievalStrategy.HYBRID,\n",
    "            \"complex\": RetrievalStrategy.RERANK\n",
    "        }\n",
    "    \n",
    "    def select_strategy(self, query: str) -> RetrievalStrategy:\n",
    "        \"\"\"\n",
    "        Select retrieval strategy based on query complexity\n",
    "        \"\"\"\n",
    "        complexity = self.classifier.classify_complexity(query)\n",
    "        strategy = self.complexity_to_strategy.get(complexity, RetrievalStrategy.HYBRID)\n",
    "        \n",
    "        print(f\"Query: '{query[:50]}...' | Complexity: {complexity} | Strategy: {strategy.value}\")\n",
    "        return strategy\n",
    "    \n",
    "    def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve documents using the selected strategy\n",
    "        \"\"\"\n",
    "        strategy = self.select_strategy(query)\n",
    "        \n",
    "        # Simulate retrieval with different strategies\n",
    "        if strategy == RetrievalStrategy.SIMPLE:\n",
    "            # Simple vector search - fastest\n",
    "            time.sleep(0.1)  # Simulate API call\n",
    "            retrieval_time = 0.1\n",
    "        elif strategy == RetrievalStrategy.HYBRID:\n",
    "            # Hybrid search - moderate speed\n",
    "            time.sleep(0.3)  # Simulate API calls\n",
    "            retrieval_time = 0.3\n",
    "        elif strategy == RetrievalStrategy.RERANK:\n",
    "            # Hybrid + rerank - slower but more accurate\n",
    "            time.sleep(0.6)  # Simulate API calls + rerank\n",
    "            retrieval_time = 0.6\n",
    "        else:\n",
    "            # Default to hybrid\n",
    "            time.sleep(0.3)\n",
    "            retrieval_time = 0.3\n",
    "        \n",
    "        # Generate mock results\n",
    "        results = []\n",
    "        for i in range(k):\n",
    "            results.append({\n",
    "                \"id\": f\"doc_{random.randint(1000, 9999)}\",\n",
    "                \"text\": f\"Relevant content for query '{query[:20]}' - chunk {i+1}\",\n",
    "                \"score\": round(random.uniform(0.5, 0.95), 3),\n",
    "                \"retrieval_time\": retrieval_time\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Test the adaptive RAG system\n",
    "adaptive_rag = AdaptiveRAG()\n",
    "\n",
    "test_queries = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How does neural networks work?\",\n",
    "    \"Compare the performance of different neural network architectures for NLP tasks\",\n",
    "    \"What are the ethical implications of AI on employment?\",\n",
    "    \"List the types of algorithms.\",\n",
    "    \"Analyze the impact of transformer models on natural language understanding benchmarks\"\n",
    "]\n",
    "\n",
    "print(\"Adaptive RAG Strategy Selection:\")\n",
    "print(\"=\" * 60)\n",
    "all_results = []\n",
    "for query in test_queries:\n",
    "    results = adaptive_rag.retrieve(query, k=3)\n",
    "    all_results.append((query, results))\n",
    "    \n",
    "    # Show first result for each query\n",
    "    print(f\"  Top result score: {results[0]['score']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Performance Optimization Strategies\n",
    "\n",
    "Let's explore various optimization strategies for RAG systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization strategies\n",
    "optimization_strategies = {\n",
    "    \"Caching\": {\n",
    "        \"description\": \"Cache embeddings, retrieved chunks, and LLM responses\",\n",
    "        \"implementation\": [\"Redis for embeddings\", \"In-memory for recent queries\", \"Persistent for common queries\"],\n",
    "        \"impact\": \"Significant reduction in latency and cost\"\n",
    "    },\n",
    "    \"Compression\": {\n",
    "        \"description\": \"Reduce embedding dimensionality to improve speed\",\n",
    "        \"implementation\": [\"PCA\", \"Autoencoders\", \"Quantization\"],\n",
    "        \"impact\": \"Faster retrieval at slight accuracy cost\"\n",
    "    },\n",
    "    \"Early Exit\": {\n",
    "        \"description\": \"Stop retrieval when confidence threshold is met\",\n",
    "        \"implementation\": [\"Confidence scoring\", \"Threshold setting\"],\n",
    "        \"impact\": \"Reduced latency for confident answers\"\n",
    "    },\n",
    "    \"Query Rewriting\": {\n",
    "        \"description\": \"Improve query formulation before retrieval\",\n",
    "        \"implementation\": [\"Query expansion\", \"Query reformulation\", \"Query decomposition\"],\n",
    "        \"impact\": \"Better retrieval accuracy\"\n",
    "    },\n",
    "    \"Hybrid Search\": {\n",
    "        \"description\": \"Combine semantic and keyword search for better results\",\n",
    "        \"implementation\": [\"RRF fusion\", \"Weighted combination\", \"Cross-encoder reranking\"],\n",
    "        \"impact\": \"Improved recall and precision\"\n",
    "    },\n",
    "    \"Multi-Tier Retrieval\": {\n",
    "        \"description\": \"Use different retrieval methods at different stages\",\n",
    "        \"implementation\": [\"Coarse-to-fine\", \"Filtering approaches\"],\n",
    "        \"impact\": \"Better performance for large collections\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the strategies\n",
    "print(\"RAG Performance Optimization Strategies:\")\n",
    "print(\"=\" * 80)\n",
    "for name, details in optimization_strategies.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Description: {details['description']}\")\n",
    "    print(f\"  Implementation: {', '.join(details['implementation'])}\")\n",
    "    print(f\"  Impact: {details['impact']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Implementing a Caching Layer\n",
    "\n",
    "Let's implement a multi-level caching system for RAG optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a multi-level cache\n",
    "class MultiLevelCache:\n",
    "    \"\"\"\n",
    "    Multi-level cache for RAG optimization\n",
    "    Level 1: In-memory cache (fastest, smallest)\n",
    "    Level 2: Redis-like cache (medium speed, larger)\n",
    "    Level 3: Persistent cache (slowest, largest)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, l1_size: int = 100, l2_size: int = 1000, l3_size: int = 10000):\n",
    "        self.l1_cache = {}  # In-memory cache\n",
    "        self.l1_access_times = {}  # Track access times\n",
    "        self.l1_size = l1_size\n",
    "        \n",
    "        self.l2_cache = {}  # Simulated Redis cache\n",
    "        self.l2_access_times = {}\n",
    "        self.l2_size = l2_size\n",
    "        \n",
    "        self.l3_cache = {}  # Simulated persistent cache\n",
    "        self.l3_access_times = {}\n",
    "        self.l3_size = l3_size\n",
    "        \n",
    "        # Stats\n",
    "        self.hits = {\"l1\": 0, \"l2\": 0, \"l3\": 0}\n",
    "        self.misses = 0\n",
    "        \n",
    "    def _evict_lru(self, cache_dict, access_times, max_size):\n",
    "        \"\"\"Evict least recently used items if cache is full\"\"\"\n",
    "        if len(cache_dict) >= max_size:\n",
    "            # Find oldest access time\n",
    "            oldest_key = min(access_times.keys(), key=lambda k: access_times[k])\n",
    "            del cache_dict[oldest_key]\n",
    "            del access_times[oldest_key]\n",
    "    \n",
    "    def get(self, key: str):\n",
    "        \"\"\"Get value from cache hierarchy\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Check L1 first\n",
    "        if key in self.l1_cache:\n",
    "            self.l1_access_times[key] = current_time\n",
    "            self.hits[\"l1\"] += 1\n",
    "            return self.l1_cache[key], \"l1\"\n",
    "        \n",
    "        # Check L2\n",
    "        if key in self.l2_cache:\n",
    "            # Move to L1 (promote)\n",
    "            value = self.l2_cache[key]\n",
    "            self._evict_lru(self.l1_cache, self.l1_access_times, self.l1_size)\n",
    "            self.l1_cache[key] = value\n",
    "            self.l1_access_times[key] = current_time\n",
    "            \n",
    "            # Remove from L2\n",
    "            del self.l2_cache[key]\n",
    "            del self.l2_access_times[key]\n",
    "            \n",
    "            self.hits[\"l2\"] += 1\n",
    "            return value, \"l2\"\n",
    "        \n",
    "        # Check L3\n",
    "        if key in self.l3_cache:\n",
    "            # Move to L1 (promote)\n",
    "            value = self.l3_cache[key]\n",
    "            self._evict_lru(self.l1_cache, self.l1_access_times, self.l1_size)\n",
    "            self.l1_cache[key] = value\n",
    "            self.l1_access_times[key] = current_time\n",
    "            \n",
    "            # Remove from L3\n",
    "            del self.l3_cache[key]\n",
    "            del self.l3_access_times[key]\n",
    "            \n",
    "            self.hits[\"l3\"] += 1\n",
    "            return value, \"l3\"\n",
    "        \n",
    "        # Miss\n",
    "        self.misses += 1\n",
    "        return None, \"miss\"\n",
    "    \n",
    "    def set(self, key: str, value: Any, level: str = \"auto\"):\n",
    "        \"\"\"Set value in cache hierarchy\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if level == \"auto\":\n",
    "            # Default to L1 for new items\n",
    "            level = \"l1\"\n",
    "        \n",
    "        if level == \"l1\":\n",
    "            self._evict_lru(self.l1_cache, self.l1_access_times, self.l1_size)\n",
    "            self.l1_cache[key] = value\n",
    "            self.l1_access_times[key] = current_time\n",
    "        elif level == \"l2\":\n",
    "            self._evict_lru(self.l2_cache, self.l2_access_times, self.l2_size)\n",
    "            self.l2_cache[key] = value\n",
    "            self.l2_access_times[key] = current_time\n",
    "        elif level == \"l3\":\n",
    "            self._evict_lru(self.l3_cache, self.l3_access_times, self.l3_size)\n",
    "            self.l3_cache[key] = value\n",
    "            self.l3_access_times[key] = current_time\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        total_requests = sum(self.hits.values()) + self.misses\n",
    "        hit_rate = sum(self.hits.values()) / total_requests if total_requests > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"total_requests\": total_requests,\n",
    "            \"l1_size\": len(self.l1_cache),\n",
    "            \"l2_size\": len(self.l2_cache),\n",
    "            \"l3_size\": len(self.l3_cache)\n",
    "        }\n",
    "\n",
    "# Test the multi-level cache\n",
    "cache = MultiLevelCache(l1_size=5, l2_size=10, l3_size=20)\n",
    "\n",
    "# Simulate some cache operations\n",
    "test_keys = [f\"query_{i}\" for i in range(15)]\n",
    "\n",
    "print(\"Testing Multi-Level Cache:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for i, key in enumerate(test_keys):\n",
    "    # Every other key is a repeat (should hit)\n",
    "    if i > 0 and random.random() < 0.3:  # 30% chance of repeat\n",
    "        key = random.choice(test_keys[:i])\n",
    "    \n",
    "    value, level = cache.get(key)\n",
    "    \n",
    "    if level == \"miss\":\n",
    "        # Simulate expensive computation\n",
    "        value = f\"Computed result for {key} at {time.time()}\"\n",
    "        cache.set(key, value)\n",
    "        print(f\"Miss: {key} - computed and stored\")\n",
    "    else:\n",
    "        print(f\"Hit: {key} - retrieved from {level}\")\n",
    "\n",
    "# Show stats\n",
    "stats = cache.get_stats()\n",
    "print(f\"\\nCache Statistics:\")\n",
    "print(f\"  Total requests: {stats['total_requests']}\")\n",
    "print(f\"  Hit rate: {stats['hit_rate']:.2%}\")\n",
    "print(f\"  Hits breakdown: {stats['hits']}\")\n",
    "print(f\"  Misses: {stats['misses']}\")\n",
    "print(f\"  Cache sizes: L1={stats['l1_size']}, L2={stats['l2_size']}, L3={stats['l3_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Query Routing and Dynamic Configuration\n",
    "\n",
    "Let's implement a system for routing queries to different retrieval configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different retrieval configurations\n",
    "@dataclass\n",
    "class RetrievalConfig:\n",
    "    name: str\n",
    "    strategy: str  # \"semantic\", \"keyword\", \"hybrid\", \"graph\"\n",
    "    k: int  # number of documents to retrieve\n",
    "    use_rerank: bool\n",
    "    use_query_expansion: bool\n",
    "    chunk_size: int  # in tokens\n",
    "    performance_cost: float  # arbitrary cost metric\n",
    "\n",
    "# Define query router\n",
    "class QueryRouter:\n",
    "    \"\"\"\n",
    "    Routes queries to optimal retrieval configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define available configurations\n",
    "        self.configs = [\n",
    "            RetrievalConfig(\n",
    "                name=\"fast_basic\",\n",
    "                strategy=\"semantic\",\n",
    "                k=3,\n",
    "                use_rerank=False,\n",
    "                use_query_expansion=False,\n",
    "                chunk_size=256,\n",
    "                performance_cost=0.3\n",
    "            ),\n",
    "            RetrievalConfig(\n",
    "                name=\"balanced_hybrid\",\n",
    "                strategy=\"hybrid\",\n",
    "                k=5,\n",
    "                use_rerank=False,\n",
    "                use_query_expansion=True,\n",
    "                chunk_size=512,\n",
    "                performance_cost=0.6\n",
    "            ),\n",
    "            RetrievalConfig(\n",
    "                name=\"accurate_rerank\",\n",
    "                strategy=\"hybrid\",\n",
    "                k=10,\n",
    "                use_rerank=True,\n",
    "                use_query_expansion=True,\n",
    "                chunk_size=512,\n",
    "                performance_cost=0.9\n",
    "            ),\n",
    "            RetrievalConfig(\n",
    "                name=\"graph_enhanced\",\n",
    "                strategy=\"graph\",\n",
    "                k=7,\n",
    "                use_rerank=False,\n",
    "                use_query_expansion=True,\n",
    "                chunk_size=512,\n",
    "                performance_cost=0.8\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Define routing rules\n",
    "        self.routing_rules = [\n",
    "            {\"condition\": lambda q: len(q.split()) <= 3, \"config\": \"fast_basic\", \"priority\": 1},\n",
    "            {\"condition\": lambda q: any(word in q.lower() for word in [\"compare\", \"analyze\", \"evaluate\"]), \n",
    "             \"config\": \"accurate_rerank\", \"priority\": 1},\n",
    "            {\"condition\": lambda q: any(word in q.lower() for word in [\"relationship\", \"connection\", \"linked to\"]), \n",
    "             \"config\": \"graph_enhanced\", \"priority\": 1},\n",
    "            {\"condition\": lambda q: True, \"config\": \"balanced_hybrid\", \"priority\": 0}  # default\n",
    "        ]\n",
    "    \n",
    "    def route_query(self, query: str) -> RetrievalConfig:\n",
    "        \"\"\"\n",
    "        Route query to optimal configuration\n",
    "        \"\"\"\n",
    "        for rule in sorted(self.routing_rules, key=lambda r: r[\"priority\"], reverse=True):\n",
    "            if rule[\"condition\"](query):\n",
    "                config_name = rule[\"config\"]\n",
    "                config = next((c for c in self.configs if c.name == config_name), None)\n",
    "                return config\n",
    "        \n",
    "        # Fallback to default\n",
    "        return self.configs[1]  # balanced_hybrid\n",
    "    \n",
    "    def get_all_configs(self) -> List[RetrievalConfig]:\n",
    "        \"\"\"\n",
    "        Get all available configurations\n",
    "        \"\"\"\n",
    "        return self.configs\n",
    "\n",
    "# Test the query router\n",
    "router = QueryRouter()\n",
    "\n",
    "test_queries = [\n",
    "    \"AI\",  # Very short\n",
    "    \"What is RAG?\",  # Short\n",
    "    \"Compare transformer and RNN architectures\",  # Complex analytical\n",
    "    \"How are neural networks connected to deep learning?\",  # Relationship query\n",
    "    \"Explain machine learning basics\",  # General\n",
    "    \"What are the ethical implications of AI?\"  # Complex\n",
    "]\n",
    "\n",
    "print(\"Query Routing System:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for query in test_queries:\n",
    "    config = router.route_query(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  â†’ Config: {config.name}\")\n",
    "    print(f\"  â†’ Strategy: {config.strategy}, k={config.k}\")\n",
    "    print(f\"  â†’ Rerank: {config.use_rerank}, Expansion: {config.use_query_expansion}\")\n",
    "    print(f\"  â†’ Cost: {config.performance_cost}\\n\")\n",
    "\n",
    "# Show all configurations\n",
    "print(\"Available Retrieval Configurations:\")\n",
    "print(\"=\" * 50)\n",
    "for config in router.get_all_configs():\n",
    "    print(f\"{config.name}: {config.strategy}, k={config.k}, rerank={config.use_rerank}, cost={config.performance_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Advanced Evaluation Techniques\n",
    "\n",
    "Let's implement advanced evaluation techniques for RAG systems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement advanced evaluation metrics\n",
    "class AdvancedRAGEvaluator:\n",
    "    \"\"\"\n",
    "    Advanced evaluation techniques for RAG systems\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def evaluate_faithfulness(self, answer: str, context: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how faithful the answer is to the provided context\n",
    "        \"\"\"\n",
    "        # Simplified implementation - in practice, use LLM-based evaluation\n",
    "        answer_words = set(answer.lower().split())\n",
    "        context_words = set(context.lower().split())\n",
    "        \n",
    "        if not answer_words:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = answer_words.intersection(context_words)\n",
    "        faithfulness = len(overlap) / len(answer_words)\n",
    "        \n",
    "        # Cap at 1.0\n",
    "        return min(1.0, faithfulness)\n",
    "    \n",
    "    def evaluate_answer_relevance(self, question: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how relevant the answer is to the question\n",
    "        \"\"\"\n",
    "        question_words = set(question.lower().split())\n",
    "        answer_words = set(answer.lower().split())\n",
    "        \n",
    "        if not question_words:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = question_words.intersection(answer_words)\n",
    "        relevance = len(overlap) / len(question_words)\n",
    "        \n",
    "        return min(1.0, relevance)\n",
    "    \n",
    "    def evaluate_context_recall(self, context: str, answer: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how much of the answer content is supported by the context\n",
    "        \"\"\"\n",
    "        answer_words = set(answer.lower().split())\n",
    "        context_words = set(context.lower().split())\n",
    "        \n",
    "        if not answer_words:\n",
    "            return 0.0\n",
    "            \n",
    "        overlap = answer_words.intersection(context_words)\n",
    "        recall = len(overlap) / len(answer_words)\n",
    "        \n",
    "        return min(1.0, recall)\n",
    "    \n",
    "    def evaluate_compression_ratio(self, original_context: str, retrieved_context: str) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how well the retrieval system compresses relevant information\n",
    "        \"\"\"\n",
    "        orig_len = len(original_context)\n",
    "        ret_len = len(retrieved_context)\n",
    "        \n",
    "        if orig_len == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Compression ratio - lower is better (more compression)\n",
    "        # But we want to balance compression with information retention\n",
    "        compression = ret_len / orig_len\n",
    "        \n",
    "        # Invert so higher is better\n",
    "        return 1 - min(compression, 1.0)\n",
    "    \n",
    "    def evaluate_latency_cost_tradeoff(self, latency: float, cost: float, quality: float) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate the tradeoff between latency, cost, and quality\n",
    "        \"\"\"\n",
    "        # Normalize values (assuming reasonable ranges)\n",
    "        norm_latency = min(latency / 5.0, 1.0)  # Assume 5s is very slow\n",
    "        norm_cost = min(cost / 0.1, 1.0)  # Assume $0.1 is very expensive\n",
    "        \n",
    "        # Calculate a combined score\n",
    "        # Penalize high latency and cost, reward high quality\n",
    "        score = quality - (norm_latency * 0.2) - (norm_cost * 0.2)\n",
    "        \n",
    "        return max(0.0, score)\n",
    "\n",
    "# Test the evaluator\n",
    "evaluator = AdvancedRAGEvaluator()\n",
    "\n",
    "# Sample data for evaluation\n",
    "samples = [\n",
    "    {\n",
    "        \"question\": \"What is machine learning?\",\n",
    "        \"context\": \"Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to learn from data without being explicitly programmed. It includes supervised, unsupervised, and reinforcement learning approaches.\",\n",
    "        \"answer\": \"Machine learning is a field of artificial intelligence that uses statistical techniques to give computer systems the ability to learn from data without being explicitly programmed.\",\n",
    "        \"latency\": 1.2,\n",
    "        \"cost\": 0.02\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does neural networks work?\",\n",
    "        \"context\": \"Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains. They consist of interconnected nodes called neurons that form layers. Each connection has a weight that adjusts during training.\",\n",
    "        \"answer\": \"Neural networks consist of interconnected nodes called neurons that form layers. Each connection has a weight that adjusts during training.\",\n",
    "        \"latency\": 2.5,\n",
    "        \"cost\": 0.05\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Advanced RAG Evaluation:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Question: {sample['question'][:50]}...\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    faithfulness = evaluator.evaluate_faithfulness(sample['answer'], sample['context'])\n",
    "    relevance = evaluator.evaluate_answer_relevance(sample['question'], sample['answer'])\n",
    "    recall = evaluator.evaluate_context_recall(sample['context'], sample['answer'])\n",
    "    compression = evaluator.evaluate_compression_ratio(sample['context'], sample['context'][:200])\n",
    "    tradeoff_score = evaluator.evaluate_latency_cost_tradeoff(\n",
    "        sample['latency'], sample['cost'], (faithfulness + relevance + recall) / 3\n",
    "    )\n",
    "    \n",
    "    print(f\"  Faithfulness: {faithfulness:.3f}\")\n",
    "    print(f\"  Answer Relevance: {relevance:.3f}\")\n",
    "    print(f\"  Context Recall: {recall:.3f}\")\n",
    "    print(f\"  Compression: {compression:.3f}\")\n",
    "    print(f\"  Trade-off Score: {tradeoff_score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Performance Benchmarking\n",
    "\n",
    "Let's create a benchmarking system to compare different RAG configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different configurations\n",
    "class RAGBenchmark:\n",
    "    \"\"\"\n",
    "    Benchmark different RAG configurations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluator = AdvancedRAGEvaluator()\n",
    "        \n",
    "    def benchmark_config(self, config: RetrievalConfig, test_samples: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Benchmark a specific configuration on test samples\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for sample in test_samples:\n",
    "            # Simulate retrieval and generation based on config\n",
    "            # This is a simplified simulation\n",
    "            \n",
    "            # Calculate latency based on config properties\n",
    "            base_latency = 0.5\n",
    "            strategy_factor = {\"semantic\": 1.0, \"keyword\": 1.2, \"hybrid\": 1.5, \"graph\": 1.8}[config.strategy]\n",
    "            rerank_factor = 1.5 if config.use_rerank else 1.0\n",
    "            expansion_factor = 1.1 if config.use_query_expansion else 1.0\n",
    "            \n",
    "            simulated_latency = base_latency * strategy_factor * rerank_factor * expansion_factor\n",
    "            \n",
    "            # Simulate quality based on config properties\n",
    "            quality = 0.5  # base quality\n",
    "            quality += 0.2 if config.use_rerank else 0.0\n",
    "            quality += 0.1 if config.use_query_expansion else 0.0\n",
    "            quality += 0.1 if config.k >= 5 else 0.0\n",
    "            quality = min(quality, 1.0)\n",
    "            \n",
    "            # Calculate cost based on operations performed\n",
    "            cost = 0.01 * config.k  # base cost per retrieved doc\n",
    "            if config.use_rerank:\n",
    "                cost += 0.005 * config.k\n",
    "            if config.use_query_expansion:\n",
    "                cost += 0.002\n",
    "            \n",
    "            # Run evaluation metrics\n",
    "            faithfulness = self.evaluator.evaluate_faithfulness(sample['answer'], sample['context'])\n",
    "            relevance = self.evaluator.evaluate_answer_relevance(sample['question'], sample['answer'])\n",
    "            \n",
    "            result = {\n",
    "                \"latency\": simulated_latency,\n",
    "                \"quality\": quality,\n",
    "                \"cost\": cost,\n",
    "                \"faithfulness\": faithfulness,\n",
    "                \"relevance\": relevance,\n",
    "                \"overall_score\": self.evaluator.evaluate_latency_cost_tradeoff(simulated_latency, cost, quality)\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "        \n",
    "        # Aggregate results\n",
    "        agg_results = {\n",
    "            \"config_name\": config.name,\n",
    "            \"avg_latency\": np.mean([r[\"latency\"] for r in results]),\n",
    "            \"avg_quality\": np.mean([r[\"quality\"] for r in results]),\n",
    "            \"avg_cost\": np.mean([r[\"cost\"] for r in results]),\n",
    "            \"avg_faithfulness\": np.mean([r[\"faithfulness\"] for r in results]),\n",
    "            \"avg_relevance\": np.mean([r[\"relevance\"] for r in results]),\n",
    "            \"avg_overall_score\": np.mean([r[\"overall_score\"] for r in results]),\n",
    "            \"throughput\": len(results) / sum(r[\"latency\"] for r in results),  # queries per second\n",
    "            \"cost_per_query\": np.mean([r[\"cost\"] for r in results])\n",
    "        }\n",
    "        \n",
    "        return agg_results\n",
    "    \n",
    "    def run_comparison(self, configs: List[RetrievalConfig], test_samples: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare multiple configurations\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for config in configs:\n",
    "            print(f\"Benchmarking {config.name}...\")\n",
    "            result = self.benchmark_config(config, test_samples)\n",
    "            results.append(result)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark = RAGBenchmark()\n",
    "configs = router.get_all_configs()\n",
    "\n",
    "# Extend our samples for benchmarking\n",
    "extended_samples = samples * 5  # Repeat samples for better statistics\n",
    "\n",
    "print(\"Running RAG Configuration Benchmarks:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "benchmark_results = benchmark.run_comparison(configs, extended_samples)\n",
    "print(benchmark_results.round(3))\n",
    "\n",
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Latency\n",
    "axes[0, 0].bar(benchmark_results['config_name'], benchmark_results['avg_latency'])\n",
    "axes[0, 0].set_title('Avg. Latency by Config')\n",
    "axes[0, 0].set_ylabel('Latency (s)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Quality\n",
    "axes[0, 1].bar(benchmark_results['config_name'], benchmark_results['avg_quality'])\n",
    "axes[0, 1].set_title('Avg. Quality by Config')\n",
    "axes[0, 1].set_ylabel('Quality Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Cost\n",
    "axes[0, 2].bar(benchmark_results['config_name'], benchmark_results['avg_cost'])\n",
    "axes[0, 2].set_title('Avg. Cost by Config')\n",
    "axes[0, 2].set_ylabel('Cost ($ per query)')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Faithfulness\n",
    "axes[1, 0].bar(benchmark_results['config_name'], benchmark_results['avg_faithfulness'])\n",
    "axes[1, 0].set_title('Avg. Faithfulness by Config')\n",
    "axes[1, 0].set_ylabel('Faithfulness Score')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Throughput\n",
    "axes[1, 1].bar(benchmark_results['config_name'], benchmark_results['throughput'])\n",
    "axes[1, 1].set_title('Throughput by Config')\n",
    "axes[1, 1].set_ylabel('Queries/sec')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Overall Score\n",
    "axes[1, 2].bar(benchmark_results['config_name'], benchmark_results['avg_overall_score'])\n",
    "axes[1, 2].set_title('Overall Score by Config')\n",
    "axes[1, 2].set_ylabel('Score')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ›ï¸ System Tuning Recommendations\n",
    "\n",
    "Based on our analysis, let's provide recommendations for tuning the RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
    "metadata": {},
   "source": [
    "# Generate system tuning recommendations\n",
    "def generate_tuning_recommendations(benchmark_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate tuning recommendations based on benchmark results\n",
    "    \"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    # Find best config for different scenarios\n",
    "    best_latency_idx = benchmark_df['avg_latency'].idxmin()\n",
    "    best_quality_idx = benchmark_df['avg_quality'].idxmax()\n",
    "    best_cost_idx = benchmark_df['avg_cost'].idxmin()\n",
    "    best_overall_idx = benchmark_df['avg_overall_score'].idxmax()\n",
    "    \n",
    "    best_latency_config = benchmark_df.loc[best_latency_idx, 'config_name']\n",
    "    best_quality_config = benchmark_df.loc[best_quality_idx, 'config_name']\n",
    "    best_cost_config = benchmark_df.loc[best_cost_idx, 'config_name']\n",
    "    best_overall_config = benchmark_df.loc[best_overall_idx, 'config_name']\n",
    "    \n",
    "    recommendations.append(\n",
    "        f\"âš¡ For lowest latency: Use {best_latency_config} configuration\"\n",
    "    )\n",
    "    recommendations.append(\n",
    "        f\"ðŸŽ¯ For highest quality: Use {best_quality_config} configuration\"\n",
    "    )\n",
    "    recommendations.append(\n",
    "        f\"ðŸ’° For lowest cost: Use {best_cost_config} configuration\"\n",
    "    )\n",
    "    recommendations.append(\n",
    "        f\"ðŸ† For best overall score: Use {best_overall_config} configuration\"\n",
    "    )\n",
    "    \n",
    "    # Additional recommendations based on data\n",
    "    if benchmark_df['avg_latency'].std() > 0.2:\n",
    "        recommendations.append(\n",
    "            \"ðŸ”„ Consider implementing adaptive configuration switching based on query characteristics\"\n",
    "        )\n",
    "    \n",
    "    if benchmark_df['avg_cost'].mean() > 0.05:\n",
    "        recommendations.append(\n",
    "            \"ðŸ’¸ Implement caching strategies to reduce API costs\"\n",
    "        )\n",
    "    \n",
    "    if benchmark_df['avg_faithfulness'].mean() < 0.7:\n",
    "        recommendations.append(\n",
    "            \"ðŸ” Consider improving the retrieval precision to enhance faithfulness\"\n",
    "        )\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate recommendations\n",
    "recommendations = generate_tuning_recommendations(benchmark_results)\n",
    "\n",
    "print(\"System Tuning Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Create a summary of the advanced RAG concepts\n",
    "print(f\"\\nAdvanced RAG Optimization Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"â€¢ Implemented Adaptive RAG with query complexity classification\")\n",
    "print(\"â€¢ Created multi-level caching system for performance\")\n",
    "print(\"â€¢ Developed query routing based on content and intent\")\n",
    "print(\"â€¢ Established advanced evaluation metrics (faithfulness, relevance)\")\n",
    "print(\"â€¢ Benchmarked configurations for performance-cost tradeoffs\")\n",
    "print(\"â€¢ Provided system tuning recommendations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Key Takeaways\n",
    "\n",
    "Advanced RAG optimization involves multiple interconnected strategies:\n",
    "\n",
    "1. **Adaptive Approaches**: Adjust retrieval strategy based on query characteristics\n",
    "2. **Caching**: Implement multi-level caching to reduce latency and cost\n",
    "3. **Query Routing**: Direct queries to optimal configurations\n",
    "4. **Advanced Evaluation**: Look beyond simple metrics to assess quality\n",
    "5. **Configuration Benchmarking**: Systematically compare different setups\n",
    "6. **System Tuning**: Optimize based on specific requirements and constraints\n",
    "\n",
    "The RAG Engine Mini project demonstrates many of these concepts with production-grade implementations including caching, hybrid search, and configurable retrieval strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ Conclusion\n",
    "\n",
    "In this notebook, we've explored advanced RAG techniques and optimization strategies:\n",
    "\n",
    "- We learned about advanced RAG paradigms like Self-RAG, CRAG, and Adaptive RAG\n",
    "- We implemented an adaptive system that selects retrieval strategies based on query complexity\n",
    "- We created a multi-level caching system for performance optimization\n",
    "- We developed advanced evaluation metrics for assessing RAG quality\n",
    "- We benchmarked different configurations to understand performance trade-offs\n",
    "- We provided system tuning recommendations based on empirical data\n",
    "\n",
    "These advanced techniques are essential for building production RAG systems that balance quality, performance, and cost. The RAG Engine Mini project provides a solid foundation for implementing these advanced techniques in real-world applications.\n",
    "\n",
    "Continue to experiment with different configurations and optimization strategies to find the best approach for your specific use case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}