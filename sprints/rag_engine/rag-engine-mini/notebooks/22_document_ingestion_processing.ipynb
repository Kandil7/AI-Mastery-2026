{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ Document Ingestion & Processing in RAG Systems\n",
    "\n",
    "> **Educational Notebook 22**: Deep dive into document ingestion, parsing, chunking, and indexing workflows\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. Understand the complete document ingestion pipeline in RAG systems\n",
    "2. Learn about different document parsers and their capabilities\n",
    "3. Explore various chunking strategies and their trade-offs\n",
    "4. Implement a custom chunking algorithm from scratch\n",
    "5. Understand multi-modal processing (text, images, tables)\n",
    "6. Learn about deduplication techniques\n",
    "7. Appreciate the production considerations for document processing\n",
    "\n",
    "## üéØ Why Document Processing Matters\n",
    "\n",
    "Document processing is the foundation of any RAG system. Poor document processing leads to:\n",
    "- Low-quality chunks that don't answer user questions effectively\n",
    "- Information loss due to inappropriate chunking\n",
    "- High latency during retrieval\n",
    "- Inconsistent retrieval results\n",
    "\n",
    "Let's dive into the complete workflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setting Up Our Environment\n",
    "\n",
    "First, let's set up our environment and import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import hashlib\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "current = Path().resolve()\n",
    "repo_root = None\n",
    "for parent in [current, *current.parents]:\n",
    "    if (parent / \"src\").exists() and (parent / \"notebooks\").exists():\n",
    "        repo_root = parent\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"Could not locate rag-engine-mini root for imports\")\n",
    "\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import required modules\n",
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(f\"Repository root: {repo_root}\")\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Understanding Document Types & Parsing\n",
    "\n",
    "Different document formats require different parsing strategies. Let's explore the key formats and their characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define supported document types\n",
    "document_types = {\n",
    "    \"PDF\": {\n",
    "        \"extensions\": [\".pdf\"],\n",
    "        \"characteristics\": [\n",
    "            \"Can contain text, images, tables\",\n",
    "            \"May have complex layouts\",\n",
    "            \"Requires specialized parsing (PyMuPDF, pdfminer)\",\n",
    "            \"OCR may be needed for scanned documents\"\n",
    "        ],\n",
    "        \"parser\": \"PyMuPDF (fitz)\"\n",
    "    },\n",
    "    \"Word\": {\n",
    "        \"extensions\": [\".docx\", \".doc\"],\n",
    "        \"characteristics\": [\n",
    "            \"Structured text with formatting\",\n",
    "            \"Can contain embedded images\",\n",
    "            \"Requires python-docx for parsing\"\n",
    "        ],\n",
    "        \"parser\": \"python-docx\"\n",
    "    },\n",
    "    \"PowerPoint\": {\n",
    "        \"extensions\": [\".pptx\", \".ppt\"],\n",
    "        \"characteristics\": [\n",
    "            \"Slide-based content\",\n",
    "            \"Often contains images and charts\",\n",
    "            \"Requires python-pptx for parsing\"\n",
    "        ],\n",
    "        \"parser\": \"python-pptx\"\n",
    "    },\n",
    "    \"Excel\": {\n",
    "        \"extensions\": [\".xlsx\", \".xls\", \".csv\"],\n",
    "        \"characteristics\": [\n",
    "            \"Tabular data\",\n",
    "            \"Requires pandas or openpyxl for parsing\",\n",
    "            \"Need special handling for table structures\"\n",
    "        ],\n",
    "        \"parser\": \"pandas/openpyxl\"\n",
    "    },\n",
    "    \"Plain Text\": {\n",
    "        \"extensions\": [\".txt\", \".md\", \".rst\"],\n",
    "        \"characteristics\": [\n",
    "            \"Simple text content\",\n",
    "            \"No complex layout\",\n",
    "            \"Fast parsing\"\n",
    "        ],\n",
    "        \"parser\": \"Built-in file reading\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display the document types information\n",
    "print(\"Supported Document Types in RAG Systems:\")\n",
    "print(\"=\" * 50)\n",
    "for doc_type, info in document_types.items():\n",
    "    print(f\"\\n{doc_type} ({', '.join(info['extensions'])}):\")\n",
    "    print(f\"  Parser: {info['parser']}\")\n",
    "    print(\"  Characteristics:\")\n",
    "    for char in info['characteristics']:\n",
    "        print(f\"    - {char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Understanding Chunking Strategies\n",
    "\n",
    "Chunking is crucial for RAG systems. Let's explore different strategies and their implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define chunking strategies\n",
    "chunking_strategies = {\n",
    "    \"Fixed Size\": {\n",
    "        \"description\": \"Split documents into fixed-size chunks based on token count or character length\",\n",
    "        \"pros\": [\n",
    "            \"Simple to implement\",\n",
    "            \"Consistent chunk sizes\",\n",
    "            \"Predictable performance\"\n",
    "        ],\n",
    "        \"cons\": [\n",
    "            \"May split related content\",\n",
    "            \"Context boundaries\",\n",
    "            \"Information fragmentation\"\n",
    "        ],\n",
    "        \"use_case\": \"General-purpose RAG with diverse document types\"\n",
    "    },\n",
    "    \"Semantic\": {\n",
    "        \"description\": \"Split documents based on semantic boundaries (sentences, paragraphs, topics)\",\n",
    "        \"pros\": [\n",
    "            \"Preserves semantic coherence\",\n",
    "            \"Better context preservation\",\n",
    "            \"More meaningful chunks\"\n",
    "        ],\n",
    "        \"cons\": [\n",
    "            \"Variable chunk sizes\",\n",
    "            \"Requires NLP processing\",\n",
    "            \"Higher computational cost\"\n",
    "        ],\n",
    "        \"use_case\": \"Documents with clear semantic structure (articles, books)\"\n",
    "    },\n",
    "    \"Hierarchical\": {\n",
    "        \"description\": \"Create multiple levels of chunks (sections, subsections, paragraphs)\",\n",
    "        \"pros\": [\n",
    "            \"Multiple resolution levels\",\n",
    "            \"Flexible retrieval\",\n",
    "            \"Context switching\"\n",
    "        ],\n",
    "        \"cons\": [\n",
    "            \"Complex implementation\",\n",
    "            \"Storage overhead\",\n",
    "            \"Indexing complexity\"\n",
    "        ],\n",
    "        \"use_case\": \"Long documents requiring both detailed and summary retrieval\"\n",
    "    },\n",
    "    \"Sliding Window\": {\n",
    "        \"description\": \"Create overlapping chunks to preserve context across boundaries\",\n",
    "        \"pros\": [\n",
    "            \"Preserves boundary context\",\n",
    "            \"Reduces information loss\",\n",
    "            \"Better for QA tasks\"\n",
    "        ],\n",
    "        \"cons\": [\n",
    "            \"Increased storage\",\n",
    "            \"Potential redundancy\",\n",
    "            \"Higher retrieval costs\"\n",
    "        ],\n",
    "        \"use_case\": \"Question answering systems requiring precise context\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display chunking strategies\n",
    "print(\"Chunking Strategies in RAG Systems:\")\n",
    "print(\"=\" * 60)\n",
    "for strategy, info in chunking_strategies.items():\n",
    "    print(f\"\\n{strategy}: {info['description']}\")\n",
    "    print(\"  Pros:\")\n",
    "    for pro in info['pros']:\n",
    "        print(f\"    ‚úì {pro}\")\n",
    "    print(\"  Cons:\")\n",
    "    for con in info['cons']:\n",
    "        print(f\"    ‚úó {con}\")\n",
    "    print(f\"  Use Case: {info['use_case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Implementing a Custom Chunking Algorithm\n",
    "\n",
    "Let's implement a token-aware chunking algorithm from scratch to understand the mechanics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAwareChunker:\n",
    "    \"\"\"\n",
    "    A token-aware chunker that splits text into chunks considering token limits.\n",
    "    This is a simplified implementation similar to what's used in the RAG engine.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 512, overlap: int = 50):\n",
    "        \"\"\"\n",
    "        Initialize the chunker.\n",
    "        \n",
    "        Args:\n",
    "            max_tokens: Maximum tokens per chunk\n",
    "            overlap: Number of overlapping tokens between chunks\n",
    "        \"\"\"\n",
    "        self.max_tokens = max_tokens\n",
    "        self.overlap = overlap\n",
    "        # For simplicity, we'll approximate tokens as words\n",
    "        # In production, use proper tokenizers like tiktoken or transformers\n",
    "        \n",
    "    def estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"\n",
    "        Estimate the number of tokens in a text.\n",
    "        This is a simple approximation - in practice, use proper tokenizers.\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace and count words\n",
    "        words = text.strip().split()\n",
    "        return len(words)\n",
    "    \n",
    "    def chunk_text(self, text: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Split text into chunks respecting token limits.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to chunk\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks with metadata\n",
    "        \"\"\"\n",
    "        # First, split by paragraphs to respect semantic boundaries\n",
    "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        current_tokens = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for para in paragraphs:\n",
    "            para_tokens = self.estimate_tokens(para)\n",
    "            \n",
    "            # If paragraph alone exceeds max tokens, split it further\n",
    "            if para_tokens > self.max_tokens:\n",
    "                # Add current chunk if it's not empty\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"id\": f\"chunk_{chunk_id}\",\n",
    "                        \"text\": current_chunk,\n",
    "                        \"tokens\": current_tokens,\n",
    "                        \"type\": \"continuation\"\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                    current_chunk = \"\"\n",
    "                    current_tokens = 0\n",
    "                \n",
    "                # Now split the long paragraph into sentences\n",
    "                sentences = self._split_into_sentences(para)\n",
    "                temp_chunk = \"\"\n",
    "                temp_tokens = 0\n",
    "                \n",
    "                for sent in sentences:\n",
    "                    sent_tokens = self.estimate_tokens(sent)\n",
    "                    \n",
    "                    if temp_tokens + sent_tokens <= self.max_tokens:\n",
    "                        temp_chunk += sent + \" \"\n",
    "                        temp_tokens += sent_tokens\n",
    "                    else:\n",
    "                        if temp_chunk:\n",
    "                            chunks.append({\n",
    "                                \"id\": f\"chunk_{chunk_id}\",\n",
    "                                \"text\": temp_chunk.strip(),\n",
    "                                \"tokens\": temp_tokens,\n",
    "                                \"type\": \"sentence_split\"\n",
    "                            })\n",
    "                            chunk_id += 1\n",
    "                        \n",
    "                        # Start new chunk with current sentence\n",
    "                        temp_chunk = sent + \" \"\n",
    "                        temp_tokens = sent_tokens\n",
    "                \n",
    "                # Add remaining text if any\n",
    "                if temp_chunk:\n",
    "                    chunks.append({\n",
    "                        \"id\": f\"chunk_{chunk_id}\",\n",
    "                        \"text\": temp_chunk.strip(),\n",
    "                        \"tokens\": temp_tokens,\n",
    "                        \"type\": \"sentence_split_continuation\"\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "            \n",
    "            # If adding this paragraph would exceed the limit, start a new chunk\n",
    "            elif current_tokens + para_tokens > self.max_tokens:\n",
    "                if current_chunk:\n",
    "                    chunks.append({\n",
    "                        \"id\": f\"chunk_{chunk_id}\",\n",
    "                        \"text\": current_chunk.strip(),\n",
    "                        \"tokens\": current_tokens,\n",
    "                        \"type\": \"paragraph_based\"\n",
    "                    })\n",
    "                    chunk_id += 1\n",
    "                \n",
    "                # Start new chunk with this paragraph\n",
    "                current_chunk = para + \"\\n\\n\"\n",
    "                current_tokens = para_tokens\n",
    "            \n",
    "            # Otherwise, add to current chunk\n",
    "            else:\n",
    "                current_chunk += para + \"\\n\\n\"\n",
    "                current_tokens += para_tokens\n",
    "        \n",
    "        # Add the last chunk if it has content\n",
    "        if current_chunk:\n",
    "            chunks.append({\n",
    "                \"id\": f\"chunk_{chunk_id}\",\n",
    "                \"text\": current_chunk.strip(),\n",
    "                \"tokens\": current_tokens,\n",
    "                \"type\": \"final_chunk\"\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _split_into_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Simple sentence splitting (in practice, use NLTK or spaCy)\n",
    "        \"\"\"\n",
    "        import re\n",
    "        # Split on sentence endings followed by whitespace and capital letter\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"\"\"\n",
    "Introduction to Machine Learning.\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed. The concept isn't new - it dates back to the 1950s when Arthur Samuel coined the term. However, recent advances in computing power and data availability have made machine learning more practical and widespread than ever before.\n",
    "\n",
    "There are three main types of machine learning: supervised learning, unsupervised learning, and reinforcement learning. Supervised learning involves training a model on labeled data, where the desired output is known. This is commonly used for classification and regression tasks. Unsupervised learning works with unlabeled data to discover hidden patterns or intrinsic structures in the data. Clustering and association are common unsupervised learning tasks.\n",
    "\n",
    "Reinforcement learning is different from the other two. It involves an agent that learns to make decisions by performing actions in an environment to maximize cumulative reward. This type of learning is used in robotics, gaming, and navigation. The agent receives feedback in the form of rewards or penalties, learning through trial and error.\n",
    "\n",
    "Deep learning, a subset of machine learning, uses neural networks with many layers to model complex patterns in data. It has revolutionized fields like computer vision, natural language processing, and speech recognition. The success of deep learning has led to breakthrough applications including autonomous vehicles, real-time translation, and sophisticated virtual assistants.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample text for chunking:\")\n",
    "print(sample_text[:200] + \"... [truncated]\")\n",
    "print(f\"\\nTotal estimated tokens: {TokenAwareChunker().estimate_tokens(sample_text)}\")\n",
    "\n",
    "# Create chunker and process text\n",
    "chunker = TokenAwareChunker(max_tokens=75, overlap=10)\n",
    "chunks = chunker.chunk_text(sample_text)\n",
    "\n",
    "print(f\"\\nNumber of chunks created: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1} (Type: {chunk['type']}, Tokens: {chunk['tokens']}):\")\n",
    "    print(f\"  Preview: {chunk['text'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Multi-Modal Processing\n",
    "\n",
    "Modern RAG systems need to handle more than just text. Let's explore multi-modal processing (text + images + tables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate multi-modal processing concepts\n",
    "print(\"Multi-Modal Processing in RAG Systems:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "multi_modal_approaches = {\n",
    "    \"Text Extraction\": {\n",
    "        \"method\": \"OCR and text extraction from documents\",\n",
    "        \"tools\": [\"PyMuPDF\", \"pdfminer\", \"Tesseract\", \"docx\"],\n",
    "        \"challenge\": \"Handling different layouts and formats\"\n",
    "    },\n",
    "    \"Image Processing\": {\n",
    "        \"method\": \"Extract and describe images using vision models\",\n",
    "        \"tools\": [\"OpenAI Vision API\", \"CLIP\", \"BLIP\"],\n",
    "        \"challenge\": \"Generating meaningful descriptions\"\n",
    "    },\n",
    "    \"Table Processing\": {\n",
    "        \"method\": \"Convert tables to structured text or vectors\",\n",
    "        \"tools\": [\"pandas\", \"camelot\", \"tabula\"],\n",
    "        \"challenge\": \"Preserving structure and relationships\"\n",
    "    },\n",
    "    \"Layout Understanding\": {\n",
    "        \"method\": \"Understanding document structure (headers, footers, columns)\",\n",
    "        \"tools\": [\"LayoutParser\", \"DocTR\", \"Donut\"],\n",
    "        \"challenge\": \"Complex layouts and formatting\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for modality, details in multi_modal_approaches.items():\n",
    "    print(f\"\\n{modality}:\")\n",
    "    print(f\"  Method: {details['method']}\")\n",
    "    print(f\"  Tools: {', '.join(details['tools'])}\")\n",
    "    print(f\"  Challenge: {details['challenge']}\")\n",
    "\n",
    "# Show how this is implemented in the RAG engine\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Real Implementation from RAG Engine Workers:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"The RAG engine handles multi-modal processing in the index_document task:\")\n",
    "print(\"1. Extract text & tables (structural extraction)\")\n",
    "print(\"2. Extract & describe images (LLM-vision)\")\n",
    "print(\"3. Hierarchical & contextual linking\")\n",
    "print(\"4. Graph triplet extraction\")\n",
    "print(\"5. Batch embedding with caching\")\n",
    "print(\"6. Detailed storage & graph extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÅ Deduplication Techniques\n",
    "\n",
    "Deduplication is essential to avoid storing redundant information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a basic deduplication technique\n",
    "class ChunkDeduplicator:\n",
    "    \"\"\"\n",
    "    Implements chunk deduplication using hash-based comparison\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunk_hashes = set()  # Store hashes of seen chunks\n",
    "        self.duplicate_count = 0\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text to reduce false duplicates\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace, convert to lowercase\n",
    "        normalized = ' '.join(text.lower().split())\n",
    "        return normalized\n",
    "    \n",
    "    def calculate_hash(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Calculate SHA256 hash of normalized text\n",
    "        \"\"\"\n",
    "        normalized = self.normalize_text(text)\n",
    "        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()\n",
    "    \n",
    "    def is_duplicate(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a text chunk is a duplicate\n",
    "        \"\"\"\n",
    "        chunk_hash = self.calculate_hash(text)\n",
    "        if chunk_hash in self.chunk_hashes:\n",
    "            self.duplicate_count += 1\n",
    "            return True\n",
    "        \n",
    "        self.chunk_hashes.add(chunk_hash)\n",
    "        return False\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get duplication statistics\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"unique_chunks\": len(self.chunk_hashes),\n",
    "            \"duplicate_count\": self.duplicate_count\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "deduplicator = ChunkDeduplicator()\n",
    "\n",
    "# Sample chunks with some duplicates\n",
    "sample_chunks = [\n",
    "    \"Artificial Intelligence is a wonderful field of computer science.\",\n",
    "    \"Machine learning enables computers to learn from data.\",\n",
    "    \"Artificial Intelligence is a wonderful field of computer science.\",  # Duplicate\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Machine learning enables computers to learn from data.\",  # Duplicate\n",
    "    \"Natural Language Processing helps machines understand human language.\",\n",
    "    \"artificial intelligence is a wonderful field of computer science.\",  # Near duplicate (different case)\n",
    "]\n",
    "\n",
    "print(\"Processing chunks for deduplication:\")\n",
    "unique_chunks = []\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    is_dup = deduplicator.is_duplicate(chunk)\n",
    "    status = \"DUPLICATE\" if is_dup else \"NEW\"\n",
    "    print(f\"Chunk {i+1}: {status}\")\n",
    "    \n",
    "    if not is_dup:\n",
    "        unique_chunks.append(chunk)\n",
    "\n",
    "stats = deduplicator.get_stats()\n",
    "print(f\"\\nDeduplication Results:\")\n",
    "print(f\"  Original chunks: {len(sample_chunks)}\")\n",
    "print(f\"  Unique chunks: {stats['unique_chunks']}\")\n",
    "print(f\"  Duplicates removed: {stats['duplicate_count']}\")\n",
    "print(f\"  Storage efficiency: {(stats['duplicate_count']/len(sample_chunks)*100):.1f}% reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Performance Considerations\n",
    "\n",
    "Document processing can be computationally expensive. Here are key performance considerations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance considerations for document processing\n",
    "performance_considerations = {\n",
    "    \"Parallel Processing\": {\n",
    "        \"technique\": \"Process multiple documents simultaneously\",\n",
    "        \"implementation\": \"Use multiprocessing or threading\",\n",
    "        \"benefit\": \"Significant speedup for I/O bound tasks\"\n",
    "    },\n",
    "    \"Caching\": {\n",
    "        \"technique\": \"Cache embeddings and processed chunks\",\n",
    "        \"implementation\": \"Redis or in-memory cache\",\n",
    "        \"benefit\": \"Avoid recomputing embeddings for identical content\"\n",
    "    },\n",
    "    \"Batch Processing\": {\n",
    "        \"technique\": \"Process multiple items together\",\n",
    "        \"implementation\": \"Batch API calls for embeddings\",\n",
    "        \"benefit\": \"Reduced API overhead and cost\"\n",
    "    },\n",
    "    \"Asynchronous Processing\": {\n",
    "        \"technique\": \"Use async/await for I/O operations\",\n",
    "        \"implementation\": \"Async document parsing and API calls\",\n",
    "        \"benefit\": \"Better resource utilization\"\n",
    "    },\n",
    "    \"Memory Management\": {\n",
    "        \"technique\": \"Process large documents in streams\",\n",
    "        \"implementation\": \"Generator functions and streaming\",\n",
    "        \"benefit\": \"Handle large documents without memory issues\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Performance Considerations for Document Processing:\")\n",
    "print(\"=\" * 60)\n",
    "for name, details in performance_considerations.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Technique: {details['technique']}\")\n",
    "    print(f\"  Implementation: {details['implementation']}\")\n",
    "    print(f\"  Benefit: {details['benefit']}\")\n",
    "\n",
    "# Show how the RAG engine implements these\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"How RAG Engine Implements Performance:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"1. Background Processing: Uses Celery for async document indexing\")\n",
    "print(\"2. Embedding Caching: Reuses embeddings for identical content\")\n",
    "print(\"3. Batch Operations: Processes multiple chunks together\")\n",
    "print(\"4. Memory Efficiency: Streams large files instead of loading entirely\")\n",
    "print(\"5. Connection Pooling: Efficient DB and vector store connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Hands-On Exercise: Building a Document Processing Pipeline\n",
    "\n",
    "Let's put everything together and create a simple document processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessorPipeline:\n",
    "    \"\"\"\n",
    "    A complete document processing pipeline combining all concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens=512, overlap=50):\n",
    "        self.chunker = TokenAwareChunker(max_tokens=max_tokens, overlap=overlap)\n",
    "        self.deduplicator = ChunkDeduplicator()\n",
    "        \n",
    "    def process_document(self, text: str, doc_id: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process a document through the complete pipeline\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Initial text cleaning\n",
    "        cleaned_text = self._clean_text(text)\n",
    "        \n",
    "        # Step 2: Chunk the text\n",
    "        chunks = self.chunker.chunk_text(cleaned_text)\n",
    "        \n",
    "        # Step 3: Deduplicate chunks\n",
    "        unique_chunks = []\n",
    "        for chunk in chunks:\n",
    "            if not self.deduplicator.is_duplicate(chunk['text']):\n",
    "                # Add document ID and other metadata\n",
    "                chunk['doc_id'] = doc_id\n",
    "                chunk['hash'] = self.deduplicator.calculate_hash(chunk['text'])\n",
    "                unique_chunks.append(chunk)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Prepare results\n",
    "        result = {\n",
    "            \"document_id\": doc_id,\n",
    "            \"original_length\": len(text),\n",
    "            \"chunks_created\": len(chunks),\n",
    "            \"unique_chunks\": len(unique_chunks),\n",
    "            \"duplication_rate\": ((len(chunks) - len(unique_chunks)) / len(chunks)) * 100 if chunks else 0,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"chunks\": unique_chunks\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Basic text cleaning\n",
    "        \"\"\"\n",
    "        # Remove extra whitespace but preserve paragraph breaks\n",
    "        lines = [line.strip() for line in text.split('\\n')]\n",
    "        cleaned = '\\n'.join(line for line in lines if line)\n",
    "        return cleaned\n",
    "\n",
    "# Test the pipeline\n",
    "pipeline = DocumentProcessorPipeline(max_tokens=60, overlap=10)\n",
    "\n",
    "# Process our sample text\n",
    "result = pipeline.process_document(sample_text, \"doc_ml_intro_001\")\n",
    "\n",
    "print(\"Document Processing Pipeline Results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Document ID: {result['document_id']}\")\n",
    "print(f\"Original Length: {result['original_length']} chars\")\n",
    "print(f\"Chunks Created: {result['chunks_created']}\")\n",
    "print(f\"Unique Chunks: {result['unique_chunks']}\")\n",
    "print(f\"Duplication Rate: {result['duplication_rate']:.1f}%\")\n",
    "print(f\"Processing Time: {result['processing_time']:.3f}s\")\n",
    "\n",
    "print(f\"\\nChunk Details:\")\n",
    "for i, chunk in enumerate(result['chunks']):\n",
    "    print(f\"  Chunk {i+1}: {chunk['tokens']} tokens, type '{chunk['type']}'\")\n",
    "    print(f\"    Preview: {chunk['text'][:70]}...\")\n",
    "\n",
    "# Show deduplication stats\n",
    "stats = pipeline.deduplicator.get_stats()\n",
    "print(f\"\\nOverall Pipeline Stats:\")\n",
    "print(f\"  Total unique chunks processed: {stats['unique_chunks']}\")\n",
    "print(f\"  Total duplicates found: {stats['duplicate_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Key Takeaways\n",
    "\n",
    "Document ingestion and processing is a critical component of RAG systems with several important considerations:\n",
    "\n",
    "1. **Format Diversity**: Different document types require specialized parsers\n",
    "2. **Chunking Strategy**: The right strategy depends on your use case and content type\n",
    "3. **Semantic Coherence**: Preserve meaning when splitting documents\n",
    "4. **Performance**: Optimize for speed and resource usage\n",
    "5. **Quality Control**: Deduplicate and validate content\n",
    "6. **Multi-Modal**: Handle text, images, and structured data appropriately\n",
    "7. **Metadata**: Preserve context and source information\n",
    "\n",
    "The RAG Engine Mini implements these concepts with production-grade features including asynchronous processing, caching, and comprehensive error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion\n",
    "\n",
    "In this notebook, we've explored the critical aspects of document ingestion and processing in RAG systems:\n",
    "\n",
    "- We learned about different document types and their parsing requirements\n",
    "- We implemented a token-aware chunking algorithm from scratch\n",
    "- We explored multi-modal processing concepts\n",
    "- We implemented a deduplication technique\n",
    "- We considered performance optimizations\n",
    "- We built a complete document processing pipeline\n",
    "\n",
    "These concepts form the foundation of any successful RAG system. The quality of your document processing directly impacts the quality of your retrievals and ultimately the effectiveness of your RAG system.\n",
    "\n",
    "Continue to the next notebooks to learn about retrieval techniques, ranking algorithms, and evaluation methodologies!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}