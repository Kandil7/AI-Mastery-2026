{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŽ“ Notebook 08: Multi-Provider LLM Strategy\n",
                "\n",
                "Welcome to the final educational module of the RAG Mastery series! In this notebook, we explore the **Provider Abstraction** layer. You will learn how to switch between different AI brains (OpenAI, Gemini, Hugging Face, and Ollama) without changing a single line of your application logic.\n",
                "\n",
                "### Learning Objectives:\n",
                "1. Understand the **Adapter Pattern** for LLMs.\n",
                "2. Learn how to configure multiple providers in RAG Engine Mini.\n",
                "3. Compare response formats and performance across backends."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "\n",
                "We will use the project's dependency injection container to see how the system swaps implementations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import asyncio\n",
                "from dotenv import load_dotenv\n",
                "from src.core.bootstrap import get_container, reset_container\n",
                "from src.core.config import get_settings\n",
                "\n",
                "load_dotenv() # Ensure you have your keys in .env"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploring the Adapters\n",
                "\n",
                "Let's look at how the same query is handled by different backends using the `LLMPort` interface."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def test_llm(backend_name: str):\n",
                "    print(f\"--- Testing Backend: {backend_name} ---\")\n",
                "    \n",
                "    # Temporarily override settings\n",
                "    os.environ[\"LLM_BACKEND\"] = backend_name\n",
                "    reset_container()\n",
                "    \n",
                "    container = get_container()\n",
                "    llm = container[\"llm\"]\n",
                "    \n",
                "    prompt = \"What are the three pillars of a robust RAG system? Answer concisely.\"\n",
                "    \n",
                "    try:\n",
                "        response = await llm.generate(prompt)\n",
                "        print(f\"Response: {response}\\n\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error with {backend_name}: {e}\\n\")\n",
                "\n",
                "# Note: These require valid API keys in your .env\n",
                "# await test_llm(\"openai\")\n",
                "# await test_llm(\"gemini\")\n",
                "# await test_llm(\"huggingface\")\n",
                "print(\"Uncomment the lines above to run live tests with your keys!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Power of `generate_stream`\n",
                "\n",
                "A premium user experience requires streaming. Our abstraction ensures that regardless of the provider's unique streaming implementation (Server-Sent Events, GRPC, etc.), the application receives a simple `AsyncGenerator`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "async def demo_streaming(backend_name: str):\n",
                "    os.environ[\"LLM_BACKEND\"] = backend_name\n",
                "    reset_container()\n",
                "    llm = get_container()[\"llm\"]\n",
                "    \n",
                "    print(f\"Streaming from {backend_name}...\")\n",
                "    async for chunk in llm.generate_stream(\"Tell me a 2-sentence story about a robot engineer.\"):\n",
                "        print(chunk, end=\"\", flush=True)\n",
                "    print(\"\\n\")\n",
                "\n",
                "# await demo_streaming(\"openai\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Why This Matters: Architectural Freedom\n",
                "\n",
                "By isolating the LLM logic into adapters:\n",
                "1. **Maintenance**: If Hugging Face changes their API tomorrow, you only fix one file (`huggingface_llm.py`).\n",
                "2. **Cost Control**: You can use cheaper models for retrieval grading and expensive models for the final answer.\n",
                "3. **Local-First**: You can develop offline using **Ollama** and deploy to **Gemini/OpenAI** for production.\n",
                "\n",
                "--- \n",
                "**Congratulations!** You have completed the RAG Engineering Mastery series. You now possess the knowledge to build, scale, and optimize production-grade search systems."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}