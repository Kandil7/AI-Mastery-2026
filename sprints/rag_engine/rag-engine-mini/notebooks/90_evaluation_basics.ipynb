{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä RAG Evaluation Basics\n",
    "\n",
    "This notebook covers the fundamentals of evaluating RAG (Retrieval-Augmented Generation) systems. We'll explore key metrics, methodologies, and practical examples to help you understand how to measure and improve your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install ragas langchain-openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    harmfulness,\n",
    "    conciseness\n",
    ")\n",
    "\n",
    "# Import other necessary libraries\n",
    "from src.core.bootstrap import get_container\n",
    "from src.application.use_cases.ask_question_hybrid import AskQuestionHybridUseCase\n",
    "\n",
    "# Initialize the container\n",
    "container = get_container()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Understanding RAG Evaluation Metrics\n",
    "\n",
    "RAG systems require specialized metrics that evaluate not just the final answer, but the entire pipeline including retrieval and generation components.\n",
    "\n",
    "### Key Metrics:\n",
    "- **Faithfulness**: Does the answer stick to the provided context?\n",
    "- **Answer Relevancy**: Is the answer relevant to the question?\n",
    "- **Context Precision**: Are the retrieved chunks relevant to the question?\n",
    "- **Context Recall**: Does the retrieved context contain the answer?\n",
    "- **Harmfulness**: Does the system generate harmful content?\n",
    "- **Conciseness**: Is the answer appropriately concise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Creating an Evaluation Dataset\n",
    "\n",
    "To evaluate our RAG system, we need a dataset with questions, ground truths, and contexts. In practice, you'd create this from your actual documents and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample evaluation data - in practice, you'd generate this from your documents\n",
    "sample_data = {\n",
    "    \"question\": [\n",
    "        \"What is the main advantage of hybrid search in RAG systems?\",\n",
    "        \"How does chunking affect RAG performance?\",\n",
    "        \"What is the purpose of re-ranking in RAG?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Hybrid search combines vector and keyword search to improve retrieval accuracy by leveraging both semantic and lexical matching.\",\n",
    "        \"Chunking affects RAG performance by determining how documents are split, impacting both retrieval relevance and context preservation.\",\n",
    "        \"Re-ranking improves the relevance of initially retrieved documents by applying a secondary ranking algorithm.\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        [\n",
    "            \"Hybrid search combines vector search with keyword search to leverage the strengths of both approaches. Vector search captures semantic meaning, while keyword search handles exact matches and named entities.\",\n",
    "            \"The effectiveness of retrieval in RAG systems depends on how well the search balances semantic and lexical matching.\",\n",
    "            \"Vector databases store embeddings that represent semantic meaning of text fragments.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Chunking strategies determine how documents are divided for storage in the vector database. Different strategies affect retrieval quality.\",\n",
    "            \"Large chunks may contain more context but could dilute relevance. Small chunks increase precision but may lose important context.\",\n",
    "            \"Optimal chunk size varies depending on the document type and query patterns.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Re-ranking is a post-processing step that re-orders initially retrieved documents based on a more sophisticated relevance model.\",\n",
    "            \"Initial retrieval might use fast but less accurate methods, while re-ranking applies more precise but computationally expensive models.\",\n",
    "            \"Cross-encoder models are commonly used for re-ranking due to their effectiveness.\"\n",
    "        ]\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"Hybrid search combines vector and keyword search to improve retrieval accuracy by leveraging both semantic and lexical matching.\",\n",
    "        \"Chunking affects RAG performance by determining how documents are split, impacting both retrieval relevance and context preservation.\",\n",
    "        \"Re-ranking improves the relevance of initially retrieved documents by applying a secondary ranking algorithm.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "eval_dataset = Dataset.from_dict(sample_data)\n",
    "print(f\"Dataset created with {eval_dataset.num_rows} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Running the Evaluation\n",
    "\n",
    "Now we'll run the evaluation using Ragas metrics to assess our RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "]\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    results = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(results)\n",
    "    \n",
    "    # Convert to a more readable format\n",
    "    results_df = pd.DataFrame([results])\n",
    "    print(\"\\nResults as DataFrame:\")\n",
    "    print(results_df.T)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")\n",
    "    print(\"This is expected if the required models/api keys are not configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ A/B Testing Different Configurations\n",
    "\n",
    "Let's compare the performance of different RAG configurations to understand the impact of various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Comparing hybrid search vs vector-only search\n",
    "def run_ab_test():\n",
    "    # This is a conceptual example - actual implementation would require\n",
    "    # different configurations of your RAG system\n",
    "    \n",
    "    print(\"Running A/B test between different configurations...\")\n",
    "    \n",
    "    # Configuration 1: Vector-only search\n",
    "    print(\"Configuration 1: Vector-only search\")\n",
    "    # results_config1 = evaluate_with_config(config=\"vector_only\")\n",
    "    \n",
    "    # Configuration 2: Hybrid search (vector + keyword)\n",
    "    print(\"Configuration 2: Hybrid search (vector + keyword)\")\n",
    "    # results_config2 = evaluate_with_config(config=\"hybrid\")\n",
    "    \n",
    "    # Configuration 3: Hybrid with re-ranking\n",
    "    print(\"Configuration 3: Hybrid search with re-ranking\")\n",
    "    # results_config3 = evaluate_with_config(config=\"hybrid_with_rerank\")\n",
    "    \n",
    "    print(\"\\nComparison would show:\")\n",
    "    print(\"- Hybrid search typically improves context recall\")\n",
    "    print(\"- Re-ranking typically improves context precision\")\n",
    "    print(\"- Vector-only might be faster but less accurate\")\n",
    "\n",
    "run_ab_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Interpreting Results\n",
    "\n",
    "Understanding what the metrics tell us about our RAG system's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_results(results):\n",
    "    \"\"\"\n",
    "    Interpret evaluation results and provide insights\n",
    "    \"\"\"\n",
    "    print(\"Interpretation of RAG Evaluation Results:\")\n",
    "    print(\"=========================================\")\n",
    "    \n",
    "    if 'faithfulness' in results:\n",
    "        faithfulness_score = results['faithfulness']\n",
    "        print(f\"Faithfulness: {faithfulness_score:.3f}\")\n",
    "        if faithfulness_score > 0.8:\n",
    "            print(\"  ‚úì Good: Answers are well-grounded in provided context\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Answers may contain hallucinations\")\n",
    "    \n",
    "    if 'answer_relevancy' in results:\n",
    "        relevancy_score = results['answer_relevancy']\n",
    "        print(f\"Answer Relevancy: {relevancy_score:.3f}\")\n",
    "        if relevancy_score > 0.7:\n",
    "            print(\"  ‚úì Good: Answers are relevant to the questions\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Answers may be off-topic\")\n",
    "            \n",
    "    if 'context_precision' in results:\n",
    "        precision_score = results['context_precision']\n",
    "        print(f\"Context Precision: {precision_score:.3f}\")\n",
    "        if precision_score > 0.7:\n",
    "            print(\"  ‚úì Good: Retrieved context is relevant to the question\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Retrieved context may contain irrelevant information\")\n",
    "            \n",
    "    if 'context_recall' in results:\n",
    "        recall_score = results['context_recall']\n",
    "        print(f\"Context Recall: {recall_score:.3f}\")\n",
    "        if recall_score > 0.7:\n",
    "            print(\"  ‚úì Good: Retrieved context contains information needed to answer\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Important information may be missing from retrieved context\")\n",
    "\n",
    "# Example interpretation (using placeholder values since we didn't run actual eval)\n",
    "example_results = {\n",
    "    'faithfulness': 0.85,\n",
    "    'answer_relevancy': 0.78,\n",
    "    'context_precision': 0.72,\n",
    "    'context_recall': 0.81\n",
    "}\n",
    "\n",
    "interpret_results(example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Practical Tips for RAG Evaluation\n",
    "\n",
    "Based on the evaluation results, here are practical steps to improve your RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_improvement_recommendations(results):\n",
    "    \"\"\"\n",
    "    Provide recommendations based on evaluation results\n",
    "    \"\"\"\n",
    "    print(\"Improvement Recommendations:\")\n",
    "    print(\"===========================\")\n",
    "    \n",
    "    if results.get('faithfulness', 1.0) < 0.8:\n",
    "        print(\"‚Ä¢ Improve faithfulness:\")\n",
    "        print(\"  - Strengthen grounding mechanisms in prompts\")\n",
    "        print(\"  - Implement self-correction/validation steps\")\n",
    "        print(\"  - Use more constrained generation parameters\")\n",
    "    \n",
    "    if results.get('context_precision', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve context precision:\")\n",
    "        print(\"  - Enhance re-ranking mechanisms\")\n",
    "        print(\"  - Fine-tune embedding models for your domain\")\n",
    "        print(\"  - Implement better query expansion techniques\")\n",
    "    \n",
    "    if results.get('context_recall', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve context recall:\")\n",
    "        print(\"  - Implement hybrid search (vector + keyword)\")\n",
    "        print(\"  - Increase retrieval depth (top-k)\")\n",
    "        print(\"  - Improve query expansion\")\n",
    "    \n",
    "    if results.get('answer_relevancy', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve answer relevancy:\")\n",
    "        print(\"  - Refine prompt engineering\")\n",
    "        print(\"  - Implement query classification and routing\")\n",
    "        print(\"  - Use more appropriate LLM for your domain\")\n",
    "\n",
    "get_improvement_recommendations(example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "1. **Comprehensive Evaluation**: RAG systems need evaluation across multiple dimensions, not just answer accuracy\n",
    "2. **Metric Selection**: Choose metrics that align with your specific use case and success criteria\n",
    "3. **Iterative Improvement**: Use evaluation results to identify bottlenecks and guide improvements\n",
    "4. **Baseline Establishment**: Establish baselines to measure improvement from changes\n",
    "5. **Realistic Testing**: Use test data that represents real-world usage patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}