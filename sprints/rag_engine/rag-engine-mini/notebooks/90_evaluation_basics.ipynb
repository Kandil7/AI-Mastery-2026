{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä RAG Evaluation Basics\n",
    "\n",
    "This notebook covers the fundamentals of evaluating RAG (Retrieval-Augmented Generation) systems. We'll explore key metrics, methodologies, and practical examples to help you understand how to measure and improve your RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:47.483906Z",
     "iopub.status.busy": "2026-01-31T14:06:47.482906Z",
     "iopub.status.idle": "2026-01-31T14:06:47.492906Z",
     "shell.execute_reply": "2026-01-31T14:06:47.490907Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "root = os.getcwd()\n",
    "rag_root = None\n",
    "\n",
    "while True:\n",
    "    candidate = os.path.join(root, 'sprints', 'rag_engine', 'rag-engine-mini')\n",
    "    if os.path.isdir(os.path.join(candidate, 'src')):\n",
    "        rag_root = candidate\n",
    "        break\n",
    "    if os.path.basename(root) == 'rag-engine-mini' and os.path.isdir(os.path.join(root, 'src')):\n",
    "        rag_root = root\n",
    "        break\n",
    "    parent = os.path.dirname(root)\n",
    "    if parent == root:\n",
    "        break\n",
    "    root = parent\n",
    "\n",
    "if rag_root:\n",
    "    sys.path.insert(0, rag_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:47.495907Z",
     "iopub.status.busy": "2026-01-31T14:06:47.495907Z",
     "iopub.status.idle": "2026-01-31T14:06:47.499942Z",
     "shell.execute_reply": "2026-01-31T14:06:47.498939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "# !pip install ragas langchain-openai datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:06:47.502562Z",
     "iopub.status.busy": "2026-01-31T14:06:47.502562Z",
     "iopub.status.idle": "2026-01-31T14:07:02.637130Z",
     "shell.execute_reply": "2026-01-31T14:07:02.636297Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amazon\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from importlib import import_module\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "\n",
    "\n",
    "def _get_metric(module_name: str, attr: str):\n",
    "    try:\n",
    "        module = import_module(module_name)\n",
    "        return getattr(module, attr)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "faithfulness = _get_metric(\"ragas.metrics.collections\", \"faithfulness\") or _get_metric(\"ragas.metrics\", \"faithfulness\")\n",
    "answer_relevancy = _get_metric(\"ragas.metrics.collections\", \"answer_relevancy\") or _get_metric(\"ragas.metrics\", \"answer_relevancy\")\n",
    "context_precision = _get_metric(\"ragas.metrics.collections\", \"context_precision\") or _get_metric(\"ragas.metrics\", \"context_precision\")\n",
    "context_recall = _get_metric(\"ragas.metrics.collections\", \"context_recall\") or _get_metric(\"ragas.metrics\", \"context_recall\")\n",
    "conciseness = _get_metric(\"ragas.metrics.collections\", \"conciseness\") or _get_metric(\"ragas.metrics\", \"conciseness\")\n",
    "harmfulness = _get_metric(\"ragas.metrics.collections\", \"harmfulness\") or _get_metric(\"ragas.metrics\", \"harmfulness\")\n",
    "\n",
    "# Import other necessary libraries\n",
    "from src.core.bootstrap import get_container\n",
    "from src.application.use_cases.ask_question_hybrid import AskQuestionHybridUseCase\n",
    "\n",
    "# Initialize the container\n",
    "container = get_container()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Understanding RAG Evaluation Metrics\n",
    "\n",
    "RAG systems require specialized metrics that evaluate not just the final answer, but the entire pipeline including retrieval and generation components.\n",
    "\n",
    "### Key Metrics:\n",
    "- **Faithfulness**: Does the answer stick to the provided context?\n",
    "- **Answer Relevancy**: Is the answer relevant to the question?\n",
    "- **Context Precision**: Are the retrieved chunks relevant to the question?\n",
    "- **Context Recall**: Does the retrieved context contain the answer?\n",
    "- **Harmfulness**: Does the system generate harmful content?\n",
    "- **Conciseness**: Is the answer appropriately concise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Creating an Evaluation Dataset\n",
    "\n",
    "To evaluate our RAG system, we need a dataset with questions, ground truths, and contexts. In practice, you'd create this from your actual documents and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:07:02.643685Z",
     "iopub.status.busy": "2026-01-31T14:07:02.641873Z",
     "iopub.status.idle": "2026-01-31T14:07:02.685308Z",
     "shell.execute_reply": "2026-01-31T14:07:02.683347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 3 samples\n"
     ]
    }
   ],
   "source": [
    "# Sample evaluation data - in practice, you'd generate this from your documents\n",
    "sample_data = {\n",
    "    \"question\": [\n",
    "        \"What is the main advantage of hybrid search in RAG systems?\",\n",
    "        \"How does chunking affect RAG performance?\",\n",
    "        \"What is the purpose of re-ranking in RAG?\"\n",
    "    ],\n",
    "    \"answer\": [\n",
    "        \"Hybrid search combines vector and keyword search to improve retrieval accuracy by leveraging both semantic and lexical matching.\",\n",
    "        \"Chunking affects RAG performance by determining how documents are split, impacting both retrieval relevance and context preservation.\",\n",
    "        \"Re-ranking improves the relevance of initially retrieved documents by applying a secondary ranking algorithm.\"\n",
    "    ],\n",
    "    \"contexts\": [\n",
    "        [\n",
    "            \"Hybrid search combines vector search with keyword search to leverage the strengths of both approaches. Vector search captures semantic meaning, while keyword search handles exact matches and named entities.\",\n",
    "            \"The effectiveness of retrieval in RAG systems depends on how well the search balances semantic and lexical matching.\",\n",
    "            \"Vector databases store embeddings that represent semantic meaning of text fragments.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Chunking strategies determine how documents are divided for storage in the vector database. Different strategies affect retrieval quality.\",\n",
    "            \"Large chunks may contain more context but could dilute relevance. Small chunks increase precision but may lose important context.\",\n",
    "            \"Optimal chunk size varies depending on the document type and query patterns.\"\n",
    "        ],\n",
    "        [\n",
    "            \"Re-ranking is a post-processing step that re-orders initially retrieved documents based on a more sophisticated relevance model.\",\n",
    "            \"Initial retrieval might use fast but less accurate methods, while re-ranking applies more precise but computationally expensive models.\",\n",
    "            \"Cross-encoder models are commonly used for re-ranking due to their effectiveness.\"\n",
    "        ]\n",
    "    ],\n",
    "    \"ground_truth\": [\n",
    "        \"Hybrid search combines vector and keyword search to improve retrieval accuracy by leveraging both semantic and lexical matching.\",\n",
    "        \"Chunking affects RAG performance by determining how documents are split, impacting both retrieval relevance and context preservation.\",\n",
    "        \"Re-ranking improves the relevance of initially retrieved documents by applying a secondary ranking algorithm.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert to HuggingFace Dataset format\n",
    "eval_dataset = Dataset.from_dict(sample_data)\n",
    "print(f\"Dataset created with {eval_dataset.num_rows} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Running the Evaluation\n",
    "\n",
    "Now we'll run the evaluation using Ragas metrics to assess our RAG system's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:07:02.688344Z",
     "iopub.status.busy": "2026-01-31T14:07:02.688344Z",
     "iopub.status.idle": "2026-01-31T14:07:03.088384Z",
     "shell.execute_reply": "2026-01-31T14:07:03.087382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation failed with error: All metrics must be initialised metric objects, e.g: metrics=[BleuScore(), AspectCritic()]\n",
      "This is expected if the required models/api keys are not configured\n"
     ]
    }
   ],
   "source": [
    "# Define the metrics to evaluate\n",
    "metrics = [m for m in [\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    conciseness,\n",
    "    harmfulness,\n",
    "] if m is not None]\n",
    "\n",
    "# Run the evaluation\n",
    "try:\n",
    "    results = evaluate(\n",
    "        dataset=eval_dataset,\n",
    "        metrics=metrics\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluation Results:\")\n",
    "    print(results)\n",
    "    \n",
    "    # Convert to a more readable format\n",
    "    results_df = pd.DataFrame([results])\n",
    "    print(\"\\nResults as DataFrame:\")\n",
    "    print(results_df.T)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Evaluation failed with error: {e}\")\n",
    "    print(\"This is expected if the required models/api keys are not configured\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ A/B Testing Different Configurations\n",
    "\n",
    "Let's compare the performance of different RAG configurations to understand the impact of various parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:07:03.092383Z",
     "iopub.status.busy": "2026-01-31T14:07:03.091388Z",
     "iopub.status.idle": "2026-01-31T14:07:03.098936Z",
     "shell.execute_reply": "2026-01-31T14:07:03.097382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running A/B test between different configurations...\n",
      "Configuration 1: Vector-only search\n",
      "Configuration 2: Hybrid search (vector + keyword)\n",
      "Configuration 3: Hybrid search with re-ranking\n",
      "\n",
      "Comparison would show:\n",
      "- Hybrid search typically improves context recall\n",
      "- Re-ranking typically improves context precision\n",
      "- Vector-only might be faster but less accurate\n"
     ]
    }
   ],
   "source": [
    "# Example: Comparing hybrid search vs vector-only search\n",
    "def run_ab_test():\n",
    "    # This is a conceptual example - actual implementation would require\n",
    "    # different configurations of your RAG system\n",
    "    \n",
    "    print(\"Running A/B test between different configurations...\")\n",
    "    \n",
    "    # Configuration 1: Vector-only search\n",
    "    print(\"Configuration 1: Vector-only search\")\n",
    "    # results_config1 = evaluate_with_config(config=\"vector_only\")\n",
    "    \n",
    "    # Configuration 2: Hybrid search (vector + keyword)\n",
    "    print(\"Configuration 2: Hybrid search (vector + keyword)\")\n",
    "    # results_config2 = evaluate_with_config(config=\"hybrid\")\n",
    "    \n",
    "    # Configuration 3: Hybrid with re-ranking\n",
    "    print(\"Configuration 3: Hybrid search with re-ranking\")\n",
    "    # results_config3 = evaluate_with_config(config=\"hybrid_with_rerank\")\n",
    "    \n",
    "    print(\"\\nComparison would show:\")\n",
    "    print(\"- Hybrid search typically improves context recall\")\n",
    "    print(\"- Re-ranking typically improves context precision\")\n",
    "    print(\"- Vector-only might be faster but less accurate\")\n",
    "\n",
    "run_ab_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Interpreting Results\n",
    "\n",
    "Understanding what the metrics tell us about our RAG system's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:07:03.100935Z",
     "iopub.status.busy": "2026-01-31T14:07:03.100935Z",
     "iopub.status.idle": "2026-01-31T14:07:03.107876Z",
     "shell.execute_reply": "2026-01-31T14:07:03.106869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpretation of RAG Evaluation Results:\n",
      "=========================================\n",
      "Faithfulness: 0.850\n",
      "  ‚úì Good: Answers are well-grounded in provided context\n",
      "Answer Relevancy: 0.780\n",
      "  ‚úì Good: Answers are relevant to the questions\n",
      "Context Precision: 0.720\n",
      "  ‚úì Good: Retrieved context is relevant to the question\n",
      "Context Recall: 0.810\n",
      "  ‚úì Good: Retrieved context contains information needed to answer\n"
     ]
    }
   ],
   "source": [
    "def interpret_results(results):\n",
    "    \"\"\"\n",
    "    Interpret evaluation results and provide insights\n",
    "    \"\"\"\n",
    "    print(\"Interpretation of RAG Evaluation Results:\")\n",
    "    print(\"=========================================\")\n",
    "    \n",
    "    if 'faithfulness' in results:\n",
    "        faithfulness_score = results['faithfulness']\n",
    "        print(f\"Faithfulness: {faithfulness_score:.3f}\")\n",
    "        if faithfulness_score > 0.8:\n",
    "            print(\"  ‚úì Good: Answers are well-grounded in provided context\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Answers may contain hallucinations\")\n",
    "    \n",
    "    if 'answer_relevancy' in results:\n",
    "        relevancy_score = results['answer_relevancy']\n",
    "        print(f\"Answer Relevancy: {relevancy_score:.3f}\")\n",
    "        if relevancy_score > 0.7:\n",
    "            print(\"  ‚úì Good: Answers are relevant to the questions\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Answers may be off-topic\")\n",
    "            \n",
    "    if 'context_precision' in results:\n",
    "        precision_score = results['context_precision']\n",
    "        print(f\"Context Precision: {precision_score:.3f}\")\n",
    "        if precision_score > 0.7:\n",
    "            print(\"  ‚úì Good: Retrieved context is relevant to the question\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Retrieved context may contain irrelevant information\")\n",
    "            \n",
    "    if 'context_recall' in results:\n",
    "        recall_score = results['context_recall']\n",
    "        print(f\"Context Recall: {recall_score:.3f}\")\n",
    "        if recall_score > 0.7:\n",
    "            print(\"  ‚úì Good: Retrieved context contains information needed to answer\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è  Improvement needed: Important information may be missing from retrieved context\")\n",
    "\n",
    "# Example interpretation (using placeholder values since we didn't run actual eval)\n",
    "example_results = {\n",
    "    'faithfulness': 0.85,\n",
    "    'answer_relevancy': 0.78,\n",
    "    'context_precision': 0.72,\n",
    "    'context_recall': 0.81\n",
    "}\n",
    "\n",
    "interpret_results(example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Practical Tips for RAG Evaluation\n",
    "\n",
    "Based on the evaluation results, here are practical steps to improve your RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-31T14:07:03.112041Z",
     "iopub.status.busy": "2026-01-31T14:07:03.112041Z",
     "iopub.status.idle": "2026-01-31T14:07:03.118598Z",
     "shell.execute_reply": "2026-01-31T14:07:03.117082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improvement Recommendations:\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "def get_improvement_recommendations(results):\n",
    "    \"\"\"\n",
    "    Provide recommendations based on evaluation results\n",
    "    \"\"\"\n",
    "    print(\"Improvement Recommendations:\")\n",
    "    print(\"===========================\")\n",
    "    \n",
    "    if results.get('faithfulness', 1.0) < 0.8:\n",
    "        print(\"‚Ä¢ Improve faithfulness:\")\n",
    "        print(\"  - Strengthen grounding mechanisms in prompts\")\n",
    "        print(\"  - Implement self-correction/validation steps\")\n",
    "        print(\"  - Use more constrained generation parameters\")\n",
    "    \n",
    "    if results.get('context_precision', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve context precision:\")\n",
    "        print(\"  - Enhance re-ranking mechanisms\")\n",
    "        print(\"  - Fine-tune embedding models for your domain\")\n",
    "        print(\"  - Implement better query expansion techniques\")\n",
    "    \n",
    "    if results.get('context_recall', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve context recall:\")\n",
    "        print(\"  - Implement hybrid search (vector + keyword)\")\n",
    "        print(\"  - Increase retrieval depth (top-k)\")\n",
    "        print(\"  - Improve query expansion\")\n",
    "    \n",
    "    if results.get('answer_relevancy', 1.0) < 0.7:\n",
    "        print(\"‚Ä¢ Improve answer relevancy:\")\n",
    "        print(\"  - Refine prompt engineering\")\n",
    "        print(\"  - Implement query classification and routing\")\n",
    "        print(\"  - Use more appropriate LLM for your domain\")\n",
    "\n",
    "get_improvement_recommendations(example_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "1. **Comprehensive Evaluation**: RAG systems need evaluation across multiple dimensions, not just answer accuracy\n",
    "2. **Metric Selection**: Choose metrics that align with your specific use case and success criteria\n",
    "3. **Iterative Improvement**: Use evaluation results to identify bottlenecks and guide improvements\n",
    "4. **Baseline Establishment**: Establish baselines to measure improvement from changes\n",
    "5. **Realistic Testing**: Use test data that represents real-world usage patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
