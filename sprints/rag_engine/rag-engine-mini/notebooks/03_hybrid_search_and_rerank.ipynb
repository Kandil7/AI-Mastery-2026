{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”€ Hybrid Search & Reranking\n",
                "\n",
                "> **Educational Notebook 03**: Deep dive into hybrid retrieval with RRF fusion.\n",
                "\n",
                "---\n",
                "\n",
                "## ðŸ“‹ Why Hybrid Search?\n",
                "\n",
                "| Method | Strengths | Weaknesses |\n",
                "|--------|-----------|------------|\n",
                "| **Vector** | Semantic similarity, synonyms | Misses exact matches |\n",
                "| **Keyword** | Exact matches, names, numbers | Misses paraphrases |\n",
                "| **Hybrid** | Best of both | More complex |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "from src.domain.entities import Chunk, TenantId, DocumentId\n",
                "from src.application.services.fusion import rrf_fusion, weighted_fusion\n",
                "from src.application.services.scoring import ScoredChunk"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š Simulating Vector vs Keyword Results\n",
                "\n",
                "Let's create sample results from both search methods."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper to create chunks\n",
                "def make_chunk(id: str, text: str) -> Chunk:\n",
                "    return Chunk(\n",
                "        id=id,\n",
                "        tenant_id=TenantId(\"demo\"),\n",
                "        document_id=DocumentId(\"doc\"),\n",
                "        text=text\n",
                "    )\n",
                "\n",
                "# Simulated vector search results (semantic similarity)\n",
                "vector_results = [\n",
                "    ScoredChunk(make_chunk(\"v1\", \"Machine learning enables computers to learn from data.\"), 0.92),\n",
                "    ScoredChunk(make_chunk(\"v2\", \"AI systems can improve through experience.\"), 0.88),\n",
                "    ScoredChunk(make_chunk(\"v3\", \"Deep learning uses neural networks.\"), 0.85),\n",
                "    ScoredChunk(make_chunk(\"v4\", \"Data science involves statistical analysis.\"), 0.80),\n",
                "]\n",
                "\n",
                "# Simulated keyword search results (BM25/FTS)\n",
                "keyword_results = [\n",
                "    ScoredChunk(make_chunk(\"k1\", \"Machine learning algorithms are widely used.\"), 15.2),\n",
                "    ScoredChunk(make_chunk(\"v1\", \"Machine learning enables computers to learn from data.\"), 12.8),  # Same as v1!\n",
                "    ScoredChunk(make_chunk(\"k2\", \"The term 'machine learning' was coined in 1959.\"), 10.5),\n",
                "    ScoredChunk(make_chunk(\"k3\", \"Learning rate is an important hyperparameter.\"), 8.3),\n",
                "]\n",
                "\n",
                "print(\"Vector Results:\")\n",
                "for r in vector_results:\n",
                "    print(f\"  {r.chunk.id}: {r.score:.2f} - {r.chunk.text[:50]}...\")\n",
                "\n",
                "print(\"\\nKeyword Results:\")\n",
                "for r in keyword_results:\n",
                "    print(f\"  {r.chunk.id}: {r.score:.2f} - {r.chunk.text[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ”€ RRF Fusion\n",
                "\n",
                "**Reciprocal Rank Fusion** merges results without needing to calibrate scores:\n",
                "\n",
                "$$\\text{RRF\\_score}(d) = \\sum_{r \\in R} \\frac{1}{k + \\text{rank}_r(d)}$$\n",
                "\n",
                "Where:\n",
                "- $k$ is a constant (default 60)\n",
                "- $\\text{rank}_r(d)$ is the rank of document $d$ in result list $r$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply RRF fusion\n",
                "fused = rrf_fusion(\n",
                "    vector_hits=vector_results,\n",
                "    keyword_hits=keyword_results,\n",
                "    k=60,\n",
                "    out_limit=10\n",
                ")\n",
                "\n",
                "print(\"RRF Fused Results:\")\n",
                "print(\"=\" * 60)\n",
                "for i, r in enumerate(fused, 1):\n",
                "    print(f\"{i}. {r.chunk.id}: RRF={r.score:.4f}\")\n",
                "    print(f\"   {r.chunk.text[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Understanding RRF Scores\n",
                "\n",
                "Notice that `v1` appears in BOTH result lists, so it gets boosted:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Manual RRF calculation for v1\n",
                "k = 60\n",
                "\n",
                "# v1 is rank 1 in vector results\n",
                "v1_vector_contribution = 1 / (k + 1)\n",
                "\n",
                "# v1 is rank 2 in keyword results\n",
                "v1_keyword_contribution = 1 / (k + 2)\n",
                "\n",
                "v1_total = v1_vector_contribution + v1_keyword_contribution\n",
                "\n",
                "print(f\"v1 RRF Score Breakdown:\")\n",
                "print(f\"  Vector (rank 1): 1/(60+1) = {v1_vector_contribution:.4f}\")\n",
                "print(f\"  Keyword (rank 2): 1/(60+2) = {v1_keyword_contribution:.4f}\")\n",
                "print(f\"  Total: {v1_total:.4f}\")\n",
                "\n",
                "# Compare to k1 (only in keyword, rank 1)\n",
                "k1_total = 1 / (k + 1)\n",
                "print(f\"\\nk1 RRF Score: {k1_total:.4f} (only in keyword list)\")\n",
                "print(f\"\\nv1 is higher because it appears in BOTH lists!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ¯ Cross-Encoder Reranking\n",
                "\n",
                "After fusion, we apply a Cross-Encoder to rerank by actual relevance.\n",
                "\n",
                "**How it works:**\n",
                "1. Take (query, passage) pairs\n",
                "2. Cross-Encoder scores each pair\n",
                "3. Sort by Cross-Encoder score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulate Cross-Encoder scoring\n",
                "# In production, use CrossEncoderReranker from src/adapters/rerank/\n",
                "\n",
                "query = \"What is machine learning?\"\n",
                "\n",
                "# Simulated Cross-Encoder scores (would come from the model)\n",
                "cross_encoder_scores = {\n",
                "    \"v1\": 0.95,  # Best match for the query\n",
                "    \"k1\": 0.88,\n",
                "    \"v2\": 0.72,\n",
                "    \"k2\": 0.85,  # Historical context, decent match\n",
                "    \"v3\": 0.65,\n",
                "    \"k3\": 0.40,  # Poor match - about learning rate\n",
                "    \"v4\": 0.50,\n",
                "}\n",
                "\n",
                "# Rerank by Cross-Encoder score\n",
                "reranked = sorted(\n",
                "    fused,\n",
                "    key=lambda x: cross_encoder_scores.get(x.chunk.id, 0),\n",
                "    reverse=True\n",
                ")[:5]  # Top 5\n",
                "\n",
                "print(f\"Query: '{query}'\")\n",
                "print(\"\\nAfter Cross-Encoder Reranking:\")\n",
                "print(\"=\" * 60)\n",
                "for i, r in enumerate(reranked, 1):\n",
                "    ce_score = cross_encoder_scores.get(r.chunk.id, 0)\n",
                "    print(f\"{i}. {r.chunk.id}: CE={ce_score:.2f}\")\n",
                "    print(f\"   {r.chunk.text[:60]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ˆ Comparison: Before vs After Reranking\n",
                "\n",
                "Notice how reranking improves precision by demoting less relevant results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"BEFORE Reranking (RRF order):\")\n",
                "for i, r in enumerate(fused[:5], 1):\n",
                "    ce_score = cross_encoder_scores.get(r.chunk.id, 0)\n",
                "    print(f\"  {i}. {r.chunk.id} (CE={ce_score:.2f})\")\n",
                "\n",
                "print(\"\\nAFTER Reranking (CE order):\")\n",
                "for i, r in enumerate(reranked, 1):\n",
                "    ce_score = cross_encoder_scores.get(r.chunk.id, 0)\n",
                "    print(f\"  {i}. {r.chunk.id} (CE={ce_score:.2f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“š Key Takeaways\n",
                "\n",
                "1. **Hybrid Search** combines semantic (vector) and lexical (keyword) retrieval\n",
                "2. **RRF Fusion** merges results without requiring score calibration\n",
                "3. **Cross-Encoder Reranking** improves precision by scoring (query, passage) pairs\n",
                "4. Items appearing in BOTH result lists get boosted by RRF\n",
                "5. Reranking is crucial for production RAG quality\n",
                "\n",
                "---\n",
                "\n",
                "ðŸŽ‰ **Congratulations!** You've completed the RAG Engine Mini educational notebooks."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}