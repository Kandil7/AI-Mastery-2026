{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 1, Day 2: RAG Evaluation Pipeline with Ragas\n",
                "\n",
                "**Sprint**: Week 1 (LLM & RAG Mastery)\n",
                "**Goal**: Build an evaluation pipeline to measure **Context Recall** and **Faithfulness**.\n",
                "\n",
                "## ðŸŽ¯ Problem Statement\n",
                "How do we know if our RAG system is actually working? \n",
                "- **Context Recall**: Did we retrieve the relevant information?\n",
                "- **Faithfulness**: Is the generated answer true to the retrieved context?\n",
                "\n",
                "We will use [Ragas](https://github.com/explodinggradients/ragas), a framework that uses an LLM (evaluation-as-a-judge) to score these metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Add project root to path\n",
                "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))\n",
                "if project_root not in sys.path:\n",
                "    sys.path.append(project_root)\n",
                "\n",
                "# Load environment variables (OPENAI_API_KEY is required for Ragas by default)\n",
                "load_dotenv()\n",
                "\n",
                "import pandas as pd\n",
                "from datasets import Dataset\n",
                "from ragas import evaluate\n",
                "from ragas.metrics import (\n",
                "    context_precision,\n",
                "    context_recall,\n",
                "    faithfulness,\n",
                "    answer_relevancy,\n",
                ")\n",
                "\n",
                "# Import our RAG components\n",
                "from src.retrieval.retrieval import HybridRetriever, Document\n",
                "from src.llm.rag import RAGModel, RetrievalStrategy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§ª 1. Setup Data & RAG System\n",
                "\n",
                "We'll use a small synthetic dataset for this demonstration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Define Knowledge Base\n",
                "docs = [\n",
                "    Document(id=\"1\", content=\"The transformer architecture was introduced in the paper 'Attention Is All You Need' by Vaswani et al. in 2017.\"),\n",
                "    Document(id=\"2\", content=\"BERT (Bidirectional Encoder Representations from Transformers) is designed to pre-train deep bidirectional representations from unlabeled text.\"),\n",
                "    Document(id=\"3\", content=\"GPT-3 is an autoregressive language model that uses deep learning to produce human-like text.\"),\n",
                "    Document(id=\"4\", content=\"RAG (Retrieval-Augmented Generation) combines an information retrieval component with a text generator model.\"),\n",
                "]\n",
                "\n",
                "# 2. Initialize RAG Model (Hybrid)\n",
                "rag = RAGModel(\n",
                "    retriever_strategy=RetrievalStrategy.HYBRID,\n",
                "    generator_model=\"gpt2\", # Using GPT2 for local demo speed, traditionally you'd use a better model\n",
                "    dense_weight=0.7,\n",
                "    sparse_weight=0.3\n",
                ")\n",
                "\n",
                "print(\"Indexing documents...\")\n",
                "rag.add_documents(docs)\n",
                "print(\"Done.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ 2. Create Evaluation Dataset\n",
                "\n",
                "Ragas expects a dataset with columns: `['question', 'answer', 'contexts', 'ground_truth']`.\n",
                "\n",
                "- **question**: The query.\n",
                "- **answer**: The generated answer from our RAG pipeline.\n",
                "- **contexts**: List of strings (retrieved document contents).\n",
                "- **ground_truth**: List of strings (the correct answer)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "eval_questions = [\n",
                "    \"When was the transformer architecture introduced?\",\n",
                "    \"What does BERT stand for?\",\n",
                "    \"How does RAG work?\"\n",
                "]\n",
                "\n",
                "eval_ground_truths = [\n",
                "    [\"The transformer architecture was introduced in 2017.\"],\n",
                "    [\"BERT stands for Bidirectional Encoder Representations from Transformers.\"],\n",
                "    [\"RAG combines retrieval with text generation.\"]\n",
                "]\n",
                "\n",
                "# Run Inference to collect 'answer' and 'contexts'\n",
                "answers = []\n",
                "contexts = []\n",
                "\n",
                "print(\"Running inference...\")\n",
                "for q in eval_questions:\n",
                "    result = rag.query(q, k=1)\n",
                "    \n",
                "    # Store the generated answer\n",
                "    answers.append(result['response'])\n",
                "    \n",
                "    # Store the retrieved contexts (list of strings)\n",
                "    retrieved_contents = [d['content'] for d in result['retrieved_documents']]\n",
                "    contexts.append(retrieved_contents)\n",
                "\n",
                "# Create Dataset\n",
                "data_dict = {\n",
                "    \"question\": eval_questions,\n",
                "    \"answer\": answers,\n",
                "    \"contexts\": contexts,\n",
                "    \"ground_truth\": eval_ground_truths\n",
                "}\n",
                "\n",
                "eval_dataset = Dataset.from_dict(data_dict)\n",
                "print(\"Dataset created.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“Š 3. Run Evaluation with Ragas\n",
                "\n",
                "> **Note**: This step requires `OPENAI_API_KEY` to be set in your environment (or `.env` file), as Ragas utilizes an LLM to act as the judge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    results = evaluate(\n",
                "        eval_dataset,\n",
                "        metrics=[\n",
                "            context_recall,\n",
                "            faithfulness,\n",
                "            answer_relevancy,\n",
                "            context_precision,\n",
                "        ],\n",
                "    )\n",
                "    \n",
                "    print(\"\\nEvaluation Results:\")\n",
                "    print(results)\n",
                "    \n",
                "    # Convert to Pandas for nicer display\n",
                "    df_results = results.to_pandas()\n",
                "    display(df_results)\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"Evaluation failed (likely due to missing OpenAI Key): {e}\")\n",
                "    print(\"Ensure OPENAI_API_KEY is set in your .env file.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Analysis & Interview Talking Points\n",
                "\n",
                "### **Metric Logic**\n",
                "1. **Faithfulness**: If low, the model is hallucinating (making things up not in context).\n",
                "   - *Fix*: Reduce temperature, improve prompt engineering (\"Answer only using the provided context\").\n",
                "2. **Context Recall**: If low, the retriever isn't finding the right documents.\n",
                "   - *Fix*: Tune Hybrid parameters (alpha), improve chunking strategy, or use a better embedding model.\n",
                "\n",
                "### **Interview Answer**\n",
                "\"In my production RAG system, I set up an automated evaluation pipeline using Ragas directly in my CI/CD. I specifically monitored **Context Recall** to validate my retrieval strategy and **Faithfulness** to ensure the LLM wasn't hallucinating. When Recall dropped, I knew I needed to adjust my hybrid search weights or re-chunk my documents.\""
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}