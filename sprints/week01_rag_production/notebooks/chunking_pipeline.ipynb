{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1023e23a",
   "metadata": {},
   "source": [
    "# Chunking Pipeline (OCR → Children)\n",
    "\n",
    "Notebook to run the current chunking stack end-to-end so you can tune chunk sizes/overlaps and inspect outputs before indexing.\n",
    "\n",
    "What it covers:\n",
    "- Loads OCR/plain text from a file or inline sample.\n",
    "- Uses the production chunkers (`Semantic`, `Recursive`, `Markdown`, `Code`, or `Auto`).\n",
    "- Emits chunk stats (counts, token lengths) and previews.\n",
    "- Optional export to JSONL for quick indexing tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18cb4a",
   "metadata": {},
   "source": [
    "## How to use\n",
    "1) Point `DOC_PATH` to your OCR text file (UTF-8) **or** edit `RAW_TEXT`.\n",
    "2) Choose `STRATEGY` (`semantic` recommended for prose; `auto` lets the analyzer decide).\n",
    "3) Tweak `CHUNK_SIZE`, `CHUNK_OVERLAP`, `MIN_CHUNK_SIZE` (defaults 550/100/120 per design).\n",
    "4) Run the pipeline cell and inspect stats/chunks; export JSONL if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.chunking.config import ChunkingConfig, ChunkingStrategy\n",
    "from src.chunking.factory import ChunkerFactory\n",
    "from src.chunking.tokenizer import build_counter\n",
    "from src.retrieval import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters you can tweak for experiments\n",
    "DOC_PATH = \"\"  # e.g., \"../data/ocr_sample.txt\". Leave empty to use RAW_TEXT below.\n",
    "\n",
    "RAW_TEXT = \"\"\"\\\n",
    "هذا نص عربي تجريبي يمثل صفحة OCR واحدة مع أكثر من جملة لاختبار التقسيم.\n",
    "يمكنك استبداله بنصك الفعلي أو ربط ملفك في DOC_PATH بالأعلى.\n",
    "يوجد أسطر متعددة للتأكد من عمل تقسيم الفقرات والجمل كما نتوقع.\n",
    "\"\"\"\n",
    "\n",
    "STRATEGY = \"semantic\"  # semantic | recursive | paragraph | sentence | code | markdown | auto\n",
    "CHUNK_SIZE = 550\n",
    "CHUNK_OVERLAP = 100\n",
    "MIN_CHUNK_SIZE = 120\n",
    "MAX_DOC_CHARS = 2_000_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8478f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "\n",
    "\n",
    "def load_text(doc_path: str, raw_text: str) -> str:\n",
    "    \"\"\"Return the text to chunk (file takes precedence).\"\"\"\n",
    "    if doc_path:\n",
    "        path = Path(doc_path).expanduser()\n",
    "        if not path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {path}\")\n",
    "        return path.read_text(encoding=\"utf-8\")\n",
    "    return raw_text.strip()\n",
    "\n",
    "\n",
    "def make_chunker(strategy: str, **overrides):\n",
    "    enum = ChunkingStrategy[strategy.upper()]\n",
    "    cfg = ChunkingConfig(\n",
    "        chunk_size=overrides.get(\"chunk_size\", CHUNK_SIZE),\n",
    "        chunk_overlap=overrides.get(\"chunk_overlap\", CHUNK_OVERLAP),\n",
    "        min_chunk_size=overrides.get(\"min_chunk_size\", MIN_CHUNK_SIZE),\n",
    "        strategy=enum,\n",
    "        max_document_chars=overrides.get(\"max_document_chars\", MAX_DOC_CHARS),\n",
    "    )\n",
    "    chunker = ChunkerFactory.create_chunker(enum, cfg)\n",
    "    return chunker, cfg\n",
    "\n",
    "\n",
    "def run_chunking(text: str, strategy: str = STRATEGY, **overrides):\n",
    "    chunker, cfg = make_chunker(strategy, **overrides)\n",
    "    doc = Document(id=\"doc-1\", content=text, doc_type=\"text/ocr\", metadata={\"source\": \"notebook\"})\n",
    "    chunks = chunker.chunk_document(doc)\n",
    "    counter = build_counter(prefer_tiktoken=True)\n",
    "    token_counts = [counter.count(c.content) for c in chunks]\n",
    "    stats = {\n",
    "        \"num_chunks\": len(chunks),\n",
    "        \"min_tokens\": min(token_counts) if token_counts else 0,\n",
    "        \"max_tokens\": max(token_counts) if token_counts else 0,\n",
    "        \"mean_tokens\": sum(token_counts) / len(token_counts) if token_counts else 0,\n",
    "    }\n",
    "    return chunks, cfg, stats, token_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28763641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chunking pipeline\n",
    "text = load_text(DOC_PATH, RAW_TEXT)\n",
    "chunks, cfg, stats, token_counts = run_chunking(\n",
    "    text,\n",
    "    strategy=STRATEGY,\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    "    min_chunk_size=MIN_CHUNK_SIZE,\n",
    "    max_document_chars=MAX_DOC_CHARS,\n",
    ")\n",
    "\n",
    "print(f\"Strategy: {STRATEGY} | chunks={stats['num_chunks']} | min={stats['min_tokens']:.1f} | max={stats['max_tokens']:.1f} | mean={stats['mean_tokens']:.1f}\")\n",
    "print(f\"chunk_size={cfg.chunk_size}, overlap={cfg.chunk_overlap}, min_chunk_size={cfg.min_chunk_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc067cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a few chunks\n",
    "counter = build_counter(prefer_tiktoken=True)\n",
    "preview_limit = 5\n",
    "for i, ch in enumerate(chunks[:preview_limit]):\n",
    "    print(f\"\n",
    "Chunk {i} | tokens≈{counter.count(ch.content)} | span=({ch.metadata.get('chunk_start')},{ch.metadata.get('chunk_end')})\")\n",
    "    print(ch.content[:400].strip())\n",
    "    print(\"-\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31fa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Export to JSONL for quick indexing experiments\n",
    "EXPORT = False\n",
    "EXPORT_PATH = Path(\"../tmp/chunks_sample.jsonl\")\n",
    "\n",
    "if EXPORT:\n",
    "    EXPORT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with EXPORT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for ch in chunks:\n",
    "            rec = {\"id\": ch.id, \"content\": ch.content, \"metadata\": ch.metadata}\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\n",
    "\")\n",
    "    print(f\"Saved {len(chunks)} chunks to {EXPORT_PATH}\")\n",
    "else:\n",
    "    print(\"Set EXPORT = True to write JSONL to disk.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
