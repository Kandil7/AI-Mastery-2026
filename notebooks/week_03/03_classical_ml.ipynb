{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 03: Classical Machine Learning\n",
                "\n",
                "Implementing classic ML algorithms from scratch.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Implement linear/logistic regression\n",
                "2. Build decision trees\n",
                "3. Understand ensemble methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import Tuple, List"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Linear Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LinearRegression:\n",
                "    \"\"\"\n",
                "    Linear Regression using Normal Equation and Gradient Descent.\n",
                "    \n",
                "    Model: y = X @ w + b\n",
                "    Loss: MSE = (1/n) * Σ(y - ŷ)²\n",
                "    \"\"\"\n",
                "    def __init__(self, method: str = 'normal'):\n",
                "        self.method = method\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "    \n",
                "    def fit_normal_equation(self, X: np.ndarray, y: np.ndarray):\n",
                "        X_b = np.c_[np.ones(X.shape[0]), X]\n",
                "        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
                "        self.bias = theta[0]\n",
                "        self.weights = theta[1:]\n",
                "    \n",
                "    def fit_gradient_descent(self, X: np.ndarray, y: np.ndarray, lr: float = 0.01, epochs: int = 1000):\n",
                "        n_samples, n_features = X.shape\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0.0\n",
                "        for _ in range(epochs):\n",
                "            y_pred = X @ self.weights + self.bias\n",
                "            error = y - y_pred\n",
                "            self.weights += lr * (2/n_samples) * X.T @ error\n",
                "            self.bias += lr * (2/n_samples) * np.sum(error)\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray, **kwargs):\n",
                "        if self.method == 'normal':\n",
                "            self.fit_normal_equation(X, y)\n",
                "        else:\n",
                "            self.fit_gradient_descent(X, y, **kwargs)\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        return X @ self.weights + self.bias\n",
                "    \n",
                "    def score(self, X: np.ndarray, y: np.ndarray) -> float:\n",
                "        y_pred = self.predict(X)\n",
                "        ss_res = np.sum((y - y_pred) ** 2)\n",
                "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
                "        return 1 - ss_res / ss_tot"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Linear Regression\n",
                "np.random.seed(42)\n",
                "X = np.random.randn(100, 3)\n",
                "true_weights = np.array([2, -1, 0.5])\n",
                "y = X @ true_weights + 3 + np.random.randn(100) * 0.1\n",
                "\n",
                "model = LinearRegression(method='normal')\n",
                "model.fit(X, y)\n",
                "print(f\"Learned weights: {model.weights}\")\n",
                "print(f\"Learned bias: {model.bias:.4f}\")\n",
                "print(f\"R² score: {model.score(X, y):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LogisticRegression:\n",
                "    \"\"\"\n",
                "    Logistic Regression for binary classification.\n",
                "    Model: p(y=1|x) = σ(x @ w + b)\n",
                "    \"\"\"\n",
                "    def __init__(self, lr: float = 0.01, epochs: int = 1000):\n",
                "        self.lr = lr\n",
                "        self.epochs = epochs\n",
                "    \n",
                "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
                "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
                "        n_samples, n_features = X.shape\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0.0\n",
                "        \n",
                "        for epoch in range(self.epochs):\n",
                "            z = X @ self.weights + self.bias\n",
                "            y_pred = self.sigmoid(z)\n",
                "            \n",
                "            error = y_pred - y\n",
                "            dw = (1/n_samples) * X.T @ error\n",
                "            db = (1/n_samples) * np.sum(error)\n",
                "            \n",
                "            self.weights -= self.lr * dw\n",
                "            self.bias -= self.lr * db\n",
                "    \n",
                "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
                "        return self.sigmoid(X @ self.weights + self.bias)\n",
                "    \n",
                "    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
                "        return (self.predict_proba(X) >= threshold).astype(int)\n",
                "    \n",
                "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
                "        return np.mean(self.predict(X) == y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import make_classification\n",
                "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
                "\n",
                "model = LogisticRegression(lr=0.1, epochs=500)\n",
                "model.fit(X, y)\n",
                "print(f\"Accuracy: {model.accuracy(X, y):.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Decision Trees"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Node:\n",
                "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
                "        self.feature = feature\n",
                "        self.threshold = threshold\n",
                "        self.left = left\n",
                "        self.right = right\n",
                "        self.value = value\n",
                "\n",
                "class DecisionTree:\n",
                "    def __init__(self, max_depth: int = 10, min_samples_split: int = 2):\n",
                "        self.max_depth = max_depth\n",
                "        self.min_samples_split = min_samples_split\n",
                "        self.root = None\n",
                "    \n",
                "    def gini(self, y: np.ndarray) -> float:\n",
                "        if len(y) == 0: return 0\n",
                "        proportions = np.bincount(y) / len(y)\n",
                "        return 1 - np.sum(proportions ** 2)\n",
                "    \n",
                "    def information_gain(self, y: np.ndarray, left_idx: np.ndarray, right_idx: np.ndarray) -> float:\n",
                "        n = len(y)\n",
                "        if n == 0: return 0\n",
                "        n_left = len(left_idx)\n",
                "        n_right = len(right_idx)\n",
                "        return self.gini(y) - ((n_left/n)*self.gini(y[left_idx]) + (n_right/n)*self.gini(y[right_idx]))\n",
                "    \n",
                "    def best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[int, float]:\n",
                "        best_gain = -1\n",
                "        best_feature = None\n",
                "        best_threshold = None\n",
                "        \n",
                "        for feature in range(X.shape[1]):\n",
                "            thresholds = np.unique(X[:, feature])\n",
                "            for threshold in thresholds:\n",
                "                left_idx = np.where(X[:, feature] <= threshold)[0]\n",
                "                right_idx = np.where(X[:, feature] > threshold)[0]\n",
                "                if len(left_idx) == 0 or len(right_idx) == 0: continue\n",
                "                gain = self.information_gain(y, left_idx, right_idx)\n",
                "                if gain > best_gain:\n",
                "                    best_gain = gain\n",
                "                    best_feature = feature\n",
                "                    best_threshold = threshold\n",
                "        return best_feature, best_threshold\n",
                "    \n",
                "    def build_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
                "        n_samples = len(y)\n",
                "        n_classes = len(np.unique(y))\n",
                "        if depth >= self.max_depth or n_classes == 1 or n_samples < self.min_samples_split:\n",
                "            return Node(value=np.argmax(np.bincount(y)))\n",
                "        feature, threshold = self.best_split(X, y)\n",
                "        if feature is None:\n",
                "            return Node(value=np.argmax(np.bincount(y)))\n",
                "        left_idx = np.where(X[:, feature] <= threshold)[0]\n",
                "        right_idx = np.where(X[:, feature] > threshold)[0]\n",
                "        left_child = self.build_tree(X[left_idx], y[left_idx], depth + 1)\n",
                "        right_child = self.build_tree(X[right_idx], y[right_idx], depth + 1)\n",
                "        return Node(feature=feature, threshold=threshold, left=left_child, right=right_child)\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
                "        self.root = self.build_tree(X, y)\n",
                "    \n",
                "    def predict_sample(self, x: np.ndarray, node: Node) -> int:\n",
                "        if node.value is not None: return node.value\n",
                "        if x[node.feature] <= node.threshold: return self.predict_sample(x, node.left)\n",
                "        return self.predict_sample(x, node.right)\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        return np.array([self.predict_sample(x, self.root) for x in X])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Decision Tree\n",
                "X, y = make_classification(n_samples=200, n_features=5, random_state=42)\n",
                "tree = DecisionTree(max_depth=5)\n",
                "tree.fit(X, y)\n",
                "accuracy = np.mean(tree.predict(X) == y)\n",
                "print(f\"Decision Tree Accuracy: {accuracy:.2%}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Random Forest & KNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RandomForest:\n",
                "    def __init__(self, n_estimators: int = 10, max_depth: int = 10):\n",
                "        self.n_estimators = n_estimators\n",
                "        self.max_depth = max_depth\n",
                "        self.trees = []\n",
                "        self.feature_indices = []\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
                "        n_samples, n_features = X.shape\n",
                "        n_selected = int(np.sqrt(n_features))\n",
                "        for _ in range(self.n_estimators):\n",
                "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
                "            feat_idx = np.random.choice(n_features, n_selected, replace=False)\n",
                "            X_sub = X[indices][:, feat_idx]\n",
                "            y_sub = y[indices]\n",
                "            tree = DecisionTree(max_depth=self.max_depth)\n",
                "            tree.fit(X_sub, y_sub)\n",
                "            self.trees.append(tree)\n",
                "            self.feature_indices.append(feat_idx)\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        preds = np.zeros((len(X), self.n_estimators))\n",
                "        for i, (tree, idx) in enumerate(zip(self.trees, self.feature_indices)):\n",
                "            preds[:, i] = tree.predict(X[:, idx])\n",
                "        return np.array([np.argmax(np.bincount(row.astype(int))) for row in preds])\n",
                "\n",
                "class KNN:\n",
                "    def __init__(self, k: int = 5):\n",
                "        self.k = k\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
                "        self.X_train = X\n",
                "        self.y_train = y\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        preds = []\n",
                "        for x in X:\n",
                "            dists = [np.sqrt(np.sum((x - xt)**2)) for xt in self.X_train]\n",
                "            k_idx = np.argsort(dists)[:self.k]\n",
                "            k_labels = self.y_train[k_idx]\n",
                "            preds.append(np.argmax(np.bincount(k_labels)))\n",
                "        return np.array(preds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test RF and KNN\n",
                "rf = RandomForest(n_estimators=10, max_depth=5)\n",
                "rf.fit(X, y)\n",
                "print(f\"Random Forest Accuracy: {np.mean(rf.predict(X) == y):.2%}\")\n",
                "\n",
                "knn = KNN(k=5)\n",
                "knn.fit(X, y)\n",
                "print(f\"KNN Accuracy: {np.mean(knn.predict(X) == y):.2%}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}