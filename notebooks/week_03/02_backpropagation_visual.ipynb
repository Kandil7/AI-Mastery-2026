{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Backpropagation: Complete Visualization\n\n## Understanding the Heart of Deep Learning\n\nComplete mathematical derivation and visual walkthrough of backpropagation.\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nprint('\u2705 Ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chain Rule Foundation\n\n$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$$\n\n**Example**: Linear layer followed by ReLU\n\n$$z = Wx + b$$\n$$a = \\text{ReLU}(z) = \\max(0, z)$$\n$$L = \\frac{1}{2}(a - y)^2$$\n\n**Backward pass**:\n1. $\\frac{\\partial L}{\\partial a} = a - y$\n2. $\\frac{\\partial a}{\\partial z} = \\mathbb{1}(z > 0)$\n3. $\\frac{\\partial z}{\\partial W} = x^T$\n\n**Result**: $\\frac{\\partial L}{\\partial W} = (a-y) \\cdot \\mathbb{1}(z>0) \\cdot x^T$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple backprop example\nclass SimpleNetwork:\n    def forward(self, x, W, b):\n        self.x = x\n        self.z = np.dot(W, x) + b\n        self.a = np.maximum(0, self.z)  # ReLU\n        return self.a\n    \n    def backward(self, grad_output, W):\n        # Gradient through ReLU\n        grad_relu = grad_output * (self.z > 0)\n        # Gradient w.r.t weights\n        grad_W = np.outer(grad_relu, self.x)\n        grad_b = grad_relu\n        # Gradient w.r.t input (for chain)\n        grad_x = np.dot(W.T, grad_relu)\n        return grad_W, grad_b, grad_x\n\nprint('\u2705 Backprop illustrated!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Insights\n\n1. **Gradient flows backward** through computational graph\n2. **Local gradients multiply** (chain rule)\n3. **ReLU kills gradients** where z \u2264 0\n4. **Vanishing gradients**: Deep networks struggle (solved by ResNet)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}