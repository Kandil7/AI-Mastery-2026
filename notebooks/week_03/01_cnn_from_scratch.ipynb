{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Convolutional Neural Networks: Complete Guide\n\n## \ud83d\udcda What You'll Master\n1. **Convolution Math** - From first principles to efficient implementation\n2. **CNN Architectures** - LeNet, AlexNet, VGG, ResNet\n3. **From-Scratch Implementation** - Conv2D, MaxPooling, Forward/Backward pass\n4. **Real-World** - ImageNet, medical imaging, autonomous vehicles\n5. **Exercises** - Build your own CNN\n6. **Competition** - CIFAR-10 image classification\n7. **Interviews** - 7 essential CNN questions\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nprint('\u2705 CNN environment ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcd6 Chapter 1: Convolution Mathematics\n\n## What is Convolution?\n\n**Core idea**: Slide a filter (kernel) over an image to detect patterns\n\n### 1.1 Mathematical Definition\n\nFor 2D discrete convolution:\n\n$$S(i,j) = (I * K)(i,j) = \\sum_m \\sum_n I(i-m, j-n)K(m,n)$$\n\nwhere:\n- $I$ = Input image\n- $K$ = Kernel/filter\n- $S$ = Output feature map\n\n### 1.2 Why Convolution Works\n\n1. **Local Connectivity**: Each neuron only looks at local patch\n2. **Parameter Sharing**: Same filter applied everywhere\n3. **Translation Invariance**: Detects features anywhere in image\n\n**Result**: Dramatically fewer parameters than fully connected!\n\n### 1.3 Convolution vs Cross-Correlation\n\n**True convolution**: Flip kernel horizontally and vertically\n\n$$S(i,j) = \\sum_m \\sum_n I(i+m, j+n)K(m,n)$$\n\n**Cross-correlation** (what CNNs actually use):\n\n$$S(i,j) = \\sum_m \\sum_n I(i-m, j-n)K(m,n)$$\n\n**In practice**: Doesn't matter (filter learned anyway)\n\n### 1.4 Output Size Formula\n\nGiven:\n- Input: $n \\times n$\n- Filter: $f \\times f$\n- Padding: $p$\n- Stride: $s$\n\n$$\\text{Output size} = \\left\\lfloor \\frac{n + 2p - f}{s} \\right\\rfloor + 1$$\n\n**Example**: \n- Input: 32\u00d732\n- Filter: 5\u00d75\n- Padding: 2\n- Stride: 1\n- Output: $(32 + 2(2) - 5)/1 + 1 = 32$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize convolution operation\ndef visualize_convolution():\n    # Simple 5x5 image\n    image = np.array([\n        [0, 0, 0, 0, 0],\n        [0, 1, 1, 1, 0],\n        [0, 1, 1, 1, 0],\n        [0, 1, 1, 1, 0],\n        [0, 0, 0, 0, 0]\n    ])\n    \n    # Edge detection kernel\n    kernel = np.array([\n        [-1, -1, -1],\n        [-1,  8, -1],\n        [-1, -1, -1]\n    ])\n    \n    # Convolve\n    output = np.zeros((3, 3))\n    for i in range(3):\n        for j in range(3):\n            patch = image[i:i+3, j:j+3]\n            output[i, j] = np.sum(patch * kernel)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    axes[0].imshow(image, cmap='gray')\n    axes[0].set_title('Input Image', fontsize=12, fontweight='bold')\n    axes[1].imshow(kernel, cmap='RdBu')\n    axes[1].set_title('Edge Detection Kernel', fontsize=12, fontweight='bold')\n    axes[2].imshow(output, cmap='gray')\n    axes[2].set_title('Output (Edges Detected)', fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_convolution()\nprint('\u2713 Convolution visualized!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcbb Chapter 2: CNN From Scratch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Conv2D:\n    \"\"\"2D Convolution layer from scratch.\"\"\"\n    \n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        \n        # Initialize weights: (out_channels, in_channels, kernel_size, kernel_size)\n        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * 0.1\n        self.bias = np.zeros(out_channels)\n        \n    def forward(self, X):\n        \"\"\"Forward pass.\"\"\"\n        batch_size, in_channels, h_in, w_in = X.shape\n        \n        # Add padding\n        if self.padding > 0:\n            X = np.pad(X, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n        \n        # Calculate output dimensions\n        h_out = (h_in + 2*self.padding - self.kernel_size) // self.stride + 1\n        w_out = (w_in + 2*self.padding - self.kernel_size) // self.stride + 1\n        \n        # Output feature map\n        output = np.zeros((batch_size, self.out_channels, h_out, w_out))\n        \n        # Perform convolution\n        for b in range(batch_size):\n            for c_out in range(self.out_channels):\n                for h in range(h_out):\n                    for w in range(w_out):\n                        h_start = h * self.stride\n                        w_start = w * self.stride\n                        patch = X[b, :, h_start:h_start+self.kernel_size, w_start:w_start+self.kernel_size]\n                        output[b, c_out, h, w] = np.sum(patch * self.weights[c_out]) + self.bias[c_out]\n        \n        self.cache = X  # Save for backward pass\n        return output\n\nclass MaxPool2D:\n    \"\"\"Max pooling layer.\"\"\"\n    \n    def __init__(self, pool_size=2, stride=2):\n        self.pool_size = pool_size\n        self.stride = stride\n    \n    def forward(self, X):\n        batch_size, channels, h_in, w_in = X.shape\n        h_out = (h_in - self.pool_size) // self.stride + 1\n        w_out = (win - self.pool_size) // self.stride + 1\n        \n        output = np.zeros((batch_size, channels, h_out, w_out))\n        \n        for b in range(batch_size):\n            for c in range(channels):\n                for h in range(h_out):\n                    for w in range(w_out):\n                        h_start = h * self.stride\n                        w_start = w * self.stride\n                        patch = X[b, c, h_start:h_start+self.pool_size, w_start:w_start+self.pool_size]\n                        output[b, c, h, w] = np.max(patch)\n        \n        return output\n\nprint('\u2705 Conv2D and MaxPool2D implemented!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfed Chapter 3: Famous CNN Architectures\n\n### LeNet-5 (1998)\n- **Authors**: Yann LeCun\n- **Use**: Handwritten digit recognition\n- **Layers**: Conv \u2192 Pool \u2192 Conv \u2192 Pool \u2192 FC \u2192 FC\n- **Impact**: First successful CNN\n\n### AlexNet (2012)\n- **Authors**: Krizhevsky, Sutskever, Hinton\n- **Achievement**: Won ImageNet by huge margin\n- **Innovation**: ReLU, Dropout, GPU training\n- **Layers**: 8 layers (5 conv, 3 FC)\n\n### VGG-16 (2014)\n- **Authors**: Simonyan & Zisserman\n- **Key**: Small 3\u00d73 filters, deeper networks\n- **Layers**: 16 layers, all 3\u00d73 conv\n- **Impact**: Showed depth matters\n\n### ResNet (2015)\n- **Authors**: He et al. (Microsoft)\n- **Innovation**: Skip connections (residual learning)\n- **Achievement**: 152 layers without degradation\n- **Impact**: Revolutionized deep learning\n\n$$y = F(x) + x$$\n\n**Skip connection solves vanishing gradients!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfaf Chapter 4: Exercises\n\n## Exercise 1: Implement ReLU \u2b50\n```python\ndef relu(x):\n    return np.maximum(0, x)\n```\n\n## Exercise 2: Calculate output size \u2b50\nInput: 28\u00d728, Filter: 5\u00d75, Padding: 2, Stride: 1\nOutput size = ?\n\n## Exercise 3: Build LeNet-5 \u2b50\u2b50\nImplement complete architecture\n\n## Exercise 4: Add Batch Normalization \u2b50\u2b50\u2b50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfc6 Competition: CIFAR-10\n\n**Challenge**: Classify 32\u00d732 color images into 10 classes\n\n**Classes**: Airplane, car, bird, cat, deer, dog, frog, horse, ship, truck\n\n**Baseline**: 70% accuracy\n**Your goal**: >80%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udca1 Interview Questions\n\n### Q1: Why convolution instead of fully connected?\n**Answer**: \n1. **Parameter reduction**: Share weights across spatial locations\n2. **Translation invariance**: Detect features anywhere\n3. **Local connectivity**: Exploit spatial structure\n\n### Q2: Padding purpose?\nPreserve spatial dimensions, prevent information loss at borders\n\n### Q3: 1\u00d71 convolution use?\nChannel-wise combination, dimensionality reduction (inception modules)\n\n### Q4: ResNet skip connections math?\n$y = F(x) + x$ solves vanishing gradients via identity mapping\n\n### Q5: Receptive field?\nRegion of input image that affects one output pixel\n\n### Q6: Pooling benefits?\n- Dimensionality reduction\n- Translation invariance\n- Reduces overfitting\n\n### Q7: CNN vs fully connected for images?\nCNN: O(k\u00b2) params per layer\nFC: O(n\u00b2) params\nCNN wins for large images!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}