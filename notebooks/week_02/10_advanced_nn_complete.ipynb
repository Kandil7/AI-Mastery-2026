{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Advanced Neural Networks: Complete Professional Guide\n\n## \ud83d\udcda What You'll Master\n1. **Optimizers** - Adam, RMSprop, SGD+Momentum (complete derivations)\n2. **Regularization** - Dropout, Batch Normalization, Weight Decay\n3. **Activation Functions** - ReLU family, SELU, Swish\n4. **Real-World** - ImageNet, BERT, GPT training techniques\n5. **Exercises** - Implement optimizers from scratch\n6. **Competition** - CIFAR-10 classification\n7. **Interviews** - 7 critical questions\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nprint('\u2705 Advanced NN ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcd6 Chapter 1: Advanced Optimizers\n\n## 1.1 SGD with Momentum\n\n**Problem**: SGD oscillates in narrow valleys\n\n**Solution**: Add momentum term\n\n$$v_t = \\gamma v_{t-1} + \\eta \\nabla_\\theta J(\\theta)$$\n$$\\theta_t = \\theta_{t-1} - v_t$$\n\n**Intuition**: \"Ball rolling downhill\" accumulates velocity\n\n## 1.2 RMSprop (Root Mean Square Propagation)\n\n**Problem**: Learning rate same for all parameters\n\n**Solution**: Adapt per-parameter learning rates\n\n$$E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta)g_t^2$$\n$$\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}}g_t$$\n\n## 1.3 Adam (Adaptive Moment Estimation)\n\n**Combines**: Momentum + RMSprop\n\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n$$\\theta_t = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}\\hat{m}_t$$\n\n**Defaults**: $\\beta_1=0.9$, $\\beta_2=0.999$, $\\eta=0.001$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Adam:\n    \"\"\"Adam optimizer from scratch.\"\"\"\n    \n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.m = None\n        self.v = None\n        self.t = 0\n    \n    def update(self, params, grads):\n        \"\"\"Update parameters using Adam.\"\"\"\n        if self.m is None:\n            self.m = np.zeros_like(params)\n            self.v = np.zeros_like(params)\n        \n        self.t += 1\n        \n        # Update biased first moment estimate\n        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n        \n        # Update biased second moment estimate  \n        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n        \n        # Bias correction\n        m_hat = self.m / (1 - self.beta1 ** self.t)\n        v_hat = self.v / (1 - self.beta2 ** self.t)\n        \n        # Update parameters\n        params -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n        \n        return params\n\nprint('\u2705 Adam optimizer complete!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfed Chapter 3: Real-World Training Techniques\n\n### 1. **ImageNet Training** \ud83d\uddbc\ufe0f\n- **Model**: ResNet-50\n- **Optimizer**: SGD with momentum (0.9)\n- **LR Schedule**: Step decay every 30 epochs\n- **Regularization**: Weight decay (1e-4)\n- **Batch size**: 256\n- **Training time**: 4 days on 8 GPUs\n\n### 2. **BERT Pretraining** \ud83d\udcdd\n- **Model**: 340M parameters\n- **Optimizer**: Adam (lr=1e-4)\n- **Regularization**: Dropout (0.1)\n- **Batch size**: 256 sequences\n- **Hardware**: 64 TPU chips\n- **Cost**: $7,000 per training run\n\n### 3. **GPT-3 Training** \ud83e\udd16\n- **Model**: 175B parameters\n- **Optimizer**: Adam with gradient clipping\n- **Batch size**: 3.2M tokens\n- **Training**: 300B tokens corpus\n- **Cost**: ~$12M for full training\n\n### 4. **Production ML (Uber)** \ud83d\ude97\n- **Problem**: ETA prediction\n- **Optimizer**: Adam\n- **Regularization**: Dropout + Early stopping\n- **Serving**: <50ms latency\n- **Scale**: Millions of predictions/sec\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfaf Chapter 4: Exercises\n\n## Exercise 1: Implement RMSprop \u2b50\u2b50\nBuild RMSprop optimizer from scratch\n\n## Exercise 2: Learning Rate Schedules \u2b50\u2b50\u2b50\nImplement cosine annealing, step decay\n\n## Exercise 3: Batch Normalization \u2b50\u2b50\u2b50\nDerive and implement batch norm layer\n\n## Exercise 4: Gradient Clipping \u2b50\nPrevent exploding gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udca1 Chapter 6: Interview Questions\n\n### Q1: Adam vs SGD - when to use?\n**Adam**: Most cases, fast convergence\n**SGD**: Sometimes better generalization, simpler\n\n### Q2: Why bias correction in Adam?\nEarly iterations have biased moment estimates \u2192 correct them\n\n### Q3: Batch Normalization benefits?\n- Faster convergence\n- Higher learning rates\n- Acts as regularizer\n- Reduces internal covariate shift\n\n### Q4: Dropout rate selection?\n**Hidden layers**: 0.5\n**Input layer**: 0.1-0.2\n\n### Q5: Why BatchNorm before or after activation?\n**Before**: More common, better performance\n**After**: Original paper placement\n\n### Q6: Learning rate warmup?\nGradually increase LR at start \u2192 stabilizes training\n\n### Q7: Gradient explosion solutions?\n- Gradient clipping\n- BatchNorm\n- Residual connections\n- Proper weight initialization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcca Summary\n\n## Key Takeaways\n\u2705 **Adam**: Default choice for most problems\n\u2705 **BatchNorm**: Accelerates training significantly\n\u2705 **Dropout**: Prevent overfitting\n\u2705 **LR schedules**: Critical for final performance\n\u26a0\ufe0f **No one-size-fits-all**: Experiment!\n\u26a0\ufe0f **Hyperparameters matter**: Grid/random search\n\n## Optimizer Comparison\n\n| Optimizer | Speed | Memory | Best For |\n|-----------|-------|--------|----------|\n| SGD | Fast | Low | Simple problems |\n| Momentum | Fast | Low | Narrow valleys |\n| RMSprop | Medium | Medium | RNNs |\n| Adam | Medium | High | Most problems |\n\n---\n\n## Next: Transformers and attention mechanisms\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}