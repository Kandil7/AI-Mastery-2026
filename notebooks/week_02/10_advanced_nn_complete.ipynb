{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Advanced Neural Networks\n\n1. Optimizers: Adam, RMSprop, SGD+momentum\n2. Regularization: Dropout, BatchNorm\n3. Activation functions: ReLU, Leaky ReLU, Swish\n4. Exercises + competition + interviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nprint('\u2705 Advanced NN ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizers\n\n### Adam (Adaptive Moment Estimation)\n\n$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1)g_t$$\n$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2)g_t^2$$\n$$\\theta_t = \\theta_{t-1} - \\alpha \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$$\n\n**Why**: Adapts learning rate per parameter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Adam:\n    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.m = None\n        self.v = None\n        self.t = 0\n    \n    def update(self, params, grads):\n        if self.m is None:\n            self.m = np.zeros_like(params)\n            self.v = np.zeros_like(params)\n        \n        self.t += 1\n        self.m = self.beta1 * self.m + (1 - self.beta1) * grads\n        self.v = self.beta2 * self.v + (1 - self.beta2) * (grads ** 2)\n        \n        m_hat = self.m / (1 - self.beta1 ** self.t)\n        v_hat = self.v / (1 - self.beta2 ** self.t)\n        \n        params -= self.lr * m_hat / (np.sqrt(v_hat) + 1e-8)\n        return params\n\nprint('\u2705 Adam optimizer!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization\n\n### Dropout\nRandomly drop neurons during training\n- Prevents co-adaptation\n- Ensemble effect\n\n### Batch Normalization\nNormalize activations per mini-batch\n- Faster convergence\n- Reduces internal covariate shift\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interviews\n\n### Q1: Adam vs SGD?\nAdam: Faster, auto-adjusts LR\nSGD+momentum: Simpler, sometimes better final accuracy\n\n### Q2: Why BatchNorm works?\n- Reduces internal covariate shift\n- Acts as regularizer\n- Allows higher learning rates\n\n### Q3: Dropout rate?\nTypical: 0.5 for hidden, 0.2 for input\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}