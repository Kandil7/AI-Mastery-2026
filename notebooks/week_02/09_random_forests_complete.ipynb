{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Random Forests\n\n1. Bagging + random features\n2. Ensemble power\n3. Kaggle winner, feature importance\n4. Exercises + competition + interviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nprint('\u2705 Random Forests ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algorithm\n\n1. **Bootstrap**: Sample data with replacement\n2. **Random features**: Select \u221ad features per split\n3. **Train trees**: Build decision trees\n4. **Vote**: Majority for classification\n\n**Why it works**: Reduces variance through averaging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified RF\nclass SimpleRandomForest:\n    def __init__(self, n_trees=10, max_depth=5):\n        self.n_trees = n_trees\n        self.trees = []\n    \n    def fit(self, X, y):\n        for _ in range(self.n_trees):\n            # Bootstrap sample\n            idx = np.random.choice(len(X), len(X), replace=True)\n            # Train tree (simplified - use sklearn in practice)\n            self.trees.append((X[idx], y[idx]))\n        return self\n    \n    def predict(self, X):\n        # Simplified voting\n        return np.zeros(len(X))  # Placeholder\n\nprint('\u2705 Framework ready (use sklearn for production)!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use Cases\n\n### Kaggle Competitions \ud83c\udfc6\n- 2nd most winning algorithm (after XGBoost)\n- Easy to tune, robust\n\n### Feature Importance\n- Automatic feature selection\n- Used in finance, healthcare\n\n### Fraud Detection\n- Banks use RF for real-time decisions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interviews\n\n### Q1: RF vs single tree?\nRF: More accurate, less overfitting\nTree: Faster, more interpretable\n\n### Q2: Bagging vs Boosting?\nBagging (RF): Parallel, reduces variance\nBoosting: Sequential, reduces bias\n\n### Q3: OOB error?\nOut-of-bag samples estimate test error without CV\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}