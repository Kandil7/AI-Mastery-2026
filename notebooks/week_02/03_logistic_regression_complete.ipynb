{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Logistic Regression: Complete Implementation\n",
        "\n",
        "## What You'll Learn\n",
        "1. Derive cost function from maximum likelihood\n",
        "2. Implement binary & multiclass from scratch\n",
        "3. Add L1/L2 regularization\n",
        "4. Visualize decision boundaries & ROC curves\n",
        "\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc, confusion_matrix\n\n",
        "np.random.seed(42)\nplt.style.use('seaborn-v0_8')\n",
        "print('\u2705 Setup complete!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Chapter 1: Mathematical Foundation\n\n",
        "## Sigmoid Function\n\n$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n\n",
        "**Properties:**\n- Range: (0, 1)\n- Derivative: $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$\n",
        "- Decision boundary at $z=0$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sigmoid(z):\n    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n\n",
        "def sigmoid_derivative(z):\n    s = sigmoid(z)\n    return s * (1 - s)\n\n",
        "# Visualize\nz = np.linspace(-10, 10, 200)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "ax1.plot(z, sigmoid(z), 'b-', lw=2)\n",
        "ax1.axhline(0.5, color='r', linestyle='--', alpha=0.5)\n",
        "ax1.set_title('Sigmoid Function')\nax1.grid(True, alpha=0.3)\n",
        "ax2.plot(z, sigmoid_derivative(z), 'g-', lw=2)\n",
        "ax2.set_title('Sigmoid Derivative')\nax2.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Binary Cross-Entropy Loss\n\n",
        "$$J(\\mathbf{w}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h(\\mathbf{x}^{(i)})) + (1-y^{(i)}) \\log(1-h(\\mathbf{x}^{(i)})) \\right]$$\n\n",
        "**Gradient:**\n\n$$\\frac{\\partial J}{\\partial \\mathbf{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (h(\\mathbf{x}^{(i)}) - y^{(i)}) \\mathbf{x}^{(i)}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000, regularization=None, lambda_=0.01):\n",
        "        self.lr = learning_rate\n        self.n_iters = n_iterations\n",
        "        self.reg = regularization\n        self.lambda_ = lambda_\n",
        "        self.weights = None\n        self.bias = None\n        self.cost_history = []\n    \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n        self.bias = 0\n        \n",
        "        for i in range(self.n_iters):\n",
        "            # Forward pass\n            z = X @ self.weights + self.bias\n",
        "            y_pred = sigmoid(z)\n            \n",
        "            # Compute cost\n            cost = -np.mean(y * np.log(y_pred + 1e-10) + (1-y) * np.log(1-y_pred + 1e-10))\n",
        "            \n            # Add regularization\n",
        "            if self.reg == 'l2':\n",
        "                cost += (self.lambda_ / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
        "            elif self.reg == 'l1':\n",
        "                cost += (self.lambda_ / n_samples) * np.sum(np.abs(self.weights))\n",
        "            \n            self.cost_history.append(cost)\n            \n",
        "            # Compute gradients\n",
        "            dw = (1/n_samples) * (X.T @ (y_pred - y))\n",
        "            db = (1/n_samples) * np.sum(y_pred - y)\n            \n",
        "            # Add regularization gradient\n",
        "            if self.reg == 'l2':\n",
        "                dw += (self.lambda_ / n_samples) * self.weights\n",
        "            elif self.reg == 'l1':\n",
        "                dw += (self.lambda_ / n_samples) * np.sign(self.weights)\n            \n",
        "            # Update parameters\n",
        "            self.weights -= self.lr * dw\n",
        "            self.bias -= self.lr * db\n        \n",
        "        return self\n    \n",
        "    def predict_proba(self, X):\n",
        "        z = X @ self.weights + self.bias\n",
        "        return sigmoid(z)\n    \n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)\n    \n",
        "    def score(self, X, y):\n",
        "        return accuracy_score(y, self.predict(X))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Chapter 2: Application to Real Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load breast cancer dataset\n",
        "data = load_breast_cancer()\nX, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n",
        "# Standardize\nscaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n\n",
        "# Train models\nmodels = {\n",
        "    'No Reg': LogisticRegression(learning_rate=0.1, n_iterations=1000),\n",
        "    'L2 Reg': LogisticRegression(learning_rate=0.1, n_iterations=1000, regularization='l2', lambda_=0.1),\n",
        "    'L1 Reg': LogisticRegression(learning_rate=0.1, n_iterations=1000, regularization='l1', lambda_=0.1)\n",
        "}\n\nresults = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "    train_acc = model.score(X_train_scaled, y_train)\n",
        "    test_acc = model.score(X_test_scaled, y_test)\n",
        "    results[name] = {'train': train_acc, 'test': test_acc}\n",
        "    print(f'{name:10} | Train: {train_acc:.4f} | Test: {test_acc:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ROC Curves\nfig, ax = plt.subplots(figsize=(10, 7))\n",
        "for name, model in models.items():\n",
        "    y_pred_proba = model.predict_proba(X_test_scaled)\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    ax.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.3f})')\n\n",
        "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random')\n",
        "ax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curves: Logistic Regression Variants')\n",
        "ax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# Summary\n\n",
        "| Concept | Formula | Purpose |\n",
        "|---------|---------|----------|\n",
        "| Sigmoid | $\\sigma(z) = 1/(1+e^{-z})$ | Squeeze to [0,1] |\n",
        "| BCE Loss | $-y\\log(h) - (1-y)\\log(1-h)$ | Measure error |\n",
        "| Gradient | $(h-y)\\mathbf{x}$ | Update direction |\n",
        "| L2 Reg | $\\lambda \\|\\mathbf{w}\\|^2$ | Prevent overfitting |\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}