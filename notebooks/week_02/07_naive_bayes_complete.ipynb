{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Naive Bayes: Complete Professional Guide\n\n## \ud83d\udcda What You'll Master\n1. **Bayes' Theorem** - Probabilistic classification from first principles\n2. **Variants** - Gaussian, Multinomial, Bernoulli\n3. **Real-World** - Gmail spam (99.9%), sentiment analysis, medical diagnosis\n4. **Exercises** - 4 problems with solutions\n5. **Competition** - Text classification\n6. **Interviews** - 7 questions\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris, fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB as SklearnGNB\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\nprint('\u2705 Naive Bayes ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcd6 Chapter 1: Bayes' Theorem\n\n## The Foundation\n\n$$P(y|X) = \\frac{P(X|y)P(y)}{P(X)}$$\n\n- $P(y|X)$: **Posterior** (what we want)\n- $P(X|y)$: **Likelihood** (from data)\n- $P(y)$: **Prior** (class frequency)\n\n## Naive Assumption\n\nFeatures are **conditionally independent**:\n\n$$P(X|y) = \\prod_{i=1}^{n} P(x_i|y)$$\n\n**\"Naive\"**: Rarely true in reality, but works surprisingly well!\n\n## Variants\n\n1. **Gaussian**: Continuous features, $P(x|y) \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)$\n2. **Multinomial**: Count data (word frequencies)\n3. **Bernoulli**: Binary features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaussianNaiveBayes:\n    def __init__(self):\n        self.classes = None\n        self.mean = {}\n        self.var = {}\n        self.priors = {}\n    \n    def fit(self, X, y):\n        self.classes = np.unique(y)\n        for c in self.classes:\n            X_c = X[y == c]\n            self.mean[c] = X_c.mean(axis=0)\n            self.var[c] = X_c.var(axis=0)\n            self.priors[c] = X_c.shape[0] / X.shape[0]\n        return self\n    \n    def _gaussian_pdf(self, x, mean, var):\n        return np.exp(-(x - mean)**2 / (2 * var)) / np.sqrt(2 * np.pi * var)\n    \n    def predict(self, X):\n        preds = []\n        for x in X:\n            posts = []\n            for c in self.classes:\n                prior = np.log(self.priors[c])\n                likelihood = np.sum(np.log(self._gaussian_pdf(x, self.mean[c], self.var[c])))\n                posts.append(prior + likelihood)\n            preds.append(self.classes[np.argmax(posts)])\n        return np.array(preds)\n    \n    def score(self, X, y):\n        return accuracy_score(y, self.predict(X))\n\nprint('\u2705 GaussianNaiveBayes complete!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfed Chapter 3: Real-World Use Cases\n\n### 1. Gmail Spam Filtering \ud83d\udce7\n- **Impact**: **99.9% accuracy**\n- **Type**: Multinomial NB on word counts\n- **Scale**: Billions of emails filtered daily\n\n### 2. Sentiment Analysis (Twitter) \ud83d\udc26\n- **Problem**: Classify tweets as positive/negative\n- **Impact**: Real-time brand monitoring\n- **Advantage**: Fast, scalable\n\n### 3. Medical Diagnosis \ud83c\udfe5\n- **Problem**: Disease prediction from symptoms\n- **Why NB**: Probabilistic output crucial for doctors\n- **Example**: Flu vs Cold classification\n\n### 4. Document Classification (Reuters) \ud83d\udcf0\n- **Problem**: Auto-categorize news articles\n- **Impact**: Content recommendation\n- **Features**: TF-IDF word vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on Iris\niris = load_iris()\nX, y = iris.data, iris.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n\nnb = GaussianNaiveBayes()\nnb.fit(X_train, y_train)\nour_acc = nb.score(X_test, y_test)\n\nsklearn_nb = SklearnGNB()\nsklearn_nb.fit(X_train, y_train)\nsklearn_acc = sklearn_nb.score(X_test, y_test)\n\nprint('='*60)\nprint(f'Our NB:      {our_acc:.4f}')\nprint(f'Sklearn:     {sklearn_acc:.4f}')\nprint('='*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfaf Exercises\n\n## Exercise 1: Laplace Smoothing \u2b50\u2b50\nAdd smoothing to handle zero probabilities\n\n## Exercise 2: Multinomial NB \u2b50\u2b50\u2b50\nImplement for text classification\n\n## Exercise 3: Compare Variants \u2b50\u2b50\nGaussian vs Multinomial vs Bernoulli\n\n## Exercise 4: Handle Missing Data \u2b50\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83c\udfc6 Competition: Text Classification\n\nClassify news articles into categories\n\n**Baseline**: 75%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udca1 Interviews\n\n### Q1: Why \"naive\"?\n**Answer**: Assumes feature independence (rarely true)\n\n### Q2: When does it work well?\n**Answer**: Text classification, despite violated assumptions!\n\n### Q3: Gaussian vs Multinomial?\n**Gaussian**: Continuous features\n**Multinomial**: Count/frequency data\n\n### Q4: Handling zero probabilities?\n**Answer**: Laplace smoothing (add $\\alpha$ to all counts)\n\n### Q5: NB vs Logistic Regression?\n**NB**: Faster, less data needed, generative\n**LR**: More accurate, discriminative\n\n### Q6: Computational complexity?\n**Answer**: Training: O(nd), Prediction: O(cd)\nExtremely fast!\n\n### Q7: Probabilistic output?\n**Answer**: Yes! Outputs P(class|features) naturally\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n# \ud83d\udcca Summary\n\n## Key Takeaways\n\u2705 **Fastest** algorithm (O(nd))\n\u2705 **Probabilistic** output\n\u2705 **Handles high dimensions** well\n\u2705 **Little training data** needed\n\u26a0\ufe0f **Naive independence** assumption\n\u26a0\ufe0f **Sensitive to feature distribution**\n\n## When to Use\n\u2705 Text classification\n\u2705 Real-time predictions\n\u2705 Baseline model\n\u2705 Small datasets\n\n---\n\n## Next: K-Means for clustering\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}