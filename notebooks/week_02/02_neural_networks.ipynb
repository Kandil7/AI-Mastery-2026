{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 02: Neural Networks from Scratch\n",
                "\n",
                "Building neural networks from first principles using only NumPy.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Understand forward propagation mathematically\n",
                "2. Implement backpropagation using chain rule\n",
                "3. Build a complete neural network class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import List, Tuple, Callable"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. The Neuron Model\n",
                "\n",
                "### 1.1 Activation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Sigmoid activation: σ(x) = 1 / (1 + e^(-x))\"\"\"\n",
                "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "\n",
                "def sigmoid_derivative(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Derivative: σ'(x) = σ(x) * (1 - σ(x))\"\"\"\n",
                "    s = sigmoid(x)\n",
                "    return s * (1 - s)\n",
                "\n",
                "def relu(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"ReLU activation: max(0, x)\"\"\"\n",
                "    return np.maximum(0, x)\n",
                "\n",
                "def relu_derivative(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"ReLU derivative: 1 if x > 0 else 0\"\"\"\n",
                "    return (x > 0).astype(float)\n",
                "\n",
                "def tanh(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Tanh activation\"\"\"\n",
                "    return np.tanh(x)\n",
                "\n",
                "def tanh_derivative(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Tanh derivative: 1 - tanh²(x)\"\"\"\n",
                "    return 1 - np.tanh(x) ** 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize activations\n",
                "x = np.linspace(-5, 5, 100)\n",
                "print(\"Activation function outputs at x=2:\")\n",
                "print(f\"  sigmoid(2) = {sigmoid(np.array([2]))[0]:.4f}\")\n",
                "print(f\"  relu(2) = {relu(np.array([2]))[0]:.4f}\")\n",
                "print(f\"  tanh(2) = {tanh(np.array([2]))[0]:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dense Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DenseLayer:\n",
                "    \"\"\"\n",
                "    Fully connected layer.\n",
                "    \n",
                "    Shape:\n",
                "    - Input: (batch_size, n_inputs)\n",
                "    - Output: (batch_size, n_outputs)\n",
                "    - Weights: (n_inputs, n_outputs)\n",
                "    \"\"\"\n",
                "    def __init__(self, n_inputs: int, n_outputs: int, activation: str = 'relu'):\n",
                "        self.W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / n_inputs)\n",
                "        self.b = np.zeros(n_outputs)\n",
                "        self.activation = activation\n",
                "    \n",
                "    def _activate(self, z: np.ndarray) -> np.ndarray:\n",
                "        if self.activation == 'relu':\n",
                "            return np.maximum(0, z)\n",
                "        elif self.activation == 'sigmoid':\n",
                "            return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
                "        elif self.activation == 'tanh':\n",
                "            return np.tanh(z)\n",
                "        else:\n",
                "            return z\n",
                "    \n",
                "    def _activate_derivative(self, z: np.ndarray) -> np.ndarray:\n",
                "        if self.activation == 'relu':\n",
                "            return (z > 0).astype(float)\n",
                "        elif self.activation == 'sigmoid':\n",
                "            s = self._activate(z)\n",
                "            return s * (1 - s)\n",
                "        elif self.activation == 'tanh':\n",
                "            return 1 - np.tanh(z) ** 2\n",
                "        else:\n",
                "            return np.ones_like(z)\n",
                "    \n",
                "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
                "        self.X = X\n",
                "        self.z = X @ self.W + self.b\n",
                "        self.a = self._activate(self.z)\n",
                "        return self.a\n",
                "    \n",
                "    def backward(self, da: np.ndarray, learning_rate: float) -> np.ndarray:\n",
                "        batch_size = self.X.shape[0]\n",
                "        dz = da * self._activate_derivative(self.z)\n",
                "        dW = (self.X.T @ dz) / batch_size\n",
                "        db = np.sum(dz, axis=0) / batch_size\n",
                "        dX = dz @ self.W.T\n",
                "        self.W -= learning_rate * dW\n",
                "        self.b -= learning_rate * db\n",
                "        return dX"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test layer\n",
                "layer = DenseLayer(3, 2, activation='relu')\n",
                "X = np.random.randn(5, 3)\n",
                "output = layer.forward(X)\n",
                "print(f\"Input shape: {X.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Complete Neural Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class NeuralNetwork:\n",
                "    def __init__(self, layer_sizes: List[int], activation: str = 'relu'):\n",
                "        self.layers = []\n",
                "        for i in range(len(layer_sizes) - 1):\n",
                "            act = 'sigmoid' if i == len(layer_sizes) - 2 else activation\n",
                "            layer = DenseLayer(layer_sizes[i], layer_sizes[i+1], act)\n",
                "            self.layers.append(layer)\n",
                "    \n",
                "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
                "        a = X\n",
                "        for layer in self.layers:\n",
                "            a = layer.forward(a)\n",
                "        return a\n",
                "    \n",
                "    def compute_loss(self, y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
                "        eps = 1e-10\n",
                "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
                "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
                "        return loss\n",
                "    \n",
                "    def backward(self, y_pred: np.ndarray, y_true: np.ndarray, learning_rate: float):\n",
                "        eps = 1e-10\n",
                "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
                "        da = (y_pred - y_true) / (y_pred * (1 - y_pred) + eps)\n",
                "        for layer in reversed(self.layers):\n",
                "            da = layer.backward(da, learning_rate)\n",
                "    \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray, epochs: int = 100,\n",
                "            learning_rate: float = 0.01, batch_size: int = 32, verbose: bool = True):\n",
                "        n_samples = X.shape[0]\n",
                "        history = []\n",
                "        for epoch in range(epochs):\n",
                "            indices = np.random.permutation(n_samples)\n",
                "            epoch_loss = 0\n",
                "            n_batches = 0\n",
                "            for start in range(0, n_samples, batch_size):\n",
                "                batch_idx = indices[start:start + batch_size]\n",
                "                X_batch = X[batch_idx]\n",
                "                y_batch = y[batch_idx]\n",
                "                y_pred = self.forward(X_batch)\n",
                "                loss = self.compute_loss(y_pred, y_batch)\n",
                "                epoch_loss += loss\n",
                "                n_batches += 1\n",
                "                self.backward(y_pred, y_batch, learning_rate)\n",
                "            avg_loss = epoch_loss / n_batches\n",
                "            history.append(avg_loss)\n",
                "            if verbose and (epoch + 1) % 100 == 0:\n",
                "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
                "        return history\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        return self.forward(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train neural network on XOR\n",
                "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y = np.array([[0], [1], [1], [0]])\n",
                "\n",
                "np.random.seed(42)\n",
                "nn = NeuralNetwork([2, 5, 1])\n",
                "history = nn.fit(X, y, epochs=2000, learning_rate=0.5, batch_size=4, verbose=False)\n",
                "\n",
                "print(f\"Final loss: {history[-1]:.6f}\")\n",
                "print(f\"Predictions: {nn.predict(X).flatten()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Backpropagation Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def backprop_visualization():\n",
                "    # Simple example visualization\n",
                "    X = np.array([[1.0, 2.0]])\n",
                "    y = np.array([[1.0]])\n",
                "    W1 = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n",
                "    W2 = np.array([[0.7], [0.8], [0.9]])\n",
                "    \n",
                "    z1 = X @ W1\n",
                "    a1 = np.maximum(0, z1)\n",
                "    z2 = a1 @ W2\n",
                "    a2 = 1 / (1 + np.exp(-z2))\n",
                "    \n",
                "    print(f\"Forward: a2={a2}\")\n",
                "    \n",
                "    dz2 = a2 - y\n",
                "    dW2 = a1.T @ dz2\n",
                "    da1 = dz2 @ W2.T\n",
                "    \n",
                "    print(f\"Backward dW2:\\n{dW2}\")\n",
                "\n",
                "backprop_visualization()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Exercises: Dropout and Batch Norm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Dropout:\n",
                "    def __init__(self, p: float = 0.5):\n",
                "        self.p = p\n",
                "        self.training = True\n",
                "    \n",
                "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
                "        if self.training:\n",
                "            self.mask = (np.random.rand(*X.shape) > self.p) / (1 - self.p)\n",
                "            return X * self.mask\n",
                "        return X\n",
                "    \n",
                "    def backward(self, da: np.ndarray) -> np.ndarray:\n",
                "        return da * self.mask\n",
                "\n",
                "class BatchNorm:\n",
                "    def __init__(self, n_features: int, eps: float = 1e-5, momentum: float = 0.1):\n",
                "        self.gamma = np.ones(n_features)\n",
                "        self.beta = np.zeros(n_features)\n",
                "        self.eps = eps\n",
                "        self.momentum = momentum\n",
                "        self.running_mean = np.zeros(n_features)\n",
                "        self.running_var = np.ones(n_features)\n",
                "    \n",
                "    def forward(self, X: np.ndarray, training: bool = True) -> np.ndarray:\n",
                "        if training:\n",
                "            mean = X.mean(axis=0)\n",
                "            var = X.var(axis=0)\n",
                "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
                "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var\n",
                "        else:\n",
                "            mean = self.running_mean\n",
                "            var = self.running_var\n",
                "        self.x_norm = (X - mean) / np.sqrt(var + self.eps)\n",
                "        return self.gamma * self.x_norm + self.beta"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}