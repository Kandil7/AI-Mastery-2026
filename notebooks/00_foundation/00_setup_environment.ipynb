{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Environment and White-Box Methodology\n",
    "\n",
    "This notebook covers setting up the environment for the AI Mastery 2026 program and introduces the white-box approach to AI engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..'))\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## White-Box Approach to AI Engineering\n",
    "\n",
    "The white-box approach emphasizes understanding the internal mechanisms of AI models rather than treating them as black boxes. This approach is crucial for:\n",
    "\n",
    "1. Debugging complex models\n",
    "2. Understanding failure modes\n",
    "3. Building trust in AI systems\n",
    "4. Optimizing performance\n",
    "5. Ensuring ethical and fair AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the white-box approach with a simple example\n",
    "# Linear regression from scratch\n",
    "\n",
    "class LinearRegressionFromScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implementation from scratch using NumPy.\n",
    "    This demonstrates the white-box approach by implementing the algorithm manually.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model using the normal equation.\n",
    "        \n",
    "        Args:\n",
    "            X: Training features of shape (n_samples, n_features)\n",
    "            y: Training targets of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        # Add bias term to X (intercept)\n",
    "        X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Calculate weights using the normal equation: Î¸ = (X^T * X)^(-1) * X^T * y\n",
    "        # This is the mathematical foundation of linear regression\n",
    "        theta = np.linalg.inv(X_with_bias.T.dot(X_with_bias)).dot(X_with_bias.T).dot(y)\n",
    "        \n",
    "        self.bias = theta[0]\n",
    "        self.weights = theta[1:]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            X: Features of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Predictions of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        return X.dot(self.weights) + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R-squared score.\n",
    "        \n",
    "        Args:\n",
    "            X: Features of shape (n_samples, n_features)\n",
    "            y: True targets of shape (n_samples,)\n",
    "            \n",
    "        Returns:\n",
    "            R-squared score\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the linear regression implementation\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train our model from scratch\n",
    "lr_model = LinearRegressionFromScratch()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate score\n",
    "score = lr_model.score(X_test, y_test)\n",
    "print(f\"R-squared score: {score:.4f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, alpha=0.6, label='True values')\n",
    "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predictions')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Linear Regression from Scratch')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Notation Reference\n",
    "\n",
    "Throughout this program, we'll use consistent mathematical notation:\n",
    "\n",
    "- $X$ - Input features matrix $(n \\times m)$ where $n$ is samples and $m$ is features\n",
    "- $y$ - Target values vector $(n \\times 1)$\n",
    "- $\\theta$ - Parameters vector\n",
    "- $h_\\theta(x)$ - Hypothesis function\n",
    "- $J(\\theta)$ - Cost function\n",
    "- $\\nabla$ - Gradient operator\n",
    "- $\\lambda$ - Regularization parameter\n",
    "\n",
    "Understanding this notation is crucial for following the mathematical derivations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Implementing gradient descent manually to understand the optimization process\n",
    "\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    \"\"\"\n",
    "    Perform gradient descent for linear regression.\n",
    "    \n",
    "    This function demonstrates the optimization process step by step.\n",
    "    \"\"\"\n",
    "    # Add bias term\n",
    "    X_with_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    theta = np.random.randn(X_with_bias.shape[1])\n",
    "    \n",
    "    # Store cost history for visualization\n",
    "    cost_history = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Calculate predictions\n",
    "        predictions = X_with_bias.dot(theta)\n",
    "        \n",
    "        # Calculate cost (Mean Squared Error)\n",
    "        cost = (1 / (2 * len(y))) * np.sum((predictions - y) ** 2)\n",
    "        cost_history.append(cost)\n",
    "        \n",
    "        # Calculate gradients\n",
    "        gradients = (1 / len(y)) * X_with_bias.T.dot(predictions - y)\n",
    "        \n",
    "        # Update parameters\n",
    "        theta = theta - learning_rate * gradients\n",
    "        \n",
    "    return theta, cost_history\n",
    "\n",
    "# Apply gradient descent to our data\n",
    "theta_opt, cost_history = gradient_descent(X_train, y_train, learning_rate=0.1, n_iterations=1000)\n",
    "\n",
    "# Plot cost history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cost_history)\n",
    "plt.title('Cost Function Over Iterations')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final cost: {cost_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Understanding over Abstraction**: Always understand what's happening under the hood before using high-level APIs\n",
    "2. **Mathematical Foundation**: Build a strong mathematical foundation to debug and optimize models\n",
    "3. **Implementation Skills**: Practice implementing algorithms from scratch to deepen understanding\n",
    "4. **Production Considerations**: Always think about how your implementation will work in production\n",
    "\n",
    "This white-box approach will be applied throughout the program as we build more complex models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
