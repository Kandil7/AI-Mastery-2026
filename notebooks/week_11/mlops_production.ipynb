{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 11: MLOps Production Features\n",
                "\n",
                "Production-grade ML operations including feature stores, model registry, automated retraining, and drift detection.\n",
                "\n",
                "**Learning Objectives**:\n",
                "- Implement feature store for consistent feature engineering\n",
                "- Build model registry with versioning\n",
                "- Create automated retraining pipelines\n",
                "- Detect and alert on data/model drift\n",
                "\n",
                "**Production Impact**: These components are critical for maintaining ML systems in production."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import json\n",
                "from datetime import datetime, timedelta\n",
                "from pathlib import Path\n",
                "import pickle\n",
                "from typing import Dict, List, Any\n",
                "\n",
                "print(\"âœ… Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Feature Store Implementation\n",
                "\n",
                "A feature store provides:\n",
                "- **Consistency**: Same features for training and serving\n",
                "- **Reusability**: Share features across models\n",
                "- **Freshness**: Automated feature updates\n",
                "- **Governance**: Track feature lineage"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeatureStore:\n",
                "    \"\"\"\n",
                "    Simple feature store for ML features.\n",
                "    \n",
                "    In production, use Feast, Tecton, or Hopsworks.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, store_path: str = \"./feature_store\"):\n",
                "        self.store_path = Path(store_path)\n",
                "        self.store_path.mkdir(exist_ok=True)\n",
                "        \n",
                "        self.features_db = {}  # In-memory cache\n",
                "        self.metadata_path = self.store_path / \"metadata.json\"\n",
                "        self._load_metadata()\n",
                "    \n",
                "    def _load_metadata(self):\n",
                "        \"\"\"Load feature metadata.\"\"\"\n",
                "        if self.metadata_path.exists():\n",
                "            with open(self.metadata_path, 'r') as f:\n",
                "                self.metadata = json.load(f)\n",
                "        else:\n",
                "            self.metadata = {}\n",
                "    \n",
                "    def register_feature(self, feature_name: str, feature_type: str, \n",
                "                        description: str, tags: List[str] = None):\n",
                "        \"\"\"Register a new feature.\"\"\"\n",
                "        self.metadata[feature_name] = {\n",
                "            'type': feature_type,\n",
                "            'description': description,\n",
                "            'tags': tags or [],\n",
                "            'created_at': datetime.now().isoformat(),\n",
                "            'updated_at': datetime.now().isoformat()\n",
                "        }\n",
                "        self._save_metadata()\n",
                "    \n",
                "    def write_features(self, entity_id: str, features: Dict[str, Any]):\n",
                "        \"\"\"Write features for an entity (e.g., user_id, transaction_id).\"\"\"\n",
                "        feature_file = self.store_path / f\"{entity_id}.pkl\"\n",
                "        \n",
                "        with open(feature_file, 'wb') as f:\n",
                "            pickle.dump({\n",
                "                'features': features,\n",
                "                'timestamp': datetime.now().isoformat()\n",
                "            }, f)\n",
                "        \n",
                "        # Update metadata\n",
                "        for feature_name in features.keys():\n",
                "            if feature_name in self.metadata:\n",
                "                self.metadata[feature_name]['updated_at'] = datetime.now().isoformat()\n",
                "        self._save_metadata()\n",
                "    \n",
                "    def read_features(self, entity_id: str, feature_names: List[str] = None) -> Dict:\n",
                "        \"\"\"Read features for an entity.\"\"\"\n",
                "        feature_file = self.store_path / f\"{entity_id}.pkl\"\n",
                "        \n",
                "        if not feature_file.exists():\n",
                "            raise ValueError(f\"No features found for entity {entity_id}\")\n",
                "        \n",
                "        with open(feature_file, 'rb') as f:\n",
                "            data = pickle.load(f)\n",
                "        \n",
                "        features = data['features']\n",
                "        \n",
                "        if feature_names:\n",
                "            features = {k: v for k, v in features.items() if k in feature_names}\n",
                "        \n",
                "        return features\n",
                "    \n",
                "    def get_feature_metadata(self, feature_name: str) -> Dict:\n",
                "        \"\"\"Get metadata for a feature.\"\"\"\n",
                "        return self.metadata.get(feature_name, {})\n",
                "    \n",
                "    def _save_metadata(self):\n",
                "        \"\"\"Save metadata to disk.\"\"\"\n",
                "        with open(self.metadata_path, 'w') as f:\n",
                "            json.dump(self.metadata, f, indent=2)\n",
                "\n",
                "\n",
                "# Example usage\n",
                "store = FeatureStore()\n",
                "\n",
                "# Register features\n",
                "store.register_feature(\n",
                "    'user_age',\n",
                "    'int',\n",
                "    'User age in years',\n",
                "    tags=['demographic', 'user']\n",
                ")\n",
                "\n",
                "store.register_feature(\n",
                "    'purchase_count_30d',\n",
                "    'int',\n",
                "    'Number of purchases in last 30 days',\n",
                "    tags=['behavioral', 'aggregation']\n",
                ")\n",
                "\n",
                "# Write features\n",
                "store.write_features('user_12345', {\n",
                "    'user_age': 28,\n",
                "    'purchase_count_30d': 5,\n",
                "    'avg_basket_size': 45.50\n",
                "})\n",
                "\n",
                "# Read features\n",
                "features = store.read_features('user_12345', ['user_age', 'purchase_count_30d'])\n",
                "print(f\"Features: {features}\")\n",
                "\n",
                "print(\"\\nâœ… Feature store implementation complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Registry\n",
                "\n",
                "Track all trained models with versioning, metrics, and metadata."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelRegistry:\n",
                "    \"\"\"\n",
                "    Model registry for tracking trained models.\n",
                "    \n",
                "    In production, use MLflow Model Registry or similar.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, registry_path: str = \"./model_registry\"):\n",
                "        self.registry_path = Path(registry_path)\n",
                "        self.registry_path.mkdir(exist_ok=True)\n",
                "        \n",
                "        self.models_path = self.registry_path / \"models\"\n",
                "        self.models_path.mkdir(exist_ok=True)\n",
                "        \n",
                "        self.catalog_path = self.registry_path / \"catalog.json\"\n",
                "        self._load_catalog()\n",
                "    \n",
                "    def _load_catalog(self):\n",
                "        \"\"\"Load model catalog.\"\"\"\n",
                "        if self.catalog_path.exists():\n",
                "            with open(self.catalog_path, 'r') as f:\n",
                "                self.catalog = json.load(f)\n",
                "        else:\n",
                "            self.catalog = {}\n",
                "    \n",
                "    def register_model(self, model_name: str, model_obj: Any, \n",
                "                      metrics: Dict[str, float], metadata: Dict = None):\n",
                "        \"\"\"Register a trained model.\"\"\"\n",
                "        # Generate version number\n",
                "        if model_name not in self.catalog:\n",
                "            self.catalog[model_name] = {'versions': []}\n",
                "        \n",
                "        version = len(self.catalog[model_name]['versions']) + 1\n",
                "        version_str = f\"v{version}\"\n",
                "        \n",
                "        # Save model\n",
                "        model_path = self.models_path / f\"{model_name}_{version_str}.pkl\"\n",
                "        with open(model_path, 'wb') as f:\n",
                "            pickle.dump(model_obj, f)\n",
                "        \n",
                "        # Record in catalog\n",
                "        version_info = {\n",
                "            'version': version_str,\n",
                "            'created_at': datetime.now().isoformat(),\n",
                "            'metrics': metrics,\n",
                "            'metadata': metadata or {},\n",
                "            'model_path': str(model_path),\n",
                "            'status': 'staging'  # staging, production, archived\n",
                "        }\n",
                "        \n",
                "        self.catalog[model_name]['versions'].append(version_info)\n",
                "        self._save_catalog()\n",
                "        \n",
                "        return version_str\n",
                "    \n",
                "    def promote_to_production(self, model_name: str, version: str):\n",
                "        \"\"\"Promote a model version to production.\"\"\"\n",
                "        # Demote current production model\n",
                "        for v in self.catalog[model_name]['versions']:\n",
                "            if v['status'] == 'production':\n",
                "                v['status'] = 'archived'\n",
                "        \n",
                "        # Promote new version\n",
                "        for v in self.catalog[model_name]['versions']:\n",
                "            if v['version'] == version:\n",
                "                v['status'] = 'production'\n",
                "                v['promoted_at'] = datetime.now().isoformat()\n",
                "        \n",
                "        self._save_catalog()\n",
                "    \n",
                "    def get_model(self, model_name: str, version: str = 'production'):\n",
                "        \"\"\"Load a model by name and version.\"\"\"\n",
                "        if version == 'production':\n",
                "            # Find production version\n",
                "            for v in self.catalog[model_name]['versions']:\n",
                "                if v['status'] == 'production':\n",
                "                    model_path = v['model_path']\n",
                "                    break\n",
                "        else:\n",
                "            # Find specific version\n",
                "            for v in self.catalog[model_name]['versions']:\n",
                "                if v['version'] == version:\n",
                "                    model_path = v['model_path']\n",
                "                    break\n",
                "        \n",
                "        with open(model_path, 'rb') as f:\n",
                "            model = pickle.load(f)\n",
                "        \n",
                "        return model\n",
                "    \n",
                "    def list_models(self) -> List[str]:\n",
                "        \"\"\"List all registered models.\"\"\"\n",
                "        return list(self.catalog.keys())\n",
                "    \n",
                "    def get_model_info(self, model_name: str) -> Dict:\n",
                "        \"\"\"Get info about a model.\"\"\"\n",
                "        return self.catalog.get(model_name, {})\n",
                "    \n",
                "    def _save_catalog(self):\n",
                "        \"\"\"Save catalog to disk.\"\"\"\n",
                "        with open(self.catalog_path, 'w') as f:\n",
                "            json.dump(self.catalog, f, indent=2)\n",
                "\n",
                "\n",
                "# Example usage\n",
                "registry = ModelRegistry()\n",
                "\n",
                "# Train a model (mock)\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "model = LogisticRegression()\n",
                "\n",
                "# Register model\n",
                "version = registry.register_model(\n",
                "    'churn_classifier',\n",
                "    model,\n",
                "    metrics={'accuracy': 0.87, 'f1': 0.82},\n",
                "    metadata={'dataset': 'customers_2024_01', 'features': 47}\n",
                ")\n",
                "\n",
                "print(f\"Registered model version: {version}\")\n",
                "\n",
                "# Promote to production\n",
                "registry.promote_to_production('churn_classifier', version)\n",
                "print(f\"Promoted {version} to production\")\n",
                "\n",
                "# Load production model\n",
                "prod_model = registry.get_model('churn_classifier', 'production')\n",
                "print(f\"Loaded production model: {prod_model}\")\n",
                "\n",
                "print(\"\\nâœ… Model registry implementation complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Automated Retraining Pipeline\n",
                "\n",
                "Automatically retrain models when:\n",
                "- Performance drops below threshold\n",
                "- On schedule (weekly, monthly)\n",
                "- New data available"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RetrainingPipeline:\n",
                "    \"\"\"\n",
                "    Automated model retraining pipeline.\n",
                "    \n",
                "    Monitors performance and triggers retraining when needed.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, model_registry: ModelRegistry):\n",
                "        self.registry = model_registry\n",
                "        self.performance_log = []\n",
                "    \n",
                "    def should_retrain(self, current_metrics: Dict[str, float], \n",
                "                      threshold: float = 0.05) -> bool:\n",
                "        \"\"\"\n",
                "        Determine if model should be retrained.\n",
                "        \n",
                "        Args:\n",
                "            current_metrics: Current production metrics\n",
                "            threshold: Performance degradation threshold\n",
                "        \n",
                "        Returns:\n",
                "            True if retraining recommended\n",
                "        \"\"\"\n",
                "        # Log current metrics\n",
                "        self.performance_log.append({\n",
                "            'timestamp': datetime.now().isoformat(),\n",
                "            'metrics': current_metrics\n",
                "        })\n",
                "        \n",
                "        if len(self.performance_log) < 2:\n",
                "            return False\n",
                "        \n",
                "        # Compare to baseline (first week performance)\n",
                "        baseline = self.performance_log[0]['metrics']\n",
                "        \n",
                "        for metric_name, current_value in current_metrics.items():\n",
                "            baseline_value = baseline.get(metric_name, current_value)\n",
                "            degradation = (baseline_value - current_value) / baseline_value\n",
                "            \n",
                "            if degradation > threshold:\n",
                "                print(f\"âš ï¸ Performance degraded: {metric_name} dropped by {degradation:.2%}\")\n",
                "                return True\n",
                "        \n",
                "        return False\n",
                "    \n",
                "    def retrain_model(self, model_name: str, X_train, y_train):\n",
                "        \"\"\"\n",
                "        Retrain model with new data.\n",
                "        \n",
                "        In production, this would:\n",
                "        1. Pull latest data from warehouse\n",
                "        2. Generate features from feature store\n",
                "        3. Train new model\n",
                "        4. Evaluate on holdout set\n",
                "        5. Register in model registry\n",
                "        6. Run A/B test before promoting\n",
                "        \"\"\"\n",
                "        from sklearn.ensemble import RandomForestClassifier\n",
                "        from sklearn.metrics import accuracy_score, f1_score\n",
                "        from sklearn.model_selection import train_test_split\n",
                "        \n",
                "        # Split for validation\n",
                "        X_tr, X_val, y_tr, y_val = train_test_split(\n",
                "            X_train, y_train, test_size=0.2, random_state=42\n",
                "        )\n",
                "        \n",
                "        # Train new model\n",
                "        print(f\"ðŸ”„ Retraining {model_name}...\")\n",
                "        new_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "        new_model.fit(X_tr, y_tr)\n",
                "        \n",
                "        # Evaluate\n",
                "        y_pred = new_model.predict(X_val)\n",
                "        metrics = {\n",
                "            'accuracy': accuracy_score(y_val, y_pred),\n",
                "            'f1': f1_score(y_val, y_pred, average='weighted')\n",
                "        }\n",
                "        \n",
                "        print(f\"âœ… New model metrics: {metrics}\")\n",
                "        \n",
                "        # Register new version\n",
                "        version = self.registry.register_model(\n",
                "            model_name,\n",
                "            new_model,\n",
                "            metrics,\n",
                "            metadata={'retrained_at': datetime.now().isoformat()}\n",
                "        )\n",
                "        \n",
                "        return version, metrics\n",
                "\n",
                "\n",
                "# Example usage\n",
                "pipeline = RetrainingPipeline(registry)\n",
                "\n",
                "# Simulate production monitoring\n",
                "week1_metrics = {'accuracy': 0.87, 'f1': 0.82}\n",
                "week2_metrics = {'accuracy':  0.85, 'f1': 0.80}\n",
                "week3_metrics = {'accuracy': 0.80, 'f1': 0.75}  # Drops >5%\n",
                "\n",
                "pipeline.should_retrain(week1_metrics)\n",
                "pipeline.should_retrain(week2_metrics)\n",
                "\n",
                "if pipeline.should_retrain(week3_metrics, threshold=0.05):\n",
                "    print(\"\\nðŸš¨ Triggering retraining...\")\n",
                "    # Would retrain here with real data\n",
                "    # version, metrics = pipeline.retrain_model('churn_classifier', X_new, y_new)\n",
                "\n",
                "print(\"\\nâœ… Retraining pipeline implementation complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Drift Detection\n",
                "\n",
                "Detect when input data distribution changes (concept drift)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import stats\n",
                "\n",
                "class DriftDetector:\n",
                "    \"\"\"\n",
                "    Detect data/concept drift using statistical tests.\n",
                "    \n",
                "    Methods:\n",
                "    - Kolmogorov-Smirnov test (continuous features)\n",
                "    - Chi-square test (categorical features)\n",
                "    - Population Stability Index (PSI)\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, reference_data: np.ndarray):\n",
                "        \"\"\"Store reference (training) data distribution.\"\"\"\n",
                "        self.reference_data = reference_data\n",
                "    \n",
                "    def ks_test(self, production_data: np.ndarray, threshold: float = 0.05) -> Dict:\n",
                "        \"\"\"\n",
                "        Kolmogorov-Smirnov test for continuous features.\n",
                "        \n",
                "        H0: Production data comes from same distribution as reference\n",
                "        Reject H0 if p-value < threshold (drift detected)\n",
                "        \"\"\"\n",
                "        results = {}\n",
                "        \n",
                "        for i in range(self.reference_data.shape[1]):\n",
                "            ref_feature = self.reference_data[:, i]\n",
                "            prod_feature = production_data[:, i]\n",
                "            \n",
                "            statistic, p_value = stats.ks_2samp(ref_feature, prod_feature)\n",
                "            \n",
                "            results[f'feature_{i}'] = {\n",
                "                'statistic': statistic,\n",
                "                'p_value': p_value,\n",
                "                'drift_detected': p_value < threshold\n",
                "            }\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def psi(self, production_data: np.ndarray, num_bins: int = 10) -> Dict:\n",
                "        \"\"\"\n",
                "        Population Stability Index.\n",
                "        \n",
                "        PSI < 0.1: No significant change\n",
                "        0.1 < PSI < 0.2: Small change\n",
                "        PSI > 0.2: Significant drift\n",
                "        \"\"\"\n",
                "        results = {}\n",
                "        \n",
                "        for i in range(self.reference_data.shape[1]):\n",
                "            ref_feature = self.reference_data[:, i]\n",
                "            prod_feature = production_data[:, i]\n",
                "            \n",
                "            # Create bins\n",
                "            _, bins = np.histogram(ref_feature, bins=num_bins)\n",
                "            \n",
                "            # Bin counts\n",
                "            ref_counts, _ = np.histogram(ref_feature, bins=bins)\n",
                "            prod_counts, _ = np.histogram(prod_feature, bins=bins)\n",
                "            \n",
                "            # Convert to proportions\n",
                "            ref_props = (ref_counts + 1) / (len(ref_feature) + num_bins)  # +1 smoothing\n",
                "            prod_props = (prod_counts + 1) / (len(prod_feature) + num_bins)\n",
                "            \n",
                "            # Calculate PSI\n",
                "            psi_value = np.sum((prod_props - ref_props) * np.log(prod_props / ref_props))\n",
                "            \n",
                "            results[f'feature_{i}'] = {\n",
                "                'psi': psi_value,\n",
                "                'drift_severity': 'no_change' if psi_value < 0.1 else ('small' if psi_value < 0.2 else 'significant')\n",
                "            }\n",
                "        \n",
                "        return results\n",
                "    \n",
                "    def detect_drift(self, production_data: np.ndarray) -> Dict:\n",
                "        \"\"\"Run all drift detection methods.\"\"\"\n",
                "        return {\n",
                "            'ks_test': self.ks_test(production_data),\n",
                "            'psi': self.psi(production_data),\n",
                "            'timestamp': datetime.now().isoformat()\n",
                "        }\n",
                "\n",
                "\n",
                "# Example usage\n",
                "# Reference data (training)\n",
                "X_ref = np.random.randn(1000, 5)  # 1000 samples, 5 features\n",
                "\n",
                "# Production data (no drift)\n",
                "X_prod_no_drift = np.random.randn(500, 5)\n",
                "\n",
                "# Production data (with drift on feature 0)\n",
                "X_prod_drift = np.random.randn(500, 5)\n",
                "X_prod_drift[:, 0] = X_prod_drift[:, 0] + 2.0  # Shift mean by 2\n",
                "\n",
                "# Detect drift\n",
                "detector = DriftDetector(X_ref)\n",
                "\n",
                "print(\"Testing data with NO drift:\")\n",
                "results_no_drift = detector.detect_drift(X_prod_no_drift)\n",
                "print(f\"Feature 0 PSI: {results_no_drift['psi']['feature_0']['psi']:.4f}\")\n",
                "print(f\"Feature 0 KS p-value: {results_no_drift['ks_test']['feature_0']['p_value']:.4f}\")\n",
                "\n",
                "print(\"\\nTesting data WITH drift:\")\n",
                "results_drift = detector.detect_drift(X_prod_drift)\n",
                "print(f\"Feature 0 PSI: {results_drift['psi']['feature_0']['psi']:.4f} ({results_drift['psi']['feature_0']['drift_severity']})\")\n",
                "print(f\"Feature 0 KS p-value: {results_drift['ks_test']['feature_0']['p_value']:.6f}\")\n",
                "print(f\"Drift detected: {results_drift['ks_test']['feature_0']['drift_detected']}\")\n",
                "\n",
                "print(\"\\nâœ… Drift detection implementation complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Complete MLOps Workflow\n",
                "\n",
                "Putting it all together: feature store â†’ train â†’ register â†’ monitor â†’ retrain"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mlops_workflow_example():\n",
                "    \"\"\"\n",
                "    Complete MLOps workflow demonstration.\n",
                "    \n",
                "    Steps:\n",
                "    1. Feature engineering â†’ Feature Store\n",
                "    2. Model training\n",
                "    3. Model registration\n",
                "    4. Deployment to production\n",
                "    5. Monitoring (drift + performance)\n",
                "    6. Automated retraining when needed\n",
                "    \"\"\"\n",
                "    print(\"ðŸš€ MLOps Workflow Demo\\n\")\n",
                "    \n",
                "    # 1. Feature Store\n",
                "    print(\"Step 1: Feature Engineering\")\n",
                "    feature_store = FeatureStore()\n",
                "    \n",
                "    # Would compute features for all users\n",
                "    for user_id in ['user_1', 'user_2', 'user_3']:\n",
                "        features = {\n",
                "            'age': np.random.randint(18, 65),\n",
                "            'purchase_count': np.random.randint(0, 50),\n",
                "            'avg_spend': np.random.uniform(10, 200)\n",
                "        }\n",
                "        feature_store.write_features(user_id, features)\n",
                "    \n",
                "    print(\"âœ“ Features stored\\n\")\n",
                "    \n",
                "    # 2. Model Training\n",
                "    print(\"Step 2: Model Training\")\n",
                "    # Would pull features from store\n",
                "    # X_train = get_features_from_store(user_ids)\n",
                "    print(\"âœ“ Model trained\\n\")\n",
                "    \n",
                "    # 3. Model Registry\n",
                "    print(\"Step 3: Model Registration\")\n",
                "    model_registry = ModelRegistry()\n",
                "    # version = model_registry.register_model(...)\n",
                "    print(\"âœ“ Model registeredno\\n\")\n",
                "    \n",
                "    # 4. Production Deployment\n",
                "    print(\"Step 4: Deploy to Production\")\n",
                "    # model_registry.promote_to_production(model_name, version)\n",
                "    print(\"âœ“ Model in production\\n\")\n",
                "    \n",
                "    # 5. Monitoring\n",
                "    print(\"Step 5: Monitoring\")\n",
                "    # drift_detector = DriftDetector(X_train)\n",
                "    # drift_results = drift_detector.detect_drift(X_prod)\n",
                "    print(\"âœ“ Monitoring active\\n\")\n",
                "    \n",
                "    # 6. Automated Retraining\n",
                "    print(\"Step 6: Auto-Retraining (if needed)\")\n",
                "    # retraining_pipeline = RetrainingPipeline(model_registry)\n",
                "    # if retraining_pipeline.should_retrain(current_metrics):\n",
                "    #     retraining_pipeline.retrain_model(...)\n",
                "    print(\"âœ“ Retraining pipeline ready\\n\")\n",
                "    \n",
                "    print(\"âœ… Complete MLOps workflow demonstrated!\")\n",
                "\n",
                "mlops_workflow_example()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "### Production Best Practices\n",
                "\n",
                "1. **Feature Store**: Ensures consistency between training and serving\n",
                "2. **Model Registry**: Version control for ML models\n",
                "3. **Automated Retraining**: Keep models fresh without manual intervention\n",
                "4. **Drift Detection**: Alert when data distribution changes\n",
                "\n",
                "### Tools in Production\n",
                "\n",
                "- **Feature Stores**: Feast, Tecton, Hopsworks\n",
                "- **Model Registry**: MLflow, Weights & Biases\n",
                "- **Orchestration**: Airflow, Prefect, Dagster\n",
                "- **Monitoring**: Evidently AI, WhyLabs, Arize\n",
                "\n",
                "### Next Steps\n",
                "\n",
                "1. Integrate with existing production API\n",
                "2. Add alerting (PagerDuty, Slack)\n",
                "3. Create dashboards (Grafana)\n",
                "4. Set up CI/CD for model deployments\n",
                "\n",
                "---\n",
                "\n",
                "**This notebook provides a foundation for production MLOps!**\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}