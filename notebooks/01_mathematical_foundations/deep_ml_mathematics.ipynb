{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§® Deep Mathematical Foundations of Machine Learning\n",
                "\n",
                "## From Theory to Industrial Applications\n",
                "\n",
                "---\n",
                "\n",
                "**Author:** AI-Mastery-2026 Project  \n",
                "**Purpose:** A comprehensive, white-box exploration of the mathematical pillars of Machine Learning.\n",
                "\n",
                "### ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "\n",
                "1. **Linear Algebra**: Understand vectors, matrices, eigenvalues, SVD, and their applications (PageRank, Netflix Recommendations)\n",
                "2. **Calculus**: Master gradients, Jacobians, backpropagation, and the Transformer's self-attention mechanism\n",
                "3. **Optimization**: Apply Lagrange multipliers, understand SVM's dual form, and learn ADMM for industrial scale\n",
                "4. **Probability & Information Theory**: Grasp entropy, KL divergence, VAEs, and t-SNE\n",
                "5. **Advanced Methods**: Implement Monte Carlo integration, importance sampling, and normalizing flows\n",
                "6. **Network Analysis**: Build link prediction models (Facebook's supervised random walks)\n",
                "7. **Bayesian Optimization**: Use Gaussian Processes and acquisition functions\n",
                "\n",
                "---\n",
                "\n",
                "### ðŸ“š Prerequisites\n",
                "\n",
                "- Basic Python programming\n",
                "- Familiarity with NumPy arrays\n",
                "- High school algebra and calculus concepts\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# IMPORTS AND SETUP\n",
                "# ============================================\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats, linalg\n",
                "from typing import Tuple, List, Callable\n",
                "import warnings\n",
                "\n",
                "# Configuration\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "np.random.seed(42)\n",
                "np.set_printoptions(precision=4, suppress=True)\n",
                "\n",
                "print(\"âœ… Setup complete! NumPy version:\", np.__version__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 1: Linear Algebra - The Language of Data\n",
                "\n",
                "---\n",
                "\n",
                "Linear algebra provides the formal framework for representing and manipulating data. In ML, we rarely work with single numbers (scalars); instead, we work with **vectors** (points in space), **matrices** (transformations), and **tensors** (higher-dimensional arrays).\n",
                "\n",
                "> **Key Insight**: A matrix is not just a table of numbersâ€”it is a *function* that transforms one vector space into another."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1 Vector Spaces and the Dot Product\n",
                "\n",
                "### Theory\n",
                "\n",
                "In ML, a **vector** $\\mathbf{x} \\in \\mathbb{R}^n$ represents an entity (an image, a user, a word). The **dot product** measures the relationship between two vectors:\n",
                "\n",
                "$$\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^n x_i y_i = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos(\\theta)$$\n",
                "\n",
                "Where:\n",
                "- $\\|\\mathbf{x}\\| = \\sqrt{\\sum x_i^2}$ is the L2 norm (vector length)\n",
                "- $\\theta$ is the angle between the vectors\n",
                "\n",
                "### ðŸ“ Mathematical Example\n",
                "\n",
                "**Given:** $\\mathbf{x} = [1, 2, 3]$, $\\mathbf{y} = [4, 5, 6]$\n",
                "\n",
                "**Compute the dot product:**\n",
                "\n",
                "$$\\mathbf{x} \\cdot \\mathbf{y} = (1 \\times 4) + (2 \\times 5) + (3 \\times 6) = 4 + 10 + 18 = 32$$\n",
                "\n",
                "**Compute the norms:**\n",
                "\n",
                "$$\\|\\mathbf{x}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14} \\approx 3.742$$\n",
                "\n",
                "$$\\|\\mathbf{y}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{77} \\approx 8.775$$\n",
                "\n",
                "**Compute the angle:**\n",
                "\n",
                "$$\\cos(\\theta) = \\frac{32}{3.742 \\times 8.775} = \\frac{32}{32.83} \\approx 0.9746$$\n",
                "\n",
                "$$\\theta = \\arccos(0.9746) \\approx 12.93Â°$$\n",
                "\n",
                "The vectors are nearly aligned (small angle)!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.1 DOT PRODUCT IMPLEMENTATION\n",
                "# ============================================\n",
                "\n",
                "def dot_product_from_scratch(x: np.ndarray, y: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Compute dot product manually.\n",
                "    \n",
                "    Args:\n",
                "        x: First vector of shape (n,)\n",
                "        y: Second vector of shape (n,)\n",
                "    \n",
                "    Returns:\n",
                "        Scalar dot product value\n",
                "    \"\"\"\n",
                "    assert len(x) == len(y), \"Vectors must have same dimension\"\n",
                "    return sum(xi * yi for xi, yi in zip(x, y))\n",
                "\n",
                "\n",
                "def compute_angle_between_vectors(x: np.ndarray, y: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Compute angle (in degrees) between two vectors.\n",
                "    \"\"\"\n",
                "    dot = np.dot(x, y)\n",
                "    norm_x = np.linalg.norm(x)\n",
                "    norm_y = np.linalg.norm(y)\n",
                "    cos_theta = dot / (norm_x * norm_y)\n",
                "    # Clamp to [-1, 1] to handle numerical errors\n",
                "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
                "    return np.degrees(np.arccos(cos_theta))\n",
                "\n",
                "\n",
                "# Example from the mathematical derivation\n",
                "x = np.array([1, 2, 3])\n",
                "y = np.array([4, 5, 6])\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"DOT PRODUCT EXAMPLE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"x = {x}\")\n",
                "print(f\"y = {y}\")\n",
                "print(f\"\\nManual dot product: {dot_product_from_scratch(x, y)}\")\n",
                "print(f\"NumPy dot product:  {np.dot(x, y)}\")\n",
                "print(f\"\\n||x|| = {np.linalg.norm(x):.4f}\")\n",
                "print(f\"||y|| = {np.linalg.norm(y):.4f}\")\n",
                "print(f\"\\nAngle between vectors: {compute_angle_between_vectors(x, y):.2f}Â°\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Cosine Similarity in NLP\n",
                "\n",
                "### Theory\n",
                "\n",
                "When vectors are **normalized** (unit length), their dot product equals the **cosine similarity**:\n",
                "\n",
                "$$\\text{sim}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$$\n",
                "\n",
                "**Interpretation**:\n",
                "- $\\text{sim} = 1$: Vectors point in the same direction (identical meaning)\n",
                "- $\\text{sim} = 0$: Vectors are perpendicular (unrelated)\n",
                "- $\\text{sim} = -1$: Vectors point in opposite directions (opposite meaning)\n",
                "\n",
                "### ðŸ­ Industrial Application: Word Embeddings\n",
                "\n",
                "In NLP models like Word2Vec or BERT, words are converted to vectors (embeddings). Semantically similar words have high cosine similarity.\n",
                "\n",
                "**Famous example**: $\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$\n",
                "\n",
                "### ðŸ“ Mathematical Example\n",
                "\n",
                "**Given embeddings:**\n",
                "- \"cat\" â†’ $[0.7, 0.5, 0.1]$\n",
                "- \"dog\" â†’ $[0.6, 0.6, 0.2]$\n",
                "- \"car\" â†’ $[0.1, 0.2, 0.9]$\n",
                "\n",
                "**Compute similarity between \"cat\" and \"dog\":**\n",
                "\n",
                "$$\\text{sim}(\\text{cat}, \\text{dog}) = \\frac{0.7 \\times 0.6 + 0.5 \\times 0.6 + 0.1 \\times 0.2}{\\sqrt{0.75} \\times \\sqrt{0.76}} = \\frac{0.74}{0.755} \\approx 0.98$$\n",
                "\n",
                "High similarity! Both are animals."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.2 COSINE SIMILARITY IMPLEMENTATION\n",
                "# ============================================\n",
                "\n",
                "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Compute cosine similarity between two vectors.\n",
                "    \n",
                "    Args:\n",
                "        a, b: Input vectors\n",
                "    \n",
                "    Returns:\n",
                "        Similarity score in [-1, 1]\n",
                "    \"\"\"\n",
                "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
                "\n",
                "\n",
                "# Simulated word embeddings\n",
                "embeddings = {\n",
                "    \"cat\": np.array([0.7, 0.5, 0.1]),\n",
                "    \"dog\": np.array([0.6, 0.6, 0.2]),\n",
                "    \"kitten\": np.array([0.65, 0.55, 0.12]),\n",
                "    \"car\": np.array([0.1, 0.2, 0.9]),\n",
                "    \"truck\": np.array([0.15, 0.18, 0.85]),\n",
                "}\n",
                "\n",
                "# Compute similarity matrix\n",
                "words = list(embeddings.keys())\n",
                "n = len(words)\n",
                "sim_matrix = np.zeros((n, n))\n",
                "\n",
                "for i, w1 in enumerate(words):\n",
                "    for j, w2 in enumerate(words):\n",
                "        sim_matrix[i, j] = cosine_similarity(embeddings[w1], embeddings[w2])\n",
                "\n",
                "# Visualization\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(sim_matrix, annot=True, fmt='.3f', \n",
                "            xticklabels=words, yticklabels=words,\n",
                "            cmap='RdYlGn', center=0.5, vmin=0, vmax=1)\n",
                "plt.title('Word Embedding Cosine Similarity Matrix', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ Insight: 'cat' and 'kitten' are most similar (both felines).\")\n",
                "print(\"   'cat' and 'car' are least similar (animal vs vehicle).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.3 Matrices as Linear Transformations\n",
                "\n",
                "### Theory\n",
                "\n",
                "A matrix $A \\in \\mathbb{R}^{m \\times n}$ can be viewed as a **function** that transforms an $n$-dimensional vector space to an $m$-dimensional space:\n",
                "\n",
                "$$\\mathbf{y} = A\\mathbf{x}$$\n",
                "\n",
                "Common transformations:\n",
                "- **Scaling**: Stretch or shrink vectors\n",
                "- **Rotation**: Rotate vectors around origin\n",
                "- **Projection**: Project onto lower-dimensional subspace\n",
                "- **Shearing**: Skew the space\n",
                "\n",
                "### ðŸ“ Mathematical Example: 2D Rotation\n",
                "\n",
                "The rotation matrix for angle $\\theta$ is:\n",
                "\n",
                "$$R_\\theta = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$$\n",
                "\n",
                "**Rotate $\\mathbf{v} = [1, 0]$ by 45Â°:**\n",
                "\n",
                "$$R_{45Â°} = \\begin{bmatrix} 0.707 & -0.707 \\\\ 0.707 & 0.707 \\end{bmatrix}$$\n",
                "\n",
                "$$\\mathbf{v}' = R_{45Â°} \\mathbf{v} = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.3 LINEAR TRANSFORMATIONS VISUALIZATION\n",
                "# ============================================\n",
                "\n",
                "def rotation_matrix(theta_deg: float) -> np.ndarray:\n",
                "    \"\"\"Create 2D rotation matrix for given angle in degrees.\"\"\"\n",
                "    theta = np.radians(theta_deg)\n",
                "    return np.array([\n",
                "        [np.cos(theta), -np.sin(theta)],\n",
                "        [np.sin(theta),  np.cos(theta)]\n",
                "    ])\n",
                "\n",
                "\n",
                "def scaling_matrix(sx: float, sy: float) -> np.ndarray:\n",
                "    \"\"\"Create 2D scaling matrix.\"\"\"\n",
                "    return np.array([[sx, 0], [0, sy]])\n",
                "\n",
                "\n",
                "def shear_matrix(k: float) -> np.ndarray:\n",
                "    \"\"\"Create 2D shear matrix.\"\"\"\n",
                "    return np.array([[1, k], [0, 1]])\n",
                "\n",
                "\n",
                "# Create a unit square\n",
                "square = np.array([\n",
                "    [0, 1, 1, 0, 0],  # x coordinates\n",
                "    [0, 0, 1, 1, 0]   # y coordinates\n",
                "])\n",
                "\n",
                "# Define transformations\n",
                "transformations = [\n",
                "    (\"Original\", np.eye(2)),\n",
                "    (\"Rotation 45Â°\", rotation_matrix(45)),\n",
                "    (\"Scale (2x, 0.5x)\", scaling_matrix(2, 0.5)),\n",
                "    (\"Shear (k=0.5)\", shear_matrix(0.5)),\n",
                "]\n",
                "\n",
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "\n",
                "for ax, (name, T) in zip(axes, transformations):\n",
                "    transformed = T @ square\n",
                "    ax.fill(transformed[0], transformed[1], alpha=0.4, color='steelblue')\n",
                "    ax.plot(transformed[0], transformed[1], 'b-', linewidth=2)\n",
                "    ax.set_xlim(-1, 3)\n",
                "    ax.set_ylim(-1, 2)\n",
                "    ax.set_aspect('equal')\n",
                "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
                "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
                "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Matrix Transformations on a Unit Square', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ”‘ Key Insight: Neural network layers are sequences of linear transformations\")\n",
                "print(\"   followed by non-linear activation functions!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.4 Convolution as Matrix Multiplication (Toeplitz Matrices)\n",
                "\n",
                "### Theory\n",
                "\n",
                "In **Convolutional Neural Networks (CNNs)**, the convolution operation can be reformulated as matrix multiplication using **Toeplitz matrices**. This allows:\n",
                "\n",
                "1. Efficient GPU computation (GPUs are optimized for matrix operations)\n",
                "2. Unified backpropagation with standard linear algebra\n",
                "\n",
                "### The Process (im2col)\n",
                "\n",
                "1. Flatten the input image patches into column vectors\n",
                "2. Arrange them as columns of a matrix\n",
                "3. Multiply by the flattened kernel\n",
                "\n",
                "### ðŸ“ Mathematical Example: 1D Convolution\n",
                "\n",
                "**Input**: $x = [1, 2, 3, 4, 5]$  \n",
                "**Kernel**: $k = [1, 0, -1]$ (edge detector)\n",
                "\n",
                "The Toeplitz matrix representation:\n",
                "\n",
                "$$T = \\begin{bmatrix} 3 & 2 & 1 \\\\ 4 & 3 & 2 \\\\ 5 & 4 & 3 \\end{bmatrix}$$\n",
                "\n",
                "$$y = T \\cdot k = \\begin{bmatrix} 3-1 \\\\ 4-2 \\\\ 5-3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.4 CONVOLUTION AS MATRIX MULTIPLICATION\n",
                "# ============================================\n",
                "\n",
                "def conv1d_standard(x: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Standard 1D convolution (valid mode).\"\"\"\n",
                "    k_size = len(kernel)\n",
                "    out_size = len(x) - k_size + 1\n",
                "    result = np.zeros(out_size)\n",
                "    for i in range(out_size):\n",
                "        result[i] = np.sum(x[i:i+k_size] * kernel)\n",
                "    return result\n",
                "\n",
                "\n",
                "def conv1d_as_matmul(x: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    1D convolution implemented as matrix multiplication.\n",
                "    Demonstrates the Toeplitz matrix approach.\n",
                "    \"\"\"\n",
                "    k_size = len(kernel)\n",
                "    out_size = len(x) - k_size + 1\n",
                "    \n",
                "    # Build the Toeplitz-like matrix (im2col)\n",
                "    # Each row contains a patch of the input\n",
                "    patches = np.zeros((out_size, k_size))\n",
                "    for i in range(out_size):\n",
                "        patches[i] = x[i:i+k_size]\n",
                "    \n",
                "    # Matrix multiplication: patches @ kernel\n",
                "    return patches @ kernel\n",
                "\n",
                "\n",
                "# Example\n",
                "x = np.array([1, 2, 3, 4, 5], dtype=float)\n",
                "kernel = np.array([1, 0, -1], dtype=float)  # Edge detector\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"CONVOLUTION AS MATRIX MULTIPLICATION\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Input x = {x}\")\n",
                "print(f\"Kernel  = {kernel}\")\n",
                "\n",
                "# Standard convolution\n",
                "y_standard = conv1d_standard(x, kernel)\n",
                "print(f\"\\nStandard convolution: {y_standard}\")\n",
                "\n",
                "# Matrix multiplication approach\n",
                "y_matmul = conv1d_as_matmul(x, kernel)\n",
                "print(f\"Matrix multiplication: {y_matmul}\")\n",
                "\n",
                "# Show the patch matrix\n",
                "out_size = len(x) - len(kernel) + 1\n",
                "patches = np.zeros((out_size, len(kernel)))\n",
                "for i in range(out_size):\n",
                "    patches[i] = x[i:i+len(kernel)]\n",
                "\n",
                "print(f\"\\nPatch matrix (im2col):\")\n",
                "print(patches)\n",
                "print(f\"\\nComputation: patches @ kernel = result\")\n",
                "\n",
                "# Verify they match\n",
                "assert np.allclose(y_standard, y_matmul), \"Results should match!\"\n",
                "print(\"\\nâœ… Both methods produce identical results!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.5 Eigenvalues and Google's PageRank\n",
                "\n",
                "### Theory\n",
                "\n",
                "An **eigenvector** $\\mathbf{v}$ of matrix $A$ is a vector that only gets scaled (not rotated) when $A$ is applied:\n",
                "\n",
                "$$A\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
                "\n",
                "Where $\\lambda$ is the **eigenvalue** (the scaling factor).\n",
                "\n",
                "### ðŸ­ Industrial Application: Google PageRank\n",
                "\n",
                "The web is modeled as a directed graph. PageRank finds the **stationary distribution** of a random surferâ€”this is the eigenvector corresponding to eigenvalue $\\lambda = 1$ of the transition matrix.\n",
                "\n",
                "**Modified PageRank equation** (with damping $d = 0.85$):\n",
                "\n",
                "$$\\mathbf{R} = d \\cdot M \\cdot \\mathbf{R} + \\frac{1-d}{N} \\mathbf{1}$$\n",
                "\n",
                "### ðŸ“ Mathematical Example: 4-Page Web\n",
                "\n",
                "Consider a web with 4 pages:\n",
                "- Page 1 â†’ links to 2, 3\n",
                "- Page 2 â†’ links to 1, 3\n",
                "- Page 3 â†’ links to 1\n",
                "- Page 4 â†’ links to 1, 2, 3\n",
                "\n",
                "Transition matrix $M$ (column-stochastic):\n",
                "\n",
                "$$M = \\begin{bmatrix} 0 & 0.5 & 1 & 0.33 \\\\ 0.5 & 0 & 0 & 0.33 \\\\ 0.5 & 0.5 & 0 & 0.33 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.5 PAGERANK FROM SCRATCH\n",
                "# ============================================\n",
                "\n",
                "def pagerank(adj_matrix: np.ndarray, damping: float = 0.85, \n",
                "             max_iter: int = 100, tol: float = 1e-6) -> Tuple[np.ndarray, List[float]]:\n",
                "    \"\"\"\n",
                "    Compute PageRank using power iteration.\n",
                "    \n",
                "    Args:\n",
                "        adj_matrix: Adjacency matrix (adj[i,j]=1 if i links to j)\n",
                "        damping: Damping factor (probability of following a link)\n",
                "        max_iter: Maximum iterations\n",
                "        tol: Convergence tolerance\n",
                "    \n",
                "    Returns:\n",
                "        ranks: PageRank scores\n",
                "        history: Convergence history\n",
                "    \"\"\"\n",
                "    n = adj_matrix.shape[0]\n",
                "    \n",
                "    # Create column-stochastic transition matrix\n",
                "    out_degree = adj_matrix.sum(axis=1)\n",
                "    out_degree[out_degree == 0] = 1  # Handle dangling nodes\n",
                "    M = (adj_matrix.T / out_degree).T\n",
                "    M = M.T  # Column stochastic\n",
                "    \n",
                "    # Initialize uniform distribution\n",
                "    r = np.ones(n) / n\n",
                "    history = []\n",
                "    \n",
                "    # Power iteration\n",
                "    for _ in range(max_iter):\n",
                "        r_new = damping * M @ r + (1 - damping) / n\n",
                "        diff = np.linalg.norm(r_new - r)\n",
                "        history.append(diff)\n",
                "        \n",
                "        if diff < tol:\n",
                "            break\n",
                "        r = r_new\n",
                "    \n",
                "    return r_new / r_new.sum(), history  # Normalize\n",
                "\n",
                "\n",
                "# Create adjacency matrix for our 4-page example\n",
                "# adj[i,j] = 1 means page i links to page j\n",
                "adj = np.array([\n",
                "    [0, 1, 1, 0],  # Page 0 â†’ 1, 2\n",
                "    [1, 0, 1, 0],  # Page 1 â†’ 0, 2\n",
                "    [1, 0, 0, 0],  # Page 2 â†’ 0\n",
                "    [1, 1, 1, 0],  # Page 3 â†’ 0, 1, 2\n",
                "])\n",
                "\n",
                "ranks, history = pagerank(adj)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"PAGERANK RESULTS\")\n",
                "print(\"=\"*50)\n",
                "for i, rank in enumerate(ranks):\n",
                "    print(f\"Page {i}: {rank:.4f} ({rank*100:.1f}%)\")\n",
                "\n",
                "print(f\"\\nðŸ† Most important page: Page {np.argmax(ranks)}\")\n",
                "print(f\"   (Page 0 receives the most incoming links)\")\n",
                "\n",
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
                "\n",
                "# Bar chart of ranks\n",
                "axes[0].bar(range(4), ranks, color='steelblue', edgecolor='black')\n",
                "axes[0].set_xlabel('Page')\n",
                "axes[0].set_ylabel('PageRank Score')\n",
                "axes[0].set_title('PageRank Scores', fontweight='bold')\n",
                "axes[0].set_xticks(range(4))\n",
                "\n",
                "# Convergence plot\n",
                "axes[1].semilogy(history, 'b-', linewidth=2)\n",
                "axes[1].set_xlabel('Iteration')\n",
                "axes[1].set_ylabel('Change (log scale)')\n",
                "axes[1].set_title('Convergence of Power Iteration', fontweight='bold')\n",
                "axes[1].axhline(y=1e-6, color='r', linestyle='--', label='Tolerance')\n",
                "axes[1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.6 SVD and Netflix Recommendation System\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Singular Value Decomposition (SVD)** factorizes any matrix $R$ into:\n",
                "\n",
                "$$R = U \\Sigma V^T$$\n",
                "\n",
                "Where:\n",
                "- $U$: Left singular vectors (user features)\n",
                "- $\\Sigma$: Diagonal matrix of singular values (importance)\n",
                "- $V^T$: Right singular vectors (item features)\n",
                "\n",
                "### ðŸ­ Industrial Application: Netflix Prize\n",
                "\n",
                "For recommendation systems, we approximate the sparse rating matrix:\n",
                "\n",
                "$$R \\approx P \\times Q^T$$\n",
                "\n",
                "Where:\n",
                "- $P$ (users Ã— k): User latent factors\n",
                "- $Q$ (items Ã— k): Item latent factors\n",
                "- $k$: Number of latent features\n",
                "\n",
                "The optimization objective (with regularization):\n",
                "\n",
                "$$\\min_{P, Q} \\sum_{(u, i) \\in \\mathcal{K}} (r_{ui} - \\mathbf{p}_u^T \\mathbf{q}_i)^2 + \\lambda (\\|\\mathbf{p}_u\\|^2 + \\|\\mathbf{q}_i\\|^2)$$\n",
                "\n",
                "### Update Rules (SGD)\n",
                "\n",
                "For each observed rating $(u, i, r_{ui})$:\n",
                "\n",
                "$$e_{ui} = r_{ui} - \\mathbf{p}_u^T \\mathbf{q}_i$$\n",
                "\n",
                "$$\\mathbf{p}_u \\leftarrow \\mathbf{p}_u + \\alpha (e_{ui} \\mathbf{q}_i - \\lambda \\mathbf{p}_u)$$\n",
                "\n",
                "$$\\mathbf{q}_i \\leftarrow \\mathbf{q}_i + \\alpha (e_{ui} \\mathbf{p}_u - \\lambda \\mathbf{q}_i)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.6 NETFLIX-STYLE RECOMMENDER (FunkSVD)\n",
                "# ============================================\n",
                "\n",
                "class MatrixFactorization:\n",
                "    \"\"\"\n",
                "    Matrix Factorization for Collaborative Filtering.\n",
                "    Implements the FunkSVD algorithm used in Netflix Prize.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, n_factors: int = 10, lr: float = 0.005, \n",
                "                 reg: float = 0.02, n_epochs: int = 100):\n",
                "        self.n_factors = n_factors\n",
                "        self.lr = lr\n",
                "        self.reg = reg\n",
                "        self.n_epochs = n_epochs\n",
                "        \n",
                "    def fit(self, R: np.ndarray) -> List[float]:\n",
                "        \"\"\"\n",
                "        Train the model on rating matrix R.\n",
                "        \n",
                "        Args:\n",
                "            R: Rating matrix (users Ã— items), 0 = missing\n",
                "            \n",
                "        Returns:\n",
                "            Training loss history\n",
                "        \"\"\"\n",
                "        self.n_users, self.n_items = R.shape\n",
                "        \n",
                "        # Initialize latent factors with small random values\n",
                "        self.P = np.random.normal(0, 0.1, (self.n_users, self.n_factors))\n",
                "        self.Q = np.random.normal(0, 0.1, (self.n_items, self.n_factors))\n",
                "        \n",
                "        # Find observed ratings\n",
                "        self.samples = [\n",
                "            (u, i, R[u, i])\n",
                "            for u in range(self.n_users)\n",
                "            for i in range(self.n_items)\n",
                "            if R[u, i] > 0\n",
                "        ]\n",
                "        \n",
                "        history = []\n",
                "        for epoch in range(self.n_epochs):\n",
                "            np.random.shuffle(self.samples)\n",
                "            \n",
                "            for u, i, r in self.samples:\n",
                "                # Prediction and error\n",
                "                pred = self.P[u] @ self.Q[i]\n",
                "                error = r - pred\n",
                "                \n",
                "                # Gradient descent updates\n",
                "                P_u = self.P[u].copy()\n",
                "                self.P[u] += self.lr * (error * self.Q[i] - self.reg * self.P[u])\n",
                "                self.Q[i] += self.lr * (error * P_u - self.reg * self.Q[i])\n",
                "            \n",
                "            # Compute training loss\n",
                "            loss = self._compute_loss(R)\n",
                "            history.append(loss)\n",
                "            \n",
                "            if (epoch + 1) % 20 == 0:\n",
                "                print(f\"Epoch {epoch+1:3d}: Loss = {loss:.4f}\")\n",
                "        \n",
                "        return history\n",
                "    \n",
                "    def _compute_loss(self, R: np.ndarray) -> float:\n",
                "        \"\"\"Compute regularized MSE loss.\"\"\"\n",
                "        loss = 0\n",
                "        for u, i, r in self.samples:\n",
                "            pred = self.P[u] @ self.Q[i]\n",
                "            loss += (r - pred) ** 2\n",
                "        # Add regularization\n",
                "        loss += self.reg * (np.sum(self.P**2) + np.sum(self.Q**2))\n",
                "        return loss / len(self.samples)\n",
                "    \n",
                "    def predict(self) -> np.ndarray:\n",
                "        \"\"\"Predict all ratings.\"\"\"\n",
                "        return self.P @ self.Q.T\n",
                "\n",
                "\n",
                "# Create sample rating matrix (0 = missing)\n",
                "R = np.array([\n",
                "    [5, 3, 0, 1, 0],\n",
                "    [4, 0, 0, 1, 0],\n",
                "    [1, 1, 0, 5, 0],\n",
                "    [0, 0, 5, 4, 0],\n",
                "    [0, 1, 5, 4, 0],\n",
                "], dtype=float)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"NETFLIX-STYLE RECOMMENDATION SYSTEM\")\n",
                "print(\"=\"*50)\n",
                "print(\"Original Rating Matrix (0 = missing):\")\n",
                "print(R)\n",
                "\n",
                "# Train model\n",
                "model = MatrixFactorization(n_factors=3, lr=0.01, reg=0.01, n_epochs=100)\n",
                "history = model.fit(R)\n",
                "\n",
                "# Get predictions\n",
                "predictions = model.predict()\n",
                "print(\"\\nPredicted Ratings:\")\n",
                "print(np.round(predictions, 2))\n",
                "\n",
                "# Highlight filled-in values\n",
                "print(\"\\nðŸŽ¬ Recommendations (previously missing ratings):\")\n",
                "for u in range(R.shape[0]):\n",
                "    for i in range(R.shape[1]):\n",
                "        if R[u, i] == 0:\n",
                "            print(f\"  User {u} â†’ Item {i}: Predicted {predictions[u, i]:.2f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}