{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 2: Calculus - The Engine of Learning\n",
                "\n",
                "---\n",
                "\n",
                "If linear algebra is the skeleton of ML, calculus is the muscle that drives learning. The process of **training** a model is fundamentally about finding parameters that minimize a loss function‚Äîthis is **optimization**, which relies entirely on derivatives.\n",
                "\n",
                "> **Key Insight**: The gradient always points toward the direction of steepest increase. To minimize, we move in the *opposite* direction."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.1 Gradients and the Jacobian Matrix\n",
                "\n",
                "### Theory\n",
                "\n",
                "For a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the **gradient** is the vector of partial derivatives:\n",
                "\n",
                "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
                "\n",
                "For a **vector-valued function** $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$, we use the **Jacobian matrix**:\n",
                "\n",
                "$$J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\n",
                "\n",
                "### üìê Mathematical Example\n",
                "\n",
                "**Function**: $f(x, y) = x^2 + xy + y^2$\n",
                "\n",
                "**Partial derivatives**:\n",
                "- $\\frac{\\partial f}{\\partial x} = 2x + y$\n",
                "- $\\frac{\\partial f}{\\partial y} = x + 2y$\n",
                "\n",
                "**At point $(x, y) = (1, 2)$**:\n",
                "\n",
                "$$\\nabla f(1, 2) = \\begin{bmatrix} 2(1) + 2 \\\\ 1 + 2(2) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}$$\n",
                "\n",
                "The gradient magnitude: $\\|\\nabla f\\| = \\sqrt{16 + 25} = \\sqrt{41} \\approx 6.4$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from typing import Callable, Tuple\n",
                "\n",
                "# ============================================\n",
                "# 2.1 GRADIENT COMPUTATION\n",
                "# ============================================\n",
                "\n",
                "def numerical_gradient(f: Callable, x: np.ndarray, h: float = 1e-5) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Compute gradient numerically using central differences.\n",
                "    \n",
                "    Args:\n",
                "        f: Scalar function\n",
                "        x: Point to evaluate gradient\n",
                "        h: Step size for finite differences\n",
                "    \n",
                "    Returns:\n",
                "        Gradient vector\n",
                "    \"\"\"\n",
                "    grad = np.zeros_like(x, dtype=float)\n",
                "    for i in range(len(x)):\n",
                "        x_plus = x.copy()\n",
                "        x_minus = x.copy()\n",
                "        x_plus[i] += h\n",
                "        x_minus[i] -= h\n",
                "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
                "    return grad\n",
                "\n",
                "\n",
                "def analytical_gradient(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Analytical gradient of f(x,y) = x^2 + xy + y^2\"\"\"\n",
                "    return np.array([2*x[0] + x[1], x[0] + 2*x[1]])\n",
                "\n",
                "\n",
                "# Define our function\n",
                "def f(x):\n",
                "    return x[0]**2 + x[0]*x[1] + x[1]**2\n",
                "\n",
                "\n",
                "# Test at point (1, 2)\n",
                "point = np.array([1.0, 2.0])\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"GRADIENT COMPUTATION\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Function: f(x,y) = x¬≤ + xy + y¬≤\")\n",
                "print(f\"Point: ({point[0]}, {point[1]})\")\n",
                "print(f\"f({point[0]}, {point[1]}) = {f(point)}\")\n",
                "print(f\"\\nNumerical gradient:  {numerical_gradient(f, point)}\")\n",
                "print(f\"Analytical gradient: {analytical_gradient(point)}\")\n",
                "print(f\"\\nGradient magnitude: {np.linalg.norm(analytical_gradient(point)):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Gradient field\n",
                "x = np.linspace(-3, 3, 20)\n",
                "y = np.linspace(-3, 3, 20)\n",
                "X, Y = np.meshgrid(x, y)\n",
                "Z = X**2 + X*Y + Y**2\n",
                "\n",
                "# Gradient components\n",
                "U = 2*X + Y  # df/dx\n",
                "V = X + 2*Y  # df/dy\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
                "plt.colorbar(label='f(x, y)')\n",
                "plt.quiver(X, Y, U, V, color='red', alpha=0.6)\n",
                "plt.scatter([1], [2], c='yellow', s=200, marker='*', edgecolors='black', zorder=5)\n",
                "plt.annotate('Point (1,2)', (1.1, 2.2), fontsize=12, fontweight='bold')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('y')\n",
                "plt.title('Gradient Field of f(x,y) = x¬≤ + xy + y¬≤', fontsize=14, fontweight='bold')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.axis('equal')\n",
                "plt.show()\n",
                "\n",
                "print(\"üî¥ Red arrows show gradient direction (steepest ascent)\")\n",
                "print(\"üìâ To minimize, move OPPOSITE to the gradient!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.2 Backpropagation via Chain Rule\n",
                "\n",
                "### Theory\n",
                "\n",
                "Neural networks are **composite functions**. If we have layers:\n",
                "\n",
                "$$L = \\text{Loss}(f_3(f_2(f_1(x))))$$\n",
                "\n",
                "The **chain rule** lets us compute derivatives through the composition:\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial x}$$\n",
                "\n",
                "### üìê Mathematical Example: 2-Layer Network\n",
                "\n",
                "**Forward pass**:\n",
                "- Input: $x = 2$\n",
                "- **Layer 1**: $h = wx = 3 \\times 2 = 6$ (weight $w = 3$)\n",
                "- **Activation**: $a = \\text{ReLU}(h) = \\max(0, 6) = 6$\n",
                "- **Layer 2**: $\\hat{y} = va = 0.5 \\times 6 = 3$ (weight $v = 0.5$)\n",
                "- **Loss**: $L = (\\hat{y} - y)^2 = (3 - 4)^2 = 1$ (target $y = 4$)\n",
                "\n",
                "**Backward pass**:\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y) = 2(3 - 4) = -2$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial v} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial v} = -2 \\times a = -2 \\times 6 = -12$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a} = -2 \\times v = -2 \\times 0.5 = -1$$\n",
                "\n",
                "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w} = -1 \\times 1 \\times x = -1 \\times 1 \\times 2 = -2$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 2.2 BACKPROPAGATION FROM SCRATCH\n",
                "# ============================================\n",
                "\n",
                "class SimpleNeuron:\n",
                "    \"\"\"A simple 2-layer network demonstrating backpropagation.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.w = 3.0  # Layer 1 weight\n",
                "        self.v = 0.5  # Layer 2 weight\n",
                "        \n",
                "    def forward(self, x: float) -> Tuple[float, dict]:\n",
                "        \"\"\"Forward pass with caching for backprop.\"\"\"\n",
                "        h = self.w * x           # Linear layer 1\n",
                "        a = max(0, h)            # ReLU activation\n",
                "        y_hat = self.v * a       # Linear layer 2\n",
                "        \n",
                "        # Cache for backward pass\n",
                "        cache = {'x': x, 'h': h, 'a': a, 'y_hat': y_hat}\n",
                "        return y_hat, cache\n",
                "    \n",
                "    def backward(self, y_true: float, cache: dict) -> dict:\n",
                "        \"\"\"Backward pass computing all gradients.\"\"\"\n",
                "        x, h, a, y_hat = cache['x'], cache['h'], cache['a'], cache['y_hat']\n",
                "        \n",
                "        # Loss: L = (y_hat - y_true)^2\n",
                "        dL_dy_hat = 2 * (y_hat - y_true)\n",
                "        \n",
                "        # Layer 2: y_hat = v * a\n",
                "        dL_dv = dL_dy_hat * a\n",
                "        dL_da = dL_dy_hat * self.v\n",
                "        \n",
                "        # ReLU: a = max(0, h)\n",
                "        dL_dh = dL_da * (1 if h > 0 else 0)\n",
                "        \n",
                "        # Layer 1: h = w * x\n",
                "        dL_dw = dL_dh * x\n",
                "        dL_dx = dL_dh * self.w\n",
                "        \n",
                "        return {\n",
                "            'dL/dy_hat': dL_dy_hat,\n",
                "            'dL/dv': dL_dv,\n",
                "            'dL/da': dL_da,\n",
                "            'dL/dh': dL_dh,\n",
                "            'dL/dw': dL_dw,\n",
                "            'dL/dx': dL_dx,\n",
                "        }\n",
                "\n",
                "\n",
                "# Example from theory\n",
                "net = SimpleNeuron()\n",
                "x, y_true = 2.0, 4.0\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"BACKPROPAGATION STEP-BY-STEP\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Forward pass\n",
                "y_hat, cache = net.forward(x)\n",
                "loss = (y_hat - y_true) ** 2\n",
                "\n",
                "print(\"üì• FORWARD PASS:\")\n",
                "print(f\"  Input x = {x}\")\n",
                "print(f\"  h = w*x = {net.w}*{x} = {cache['h']}\")\n",
                "print(f\"  a = ReLU(h) = {cache['a']}\")\n",
                "print(f\"  ≈∑ = v*a = {net.v}*{cache['a']} = {y_hat}\")\n",
                "print(f\"  Loss L = (≈∑ - y)¬≤ = ({y_hat} - {y_true})¬≤ = {loss}\")\n",
                "\n",
                "# Backward pass\n",
                "grads = net.backward(y_true, cache)\n",
                "\n",
                "print(\"\\nüì§ BACKWARD PASS (Chain Rule):\")\n",
                "for name, value in grads.items():\n",
                "    print(f\"  {name} = {value}\")\n",
                "\n",
                "print(\"\\n‚úÖ These gradients tell us how to update w and v to reduce loss!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2.3 Self-Attention Mechanism (Transformers)\n",
                "\n",
                "### Theory\n",
                "\n",
                "The **Self-Attention** mechanism is the heart of Transformers (GPT, BERT). It computes relationships between all positions in a sequence using:\n",
                "\n",
                "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
                "\n",
                "Where:\n",
                "- $Q$ (Query): \"What am I looking for?\"\n",
                "- $K$ (Key): \"What do I contain?\"\n",
                "- $V$ (Value): \"What information do I provide?\"\n",
                "- $d_k$: Dimension of keys (for scaling)\n",
                "\n",
                "### Why $\\sqrt{d_k}$?\n",
                "\n",
                "The dot product $QK^T$ grows with dimension $d_k$. Large values cause softmax to saturate (output near 0 or 1), leading to **vanishing gradients**. Scaling by $\\sqrt{d_k}$ keeps values in a good range.\n",
                "\n",
                "### üìê Mathematical Example: 3-Word Sentence\n",
                "\n",
                "**Sentence**: \"The cat sat\"\n",
                "\n",
                "Suppose each word has a 4-dimensional embedding:\n",
                "- \"The\" ‚Üí $[0.1, 0.2, 0.1, 0.3]$\n",
                "- \"cat\" ‚Üí $[0.5, 0.4, 0.6, 0.2]$\n",
                "- \"sat\" ‚Üí $[0.3, 0.5, 0.2, 0.4]$\n",
                "\n",
                "We compute Q, K, V by multiplying with learned weight matrices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 2.3 SELF-ATTENTION FROM SCRATCH\n",
                "# ============================================\n",
                "\n",
                "def softmax(x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Numerically stable softmax.\"\"\"\n",
                "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
                "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
                "\n",
                "\n",
                "def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
                "    \"\"\"\n",
                "    Compute scaled dot-product attention.\n",
                "    \n",
                "    Args:\n",
                "        Q: Query matrix (seq_len, d_k)\n",
                "        K: Key matrix (seq_len, d_k)\n",
                "        V: Value matrix (seq_len, d_v)\n",
                "    \n",
                "    Returns:\n",
                "        output: Attention output\n",
                "        attention_weights: Attention weights matrix\n",
                "    \"\"\"\n",
                "    d_k = Q.shape[-1]\n",
                "    \n",
                "    # Step 1: Compute attention scores\n",
                "    scores = Q @ K.T  # (seq_len, seq_len)\n",
                "    \n",
                "    # Step 2: Scale by sqrt(d_k)\n",
                "    scaled_scores = scores / np.sqrt(d_k)\n",
                "    \n",
                "    # Step 3: Apply softmax\n",
                "    attention_weights = softmax(scaled_scores)\n",
                "    \n",
                "    # Step 4: Weighted sum of values\n",
                "    output = attention_weights @ V\n",
                "    \n",
                "    return output, attention_weights\n",
                "\n",
                "\n",
                "# Example: 3-word sentence with 4-dim embeddings\n",
                "np.random.seed(42)\n",
                "\n",
                "# Word embeddings (in practice, these come from an embedding layer)\n",
                "X = np.array([\n",
                "    [0.1, 0.2, 0.1, 0.3],  # \"The\"\n",
                "    [0.5, 0.4, 0.6, 0.2],  # \"cat\"\n",
                "    [0.3, 0.5, 0.2, 0.4],  # \"sat\"\n",
                "])\n",
                "\n",
                "# Weight matrices (in practice, these are learned)\n",
                "d_model = 4\n",
                "d_k = 3  # Query/Key dimension\n",
                "d_v = 3  # Value dimension\n",
                "\n",
                "W_Q = np.random.randn(d_model, d_k) * 0.5\n",
                "W_K = np.random.randn(d_model, d_k) * 0.5\n",
                "W_V = np.random.randn(d_model, d_v) * 0.5\n",
                "\n",
                "# Compute Q, K, V\n",
                "Q = X @ W_Q\n",
                "K = X @ W_K\n",
                "V = X @ W_V\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"SELF-ATTENTION MECHANISM\")\n",
                "print(\"=\"*60)\n",
                "print(\"Sentence: ['The', 'cat', 'sat']\")\n",
                "print(f\"\\nInput embeddings X (3 words √ó 4 dims):\\n{X}\")\n",
                "\n",
                "# Compute attention\n",
                "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
                "\n",
                "print(f\"\\nüìä Attention Weights (each row sums to 1):\")\n",
                "print(f\"          The    cat    sat\")\n",
                "for i, word in enumerate(['The', 'cat', 'sat']):\n",
                "    print(f\"{word:>6}:  {attn_weights[i]}\")\n",
                "\n",
                "print(f\"\\n‚úÖ Row sums (should be 1.0): {attn_weights.sum(axis=1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization of attention weights\n",
                "import seaborn as sns\n",
                "\n",
                "words = ['The', 'cat', 'sat']\n",
                "\n",
                "plt.figure(figsize=(6, 5))\n",
                "sns.heatmap(attn_weights, annot=True, fmt='.3f', \n",
                "            xticklabels=words, yticklabels=words,\n",
                "            cmap='Blues', cbar_kws={'label': 'Attention Weight'})\n",
                "plt.xlabel('Key (attending to)')\n",
                "plt.ylabel('Query (from)')\n",
                "plt.title('Self-Attention Weights\\n\"How much does each word attend to others?\"', fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"üí° Each word (Query) distributes its attention across all words (Keys).\")\n",
                "print(\"   The attention weights determine how much information flows from each position.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 3: Optimization - Finding the Best Solution\n",
                "\n",
                "---\n",
                "\n",
                "Once we have a model and a loss function, the goal is to find parameters that minimize the loss. This is **optimization**.\n",
                "\n",
                "> **Key Insight**: Convex problems have a single global minimum. Non-convex problems (deep learning) have many local minima, but good optimizers find good solutions anyway."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.1 Convex Optimization and Lagrange Multipliers\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Convex optimization** is the \"easy\" case: any local minimum is a global minimum. Many classical ML algorithms (linear regression, SVM, logistic regression) are convex.\n",
                "\n",
                "For **constrained optimization**, we use **Lagrange multipliers**. Convert:\n",
                "\n",
                "$$\\min_x f(x) \\quad \\text{subject to} \\quad g(x) = 0$$\n",
                "\n",
                "Into the **Lagrangian**:\n",
                "\n",
                "$$\\mathcal{L}(x, \\lambda) = f(x) + \\lambda g(x)$$\n",
                "\n",
                "Then solve:\n",
                "$$\\nabla_x \\mathcal{L} = 0 \\quad \\text{and} \\quad \\nabla_\\lambda \\mathcal{L} = 0$$\n",
                "\n",
                "### üìê Mathematical Example\n",
                "\n",
                "**Minimize** $f(x, y) = x^2 + y^2$ **subject to** $x + y = 1$\n",
                "\n",
                "**Lagrangian**:\n",
                "$$\\mathcal{L} = x^2 + y^2 + \\lambda(x + y - 1)$$\n",
                "\n",
                "**Solve**:\n",
                "- $\\frac{\\partial \\mathcal{L}}{\\partial x} = 2x + \\lambda = 0 \\Rightarrow x = -\\lambda/2$\n",
                "- $\\frac{\\partial \\mathcal{L}}{\\partial y} = 2y + \\lambda = 0 \\Rightarrow y = -\\lambda/2$\n",
                "- $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = x + y - 1 = 0$\n",
                "\n",
                "From equations 1 and 2: $x = y$. From equation 3: $2x = 1 \\Rightarrow x = y = 0.5$\n",
                "\n",
                "**Solution**: $(x^*, y^*) = (0.5, 0.5)$ with $f^* = 0.5$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 3.1 LAGRANGE MULTIPLIERS VISUALIZATION\n",
                "# ============================================\n",
                "\n",
                "from scipy.optimize import minimize\n",
                "\n",
                "# Objective function\n",
                "def objective(vars):\n",
                "    x, y = vars\n",
                "    return x**2 + y**2\n",
                "\n",
                "# Constraint: x + y = 1 (equality, so we write it as x + y - 1 = 0)\n",
                "constraint = {'type': 'eq', 'fun': lambda vars: vars[0] + vars[1] - 1}\n",
                "\n",
                "# Solve\n",
                "result = minimize(objective, [0, 0], constraints=constraint)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"LAGRANGE MULTIPLIERS EXAMPLE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Minimize: f(x,y) = x¬≤ + y¬≤\")\n",
                "print(f\"Subject to: x + y = 1\")\n",
                "print(f\"\\nSolution: x* = {result.x[0]:.4f}, y* = {result.x[1]:.4f}\")\n",
                "print(f\"Optimal value: f* = {result.fun:.4f}\")\n",
                "\n",
                "# Visualization\n",
                "fig, ax = plt.subplots(figsize=(8, 8))\n",
                "\n",
                "# Contours of objective function\n",
                "x = np.linspace(-1, 2, 100)\n",
                "y = np.linspace(-1, 2, 100)\n",
                "X, Y = np.meshgrid(x, y)\n",
                "Z = X**2 + Y**2\n",
                "\n",
                "contours = ax.contour(X, Y, Z, levels=20, cmap='viridis')\n",
                "ax.clabel(contours, inline=True, fontsize=8)\n",
                "\n",
                "# Constraint line\n",
                "ax.plot(x, 1 - x, 'r-', linewidth=2, label='Constraint: x + y = 1')\n",
                "\n",
                "# Optimal point\n",
                "ax.scatter([0.5], [0.5], c='red', s=200, marker='*', \n",
                "           edgecolors='black', zorder=5, label=f'Optimal: (0.5, 0.5)')\n",
                "\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('y')\n",
                "ax.set_title('Constrained Optimization with Lagrange Multipliers', fontweight='bold')\n",
                "ax.legend()\n",
                "ax.set_xlim(-0.5, 1.5)\n",
                "ax.set_ylim(-0.5, 1.5)\n",
                "ax.set_aspect('equal')\n",
                "ax.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n‚≠ê The optimal point is where the constraint line is TANGENT to a contour.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 SVM and the Dual Formulation\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Support Vector Machines (SVM)** find the maximum-margin hyperplane. The **primal** problem:\n",
                "\n",
                "$$\\min_{w, b} \\frac{1}{2}\\|w\\|^2 \\quad \\text{s.t.} \\quad y_i(w^T x_i + b) \\geq 1$$\n",
                "\n",
                "Using Lagrange multipliers, we get the **dual** problem:\n",
                "\n",
                "$$\\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)$$\n",
                "\n",
                "**Key insight**: The dual form uses only **dot products** $K(x_i, x_j) = x_i^T x_j$, which can be replaced by any kernel function (the **kernel trick**).\n",
                "\n",
                "### üìê Mathematical Example\n",
                "\n",
                "**Data** (simple 2D case):\n",
                "- Class +1: $(1, 2), (2, 3)$\n",
                "- Class -1: $(0, 0), (1, 0)$\n",
                "\n",
                "The dual problem becomes a **Quadratic Programming** problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 3.2 SVM FROM SCRATCH (SIMPLIFIED)\n",
                "# ============================================\n",
                "\n",
                "from scipy.optimize import minimize\n",
                "\n",
                "class SimpleSVM:\n",
                "    \"\"\"Simplified SVM using scipy optimization.\"\"\"\n",
                "    \n",
                "    def __init__(self, C: float = 1.0):\n",
                "        self.C = C\n",
                "        \n",
                "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
                "        n_samples, n_features = X.shape\n",
                "        \n",
                "        # Compute Gram matrix K[i,j] = x_i ¬∑ x_j\n",
                "        K = X @ X.T\n",
                "        \n",
                "        # Dual objective (to maximize, so we negate for minimization)\n",
                "        def dual_objective(alpha):\n",
                "            return 0.5 * np.sum((alpha * y)[:, None] * (alpha * y)[None, :] * K) - np.sum(alpha)\n",
                "        \n",
                "        # Gradients\n",
                "        def dual_gradient(alpha):\n",
                "            return (alpha * y) @ (K * (y[:, None] * y[None, :])) - 1\n",
                "        \n",
                "        # Constraints: 0 <= alpha <= C and sum(alpha * y) = 0\n",
                "        constraints = [\n",
                "            {'type': 'eq', 'fun': lambda a: np.dot(a, y)}\n",
                "        ]\n",
                "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
                "        \n",
                "        # Solve\n",
                "        result = minimize(\n",
                "            dual_objective,\n",
                "            np.zeros(n_samples),\n",
                "            jac=dual_gradient,\n",
                "            bounds=bounds,\n",
                "            constraints=constraints,\n",
                "            method='SLSQP'\n",
                "        )\n",
                "        \n",
                "        self.alpha = result.x\n",
                "        \n",
                "        # Find support vectors (alpha > threshold)\n",
                "        sv_mask = self.alpha > 1e-5\n",
                "        self.support_vectors = X[sv_mask]\n",
                "        self.support_labels = y[sv_mask]\n",
                "        self.support_alphas = self.alpha[sv_mask]\n",
                "        \n",
                "        # Compute w = sum(alpha_i * y_i * x_i)\n",
                "        self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)\n",
                "        \n",
                "        # Compute b using support vectors\n",
                "        self.b = np.mean(self.support_labels - self.support_vectors @ self.w)\n",
                "        \n",
                "        return self\n",
                "    \n",
                "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
                "        return np.sign(X @ self.w + self.b)\n",
                "\n",
                "\n",
                "# Example data\n",
                "X = np.array([\n",
                "    [1, 2],   # +1\n",
                "    [2, 3],   # +1  \n",
                "    [0, 0],   # -1\n",
                "    [1, 0],   # -1\n",
                "], dtype=float)\n",
                "y = np.array([1, 1, -1, -1], dtype=float)\n",
                "\n",
                "# Train SVM\n",
                "svm = SimpleSVM(C=100.0)\n",
                "svm.fit(X, y)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"SUPPORT VECTOR MACHINE (DUAL FORM)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Data points: {len(X)}\")\n",
                "print(f\"Lagrange multipliers (Œ±): {svm.alpha}\")\n",
                "print(f\"\\nSupport vectors (Œ± > 0):\")\n",
                "for sv, label, alpha in zip(svm.support_vectors, svm.support_labels, svm.support_alphas):\n",
                "    print(f\"  {sv} (y={int(label)}, Œ±={alpha:.4f})\")\n",
                "print(f\"\\nWeight vector w: {svm.w}\")\n",
                "print(f\"Bias b: {svm.b:.4f}\")\n",
                "print(f\"\\nDecision boundary: {svm.w[0]:.3f}x + {svm.w[1]:.3f}y + {svm.b:.3f} = 0\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(8, 6))\n",
                "\n",
                "# Plot data points\n",
                "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', s=100, label='Class +1', edgecolors='black')\n",
                "plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', s=100, label='Class -1', edgecolors='black')\n",
                "\n",
                "# Plot support vectors\n",
                "plt.scatter(svm.support_vectors[:, 0], svm.support_vectors[:, 1], \n",
                "            s=200, facecolors='none', edgecolors='green', linewidths=2, label='Support Vectors')\n",
                "\n",
                "# Plot decision boundary and margins\n",
                "x_range = np.linspace(-0.5, 3, 100)\n",
                "y_boundary = -(svm.w[0] * x_range + svm.b) / svm.w[1]\n",
                "y_margin_pos = -(svm.w[0] * x_range + svm.b - 1) / svm.w[1]\n",
                "y_margin_neg = -(svm.w[0] * x_range + svm.b + 1) / svm.w[1]\n",
                "\n",
                "plt.plot(x_range, y_boundary, 'k-', linewidth=2, label='Decision Boundary')\n",
                "plt.plot(x_range, y_margin_pos, 'k--', linewidth=1)\n",
                "plt.plot(x_range, y_margin_neg, 'k--', linewidth=1)\n",
                "plt.fill_between(x_range, y_margin_neg, y_margin_pos, alpha=0.1, color='yellow')\n",
                "\n",
                "plt.xlabel('x‚ÇÅ')\n",
                "plt.ylabel('x‚ÇÇ')\n",
                "plt.title('SVM: Maximum Margin Classifier', fontweight='bold')\n",
                "plt.legend(loc='upper left')\n",
                "plt.xlim(-0.5, 3)\n",
                "plt.ylim(-1, 4)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"üí° The margin (yellow area) is maximized.\")\n",
                "print(\"   Only support vectors determine the decision boundary!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.3 ADMM: Industrial-Scale Optimization (Uber Case)\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Alternating Direction Method of Multipliers (ADMM)** solves problems of the form:\n",
                "\n",
                "$$\\min_x f(x) + g(z) \\quad \\text{s.t.} \\quad Ax + Bz = c$$\n",
                "\n",
                "The algorithm alternates between:\n",
                "\n",
                "1. **x-update**: $x^{k+1} = \\arg\\min_x (f(x) + \\frac{\\rho}{2}\\|Ax + Bz^k - c + u^k\\|_2^2)$\n",
                "2. **z-update**: $z^{k+1} = \\arg\\min_z (g(z) + \\frac{\\rho}{2}\\|Ax^{k+1} + Bz - c + u^k\\|_2^2)$\n",
                "3. **Dual update**: $u^{k+1} = u^k + Ax^{k+1} + Bz^{k+1} - c$\n",
                "\n",
                "### üè≠ Industrial Application: Uber\n",
                "\n",
                "Uber uses ADMM for budget allocation across cities. Each city solves a local problem, while the global constraint ensures total budget is respected.\n",
                "\n",
                "### üìê Mathematical Example: LASSO Regression\n",
                "\n",
                "$$\\min_x \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\|x\\|_1$$\n",
                "\n",
                "We introduce $z = x$ and solve using ADMM."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 3.3 ADMM FOR LASSO\n",
                "# ============================================\n",
                "\n",
                "def soft_threshold(x: np.ndarray, threshold: float) -> np.ndarray:\n",
                "    \"\"\"Soft thresholding operator (proximal operator for L1).\"\"\"\n",
                "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
                "\n",
                "\n",
                "def admm_lasso(A: np.ndarray, b: np.ndarray, lam: float, \n",
                "               rho: float = 1.0, max_iter: int = 100, tol: float = 1e-6):\n",
                "    \"\"\"\n",
                "    Solve LASSO using ADMM.\n",
                "    \n",
                "    min (1/2)||Ax - b||¬≤ + Œª||x||‚ÇÅ\n",
                "    \"\"\"\n",
                "    n = A.shape[1]\n",
                "    \n",
                "    # Initialize\n",
                "    x = np.zeros(n)\n",
                "    z = np.zeros(n)\n",
                "    u = np.zeros(n)\n",
                "    \n",
                "    # Precompute (A^T A + œÅI)^{-1} A^T b for efficiency\n",
                "    AtA = A.T @ A\n",
                "    Atb = A.T @ b\n",
                "    L = AtA + rho * np.eye(n)\n",
                "    L_inv = np.linalg.inv(L)\n",
                "    \n",
                "    history = []\n",
                "    \n",
                "    for k in range(max_iter):\n",
                "        # x-update: solve (A^T A + œÅI)x = A^T b + œÅ(z - u)\n",
                "        x = L_inv @ (Atb + rho * (z - u))\n",
                "        \n",
                "        # z-update: soft thresholding\n",
                "        z_new = soft_threshold(x + u, lam / rho)\n",
                "        \n",
                "        # u-update (dual variable)\n",
                "        u = u + x - z_new\n",
                "        \n",
                "        # Check convergence\n",
                "        primal_residual = np.linalg.norm(x - z_new)\n",
                "        history.append(primal_residual)\n",
                "        \n",
                "        z = z_new\n",
                "        \n",
                "        if primal_residual < tol:\n",
                "            break\n",
                "    \n",
                "    return x, history\n",
                "\n",
                "\n",
                "# Generate example data\n",
                "np.random.seed(42)\n",
                "n_samples, n_features = 50, 20\n",
                "A = np.random.randn(n_samples, n_features)\n",
                "true_x = np.zeros(n_features)\n",
                "true_x[:5] = [3, -2, 1.5, -1, 0.5]  # Only 5 non-zero coefficients\n",
                "b = A @ true_x + 0.1 * np.random.randn(n_samples)\n",
                "\n",
                "# Solve LASSO\n",
                "lam = 0.5\n",
                "x_lasso, history = admm_lasso(A, b, lam, rho=1.0, max_iter=200)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ADMM FOR LASSO REGRESSION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Problem: min (1/2)||Ax - b||¬≤ + {lam}||x||‚ÇÅ\")\n",
                "print(f\"True sparse coefficients:   {true_x[:8]}...\")\n",
                "print(f\"LASSO solution (rounded):   {np.round(x_lasso[:8], 3)}...\")\n",
                "print(f\"\\nConverged in {len(history)} iterations\")\n",
                "print(f\"Non-zero coefficients: {np.sum(np.abs(x_lasso) > 0.01)} (true: 5)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}