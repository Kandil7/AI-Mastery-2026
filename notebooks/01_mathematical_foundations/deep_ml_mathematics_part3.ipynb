{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 4: Probability & Information Theory\n",
                "\n",
                "---\n",
                "\n",
                "Dealing with **uncertainty** is fundamental to AI. Probability provides the language to describe uncertainty, and **information theory** provides measures of it.\n",
                "\n",
                "> **Key Insight**: The real world is noisy and uncertain. Probabilistic models embrace this reality rather than fighting it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "\n",
                "np.random.seed(42)\n",
                "plt.style.use('seaborn-v0_8-whitegrid')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.1 Entropy and KL Divergence\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Entropy** $H(P)$ measures the uncertainty or randomness in a distribution:\n",
                "\n",
                "$$H(P) = -\\sum_x P(x) \\log P(x)$$\n",
                "\n",
                "Properties:\n",
                "- Maximum for uniform distribution\n",
                "- Zero for deterministic distribution (one outcome has probability 1)\n",
                "\n",
                "**KL Divergence** $D_{KL}(P||Q)$ measures how different distribution $Q$ is from $P$:\n",
                "\n",
                "$$D_{KL}(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
                "\n",
                "Properties:\n",
                "- Always non-negative ($\\geq 0$)\n",
                "- Zero if and only if $P = Q$\n",
                "- **Not symmetric**: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$\n",
                "\n",
                "### ðŸ“ Mathematical Example\n",
                "\n",
                "**Distributions**:\n",
                "- $P = [0.25, 0.75]$ (biased coin)\n",
                "- $Q = [0.5, 0.5]$ (fair coin)\n",
                "\n",
                "**Entropy of P**:\n",
                "$$H(P) = -0.25 \\log(0.25) - 0.75 \\log(0.75) = 0.562 \\text{ bits}$$\n",
                "\n",
                "**KL Divergence**:\n",
                "$$D_{KL}(P||Q) = 0.25 \\log\\frac{0.25}{0.5} + 0.75 \\log\\frac{0.75}{0.5} = 0.0589 \\text{ bits}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 4.1 ENTROPY AND KL DIVERGENCE\n",
                "# ============================================\n",
                "\n",
                "def entropy(p: np.ndarray) -> float:\n",
                "    \"\"\"Compute entropy in bits.\"\"\"\n",
                "    p = np.array(p)\n",
                "    p = p[p > 0]  # Avoid log(0)\n",
                "    return -np.sum(p * np.log2(p))\n",
                "\n",
                "\n",
                "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
                "    \"\"\"Compute KL divergence D_KL(P||Q) in bits.\"\"\"\n",
                "    p, q = np.array(p), np.array(q)\n",
                "    # Only consider where p > 0\n",
                "    mask = p > 0\n",
                "    return np.sum(p[mask] * np.log2(p[mask] / q[mask]))\n",
                "\n",
                "\n",
                "# Example distributions\n",
                "P = np.array([0.25, 0.75])  # Biased coin\n",
                "Q = np.array([0.5, 0.5])   # Fair coin\n",
                "R = np.array([0.1, 0.9])   # Very biased\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ENTROPY AND KL DIVERGENCE\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "print(f\"\\nðŸ“Š Distributions:\")\n",
                "print(f\"  P (biased):      {P}\")\n",
                "print(f\"  Q (fair coin):   {Q}\")\n",
                "print(f\"  R (very biased): {R}\")\n",
                "\n",
                "print(f\"\\nðŸ“ Entropy (uncertainty):\")\n",
                "print(f\"  H(P) = {entropy(P):.4f} bits\")\n",
                "print(f\"  H(Q) = {entropy(Q):.4f} bits (maximum for 2 outcomes)\")\n",
                "print(f\"  H(R) = {entropy(R):.4f} bits (low uncertainty)\")\n",
                "\n",
                "print(f\"\\nðŸ“ KL Divergence (difference from Q):\")\n",
                "print(f\"  D_KL(P||Q) = {kl_divergence(P, Q):.4f} bits\")\n",
                "print(f\"  D_KL(R||Q) = {kl_divergence(R, Q):.4f} bits (R is more different from Q)\")\n",
                "print(f\"  D_KL(Q||Q) = {kl_divergence(Q, Q):.4f} bits (same distribution = 0)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Entropy vs probability\n",
                "p_range = np.linspace(0.001, 0.999, 100)\n",
                "entropy_values = [entropy([p, 1-p]) for p in p_range]\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(p_range, entropy_values, 'b-', linewidth=2)\n",
                "plt.axvline(x=0.5, color='r', linestyle='--', label='Maximum entropy (p=0.5)')\n",
                "plt.scatter([0.25, 0.5, 0.1], [entropy([0.25, 0.75]), entropy([0.5, 0.5]), entropy([0.1, 0.9])],\n",
                "            c=['green', 'red', 'orange'], s=100, zorder=5)\n",
                "plt.annotate('P', (0.25, entropy([0.25, 0.75])+0.05), fontsize=12, fontweight='bold')\n",
                "plt.annotate('Q', (0.5, entropy([0.5, 0.5])+0.05), fontsize=12, fontweight='bold')\n",
                "plt.annotate('R', (0.1, entropy([0.1, 0.9])+0.05), fontsize=12, fontweight='bold')\n",
                "plt.xlabel('Probability p (for outcome 1)', fontsize=12)\n",
                "plt.ylabel('Entropy H(p) [bits]', fontsize=12)\n",
                "plt.title('Binary Entropy Function', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ Entropy is maximum when outcomes are equally likely (maximum uncertainty).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.2 Variational Autoencoders (VAE)\n",
                "\n",
                "### Theory\n",
                "\n",
                "**VAEs** are generative models that learn to map data to a latent space that follows a standard Gaussian $N(0, I)$.\n",
                "\n",
                "The loss function is the **Evidence Lower Bound (ELBO)**:\n",
                "\n",
                "$$\\mathcal{L} = \\underbrace{-\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q(z|x) || p(z))}_{\\text{KL Regularization}}$$\n",
                "\n",
                "### KL Between Two Gaussians (Closed Form)\n",
                "\n",
                "For $q(z|x) = N(\\mu, \\sigma^2)$ and $p(z) = N(0, 1)$:\n",
                "\n",
                "$$D_{KL}(q||p) = -\\frac{1}{2} \\sum_{j=1}^{d} (1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$$\n",
                "\n",
                "### ðŸ“ Mathematical Example\n",
                "\n",
                "**Encoder output**: $\\mu = 0.5$, $\\sigma = 0.8$\n",
                "\n",
                "$$D_{KL} = -\\frac{1}{2}(1 + \\log(0.64) - 0.25 - 0.64) = -\\frac{1}{2}(1 - 0.446 - 0.25 - 0.64) = 0.168$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 4.2 VAE KL DIVERGENCE COMPUTATION\n",
                "# ============================================\n",
                "\n",
                "def kl_divergence_gaussian(mu: np.ndarray, log_var: np.ndarray) -> float:\n",
                "    \"\"\"\n",
                "    Compute KL divergence between N(mu, exp(log_var)) and N(0, 1).\n",
                "    This is the closed-form solution used in VAEs.\n",
                "    \n",
                "    Args:\n",
                "        mu: Mean of the encoder distribution\n",
                "        log_var: Log variance of the encoder distribution\n",
                "    \n",
                "    Returns:\n",
                "        KL divergence value\n",
                "    \"\"\"\n",
                "    return -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))\n",
                "\n",
                "\n",
                "def reparameterize(mu: np.ndarray, log_var: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Reparameterization trick: z = mu + sigma * epsilon\n",
                "    where epsilon ~ N(0, 1)\n",
                "    \"\"\"\n",
                "    std = np.exp(0.5 * log_var)\n",
                "    eps = np.random.randn(*mu.shape)\n",
                "    return mu + std * eps\n",
                "\n",
                "\n",
                "# Example: encoder outputs\n",
                "mu = np.array([0.5, -0.3, 0.8])\n",
                "sigma = np.array([0.8, 0.5, 1.2])\n",
                "log_var = 2 * np.log(sigma)  # log(sigma^2)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"VAE KL DIVERGENCE (CLOSED FORM)\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Encoder output:\")\n",
                "print(f\"  Î¼ = {mu}\")\n",
                "print(f\"  Ïƒ = {sigma}\")\n",
                "print(f\"  log(ÏƒÂ²) = {log_var}\")\n",
                "\n",
                "kl = kl_divergence_gaussian(mu, log_var)\n",
                "print(f\"\\nD_KL(q(z|x) || p(z)) = {kl:.4f}\")\n",
                "\n",
                "# Show reparameterization trick\n",
                "print(f\"\\nðŸŽ² Reparameterization samples:\")\n",
                "for i in range(3):\n",
                "    z = reparameterize(mu, log_var)\n",
                "    print(f\"  z_{i+1} = {z}\")\n",
                "\n",
                "print(\"\\nðŸ’¡ The reparameterization trick allows gradients to flow through sampling!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Effect of KL divergence on latent space\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Different encoder distributions\n",
                "configs = [\n",
                "    (0, 1, 'Standard Normal N(0,1)'),\n",
                "    (0.5, 0.8, 'Shifted: N(0.5, 0.64)'),\n",
                "    (2, 0.3, 'Far from prior: N(2, 0.09)'),\n",
                "]\n",
                "\n",
                "x = np.linspace(-4, 5, 200)\n",
                "\n",
                "for ax, (mu, sigma, title) in zip(axes, configs):\n",
                "    # Prior p(z)\n",
                "    prior = stats.norm.pdf(x, 0, 1)\n",
                "    # Encoder q(z|x)\n",
                "    encoder = stats.norm.pdf(x, mu, sigma)\n",
                "    \n",
                "    ax.fill_between(x, prior, alpha=0.3, label='Prior p(z) = N(0,1)')\n",
                "    ax.fill_between(x, encoder, alpha=0.3, label=f'Encoder q(z|x)')\n",
                "    ax.plot(x, prior, 'b-', linewidth=2)\n",
                "    ax.plot(x, encoder, 'r-', linewidth=2)\n",
                "    \n",
                "    # Compute KL\n",
                "    log_var = 2 * np.log(sigma)\n",
                "    kl = kl_divergence_gaussian(np.array([mu]), np.array([log_var]))\n",
                "    \n",
                "    ax.set_title(f'{title}\\nD_KL = {kl:.3f}', fontweight='bold')\n",
                "    ax.set_xlabel('z')\n",
                "    ax.legend(loc='upper right')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('KL Divergence in VAE: Encoder vs Prior', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ“ˆ Higher KL = encoder distribution is far from the prior.\")\n",
                "print(\"ðŸ“‰ VAE training minimizes KL to keep latent space regular.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4.3 t-SNE Algorithm\n",
                "\n",
                "### Theory\n",
                "\n",
                "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) visualizes high-dimensional data in 2D/3D.\n",
                "\n",
                "**Key idea**: Convert distances to probabilities, then minimize the difference between high-dim and low-dim probability distributions.\n",
                "\n",
                "**High-dimensional space**: Gaussian similarities\n",
                "$$p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}$$\n",
                "\n",
                "**Low-dimensional space**: t-distribution (heavier tails to prevent crowding)\n",
                "$$q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}$$\n",
                "\n",
                "**Loss**: KL divergence between P and Q\n",
                "$$C = \\sum_i D_{KL}(P_i || Q_i) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 4.3 t-SNE SIMPLIFIED DEMO\n",
                "# ============================================\n",
                "\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.datasets import make_blobs\n",
                "\n",
                "# Generate high-dimensional clustered data\n",
                "np.random.seed(42)\n",
                "n_samples = 300\n",
                "n_features = 50  # High dimensional\n",
                "n_clusters = 4\n",
                "\n",
                "X, y = make_blobs(n_samples=n_samples, n_features=n_features, \n",
                "                  centers=n_clusters, cluster_std=2.0, random_state=42)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"t-SNE DIMENSIONALITY REDUCTION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Original data: {X.shape[0]} samples Ã— {X.shape[1]} dimensions\")\n",
                "\n",
                "# Apply t-SNE\n",
                "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
                "X_tsne = tsne.fit_transform(X)\n",
                "\n",
                "print(f\"t-SNE output: {X_tsne.shape[0]} samples Ã— {X_tsne.shape[1]} dimensions\")\n",
                "\n",
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Original (first 2 dimensions only)\n",
                "scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
                "axes[0].set_title('Original Data (First 2 of 50 dims)\\nClusters not clearly separated', fontweight='bold')\n",
                "axes[0].set_xlabel('Dimension 1')\n",
                "axes[0].set_ylabel('Dimension 2')\n",
                "\n",
                "# t-SNE\n",
                "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
                "axes[1].set_title('t-SNE Projection (2D)\\nClusters clearly separated!', fontweight='bold')\n",
                "axes[1].set_xlabel('t-SNE Dimension 1')\n",
                "axes[1].set_ylabel('t-SNE Dimension 2')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ t-SNE reveals cluster structure hidden in high dimensions!\")\n",
                "print(\"   It uses KL divergence to preserve local neighborhood structure.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 5: Advanced Integration Methods\n",
                "\n",
                "---\n",
                "\n",
                "Many ML problems require computing integrals that have no closed-form solution. **Monte Carlo methods** provide numerical approximations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.1 Monte Carlo Integration and Importance Sampling\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Monte Carlo Integration** approximates expectations:\n",
                "\n",
                "$$\\mathbb{E}_{x \\sim p}[f(x)] = \\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i), \\quad x_i \\sim p$$\n",
                "\n",
                "**Problem**: What if $p(x)$ is hard to sample from?\n",
                "\n",
                "**Solution**: **Importance Sampling** - sample from an easier distribution $q(x)$:\n",
                "\n",
                "$$\\mathbb{E}_p[f(x)] = \\mathbb{E}_q\\left[f(x) \\frac{p(x)}{q(x)}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\frac{p(x_i)}{q(x_i)}, \\quad x_i \\sim q$$\n",
                "\n",
                "The ratio $w_i = p(x_i)/q(x_i)$ is called the **importance weight**.\n",
                "\n",
                "### ðŸ“ Mathematical Example\n",
                "\n",
                "Estimate $I = \\int_0^1 e^x dx$ (true value: $e - 1 \\approx 1.718$)\n",
                "\n",
                "Using uniform proposal $q(x) = 1$ on $[0, 1]$:\n",
                "\n",
                "$$\\hat{I} = \\frac{1}{N} \\sum_{i=1}^N e^{x_i}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 5.1 MONTE CARLO AND IMPORTANCE SAMPLING\n",
                "# ============================================\n",
                "\n",
                "def monte_carlo_estimate(f, n_samples=10000):\n",
                "    \"\"\"Simple Monte Carlo: sample from Uniform[0,1].\"\"\"\n",
                "    x = np.random.uniform(0, 1, n_samples)\n",
                "    return np.mean(f(x))\n",
                "\n",
                "\n",
                "def importance_sampling_estimate(f, p, q_sample, q_pdf, n_samples=10000):\n",
                "    \"\"\"\n",
                "    Importance sampling estimate.\n",
                "    \n",
                "    Args:\n",
                "        f: Function to integrate\n",
                "        p: Target probability density\n",
                "        q_sample: Function to sample from proposal\n",
                "        q_pdf: Proposal probability density\n",
                "    \"\"\"\n",
                "    x = q_sample(n_samples)\n",
                "    weights = p(x) / q_pdf(x)\n",
                "    return np.mean(f(x) * weights)\n",
                "\n",
                "\n",
                "# Example: Integrate e^x from 0 to 1\n",
                "f = lambda x: np.exp(x)\n",
                "true_value = np.e - 1  # â‰ˆ 1.718\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"MONTE CARLO INTEGRATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Integral: âˆ«â‚€Â¹ eË£ dx\")\n",
                "print(f\"True value: {true_value:.6f}\")\n",
                "\n",
                "# Simple Monte Carlo\n",
                "estimates = [monte_carlo_estimate(f, n) for n in [100, 1000, 10000, 100000]]\n",
                "print(f\"\\nðŸ“Š Simple Monte Carlo estimates:\")\n",
                "for n, est in zip([100, 1000, 10000, 100000], estimates):\n",
                "    print(f\"  N={n:6d}: {est:.6f} (error: {abs(est - true_value):.6f})\")\n",
                "\n",
                "print(\"\\nâœ… Error decreases as âˆšN (law of large numbers)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization: Convergence of Monte Carlo\n",
                "np.random.seed(42)\n",
                "n_max = 5000\n",
                "samples = np.random.uniform(0, 1, n_max)\n",
                "cumulative_mean = np.cumsum(np.exp(samples)) / (np.arange(1, n_max + 1))\n",
                "\n",
                "plt.figure(figsize=(10, 5))\n",
                "plt.plot(cumulative_mean, 'b-', alpha=0.7, linewidth=1)\n",
                "plt.axhline(y=true_value, color='r', linestyle='--', linewidth=2, label=f'True value = {true_value:.4f}')\n",
                "plt.fill_between(range(n_max), true_value - 0.1, true_value + 0.1, alpha=0.2, color='red')\n",
                "plt.xlabel('Number of samples', fontsize=12)\n",
                "plt.ylabel('Estimate', fontsize=12)\n",
                "plt.title('Monte Carlo Convergence: âˆ«â‚€Â¹ eË£ dx', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.xlim(0, n_max)\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ The estimate fluctuates but converges to the true value.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5.2 Normalizing Flows\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Normalizing Flows** transform a simple distribution (e.g., Gaussian) into a complex one through a series of invertible transformations.\n",
                "\n",
                "Given $z \\sim p_z(z)$ and an invertible function $x = f(z)$, the density of $x$ is:\n",
                "\n",
                "$$p_x(x) = p_z(f^{-1}(x)) \\left| \\det \\frac{\\partial f^{-1}}{\\partial x} \\right| = p_z(z) \\left| \\det \\frac{\\partial f}{\\partial z} \\right|^{-1}$$\n",
                "\n",
                "The key is choosing $f$ such that the **Jacobian determinant** is easy to compute.\n",
                "\n",
                "### ðŸ“ Mathematical Example: Affine Flow\n",
                "\n",
                "Simplest flow: $x = az + b$ (scale and shift)\n",
                "\n",
                "Jacobian determinant: $|a|$\n",
                "\n",
                "$$p_x(x) = p_z\\left(\\frac{x-b}{a}\\right) \\cdot \\frac{1}{|a|}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 5.2 NORMALIZING FLOWS DEMONSTRATION\n",
                "# ============================================\n",
                "\n",
                "class AffineFlow:\n",
                "    \"\"\"Simple affine flow: x = a*z + b\"\"\"\n",
                "    \n",
                "    def __init__(self, a: float, b: float):\n",
                "        self.a = a\n",
                "        self.b = b\n",
                "    \n",
                "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
                "        return self.a * z + self.b\n",
                "    \n",
                "    def inverse(self, x: np.ndarray) -> np.ndarray:\n",
                "        return (x - self.b) / self.a\n",
                "    \n",
                "    def log_det_jacobian(self) -> float:\n",
                "        return np.log(np.abs(self.a))\n",
                "\n",
                "\n",
                "# Start with standard normal\n",
                "np.random.seed(42)\n",
                "z = np.random.randn(5000)\n",
                "\n",
                "# Apply flow: x = 2z + 3 (scale by 2, shift by 3)\n",
                "flow = AffineFlow(a=2.0, b=3.0)\n",
                "x = flow.forward(z)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"NORMALIZING FLOWS\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Base distribution: z ~ N(0, 1)\")\n",
                "print(f\"Flow: x = 2z + 3\")\n",
                "print(f\"Resulting distribution: x ~ N(3, 4)\")\n",
                "print(f\"\\nlog|det(J)| = log|2| = {flow.log_det_jacobian():.4f}\")\n",
                "\n",
                "# Verification\n",
                "print(f\"\\nðŸ“Š Empirical statistics:\")\n",
                "print(f\"  z: mean={np.mean(z):.3f}, std={np.std(z):.3f}\")\n",
                "print(f\"  x: mean={np.mean(x):.3f}, std={np.std(x):.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# Base distribution\n",
                "axes[0].hist(z, bins=50, density=True, alpha=0.7, color='blue')\n",
                "x_plot = np.linspace(-4, 4, 100)\n",
                "axes[0].plot(x_plot, stats.norm.pdf(x_plot), 'k-', linewidth=2)\n",
                "axes[0].set_title('Base: z ~ N(0, 1)', fontweight='bold')\n",
                "axes[0].set_xlabel('z')\n",
                "\n",
                "# Flow transformation\n",
                "axes[1].annotate('', xy=(0.9, 0.5), xytext=(0.1, 0.5),\n",
                "                 arrowprops=dict(arrowstyle='->', lw=3, color='green'),\n",
                "                 xycoords='axes fraction')\n",
                "axes[1].text(0.5, 0.6, 'x = 2z + 3', fontsize=16, ha='center', transform=axes[1].transAxes, fontweight='bold')\n",
                "axes[1].text(0.5, 0.4, 'Invertible!', fontsize=12, ha='center', transform=axes[1].transAxes)\n",
                "axes[1].axis('off')\n",
                "axes[1].set_title('Affine Flow', fontweight='bold')\n",
                "\n",
                "# Transformed distribution\n",
                "axes[2].hist(x, bins=50, density=True, alpha=0.7, color='orange')\n",
                "x_plot = np.linspace(-3, 9, 100)\n",
                "axes[2].plot(x_plot, stats.norm.pdf(x_plot, loc=3, scale=2), 'k-', linewidth=2)\n",
                "axes[2].set_title('Result: x ~ N(3, 4)', fontweight='bold')\n",
                "axes[2].set_xlabel('x')\n",
                "\n",
                "plt.suptitle('Normalizing Flow: Transform Simple â†’ Complex', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ By stacking many invertible layers, flows can model very complex distributions!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 6: Network Analysis\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6.1 Supervised Random Walks (Facebook Link Prediction)\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Link Prediction**: Given a social network, predict which new edges (friendships) will form.\n",
                "\n",
                "**Supervised Random Walks** (Backstrom & Leskovec, 2011) learn edge weights based on features to bias a random walk toward future friends.\n",
                "\n",
                "**Key idea**: Learn a function $\\text{strength}(u, v) = f_w(\\text{features}(u, v))$ that weights edges.\n",
                "\n",
                "The optimization objective:\n",
                "\n",
                "$$\\min_w \\|w\\|^2 + \\lambda \\sum_{(d, l) \\in D} h(p_l - p_d)$$\n",
                "\n",
                "Where:\n",
                "- $d$: A node that becomes a friend (destination)\n",
                "- $l$: A node that does not (non-link)\n",
                "- $p_d, p_l$: Random walk probabilities of reaching these nodes\n",
                "- $h$: Hinge-like loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 6.1 LINK PREDICTION DEMO\n",
                "# ============================================\n",
                "\n",
                "def compute_common_neighbors(adj: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Compute common neighbors score for all node pairs.\"\"\"\n",
                "    # CN(u,v) = |N(u) âˆ© N(v)| = (A @ A)[u,v]\n",
                "    return adj @ adj\n",
                "\n",
                "\n",
                "def compute_adamic_adar(adj: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"Compute Adamic-Adar score for all node pairs.\"\"\"\n",
                "    n = adj.shape[0]\n",
                "    degrees = adj.sum(axis=1)\n",
                "    scores = np.zeros((n, n))\n",
                "    \n",
                "    for u in range(n):\n",
                "        for v in range(n):\n",
                "            if u != v:\n",
                "                # Find common neighbors\n",
                "                common = np.where((adj[u] == 1) & (adj[v] == 1))[0]\n",
                "                # Sum 1/log(degree) for each common neighbor\n",
                "                for w in common:\n",
                "                    if degrees[w] > 1:\n",
                "                        scores[u, v] += 1 / np.log(degrees[w])\n",
                "    \n",
                "    return scores\n",
                "\n",
                "\n",
                "# Create a sample social network\n",
                "# 6 users: edges represent friendships\n",
                "adj = np.array([\n",
                "    [0, 1, 1, 0, 0, 0],  # User 0: friends with 1, 2\n",
                "    [1, 0, 1, 1, 0, 0],  # User 1: friends with 0, 2, 3\n",
                "    [1, 1, 0, 1, 1, 0],  # User 2: friends with 0, 1, 3, 4\n",
                "    [0, 1, 1, 0, 1, 1],  # User 3: friends with 1, 2, 4, 5\n",
                "    [0, 0, 1, 1, 0, 1],  # User 4: friends with 2, 3, 5\n",
                "    [0, 0, 0, 1, 1, 0],  # User 5: friends with 3, 4\n",
                "])\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"LINK PREDICTION IN SOCIAL NETWORKS\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Compute scores\n",
                "cn_scores = compute_common_neighbors(adj)\n",
                "aa_scores = compute_adamic_adar(adj)\n",
                "\n",
                "# Find potential links (pairs that aren't already connected)\n",
                "print(\"\\nðŸ“Š Potential new friendships (ranked by Common Neighbors):\")\n",
                "potential_links = []\n",
                "for i in range(6):\n",
                "    for j in range(i+1, 6):\n",
                "        if adj[i, j] == 0:  # Not already friends\n",
                "            potential_links.append((i, j, cn_scores[i, j], aa_scores[i, j]))\n",
                "\n",
                "# Sort by common neighbors\n",
                "potential_links.sort(key=lambda x: x[2], reverse=True)\n",
                "\n",
                "print(f\"{'Pair':<10} {'Common Neighbors':<18} {'Adamic-Adar':<12}\")\n",
                "print(\"-\" * 40)\n",
                "for i, j, cn, aa in potential_links[:5]:\n",
                "    print(f\"({i}, {j})     {int(cn):<18} {aa:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization of the network\n",
                "import matplotlib.patches as mpatches\n",
                "\n",
                "# Node positions (manually arranged for visualization)\n",
                "pos = {\n",
                "    0: (0, 1),\n",
                "    1: (1, 2),\n",
                "    2: (2, 1),\n",
                "    3: (3, 2),\n",
                "    4: (4, 1),\n",
                "    5: (4, 2.5)\n",
                "}\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "\n",
                "# Draw existing edges\n",
                "for i in range(6):\n",
                "    for j in range(i+1, 6):\n",
                "        if adj[i, j] == 1:\n",
                "            plt.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], \n",
                "                     'b-', linewidth=2, alpha=0.6)\n",
                "\n",
                "# Draw predicted edge (highest score)\n",
                "best_i, best_j = potential_links[0][0], potential_links[0][1]\n",
                "plt.plot([pos[best_i][0], pos[best_j][0]], [pos[best_i][1], pos[best_j][1]], \n",
                "         'g--', linewidth=3, label='Predicted friendship')\n",
                "\n",
                "# Draw nodes\n",
                "for node, (x, y) in pos.items():\n",
                "    plt.scatter(x, y, c='steelblue', s=800, zorder=5, edgecolors='black', linewidths=2)\n",
                "    plt.annotate(f'User {node}', (x, y), fontsize=10, ha='center', va='center', color='white', fontweight='bold')\n",
                "\n",
                "plt.title('Social Network with Predicted Link', fontsize=14, fontweight='bold')\n",
                "plt.legend(loc='upper left')\n",
                "plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nðŸŽ¯ Most likely new friendship: Users {best_i} and {best_j}\")\n",
                "print(f\"   (They have {int(potential_links[0][2])} common friends!)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 7: Bayesian Optimization\n",
                "\n",
                "---\n",
                "\n",
                "When the objective function is expensive to evaluate (hyperparameter tuning, drug discovery), gradient-based methods are impractical. **Bayesian Optimization** uses a probabilistic model to guide the search."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7.1 Gaussian Processes\n",
                "\n",
                "### Theory\n",
                "\n",
                "A **Gaussian Process (GP)** is a distribution over functions:\n",
                "\n",
                "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
                "\n",
                "Where:\n",
                "- $m(x)$: Mean function\n",
                "- $k(x, x')$: Kernel (covariance) function\n",
                "\n",
                "**Key property**: GP gives both a prediction AND uncertainty!\n",
                "\n",
                "### Squared Exponential (RBF) Kernel\n",
                "\n",
                "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)$$\n",
                "\n",
                "Parameters:\n",
                "- $\\sigma^2$: Signal variance (amplitude)\n",
                "- $\\ell$: Length scale (how quickly function varies)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 7.1 GAUSSIAN PROCESS REGRESSION\n",
                "# ============================================\n",
                "\n",
                "def rbf_kernel(X1: np.ndarray, X2: np.ndarray, \n",
                "               length_scale: float = 1.0, variance: float = 1.0) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Compute RBF (squared exponential) kernel matrix.\n",
                "    \"\"\"\n",
                "    X1 = X1.reshape(-1, 1) if X1.ndim == 1 else X1\n",
                "    X2 = X2.reshape(-1, 1) if X2.ndim == 1 else X2\n",
                "    \n",
                "    # Compute squared Euclidean distances\n",
                "    sqdist = np.sum(X1**2, axis=1, keepdims=True) + \\\n",
                "             np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n",
                "    \n",
                "    return variance * np.exp(-0.5 * sqdist / (length_scale**2))\n",
                "\n",
                "\n",
                "def gp_predict(X_train: np.ndarray, y_train: np.ndarray, \n",
                "               X_test: np.ndarray, length_scale: float = 1.0,\n",
                "               noise: float = 1e-6):\n",
                "    \"\"\"\n",
                "    GP prediction with posterior mean and variance.\n",
                "    \"\"\"\n",
                "    K = rbf_kernel(X_train, X_train, length_scale) + noise * np.eye(len(X_train))\n",
                "    K_star = rbf_kernel(X_train, X_test, length_scale)\n",
                "    K_star_star = rbf_kernel(X_test, X_test, length_scale)\n",
                "    \n",
                "    # Compute posterior\n",
                "    K_inv = np.linalg.inv(K)\n",
                "    mu = K_star.T @ K_inv @ y_train\n",
                "    cov = K_star_star - K_star.T @ K_inv @ K_star\n",
                "    \n",
                "    return mu, np.sqrt(np.diag(cov))\n",
                "\n",
                "\n",
                "# Generate training data\n",
                "np.random.seed(42)\n",
                "X_train = np.array([1, 3, 5, 6, 7])\n",
                "y_train = np.sin(X_train) + 0.1 * np.random.randn(len(X_train))\n",
                "\n",
                "# Test points\n",
                "X_test = np.linspace(0, 10, 100)\n",
                "\n",
                "# GP prediction\n",
                "mu, std = gp_predict(X_train, y_train, X_test, length_scale=1.0)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"GAUSSIAN PROCESS REGRESSION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Training points: {len(X_train)}\")\n",
                "print(f\"Test points: {len(X_test)}\")\n",
                "print(f\"\\nGP provides:\")\n",
                "print(f\"  - Mean prediction Î¼(x)\")\n",
                "print(f\"  - Uncertainty Ïƒ(x) at each point!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# True function\n",
                "plt.plot(X_test, np.sin(X_test), 'k--', label='True function: sin(x)', linewidth=2)\n",
                "\n",
                "# GP prediction\n",
                "plt.plot(X_test, mu, 'b-', label='GP Mean prediction', linewidth=2)\n",
                "plt.fill_between(X_test, mu - 2*std, mu + 2*std, alpha=0.3, color='blue', label='95% confidence')\n",
                "\n",
                "# Training points\n",
                "plt.scatter(X_train, y_train, c='red', s=100, zorder=5, label='Training data', edgecolors='black')\n",
                "\n",
                "plt.xlabel('x', fontsize=12)\n",
                "plt.ylabel('f(x)', fontsize=12)\n",
                "plt.title('Gaussian Process: Prediction with Uncertainty', fontsize=14, fontweight='bold')\n",
                "plt.legend(loc='upper right')\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ Notice: Uncertainty is LOW near training points, HIGH far from them!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7.2 Acquisition Functions\n",
                "\n",
                "### Theory\n",
                "\n",
                "**Acquisition functions** decide where to sample next by balancing:\n",
                "- **Exploitation**: Sample where we expect good values\n",
                "- **Exploration**: Sample where we're uncertain\n",
                "\n",
                "### Expected Improvement (EI)\n",
                "\n",
                "$$\\alpha_{EI}(x) = (\\mu(x) - f^* - \\xi) \\Phi(Z) + \\sigma(x) \\phi(Z)$$\n",
                "\n",
                "Where:\n",
                "- $Z = \\frac{\\mu(x) - f^* - \\xi}{\\sigma(x)}$\n",
                "- $f^*$: Best value observed so far\n",
                "- $\\xi$: Exploration parameter\n",
                "- $\\Phi$, $\\phi$: CDF and PDF of standard normal"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 7.2 BAYESIAN OPTIMIZATION WITH EI\n",
                "# ============================================\n",
                "\n",
                "def expected_improvement(mu: np.ndarray, std: np.ndarray, \n",
                "                         f_best: float, xi: float = 0.01) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Compute Expected Improvement acquisition function.\n",
                "    \"\"\"\n",
                "    std = np.maximum(std, 1e-9)  # Avoid division by zero\n",
                "    Z = (mu - f_best - xi) / std\n",
                "    ei = (mu - f_best - xi) * stats.norm.cdf(Z) + std * stats.norm.pdf(Z)\n",
                "    return ei\n",
                "\n",
                "\n",
                "# Objective function to optimize (expensive black-box function)\n",
                "def objective(x):\n",
                "    return -((x - 2)**2 * np.sin(3*x))\n",
                "\n",
                "\n",
                "# Initial samples\n",
                "X_train = np.array([0.5, 2.0, 4.5])\n",
                "y_train = objective(X_train)\n",
                "f_best = np.max(y_train)\n",
                "\n",
                "# Test points\n",
                "X_test = np.linspace(0, 5, 200)\n",
                "\n",
                "# GP prediction\n",
                "mu, std = gp_predict(X_train, y_train, X_test, length_scale=0.5)\n",
                "\n",
                "# Expected Improvement\n",
                "ei = expected_improvement(mu, std, f_best)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"BAYESIAN OPTIMIZATION\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Current best: {f_best:.4f} at x = {X_train[np.argmax(y_train)]:.2f}\")\n",
                "print(f\"Next point to sample: x = {X_test[np.argmax(ei)]:.4f}\")\n",
                "print(f\"Expected Improvement at that point: {np.max(ei):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
                "\n",
                "# Top: GP surrogate\n",
                "ax1 = axes[0]\n",
                "ax1.plot(X_test, objective(X_test), 'k--', label='True objective', linewidth=2)\n",
                "ax1.plot(X_test, mu, 'b-', label='GP prediction', linewidth=2)\n",
                "ax1.fill_between(X_test, mu - 2*std, mu + 2*std, alpha=0.3, color='blue')\n",
                "ax1.scatter(X_train, y_train, c='red', s=100, zorder=5, label='Observations', edgecolors='black')\n",
                "ax1.axhline(y=f_best, color='green', linestyle=':', label=f'Best so far = {f_best:.2f}')\n",
                "ax1.set_ylabel('f(x)', fontsize=12)\n",
                "ax1.set_title('Gaussian Process Surrogate Model', fontsize=14, fontweight='bold')\n",
                "ax1.legend(loc='upper right')\n",
                "ax1.grid(True, alpha=0.3)\n",
                "\n",
                "# Bottom: Acquisition function\n",
                "ax2 = axes[1]\n",
                "ax2.fill_between(X_test, 0, ei, alpha=0.5, color='orange')\n",
                "ax2.plot(X_test, ei, 'orange', linewidth=2)\n",
                "next_x = X_test[np.argmax(ei)]\n",
                "ax2.axvline(x=next_x, color='red', linestyle='--', linewidth=2, label=f'Next sample: x={next_x:.2f}')\n",
                "ax2.set_xlabel('x', fontsize=12)\n",
                "ax2.set_ylabel('Expected Improvement', fontsize=12)\n",
                "ax2.set_title('Acquisition Function (Expected Improvement)', fontsize=14, fontweight='bold')\n",
                "ax2.legend()\n",
                "ax2.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸŽ¯ Bayesian Optimization balances:\")\n",
                "print(\"   - Exploitation: Sample where Î¼(x) is high\")\n",
                "print(\"   - Exploration: Sample where Ïƒ(x) is high (uncertain regions)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# ðŸ“š Conclusion\n",
                "\n",
                "---\n",
                "\n",
                "This notebook has taken you through the **deep mathematical foundations** that power modern machine learning:\n",
                "\n",
                "1. **Linear Algebra**: Vectors, matrices, eigenvalues, SVD â†’ PageRank, recommendations\n",
                "2. **Calculus**: Gradients, chain rule, Jacobian â†’ Backpropagation, Transformers\n",
                "3. **Optimization**: Lagrange, convex, ADMM â†’ SVMs, industrial-scale systems\n",
                "4. **Probability**: Entropy, KL, Bayes â†’ VAEs, generative models\n",
                "5. **Advanced Methods**: Monte Carlo, flows â†’ Sampling, density estimation\n",
                "6. **Network Analysis**: Random walks â†’ Link prediction\n",
                "7. **Bayesian Optimization**: GPs, acquisition â†’ Hyperparameter tuning\n",
                "\n",
                "> **The Key Insight**: Mathematics is not just a prerequisiteâ€”it is the *language* in which ML discoveries are written. Mastering these foundations enables you to:\n",
                "> - Debug models effectively\n",
                "> - Design novel architectures\n",
                "> - Understand cutting-edge research papers\n",
                "> - Build production-grade ML systems\n",
                "\n",
                "---\n",
                "\n",
                "**Next Steps**: Apply these concepts to real datasets and implement more complex models!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}