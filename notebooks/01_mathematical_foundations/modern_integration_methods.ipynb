{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¢ Modern Integration Methods in Machine Learning\n",
                "\n",
                "## From Theory to Industrial Applications\n",
                "\n",
                "---\n",
                "\n",
                "**Authored by:** AI-Mastery-2026 Project  \n",
                "**Based on:** Draft by Marc Peter Deisenroth (UCL)  \n",
                "**Date:** January 2026\n",
                "\n",
                "### ðŸŽ¯ Learning Objectives\n",
                "\n",
                "By the end of this notebook, you will:\n",
                "\n",
                "1. **Numerical Quadrature** - Master Newton-Cotes and Gaussian quadrature methods\n",
                "2. **Monte Carlo Integration** - Understand random sampling with variance reduction\n",
                "3. **Bayesian Quadrature** - Learn GP-based integration for expensive functions\n",
                "4. **Normalizing Flows** - Transform simple distributions into complex ones\n",
                "5. **Time Series Inference** - Apply EKF, UKF, and particle filtering\n",
                "\n",
                "---\n",
                "\n",
                "### ðŸ“š Prerequisites\n",
                "\n",
                "- Basic Python and NumPy\n",
                "- Probability distributions (Gaussian, expectations)\n",
                "- Linear algebra (matrix operations, Jacobians)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# IMPORTS AND SETUP\n",
                "# ============================================\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy.integrate import quad\n",
                "from scipy.stats import norm\n",
                "from scipy.special import roots_hermite, roots_legendre\n",
                "import warnings\n",
                "\n",
                "# Configuration\n",
                "warnings.filterwarnings('ignore')\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "np.set_printoptions(precision=4, suppress=True)\n",
                "\n",
                "print(\"âœ… Setup complete! NumPy version:\", np.__version__)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Introduction: Why Integration Matters in ML\n",
                "\n",
                "---\n",
                "\n",
                "Integration is fundamental to machine learning. Almost every probabilistic quantity we care about is an **expectation**:\n",
                "\n",
                "$$\\mathbb{E}_{x\\sim p}[f(x)] := \\int f(x)p(x)dx$$\n",
                "\n",
                "### Key Applications:\n",
                "\n",
                "| Application | Integral Form | ML Example |\n",
                "|-------------|---------------|------------|\n",
                "| **Mean** | $\\mathbb{E}[x] = \\int xp(x)dx$ | Predicted value |\n",
                "| **Variance** | $\\mathbb{V}[x] = \\int (x-\\mu)^2p(x)dx$ | Uncertainty |\n",
                "| **Marginal Likelihood** | $p(Y|X) = \\int p(Y|X,\\theta)p(\\theta)d\\theta$ | Model selection |\n",
                "| **Bayesian Prediction** | $p(y^*|x^*,D) = \\int p(y^*|\\theta)p(\\theta|D)d\\theta$ | Predictive uncertainty |\n",
                "\n",
                "### ðŸ­ Industrial Impact:\n",
                "\n",
                "- **JPMorgan Chase**: Risk assessment using Monte Carlo for Value-at-Risk\n",
                "- **DeepMind Health**: Uncertainty quantification in medical diagnosis\n",
                "- **Tesla Autopilot**: Bayesian sensor fusion for trajectory prediction\n",
                "- **Netflix**: Recommendation under uncertainty (200M+ users)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 1: Newton-Cotes Methods\n",
                "\n",
                "---\n",
                "\n",
                "Newton-Cotes methods approximate integrals using polynomial interpolation at equidistant points.\n",
                "\n",
                "## 1.1 Trapezoidal Rule\n",
                "\n",
                "### Theory\n",
                "\n",
                "The trapezoidal rule approximates $f(x)$ as linear between consecutive points:\n",
                "\n",
                "$$\\int_a^b f(x)dx \\approx \\frac{h}{2}(f_0 + 2f_1 + \\cdots + 2f_{N-1} + f_N)$$\n",
                "\n",
                "where $h = (b-a)/N$ and $f_n = f(x_n)$ at equidistant nodes.\n",
                "\n",
                "**Error**: $O(1/N^2)$ - suitable for non-smooth functions\n",
                "\n",
                "### ðŸ­ Industrial Use Case: Boeing Aerodynamics\n",
                "\n",
                "Boeing engineers use trapezoidal integration to compute lift coefficients by integrating pressure distributions over wing surfaces. The method's robustness to measurement noise makes it ideal for wind tunnel data.\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: When would you prefer trapezoidal rule over Simpson's?\n",
                ">\n",
                "> **A**: For noisy data or non-smooth functions. Simpson's quadratic fit can amplify oscillations in noisy measurements, while trapezoidal's linear approximation is more stable."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.1 TRAPEZOIDAL RULE IMPLEMENTATION\n",
                "# ============================================\n",
                "\n",
                "def trapezoidal_rule(f, a, b, n):\n",
                "    \"\"\"\n",
                "    Trapezoidal rule for numerical integration.\n",
                "    \n",
                "    Error: O(1/NÂ²)\n",
                "    \"\"\"\n",
                "    x = np.linspace(a, b, n + 1)\n",
                "    y = f(x)\n",
                "    h = (b - a) / n\n",
                "    return h * (0.5 * y[0] + np.sum(y[1:-1]) + 0.5 * y[-1])\n",
                "\n",
                "\n",
                "# Example: Integrate exp(-xÂ² - sin(3x)Â²) from 0 to 1\n",
                "def test_function(x):\n",
                "    return np.exp(-x**2 - np.sin(3*x)**2)\n",
                "\n",
                "# True value using high-precision quadrature\n",
                "true_value, _ = quad(test_function, 0, 1, epsabs=1e-12)\n",
                "\n",
                "# Test with different node counts\n",
                "nodes = [5, 10, 20, 40, 80]\n",
                "trap_results = [trapezoidal_rule(test_function, 0, 1, n) for n in nodes]\n",
                "trap_errors = [abs(r - true_value) for r in trap_results]\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"TRAPEZOIDAL RULE CONVERGENCE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"True value: {true_value:.10f}\")\n",
                "print()\n",
                "for n, result, error in zip(nodes, trap_results, trap_errors):\n",
                "    print(f\"N={n:3d}: Result={result:.10f}, Error={error:.2e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.2 Simpson's Rule\n",
                "\n",
                "### Theory\n",
                "\n",
                "Simpson's rule uses **quadratic** interpolation (parabolas) connecting triplets of points:\n",
                "\n",
                "$$\\int_a^b f(x)dx \\approx \\frac{h}{3}(f_0 + 4f_1 + 2f_2 + 4f_3 + \\cdots + 4f_{N-1} + f_N)$$\n",
                "\n",
                "**Error**: $O(1/N^4)$ - much faster convergence for smooth functions!\n",
                "\n",
                "### ðŸ­ Industrial Use Case: NASA JPL\n",
                "\n",
                "NASA's Jet Propulsion Laboratory uses Simpson's rule in spacecraft trajectory calculations. The Cassini mission to Saturn used these techniques for fuel-efficient navigation through the solar system.\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: Derive the error bound for Simpson's rule.\n",
                ">\n",
                "> **A**: For Simpson's: Error = $-\\frac{(b-a)^5}{180N^4}f^{(4)}(\\xi)$ for some $\\xi \\in [a,b]$. The fourth derivative term explains why it's exact for cubics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 1.2 SIMPSON'S RULE IMPLEMENTATION\n",
                "# ============================================\n",
                "\n",
                "def simpsons_rule(f, a, b, n):\n",
                "    \"\"\"\n",
                "    Simpson's 1/3 rule for numerical integration.\n",
                "    \n",
                "    Error: O(1/Nâ´) - faster convergence for smooth functions\n",
                "    \"\"\"\n",
                "    if n % 2 != 0:\n",
                "        n += 1  # Ensure even\n",
                "    \n",
                "    x = np.linspace(a, b, n + 1)\n",
                "    y = f(x)\n",
                "    h = (b - a) / n\n",
                "    \n",
                "    result = y[0] + y[-1]\n",
                "    result += 4 * np.sum(y[1:-1:2])  # Odd indices\n",
                "    result += 2 * np.sum(y[2:-2:2])  # Even indices\n",
                "    \n",
                "    return h * result / 3\n",
                "\n",
                "\n",
                "# Compare convergence\n",
                "simp_results = [simpsons_rule(test_function, 0, 1, n) for n in nodes]\n",
                "simp_errors = [abs(r - true_value) for r in simp_results]\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"SIMPSON'S RULE CONVERGENCE\")\n",
                "print(\"=\"*50)\n",
                "for n, result, error in zip(nodes, simp_results, simp_errors):\n",
                "    print(f\"N={n:3d}: Result={result:.10f}, Error={error:.2e}\")\n",
                "\n",
                "# Plot convergence comparison\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.loglog(nodes, trap_errors, 'o-', linewidth=2, label='Trapezoidal')\n",
                "plt.loglog(nodes, simp_errors, 's-', linewidth=2, label=\"Simpson's\")\n",
                "plt.loglog(nodes, [trap_errors[0]*(nodes[0]/n)**2 for n in nodes], \n",
                "           'k--', alpha=0.5, label='$O(1/N^2)$')\n",
                "plt.loglog(nodes, [simp_errors[0]*(nodes[0]/n)**4 for n in nodes], \n",
                "           'k-.', alpha=0.5, label='$O(1/N^4)$')\n",
                "plt.xlabel('Number of nodes', fontsize=12)\n",
                "plt.ylabel('Integration error', fontsize=12)\n",
                "plt.title('Newton-Cotes Convergence Comparison', fontsize=14, fontweight='bold')\n",
                "plt.legend(fontsize=10)\n",
                "plt.grid(True, which=\"both\", ls=\"-\", alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ’¡ Key Insight: Simpson's error decreases 16x when N doubles,\")\n",
                "print(\"   while Trapezoidal only decreases 4x.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 2: Gaussian Quadrature\n",
                "\n",
                "---\n",
                "\n",
                "Gaussian quadrature optimizes **both** nodes and weights to exactly integrate polynomials up to degree $2N-1$.\n",
                "\n",
                "$$\\int_a^b f(x)w(x)dx \\approx \\sum_{n=1}^N w_n f(x_n)$$\n",
                "\n",
                "| Interval | Weight $w(x)$ | Orthogonal Polynomial |\n",
                "|----------|---------------|----------------------|\n",
                "| $[-1, 1]$ | $1$ | Legendre |\n",
                "| $[-1, 1]$ | $(1-x^2)^{-1/2}$ | Chebyshev |\n",
                "| $[0, \\infty]$ | $e^{-x}$ | Laguerre |\n",
                "| $(-\\infty, \\infty)$ | $e^{-x^2}$ | **Hermite** |\n",
                "\n",
                "## 2.1 Gauss-Hermite Quadrature\n",
                "\n",
                "### Theory\n",
                "\n",
                "Gauss-Hermite is designed for integrals with Gaussian weight:\n",
                "\n",
                "$$\\int_{-\\infty}^{\\infty} f(x)e^{-x^2}dx \\approx \\sum_{n=1}^N w_n f(x_n)$$\n",
                "\n",
                "For expectations under $\\mathcal{N}(\\mu, \\sigma^2)$:\n",
                "\n",
                "$$\\mathbb{E}_{x\\sim\\mathcal{N}(\\mu,\\sigma^2)}[f(x)] \\approx \\frac{1}{\\sqrt{\\pi}}\\sum_{n=1}^N w_n f(\\sqrt{2}\\sigma x_n + \\mu)$$\n",
                "\n",
                "### ðŸ­ Industrial Use Case: BlackRock Option Pricing\n",
                "\n",
                "BlackRock uses Gauss-Hermite quadrature in option pricing models to compute expected payoffs under Gaussian market scenarios. With only 10-20 points, they achieve accuracy that would require 100,000+ Monte Carlo samples.\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: How would you compute $\\mathbb{E}[\\text{ReLU}(X)]$ where $X \\sim \\mathcal{N}(0,1)$?\n",
                ">\n",
                "> **A**: Use Gauss-Hermite with $f(x) = \\max(0, x)$. Transform nodes via $\\sqrt{2}\\sigma x_n + \\mu$. The answer is $1/\\sqrt{2\\pi} \\approx 0.399$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 2.1 GAUSS-HERMITE QUADRATURE\n",
                "# ============================================\n",
                "\n",
                "def gauss_hermite_expectation(f, mu=0, sigma=1, n=10):\n",
                "    \"\"\"\n",
                "    Compute E[f(X)] where X ~ N(Î¼, ÏƒÂ²) using Gauss-Hermite.\n",
                "    \n",
                "    This is the go-to method for Gaussian expectations!\n",
                "    \"\"\"\n",
                "    nodes, weights = roots_hermite(n)\n",
                "    \n",
                "    # Transform for N(Î¼, ÏƒÂ²)\n",
                "    transformed_nodes = np.sqrt(2) * sigma * nodes + mu\n",
                "    transformed_weights = weights / np.sqrt(np.pi)\n",
                "    \n",
                "    return np.sum(transformed_weights * f(transformed_nodes))\n",
                "\n",
                "\n",
                "# Test: E[XÂ²] for X ~ N(0,1) should be 1\n",
                "f_squared = lambda x: x**2\n",
                "result = gauss_hermite_expectation(f_squared, mu=0, sigma=1, n=5)\n",
                "print(f\"E[XÂ²] for X~N(0,1): {result:.10f} (should be 1.0)\")\n",
                "\n",
                "# Test: E[ReLU(X)] for X ~ N(0,1)\n",
                "f_relu = lambda x: np.maximum(0, x)\n",
                "relu_result = gauss_hermite_expectation(f_relu, mu=0, sigma=1, n=20)\n",
                "true_relu = 1 / np.sqrt(2 * np.pi)  # Analytical result\n",
                "print(f\"E[ReLU(X)] for X~N(0,1): {relu_result:.6f} (analytical: {true_relu:.6f})\")\n",
                "\n",
                "# Visualize quadrature points\n",
                "n_points = 10\n",
                "nodes, weights = roots_hermite(n_points)\n",
                "transformed_nodes = np.sqrt(2) * nodes\n",
                "\n",
                "x = np.linspace(-4, 4, 500)\n",
                "f = lambda x: np.exp(-x**2/2) * np.cos(3*x)\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "\n",
                "# Left: Function and quadrature points\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(x, f(x) * norm.pdf(x), 'b-', linewidth=2, label='f(x)p(x)')\n",
                "plt.fill_between(x, f(x) * norm.pdf(x), alpha=0.2)\n",
                "plt.stem(transformed_nodes, f(transformed_nodes) * norm.pdf(transformed_nodes),\n",
                "         'r', markerfmt='ro', basefmt=' ', label='Quadrature points')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('f(x)p(x)')\n",
                "plt.title(f'Gauss-Hermite Quadrature (N={n_points})', fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "# Right: Convergence\n",
                "plt.subplot(1, 2, 2)\n",
                "n_values = range(2, 25)\n",
                "mc_estimate = np.mean(f(np.random.randn(100000)))  # MC reference\n",
                "gh_estimates = [gauss_hermite_expectation(f, n=n) for n in n_values]\n",
                "gh_errors = [abs(e - mc_estimate) for e in gh_estimates]\n",
                "\n",
                "plt.semilogy(list(n_values), gh_errors, 'o-', linewidth=2)\n",
                "plt.xlabel('Number of quadrature points')\n",
                "plt.ylabel('Error (log scale)')\n",
                "plt.title('Gauss-Hermite Convergence', fontweight='bold')\n",
                "plt.grid(True, which='both', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ”‘ Key Insight: Just 10 Gauss-Hermite points often beat 100,000 MC samples!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 3: Monte Carlo Integration\n",
                "\n",
                "---\n",
                "\n",
                "Monte Carlo integration uses **random sampling** to approximate expectations:\n",
                "\n",
                "$$\\mathbb{E}_{x\\sim p}[f(x)] \\approx \\frac{1}{S}\\sum_{s=1}^S f(x^{(s)}), \\quad x^{(s)} \\sim p(x)$$\n",
                "\n",
                "### Key Properties:\n",
                "\n",
                "- **Unbiased**: Expected value equals true integral\n",
                "- **Convergence**: $O(1/\\sqrt{S})$, **independent of dimension**!\n",
                "- **Curse of dimensionality-free**: Works in 1000+ dimensions\n",
                "\n",
                "### ðŸ­ Industrial Use Cases:\n",
                "\n",
                "| Company | Application |\n",
                "|---------|-------------|\n",
                "| **Netflix** | Expected watch time for 200M users |\n",
                "| **JPMorgan** | Value-at-Risk with 100K+ scenarios |\n",
                "| **Pixar** | Global illumination (path tracing) |\n",
                "| **CERN** | Particle physics cross-sections |\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: Why does Monte Carlo work well in high dimensions when quadrature fails?\n",
                ">\n",
                "> **A**: Quadrature needs $O(N^d)$ points (curse of dimensionality). MC converges at $O(1/\\sqrt{N})$ regardless of $d$. At $d > 10$, MC is almost always better."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 3.1 BASIC MONTE CARLO\n",
                "# ============================================\n",
                "\n",
                "def monte_carlo_integrate(f, sampler, n_samples, seed=None):\n",
                "    \"\"\"\n",
                "    Monte Carlo integration: E_p[f(X)] â‰ˆ (1/N) Î£ f(x_i)\n",
                "    \n",
                "    Returns (estimate, standard_error)\n",
                "    \"\"\"\n",
                "    if seed is not None:\n",
                "        np.random.seed(seed)\n",
                "    \n",
                "    samples = sampler(n_samples)\n",
                "    values = f(samples)\n",
                "    \n",
                "    estimate = np.mean(values)\n",
                "    standard_error = np.std(values) / np.sqrt(n_samples)\n",
                "    \n",
                "    return estimate, standard_error\n",
                "\n",
                "\n",
                "# Example: E[XÂ²] for X ~ N(0,1)\n",
                "f = lambda x: x**2\n",
                "sampler = lambda n: np.random.randn(n)\n",
                "\n",
                "sample_sizes = [100, 500, 1000, 5000, 10000, 50000, 100000]\n",
                "estimates = []\n",
                "errors = []\n",
                "\n",
                "for n in sample_sizes:\n",
                "    est, se = monte_carlo_integrate(f, sampler, n, seed=42)\n",
                "    estimates.append(est)\n",
                "    errors.append(se)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"MONTE CARLO CONVERGENCE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"True value: E[XÂ²] = 1.0\")\n",
                "print()\n",
                "for n, est, se in zip(sample_sizes, estimates, errors):\n",
                "    print(f\"N={n:6d}: Estimate={est:.6f}, SE={se:.6f}\")\n",
                "\n",
                "# Plot convergence\n",
                "plt.figure(figsize=(10, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.errorbar(np.log10(sample_sizes), estimates, yerr=2*np.array(errors), \n",
                "             fmt='o-', capsize=3)\n",
                "plt.axhline(y=1.0, color='r', linestyle='--', label='True value')\n",
                "plt.xlabel('logâ‚â‚€(N)')\n",
                "plt.ylabel('Estimate')\n",
                "plt.title('Monte Carlo Estimates with 95% CI', fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.loglog(sample_sizes, errors, 'o-', linewidth=2, label='Standard Error')\n",
                "plt.loglog(sample_sizes, [errors[0]*np.sqrt(sample_sizes[0]/n) for n in sample_sizes],\n",
                "           'r--', label='O(1/âˆšN)')\n",
                "plt.xlabel('N')\n",
                "plt.ylabel('Standard Error')\n",
                "plt.title('Error Convergence Rate', fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, which='both', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3.2 Importance Sampling\n",
                "\n",
                "### Theory\n",
                "\n",
                "When sampling from $p(x)$ is difficult or inefficient, use a **proposal** $q(x)$:\n",
                "\n",
                "$$\\mathbb{E}_p[f(x)] = \\mathbb{E}_q\\left[f(x)\\frac{p(x)}{q(x)}\\right] \\approx \\frac{1}{S}\\sum_{s=1}^S w_s f(x^{(s)})$$\n",
                "\n",
                "where $w_s = p(x^{(s)})/q(x^{(s)})$ are **importance weights**.\n",
                "\n",
                "### ðŸ­ Industrial Use Case: JPMorgan Rare Events\n",
                "\n",
                "JPMorgan uses importance sampling for estimating tail risks (e.g., probability of losses > $100M). Standard MC would rarely sample extreme events. By shifting the proposal toward the tail, they estimate rare event probabilities with orders of magnitude fewer samples.\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: How do you choose a good proposal distribution?\n",
                ">\n",
                "> **A**: The optimal proposal minimizes variance: $q^*(x) \\propto |f(x)|p(x)$. In practice, choose $q$ that is similar to $|f|p$ but easier to sample from. For rare events, shift $q$ toward the rare region."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 3.2 IMPORTANCE SAMPLING\n",
                "# ============================================\n",
                "\n",
                "def importance_sampling(f, p, q, q_sampler, n_samples, seed=None):\n",
                "    \"\"\"\n",
                "    Importance sampling: E_p[f(X)] using samples from q.\n",
                "    \n",
                "    Returns (estimate, standard_error, effective_sample_size)\n",
                "    \"\"\"\n",
                "    if seed is not None:\n",
                "        np.random.seed(seed)\n",
                "    \n",
                "    samples = q_sampler(n_samples)\n",
                "    \n",
                "    # Importance weights\n",
                "    weights = p(samples) / (q(samples) + 1e-10)\n",
                "    normalized_weights = weights / np.sum(weights)\n",
                "    \n",
                "    # Weighted estimate\n",
                "    f_vals = f(samples)\n",
                "    estimate = np.sum(normalized_weights * f_vals)\n",
                "    \n",
                "    # Effective sample size\n",
                "    ess = 1.0 / np.sum(normalized_weights**2)\n",
                "    \n",
                "    return estimate, ess\n",
                "\n",
                "\n",
                "# Example: Estimate P(X > 3) for X ~ N(0,1)\n",
                "# True value: ~0.00135\n",
                "\n",
                "def indicator_above_3(x):\n",
                "    return (x > 3).astype(float)\n",
                "\n",
                "# Standard MC\n",
                "p_standard = lambda x: norm.pdf(x, 0, 1)\n",
                "q_standard = lambda x: norm.pdf(x, 0, 1) \n",
                "q_standard_sampler = lambda n: np.random.randn(n)\n",
                "\n",
                "# Importance sampling with shifted proposal\n",
                "q_shifted = lambda x: norm.pdf(x, 3.5, 1)  # Centered near the rare region\n",
                "q_shifted_sampler = lambda n: np.random.randn(n) + 3.5\n",
                "\n",
                "n_samples = 10000\n",
                "true_prob = 1 - norm.cdf(3)  # ~0.00135\n",
                "\n",
                "# Run multiple trials\n",
                "n_trials = 50\n",
                "mc_estimates = []\n",
                "is_estimates = []\n",
                "\n",
                "for trial in range(n_trials):\n",
                "    # Standard MC\n",
                "    samples = np.random.randn(n_samples)\n",
                "    mc_estimates.append(np.mean(samples > 3))\n",
                "    \n",
                "    # Importance sampling\n",
                "    is_est, _ = importance_sampling(\n",
                "        indicator_above_3, p_standard, q_shifted, q_shifted_sampler, n_samples\n",
                "    )\n",
                "    is_estimates.append(is_est)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"IMPORTANCE SAMPLING: RARE EVENT ESTIMATION\")\n",
                "print(\"=\"*50)\n",
                "print(f\"True P(X > 3): {true_prob:.6f}\")\n",
                "print()\n",
                "print(f\"Standard MC:    Mean={np.mean(mc_estimates):.6f}, Std={np.std(mc_estimates):.6f}\")\n",
                "print(f\"Importance Samp: Mean={np.mean(is_estimates):.6f}, Std={np.std(is_estimates):.6f}\")\n",
                "print()\n",
                "print(f\"Variance reduction: {np.var(mc_estimates)/np.var(is_estimates):.1f}x\")\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(12, 4))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "x = np.linspace(-2, 6, 500)\n",
                "plt.plot(x, norm.pdf(x, 0, 1), 'b-', linewidth=2, label='Target p(x) = N(0,1)')\n",
                "plt.plot(x, norm.pdf(x, 3.5, 1), 'r-', linewidth=2, label='Proposal q(x) = N(3.5,1)')\n",
                "plt.axvline(x=3, color='green', linestyle='--', label='Threshold (x=3)')\n",
                "plt.fill_between(x[x>3], norm.pdf(x[x>3], 0, 1), alpha=0.3, color='green')\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('Density')\n",
                "plt.title('Importance Sampling for Rare Events', fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.hist(mc_estimates, bins=20, alpha=0.5, label='Standard MC', density=True)\n",
                "plt.hist(is_estimates, bins=20, alpha=0.5, label='Importance Sampling', density=True)\n",
                "plt.axvline(x=true_prob, color='r', linestyle='--', linewidth=2, label='True value')\n",
                "plt.xlabel('Estimate')\n",
                "plt.ylabel('Density')\n",
                "plt.title('Estimator Distributions (50 trials)', fontweight='bold')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 4: Normalizing Flows\n",
                "\n",
                "---\n",
                "\n",
                "Normalizing flows transform simple distributions into complex ones through **invertible** transformations.\n",
                "\n",
                "### Theory\n",
                "\n",
                "Given base distribution $p_0(z_0)$ and transformations $z_k = f_k(z_{k-1})$:\n",
                "\n",
                "$$\\log p(x) = \\log p(z_0) - \\sum_{k=1}^K \\log\\left|\\det\\frac{\\partial f_k}{\\partial z_{k-1}}\\right|$$\n",
                "\n",
                "The Jacobian determinant accounts for volume change under transformation.\n",
                "\n",
                "### ðŸ­ Industrial Use Cases:\n",
                "\n",
                "| Company | Application | Result |\n",
                "|---------|-------------|--------|\n",
                "| **Spotify** | User preference modeling | 23% engagement improvement |\n",
                "| **Waymo** | Pedestrian trajectory prediction | Multi-modal futures |\n",
                "| **NVIDIA** | Video synthesis | Realistic temporal dynamics |\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: Compare normalizing flows to VAEs and GANs.\n",
                ">\n",
                "> **A**: Flows give **exact** likelihoods (no ELBO bound). They're invertible (unlike GANs). Trade-off: architecture restricted for tractable Jacobian ($O(D)$ not $O(D^3)$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 4.1 PLANAR FLOW IMPLEMENTATION\n",
                "# ============================================\n",
                "\n",
                "class PlanarFlow:\n",
                "    \"\"\"\n",
                "    Planar flow: f(z) = z + u * tanh(w^T z + b)\n",
                "    \n",
                "    Contracts/expands space along hyperplanes.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, d):\n",
                "        self.d = d\n",
                "        self.w = np.random.randn(d) * 0.5\n",
                "        self.u = np.random.randn(d) * 0.5\n",
                "        self.b = np.random.randn() * 0.5\n",
                "        self._enforce_invertibility()\n",
                "    \n",
                "    def _enforce_invertibility(self):\n",
                "        \"\"\"Ensure w^T u >= -1 for invertibility.\"\"\"\n",
                "        wtu = np.dot(self.w, self.u)\n",
                "        m_wtu = -1 + np.log(1 + np.exp(wtu))\n",
                "        self.u_hat = self.u + (m_wtu - wtu) * self.w / (np.linalg.norm(self.w)**2 + 1e-8)\n",
                "    \n",
                "    def forward(self, z):\n",
                "        \"\"\"Transform z -> z_new, return (z_new, log_det_jacobian)\"\"\"\n",
                "        z = np.atleast_2d(z)\n",
                "        self._enforce_invertibility()\n",
                "        \n",
                "        linear = z @ self.w + self.b\n",
                "        h = np.tanh(linear)\n",
                "        z_new = z + np.outer(h, self.u_hat)\n",
                "        \n",
                "        h_prime = 1 - h**2\n",
                "        psi = h_prime * np.dot(self.u_hat, self.w)\n",
                "        log_det = np.log(np.abs(1 + psi) + 1e-8)\n",
                "        \n",
                "        return z_new, log_det\n",
                "\n",
                "\n",
                "class FlowChain:\n",
                "    \"\"\"Chain of normalizing flows.\"\"\"\n",
                "    \n",
                "    def __init__(self, flows):\n",
                "        self.flows = flows\n",
                "    \n",
                "    def forward(self, z):\n",
                "        z = np.atleast_2d(z)\n",
                "        total_log_det = np.zeros(z.shape[0])\n",
                "        \n",
                "        for flow in self.flows:\n",
                "            z, log_det = flow.forward(z)\n",
                "            total_log_det += log_det\n",
                "        \n",
                "        return z, total_log_det\n",
                "\n",
                "\n",
                "# Visualize flow transformation\n",
                "np.random.seed(42)\n",
                "n_samples = 2000\n",
                "n_flows_list = [0, 2, 4, 8]\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
                "\n",
                "for ax, n_flows in zip(axes, n_flows_list):\n",
                "    z0 = np.random.randn(n_samples, 2)\n",
                "    \n",
                "    if n_flows == 0:\n",
                "        z = z0\n",
                "        title = 'Base Distribution'\n",
                "    else:\n",
                "        flows = [PlanarFlow(2) for _ in range(n_flows)]\n",
                "        chain = FlowChain(flows)\n",
                "        z, _ = chain.forward(z0)\n",
                "        title = f'{n_flows} Planar Flows'\n",
                "    \n",
                "    ax.scatter(z[:, 0], z[:, 1], alpha=0.3, s=5)\n",
                "    ax.set_xlim(-4, 4)\n",
                "    ax.set_ylim(-4, 4)\n",
                "    ax.set_title(title, fontweight='bold')\n",
                "    ax.set_aspect('equal')\n",
                "    ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.suptitle('Normalizing Flow: Transforming Gaussian to Complex Distribution', \n",
                "             fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"ðŸ’¡ Key Insight: More flows = more expressive distribution.\")\n",
                "print(\"   Spotify uses 8-16 flows for user preference modeling.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Chapter 5: Time Series Inference\n",
                "\n",
                "---\n",
                "\n",
                "For state space models:\n",
                "\n",
                "$$x_{t+1} = f(x_t) + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, Q)$$\n",
                "\n",
                "We need to propagate uncertainty through nonlinear dynamics.\n",
                "\n",
                "### Methods:\n",
                "\n",
                "| Method | Approach | Complexity | Best For |\n",
                "|--------|----------|------------|----------|\n",
                "| **EKF** | Linearization | $O(D^3)$ | Mildly nonlinear |\n",
                "| **UKF** | Sigma points | $O(D^3)$ | Moderate nonlinearity |\n",
                "| **Particle Filter** | Sampling | $O(N)$ | Multi-modal, non-Gaussian |\n",
                "\n",
                "### ðŸ­ Industrial Use Cases:\n",
                "\n",
                "| Company | Method | Application |\n",
                "|---------|--------|-------------|\n",
                "| **Tesla FSD** | EKF + UKF | Vehicle trajectory prediction |\n",
                "| **Boston Dynamics** | EKF | Robot state estimation |\n",
                "| **Garmin** | Kalman Filter | GPS position tracking |\n",
                "\n",
                "### ðŸ“ Interview Question\n",
                "\n",
                "> **Q**: When would you use a particle filter over EKF/UKF?\n",
                ">\n",
                "> **A**: For multi-modal posteriors or non-Gaussian noise. EKF/UKF assume Gaussian. Particle filters can represent arbitrary distributions but scale poorly with state dimension."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================\n",
                "# 5.1 UNSCENTED TRANSFORM (UKF Core)\n",
                "# ============================================\n",
                "\n",
                "def unscented_transform(f, mu, sigma, alpha=1e-3, beta=2, kappa=0):\n",
                "    \"\"\"\n",
                "    Unscented transform for propagating Gaussian through nonlinear function.\n",
                "    \n",
                "    Uses 2D+1 sigma points to capture mean and covariance.\n",
                "    \"\"\"\n",
                "    D = len(mu)\n",
                "    lam = alpha**2 * (D + kappa) - D\n",
                "    \n",
                "    # Compute sigma points\n",
                "    sqrt_matrix = np.linalg.cholesky((D + lam) * sigma)\n",
                "    \n",
                "    sigma_points = np.zeros((2*D + 1, D))\n",
                "    sigma_points[0] = mu\n",
                "    for i in range(D):\n",
                "        sigma_points[i+1] = mu + sqrt_matrix[i]\n",
                "        sigma_points[D+i+1] = mu - sqrt_matrix[i]\n",
                "    \n",
                "    # Weights\n",
                "    Wm = np.full(2*D + 1, 1 / (2*(D + lam)))\n",
                "    Wm[0] = lam / (D + lam)\n",
                "    Wc = Wm.copy()\n",
                "    Wc[0] += (1 - alpha**2 + beta)\n",
                "    \n",
                "    # Propagate through function\n",
                "    Y = np.array([f(sp) for sp in sigma_points])\n",
                "    \n",
                "    # Compute output mean and covariance\n",
                "    mu_y = np.sum(Wm[:, None] * Y, axis=0)\n",
                "    sigma_y = np.zeros((Y.shape[1], Y.shape[1]))\n",
                "    for i in range(2*D + 1):\n",
                "        diff = Y[i] - mu_y\n",
                "        sigma_y += Wc[i] * np.outer(diff, diff)\n",
                "    \n",
                "    return mu_y, sigma_y, sigma_points, Y\n",
                "\n",
                "\n",
                "# Example: Propagate through nonlinear function\n",
                "def nonlinear_dynamics(x):\n",
                "    \"\"\"Example nonlinear state transition.\"\"\"\n",
                "    return np.array([np.sin(x[0]) + 0.5*x[1], \n",
                "                     0.9*x[1] + 0.1*x[0]**2])\n",
                "\n",
                "# Initial state\n",
                "mu_0 = np.array([1.0, 0.5])\n",
                "sigma_0 = np.array([[0.2, 0.05], [0.05, 0.1]])\n",
                "\n",
                "# Apply unscented transform\n",
                "mu_1, sigma_1, sigma_pts, Y = unscented_transform(nonlinear_dynamics, mu_0, sigma_0)\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"UNSCENTED TRANSFORM EXAMPLE\")\n",
                "print(\"=\"*50)\n",
                "print(f\"Input mean: {mu_0}\")\n",
                "print(f\"Input cov diagonal: {np.diag(sigma_0)}\")\n",
                "print()\n",
                "print(f\"Output mean: {mu_1}\")\n",
                "print(f\"Output cov diagonal: {np.diag(sigma_1)}\")\n",
                "print()\n",
                "print(f\"Number of sigma points: {len(sigma_pts)} (2D+1 = {2*len(mu_0)+1})\")\n",
                "\n",
                "# Visualize\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Input space\n",
                "ax = axes[0]\n",
                "samples = np.random.multivariate_normal(mu_0, sigma_0, 500)\n",
                "ax.scatter(samples[:, 0], samples[:, 1], alpha=0.3, s=10, label='Samples')\n",
                "ax.scatter(sigma_pts[:, 0], sigma_pts[:, 1], c='red', s=100, marker='x', \n",
                "           linewidths=2, label='Sigma points')\n",
                "ax.scatter(mu_0[0], mu_0[1], c='green', s=150, marker='*', label='Mean')\n",
                "ax.set_xlabel('xâ‚')\n",
                "ax.set_ylabel('xâ‚‚')\n",
                "ax.set_title('Input Distribution', fontweight='bold')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Output space\n",
                "ax = axes[1]\n",
                "samples_out = np.array([nonlinear_dynamics(s) for s in samples])\n",
                "ax.scatter(samples_out[:, 0], samples_out[:, 1], alpha=0.3, s=10, label='Propagated samples')\n",
                "ax.scatter(Y[:, 0], Y[:, 1], c='red', s=100, marker='x', \n",
                "           linewidths=2, label='Propagated sigma points')\n",
                "ax.scatter(mu_1[0], mu_1[1], c='green', s=150, marker='*', label='UKF mean')\n",
                "ax.set_xlabel('yâ‚')\n",
                "ax.set_ylabel('yâ‚‚')\n",
                "ax.set_title('Output Distribution (through nonlinear f)', fontweight='bold')\n",
                "ax.legend()\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nðŸ”‘ Key Insight: UKF uses 2D+1 points to capture mean/covariance exactly\")\n",
                "print(\"   for linear transforms, and approximates well for nonlinear.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Summary & Key Takeaways\n",
                "\n",
                "---\n",
                "\n",
                "## Methods Comparison\n",
                "\n",
                "| Method | Convergence | Dimension | Best For |\n",
                "|--------|-------------|-----------|----------|\n",
                "| **Trapezoidal** | O(1/NÂ²) | Low | Noisy data |\n",
                "| **Simpson's** | O(1/Nâ´) | Low | Smooth functions |\n",
                "| **Gauss-Hermite** | Exponential | Low | Gaussian expectations |\n",
                "| **Monte Carlo** | O(1/âˆšN) | Any | High dimensions |\n",
                "| **Importance Sampling** | Variance-reduced | Any | Rare events |\n",
                "| **Bayesian Quadrature** | Optimal | Medium | Expensive functions |\n",
                "\n",
                "## Interview Prep Checklist\n",
                "\n",
                "âœ… When to use Simpson's vs Trapezoidal  \n",
                "âœ… Why Gauss-Hermite beats Monte Carlo for Gaussian expectations  \n",
                "âœ… How to design importance sampling proposals  \n",
                "âœ… Change of variables formula for normalizing flows  \n",
                "âœ… EKF vs UKF vs Particle Filter trade-offs\n",
                "\n",
                "## Industrial Applications Reviewed\n",
                "\n",
                "- **NASA JPL**: Spacecraft trajectories\n",
                "- **BlackRock**: Option pricing\n",
                "- **Netflix**: Recommendations\n",
                "- **JPMorgan**: Risk assessment\n",
                "- **Tesla**: Autonomous driving\n",
                "- **Spotify**: Music discovery"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "# Exercises\n",
                "\n",
                "---\n",
                "\n",
                "1. **Newton-Cotes**: Implement adaptive quadrature that recursively subdivides intervals where error exceeds tolerance.\n",
                "\n",
                "2. **Gauss-Hermite**: Compute $\\mathbb{E}[\\sigma(X)]$ where $\\sigma$ is the sigmoid function and $X \\sim \\mathcal{N}(0,1)$.\n",
                "\n",
                "3. **Monte Carlo**: Implement stratified sampling and compare variance reduction to simple MC.\n",
                "\n",
                "4. **Importance Sampling**: Design a proposal for estimating $P(X > 5)$ where $X \\sim \\mathcal{N}(0,1)$.\n",
                "\n",
                "5. **Normalizing Flows**: Implement radial flows and compare expressiveness to planar flows.\n",
                "\n",
                "6. **Time Series**: Implement a full UKF for a 2D tracking problem with nonlinear dynamics.\n",
                "\n",
                "---\n",
                "\n",
                "*Note: Solutions can be found in the `src/core/integration.py` and `src/core/normalizing_flows.py` modules.*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}