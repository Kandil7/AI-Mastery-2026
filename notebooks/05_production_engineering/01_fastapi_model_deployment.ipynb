{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI Model Deployment\n",
    "\n",
    "This notebook demonstrates how to deploy machine learning models using FastAPI, focusing on production-ready deployment patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Any, Optional\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Model Deployment with FastAPI\n",
    "\n",
    "Deploying ML models in production requires:\n",
    "\n",
    "1. **API Design**: Clean, well-documented endpoints\n",
    "2. **Performance**: Efficient inference and request handling\n",
    "3. **Monitoring**: Metrics and logging for model performance\n",
    "4. **Scalability**: Ability to handle multiple requests\n",
    "5. **Security**: Input validation and protection against attacks\n",
    "6. **Reliability**: Error handling and graceful degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model to deploy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_regression(n_samples=1000, n_features=10, noise=10, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Model Performance:\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"RÂ²: {r2:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "model_path = \"temp_model.pkl\"\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FastAPI application\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import logging\n",
    "from contextlib import asynccontextmanager\n",
    "import joblib\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Define Pydantic models for request/response\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: List[List[float]]  # 2D array for batch predictions\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    predictions: List[float]\n",
    "    processing_time: float\n",
    "    timestamp: float\n",
    "\n",
    "class HealthCheckResponse(BaseModel):\n",
    "    status: str\n",
    "    timestamp: float\n",
    "\n",
    "\n",
    "# Global model variable\n",
    "model = None\n",
    "\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Lifespan manager for the FastAPI app.\"\"\"\n",
    "    global model\n",
    "    logger.info(\"Loading model...\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = joblib.load(model_path)\n",
    "    logger.info(\"Model loaded successfully\")\n",
    "    \n",
    "    yield\n",
    "    \n",
    "    # Cleanup\n",
    "    logger.info(\"Shutting down...\")\n",
    "\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"ML Model API\",\n",
    "    description=\"Production-ready API for ML model inference\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the ML Model API\"}\n",
    "\n",
    "\n",
    "@app.get(\"/health\", response_model=HealthCheckResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return HealthCheckResponse(\n",
    "        status=\"healthy\",\n",
    "        timestamp=time.time()\n",
    "    )\n",
    "\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"Make predictions using the model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Convert features to numpy array\n",
    "        features = np.array(request.features)\n",
    "        \n",
    "        # Validate input shape\n",
    "        if features.ndim != 2:\n",
    "            raise HTTPException(status_code=400, detail=\"Features must be a 2D array\")\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = model.predict(features).tolist()\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Log the prediction\n",
    "        logger.info(f\"Prediction made for {len(predictions)} samples, processing_time: {processing_time:.4f}s\")\n",
    "        \n",
    "        return PredictionResponse(\n",
    "            predictions=predictions,\n",
    "            processing_time=processing_time,\n",
    "            timestamp=time.time()\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n",
    "\n",
    "\n",
    "# Test the API endpoints\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Define a function to test the API\n",
    "def test_api():\n",
    "    # Test health check\n",
    "    try:\n",
    "        response = requests.get(\"http://localhost:8000/health\")\n",
    "        print(f\"Health check: {response.status_code}, {response.json()}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"API is not running. Start it with: uvicorn this_app:app --reload\")\n",
    "    \n",
    "    # Test prediction\n",
    "    try:\n",
    "        test_data = {\"features\": X_test[:5].tolist()}  # Take first 5 samples\n",
    "        response = requests.post(\"http://localhost:8000/predict\", json=test_data)\n",
    "        print(f\"Prediction: {response.status_code}, {response.json()}\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"API is not running. Start it with: uvicorn this_app:app --reload\")\n",
    "\n",
    "# Note: To run the API, you would typically use:\n",
    "# uvicorn this_notebook_app:app --reload --port 8000\n",
    "# But in a notebook, we'll just define the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a more advanced API with monitoring and caching\n",
    "from fastapi import FastAPI, HTTPException, BackgroundTasks, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import asyncio\n",
    "from collections import deque\n",
    "import threading\n",
    "import statistics\n",
    "\n",
    "# Advanced API with monitoring\n",
    "class AdvancedModelAPI:\n",
    "    def __init__(self):\n",
    "        self.app = FastAPI(title=\"Advanced ML Model API\")\n",
    "        self.model = None\n",
    "        self.prediction_times = deque(maxlen=1000)  # Keep last 1000 prediction times\n",
    "        self.request_count = 0\n",
    "        self.error_count = 0\n",
    "        \n",
    "        # Add CORS middleware\n",
    "        self.app.add_middleware(\n",
    "            CORSMiddleware,\n",
    "            allow_origins=[\"*\"],\n",
    "            allow_credentials=True,\n",
    "            allow_methods=[\"*\"],\n",
    "            allow_headers=[\"*\"],\n",
    "        )\n",
    "        \n",
    "        # Add routes\n",
    "        self._add_routes()\n",
    "    \n",
    "    def _add_routes(self):\n",
    "        @self.app.get(\"/\")\n",
    "        async def root():\n",
    "            return {\"message\": \"Advanced ML Model API\"}\n",
    "        \n",
    "        @self.app.get(\"/health\")\n",
    "        async def health_check():\n",
    "            return {\n",
    "                \"status\": \"healthy\",\n",
    "                \"timestamp\": time.time(),\n",
    "                \"metrics\": {\n",
    "                    \"request_count\": self.request_count,\n",
    "                    \"error_count\": self.error_count,\n",
    "                    \"avg_prediction_time\": statistics.mean(self.prediction_times) if self.prediction_times else 0,\n",
    "                    \"p95_prediction_time\": float(np.percentile(self.prediction_times, 95)) if self.prediction_times else 0\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        @self.app.post(\"/predict\")\n",
    "        async def predict(request: PredictionRequest):\n",
    "            start_time = time.time()\n",
    "            self.request_count += 1\n",
    "            \n",
    "            try:\n",
    "                # Convert features to numpy array\n",
    "                features = np.array(request.features)\n",
    "                \n",
    "                # Validate input shape\n",
    "                if features.ndim != 2:\n",
    "                    raise HTTPException(status_code=400, detail=\"Features must be a 2D array\")\n",
    "                \n",
    "                # Make predictions (using our pre-trained model)\n",
    "                # In a real implementation, you would load the actual model\n",
    "                predictions = model.predict(features).tolist()\n",
    "                \n",
    "                # Record prediction time\n",
    "                prediction_time = time.time() - start_time\n",
    "                self.prediction_times.append(prediction_time)\n",
    "                \n",
    "                return {\n",
    "                    \"predictions\": predictions,\n",
    "                    \"processing_time\": prediction_time,\n",
    "                    \"timestamp\": time.time()\n",
    "                }\n",
    "            except Exception as e:\n",
    "                self.error_count += 1\n",
    "                raise HTTPException(status_code=500, detail=f\"Prediction failed: {str(e)}\")\n",
    "        \n",
    "        @self.app.get(\"/metrics\")\n",
    "        async def get_metrics():\n",
    "            return {\n",
    "                \"request_count\": self.request_count,\n",
    "                \"error_count\": self.error_count,\n",
    "                \"error_rate\": self.error_count / max(self.request_count, 1),\n",
    "                \"avg_prediction_time\": statistics.mean(self.prediction_times) if self.prediction_times else 0,\n",
    "                \"p50_prediction_time\": statistics.median(self.prediction_times) if self.prediction_times else 0,\n",
    "                \"p95_prediction_time\": float(np.percentile(self.prediction_times, 95)) if self.prediction_times else 0,\n",
    "                \"p99_prediction_time\": float(np.percentile(self.prediction_times, 99)) if self.prediction_times else 0,\n",
    "                \"active_requests\": 0  # In a real implementation, you'd track this\n",
    "            }\n",
    "    \n",
    "    def run(self, host=\"0.0.0.0\", port=8000):\n",
    "        import uvicorn\n",
    "        uvicorn.run(self.app, host=host, port=port)\n",
    "\n",
    "# Create the advanced API\n",
    "advanced_api = AdvancedModelAPI()\n",
    "\n",
    "# Print the API info\n",
    "print(\"Advanced Model API created with monitoring capabilities\")\n",
    "print(\"Endpoints:\")\n",
    "print(\"  GET  / - Root endpoint\")\n",
    "print(\"  GET  /health - Health check with metrics\")\n",
    "print(\"  POST /predict - Make predictions\")\n",
    "print(\"  GET  /metrics - Performance metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate model serialization and versioning\n",
    "import joblib\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class ModelRegistry:\n",
    "    \"\"\"Simple model registry for versioning and management.\"\"\"\n",
    "    \n",
    "    def __init__(self, registry_path=\"model_registry\"):\n",
    "        self.registry_path = registry_path\n",
    "        os.makedirs(registry_path, exist_ok=True)\n",
    "        self.registry_file = os.path.join(registry_path, \"registry.json\")\n",
    "        self.models = self._load_registry()\n",
    "    \n",
    "    def _load_registry(self):\n",
    "        \"\"\"Load the model registry from file.\"\"\"\n",
    "        if os.path.exists(self.registry_file):\n",
    "            with open(self.registry_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _save_registry(self):\n",
    "        \"\"\"Save the model registry to file.\"\"\"\n",
    "        with open(self.registry_file, 'w') as f:\n",
    "            json.dump(self.models, f, indent=2)\n",
    "    \n",
    "    def register_model(self, model, model_name, metrics=None, description=\"\"):\n",
    "        \"\"\"Register a model in the registry.\"\"\"\n",
    "        # Create a unique version ID\n",
    "        version_id = f\"{model_name}-v{len([k for k in self.models.keys() if k.startswith(model_name)]) + 1}-{int(time.time())}\"\n",
    "        \n",
    "        # Save the model\n",
    "        model_path = os.path.join(self.registry_path, f\"{version_id}.pkl\")\n",
    "        joblib.dump(model, model_path)\n",
    "        \n",
    "        # Calculate model hash\n",
    "        with open(model_path, 'rb') as f:\n",
    "            model_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        \n",
    "        # Register the model\n",
    "        self.models[version_id] = {\n",
    "            \"name\": model_name,\n",
    "            \"version_id\": version_id,\n",
    "            \"path\": model_path,\n",
    "            \"hash\": model_hash,\n",
    "            \"registered_at\": datetime.now().isoformat(),\n",
    "            \"metrics\": metrics or {},\n",
    "            \"description\": description\n",
    "        }\n",
    "        \n",
    "        self._save_registry()\n",
    "        return version_id\n",
    "    \n",
    "    def get_model(self, version_id):\n",
    "        \"\"\"Load a model by version ID.\"\"\"\n",
    "        if version_id not in self.models:\n",
    "            raise ValueError(f\"Model {version_id} not found in registry\")\n",
    "        \n",
    "        model_path = self.models[version_id][\"path\"]\n",
    "        return joblib.load(model_path)\n",
    "    \n",
    "    def list_models(self, model_name=None):\n",
    "        \"\"\"List all models or models with a specific name.\"\"\"\n",
    "        if model_name:\n",
    "            return {k: v for k, v in self.models.items() if v[\"name\"] == model_name}\n",
    "        return self.models\n",
    "\n",
    "# Create a model registry and register our model\n",
    "registry = ModelRegistry()\n",
    "\n",
    "# Register the model we trained earlier\n",
    "version_id = registry.register_model(\n",
    "    model=model,\n",
    "    model_name=\"random_forest_regressor\",\n",
    "    metrics={\"mse\": mse, \"r2\": r2},\n",
    "    description=\"Random Forest Regressor trained on synthetic data\"\n",
    ")\n",
    "\n",
    "print(f\"Model registered with version ID: {version_id}\")\n",
    "print(f\"Registry contents:\")\n",
    "for vid, info in registry.list_models().items():\n",
    "    print(f\"  {vid}: {info['description']}\")\n",
    "\n",
    "# Load the model back from the registry\n",
    "loaded_model = registry.get_model(version_id)\n",
    "print(f\"Model loaded from registry: {type(loaded_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complete deployment configuration\n",
    "import yaml\n",
    "\n",
    "# Create a deployment configuration\n",
    "deployment_config = {\n",
    "    \"api\": {\n",
    "        \"name\": \"ml-model-api\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"host\": \"0.0.0.0\",\n",
    "        \"port\": 8000,\n",
    "        \"workers\": 4,\n",
    "        \"timeout\": 30,\n",
    "        \"keep_alive\": 5\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"version_id\": version_id,\n",
    "        \"registry_path\": \"model_registry\",\n",
    "        \"max_batch_size\": 1000,\n",
    "        \"cache_enabled\": True,\n",
    "        \"cache_size\": 10000\n",
    "    },\n",
    "    \"monitoring\": {\n",
    "        \"metrics_enabled\": True,\n",
    "        \"logging_level\": \"INFO\",\n",
    "        \"prometheus_enabled\": True,\n",
    "        \"prometheus_port\": 9090\n",
    "    },\n",
    "    \"security\": {\n",
    "        \"rate_limiting\": {\n",
    "            \"enabled\": True,\n",
    "            \"requests_per_minute\": 1000\n",
    "        },\n",
    "        \"authentication\": {\n",
    "            \"enabled\": False,\n",
    "            \"api_keys\": []\n",
    "        }\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"max_memory_usage\": \"2GB\",\n",
    "        \"cpu_affinity\": True,\n",
    "        \"threading\": {\n",
    "            \"enabled\": True,\n",
    "            \"max_workers\": 10\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save the configuration\n",
    "with open(\"deployment_config.yaml\", \"w\") as f:\n",
    "    yaml.dump(deployment_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"Deployment configuration saved to deployment_config.yaml\")\n",
    "print(\"\\nConfiguration:\")\n",
    "print(yaml.dump(deployment_config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **API Design**: Clean, well-documented endpoints with proper request/response models\n",
    "2. **Model Serialization**: Proper model saving/loading with versioning and registry\n",
    "3. **Monitoring**: Metrics collection for performance and error tracking\n",
    "4. **Configuration**: Externalized configuration for different deployment environments\n",
    "5. **Scalability**: Considerations for handling multiple requests and resource usage\n",
    "6. **Security**: Input validation and rate limiting for production safety"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}