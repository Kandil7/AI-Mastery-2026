{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfaf Transformers & Attention: Complete Guide\n\n## The Architecture That Changed Everything\n\n**\"Attention is All You Need\"** - Vaswani et al., 2017\n\nComprehensive breakdown of the Transformer architecture and self-attention mechanism.\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\nprint('\u2705 Transformers ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Self-Attention Mathematics\n\n### The Core Innovation\n\n**Input**: Sequence of vectors $X = [x_1, x_2, ..., x_n]$\n\n**Three learned projections**:\n- **Query**: $Q = XW_Q$\n- **Key**: $K = XW_K$  \n- **Value**: $V = XW_V$\n\n**Attention formula**:\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\n**Why $\\sqrt{d_k}$?** Prevents dot products from growing too large.\n\n### Multi-Head Attention\n\nRun h parallel attention \"heads\":\n\n$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\nwhere each $\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n\n**Benefit**: Learn different representation subspaces\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified self-attention\ndef self_attention(Q, K, V):\n    \"\"\"Scaled dot-product attention.\"\"\"\n    d_k = Q.shape[-1]\n    scores = np.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n    attention_weights = np.exp(scores) / np.sum(np.exp(scores), axis=-1, keepdims=True)\n    output = np.matmul(attention_weights, V)\n    return output, attention_weights\n\n# Example\nseq_len, d_model = 4, 8\nX = np.random.randn(seq_len, d_model)\nQ = K = V = X  # Self-attention\n\noutput, weights = self_attention(Q, K, V)\nprint(f'Input shape: {X.shape}')\nprint(f'Output shape: {output.shape}')\nprint(f'Attention weights shape: {weights.shape}')\nprint('\u2705 Self-attention computed!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer Architecture\n\n### Encoder\n1. Input Embedding + Positional Encoding\n2. **N layers** of:\n   - Multi-Head Self-Attention\n   - Add & Norm\n   - Feed-Forward Network\n   - Add & Norm\n\n### Decoder  \n1. Output Embedding + Positional Encoding\n2. **N layers** of:\n   - Masked Multi-Head Self-Attention\n   - Add & Norm\n   - Multi-Head Cross-Attention (with encoder)\n   - Add & Norm\n   - Feed-Forward\n   - Add & Norm\n\n### Why Transformers Won\n\n\u2705 **Parallelizable** (unlike RNNs)\n\u2705 **Long-range dependencies** (O(1) vs O(n) for RNNs)\n\u2705 **Scalable** to billions of parameters\n\u2705 **Transfer learning** (BERT, GPT)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}