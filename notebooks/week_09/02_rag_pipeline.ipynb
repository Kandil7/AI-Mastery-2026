{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\udd0d RAG: Retrieval Augmented Generation\n\n## Giving LLMs Long-Term Memory\n\n**Problem**: LLMs have cutoff knowledge dates\n**Solution**: Retrieve relevant context dynamically\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nprint('\u2705 RAG concepts ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG Architecture\n\n### The Pipeline\n\n1. **Ingestion**\n   - Chunk documents\n   - Generate embeddings\n   - Store in vector DB\n\n2. **Retrieval**\n   - User query \u2192 embedding\n   - Similarity search\n   - Top-k relevant chunks\n\n3. **Generation**\n   - Inject chunks into prompt\n   - LLM generates answer\n   - Cite sources\n\n### Chunking Strategies\n\n**Fixed-size**: 512 tokens, overlap 50\n**Semantic**: Split on paragraphs/sentences\n**Recursive**: Hierarchical chunking\n\n### Embedding Models\n\n| Model | Dimensions | Speed | Quality |\n|-------|-----------|-------|----------|\n| **all-MiniLM-L6-v2** | 384 | Fast | Good |\n| **text-embedding-ada-002** | 1536 | Medium | Excellent |\n| **BGE-large** | 1024 | Slow | Best |\n\n### Vector Databases\n\n**Chroma**: Simple, local, great for prototyping\n**Qdrant**: Production-ready, scalable\n**Pinecone**: Managed, serverless\n**Weaviate**: Hybrid search (dense + sparse)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG Pipeline (simplified)\nclass SimpleRAG:\n    def __init__(self, documents):\n        self.documents = documents\n        self.embeddings = self._embed_documents(documents)\n    \n    def _embed_documents(self, docs):\n        # Simplified: random embeddings\n        # In practice: use SentenceTransformer\n        return np.random.randn(len(docs), 384)\n    \n    def retrieve(self, query, k=3):\n        # Embed query\n        query_emb = np.random.randn(384)  # Simplified\n        \n        # Compute similarities (cosine)\n        similarities = np.dot(self.embeddings, query_emb)\n        similarities /= (np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_emb))\n        \n        # Top-k\n        top_k_idx = np.argsort(similarities)[-k:][::-1]\n        return [self.documents[i] for i in top_k_idx]\n    \n    def generate(self, query):\n        # Retrieve context\n        context = self.retrieve(query)\n        \n        # Build prompt\n        prompt = f\"Context: {' '.join(context)}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n        \n        # In practice: call LLM API\n        return \"[LLM would generate answer here]\"\n\nprint('\u2705 RAG pipeline structure!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced RAG Techniques\n\n### HyDE (Hypothetical Document Embeddings)\n1. Generate hypothetical answer\n2. Embed hypothetical doc\n3. Retrieve similar docs\n\n### Multi-Query\n1. Generate multiple query variations\n2. Retrieve for each\n3. Combine results\n\n### Reranking\n1. Retrieve top-20 with fast method\n2. Rerank with slower, better model\n3. Return top-5\n\n### Production Considerations\n\n\u2705 **Caching**: Cache embeddings & retrievals\n\u2705 **Monitoring**: Track retrieval quality\n\u2705 **Evaluation**: Use RAGAS metrics\n\u2705 **Cost**: Embedding API calls add up\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}