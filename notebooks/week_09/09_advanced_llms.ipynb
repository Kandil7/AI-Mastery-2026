{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 09: Advanced LLM Architectures\n",
                "\n",
                "Exploring modern efficiency techniques: Mixture of Experts (MoE) and Rotary Embeddings (RoPE).\n",
                "\n",
                "## Learning Objectives\n",
                "1. Implement Rotary Positional Embeddings (RoPE)\n",
                "2. Build a Mixture of Experts (MoE) Layer\n",
                "3. Understand Gated Linear Units (GLU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "import math\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Rotary Positional Embeddings (RoPE)\n",
                "\n",
                "RoPE rotates the query and key vectors to encode relative position."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
                "    \"\"\"\n",
                "    Precompute the frequency tensor for complex exponentials (cis).\n",
                "    \"\"\"\n",
                "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
                "    t = torch.arange(end, device=freqs.device)\n",
                "    freqs = torch.outer(t, freqs).float()  # (seq_len, dim/2)\n",
                "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
                "    return freqs_cis\n",
                "\n",
                "def apply_rotary_emb(xq, xk, freqs_cis):\n",
                "    \"\"\"\n",
                "    Apply RoPE to queries and keys.\n",
                "    Input shape: (batch, seq_len, n_head, head_dim)\n",
                "    \"\"\"\n",
                "    # Reshape for broadcast\n",
                "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
                "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
                "    \n",
                "    freqs_cis = freqs_cis.view(1, xq_.shape[1], 1, xq_.shape[-1])\n",
                "    \n",
                "    # Rotate\n",
                "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
                "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
                "    \n",
                "    return xq_out.type_as(xq), xk_out.type_as(xk)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test RoPE\n",
                "dim = 64\n",
                "seq_len = 10\n",
                "freqs_cis = precompute_freqs_cis(dim, seq_len)\n",
                "\n",
                "q = torch.randn(1, seq_len, 4, dim)\n",
                "k = torch.randn(1, seq_len, 4, dim)\n",
                "\n",
                "q_rot, k_rot = apply_rotary_emb(q, k, freqs_cis)\n",
                "print(f\"Original Q norm: {torch.norm(q):.4f}\")\n",
                "print(f\"Rotated Q norm:  {torch.norm(q_rot):.4f} (should be same)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. SwiGLU Activation\n",
                "\n",
                "SwiGLU(x) = Swish(xW) * (xV)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SwiGLU(nn.Module):\n",
                "    def __init__(self, dim, hidden_dim):\n",
                "        super().__init__()\n",
                "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
                "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
                "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
                "\n",
                "    def forward(self, x):\n",
                "        # silu(x * w1) * (x * w3) -> * w2\n",
                "        return F.silu(self.w1(x)) * self.w3(x) @ self.w2.weight.T"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Mixture of Experts (MoE)\n",
                "\n",
                "Sparse MoE layer with Top-K gating."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MoELayer(nn.Module):\n",
                "    def __init__(self, dim, num_experts, top_k=2):\n",
                "        super().__init__()\n",
                "        self.num_experts = num_experts\n",
                "        self.top_k = top_k\n",
                "        \n",
                "        # Experts: simple FFN or MLP\n",
                "        self.experts = nn.ModuleList([\n",
                "            nn.Sequential(\n",
                "                nn.Linear(dim, 4*dim),\n",
                "                nn.ReLU(),\n",
                "                nn.Linear(4*dim, dim)\n",
                "            ) for _ in range(num_experts)\n",
                "        ])\n",
                "        \n",
                "        # Gating network\n",
                "        self.gate = nn.Linear(dim, num_experts)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        batch, seq_len, dim = x.shape\n",
                "        x_flat = x.view(-1, dim)\n",
                "        \n",
                "        # Gate scores\n",
                "        gate_logits = self.gate(x_flat)\n",
                "        probs = F.softmax(gate_logits, dim=-1)\n",
                "        \n",
                "        # Top-K experts\n",
                "        top_probs, top_indices = torch.topk(probs, self.top_k, dim=-1)\n",
                "        # Normalize probs\n",
                "        top_probs = top_probs / top_probs.sum(dim=-1, keepdim=True)\n",
                "        \n",
                "        out = torch.zeros_like(x_flat)\n",
                "        \n",
                "        # Route tokens to experts\n",
                "        for k in range(self.top_k):\n",
                "            expert_idx = top_indices[:, k]\n",
                "            prob = top_probs[:, k].unsqueeze(-1)\n",
                "            \n",
                "            for i in range(self.num_experts):\n",
                "                mask = (expert_idx == i)\n",
                "                if mask.sum() == 0:\n",
                "                    continue\n",
                "                \n",
                "                expert_input = x_flat[mask]\n",
                "                expert_output = self.experts[i](expert_input)\n",
                "                \n",
                "                # Add weighted expert output\n",
                "                out[mask] += prob[mask] * expert_output\n",
                "                \n",
                "        return out.view(batch, seq_len, dim)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test MoE\n",
                "moe = MoELayer(dim=128, num_experts=8, top_k=2)\n",
                "x = torch.randn(4, 10, 128)\n",
                "output = moe(x)\n",
                "print(f\"MoE Input: {x.shape}\")\n",
                "print(f\"MoE Output: {output.shape}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}