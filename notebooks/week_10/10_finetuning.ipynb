{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 10: Low-Rank Adaptation (LoRA)\n",
                "\n",
                "Implementing LoRA from scratch to fine-tune large models efficiently.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Understand reparameterization\n",
                "2. Implement a LoRA Linear Layer\n",
                "3. Fine-tune a small BERT-like model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import math"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. LoRA Layer Implementation\n",
                "\n",
                "Key concept: W = W_0 + BA\n",
                "Where B is (d_out, r) and A is (r, d_in), r << d."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LoRALinear(nn.Module):\n",
                "    def __init__(self, in_features, out_features, rank=4, alpha=16):\n",
                "        super().__init__()\n",
                "        # Frozen pretrained weights\n",
                "        self.linear = nn.Linear(in_features, out_features)\n",
                "        self.linear.weight.requires_grad = False\n",
                "        self.linear.bias.requires_grad = False\n",
                "        \n",
                "        # LoRA weights\n",
                "        self.lora_rank = rank\n",
                "        self.lora_alpha = alpha\n",
                "        self.scaling = alpha / rank\n",
                "        \n",
                "        # A: Gaussian init, B: Zero init\n",
                "        self.lora_A = nn.Parameter(torch.randn(rank, in_features) * (1/math.sqrt(rank)))\n",
                "        self.lora_B = nn.Parameter(torch.zeros(out_features, rank))\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # Original path\n",
                "        h_frozen = self.linear(x)\n",
                "        \n",
                "        # LoRA path: (x @ A.T) @ B.T * scaling\n",
                "        h_lora = (x @ self.lora_A.T) @ self.lora_B.T * self.scaling\n",
                "        \n",
                "        return h_frozen + h_lora"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test LoRA\n",
                "in_dim = 768\n",
                "out_dim = 768\n",
                "rank = 8\n",
                "\n",
                "layer = LoRALinear(in_dim, out_dim, rank=rank)\n",
                "x = torch.randn(1, 10, in_dim)\n",
                "\n",
                "output = layer(x)\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Trainable parameters: {sum(p.numel() for p in layer.parameters() if p.requires_grad)}\")\n",
                "print(f\"Total parameters: {sum(p.numel() for p in layer.parameters())}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Replacing Layers in a Model\n",
                "\n",
                "Function to recursively replace Linear layers with LoRALinear."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def apply_lora(model, rank=4, alpha=16):\n",
                "    \"\"\"Replace all linear layers with LoRA layers.\"\"\"\n",
                "    for name, module in model.named_children():\n",
                "        if isinstance(module, nn.Linear):\n",
                "            # Create LoRA layer\n",
                "            lora_layer = LoRALinear(\n",
                "                module.in_features, \n",
                "                module.out_features, \n",
                "                rank=rank, \n",
                "                alpha=alpha\n",
                "            )\n",
                "            # Copy weights to frozen linear\n",
                "            lora_layer.linear.weight.data = module.weight.data.clone()\n",
                "            if module.bias is not None:\n",
                "                lora_layer.linear.bias.data = module.bias.data.clone()\n",
                "            \n",
                "            # Replace\n",
                "            setattr(model, name, lora_layer)\n",
                "        else:\n",
                "            # Recurse\n",
                "            apply_lora(module, rank, alpha)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example Model\n",
                "class SimpleMLP(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(128, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(256, 10)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "model = SimpleMLP()\n",
                "print(\"Before LoRA:\", model)\n",
                "\n",
                "apply_lora(model, rank=8)\n",
                "print(\"\\nAfter LoRA:\", model)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}