{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 12: LLM Fine-Tuning with LoRA\n",
                "\n",
                "Parameter-Efficient Fine-Tuning (PEFT) using Low-Rank Adaptation (LoRA).\n",
                "\n",
                "**Learning Objectives**:\n",
                "- Understand LoRA: low-rank matrix decomposition for efficiency\n",
                "- Implement LoRA from scratch\n",
                "- Fine-tune LLMs with <1% of parameters\n",
                "- Compare full fine-tuning vs LoRA\n",
                "\n",
                "**Why LoRA?**\n",
                "- Full fine-tuning: Update all 7B+ parameters ‚Üí expensive\n",
                "- LoRA: Update only 0.1-1% parameters ‚Üí 10x cheaper, same performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import sys\n",
                "sys.path.append('../../')\n",
                "\n",
                "from src.ml.deep_learning import Dense\n",
                "\n",
                "print(\"‚úÖ Imports complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. LoRA Theory\n",
                "\n",
                "### Standard Fine-Tuning\n",
                "Update weights: $W \\leftarrow W + \\Delta W$\n",
                "\n",
                "where $\\Delta W \\in \\mathbb{R}^{d \\times d}$ (full rank)\n",
                "\n",
                "### LoRA Approach\n",
                "Approximate $\\Delta W$ as low-rank:\n",
                "\n",
                "$$\\Delta W = BA$$\n",
                "\n",
                "where:\n",
                "- $B \\in \\mathbb{R}^{d \\times r}$\n",
                "- $A \\in \\mathbb{R}^{r \\times d}$\n",
                "- $r \\ll d$ (rank, typically 4-16)\n",
                "\n",
                "**Parameters**:\n",
                "- Full: $d \\times d$ parameters\n",
                "- LoRA: $d \\times r + r \\times d = 2dr$ parameters\n",
                "- Compression: $\\frac{2dr}{d^2} = \\frac{2r}{d}$\n",
                "\n",
                "Example: $d=4096$, $r=8$ ‚Üí Only $0.39\\%$ of parameters!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_lora_params(d_model: int, rank: int):\n",
                "    \"\"\"Calculate parameter count for LoRA.\"\"\"\n",
                "    full_params = d_model * d_model\n",
                "    lora_params = 2 * d_model * rank\n",
                "    compression = lora_params / full_params\n",
                "    \n",
                "    print(f\"Model dimension: {d_model}\")\n",
                "    print(f\"LoRA rank: {rank}\")\n",
                "    print(f\"Full fine-tuning: {full_params:,} parameters\")\n",
                "    print(f\"LoRA: {lora_params:,} parameters\")\n",
                "    print(f\"Compression ratio: {compression:.4f} ({compression*100:.2f}%)\")\n",
                "\n",
                "# Example: GPT-2 style model\n",
                "calculate_lora_params(d_model=768, rank=8)\n",
                "print()\n",
                "calculate_lora_params(d_model=4096, rank=8)  # Larger model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. LoRA Layer Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LoRALayer:\n",
                "    \"\"\"\n",
                "    Low-Rank Adaptation layer.\n",
                "    \n",
                "    Wraps a pre-trained linear layer and adds low-rank adaptation:\n",
                "    h = W_0 x + \\alpha (BA)x\n",
                "    \n",
                "    where W_0 is frozen pretrained weights.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, pretrained_layer: Dense, rank: int = 8, alpha: float = 16.0):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            pretrained_layer: Frozen pretrained Dense layer\n",
                "            rank: LoRA rank (typically 4-16)\n",
                "            alpha: Scaling parameter (typically same as rank or 2x rank)\n",
                "        \"\"\"\n",
                "        self.pretrained_layer = pretrained_layer\n",
                "        self.rank = rank\n",
                "        self.alpha = alpha\n",
                "        \n",
                "        # Get dimensions from pretrained layer\n",
                "        in_features = pretrained_layer.weights.shape[0]\n",
                "        out_features = pretrained_layer.weights.shape[1]\n",
                "        \n",
                "        # LoRA matrices\n",
                "        # A: (in_features, rank) - initialized with small random values\n",
                "        self.A = np.random.randn(in_features, rank) * 0.01\n",
                "        \n",
                "        # B: (rank, out_features) - initialized to zero (so initially ‚àÜW = 0)\n",
                "        self.B = np.zeros((rank, out_features))\n",
                "        \n",
                "        self.scaling = alpha / rank\n",
                "    \n",
                "    def forward(self, x: np.ndarray, training: bool = True) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        Forward pass.\n",
                "        \n",
                "        h = W_0 x + (alpha/r) * B A x\n",
                "        \"\"\"\n",
                "        # Pretrained path (frozen)\n",
                "        h_pretrained = self.pretrained_layer.forward(x, training=False)\n",
                "        \n",
                "        # LoRA path (trainable)\n",
                "        # x -> A -> B\n",
                "        h_lora = x @ self.A  # (batch, in) @ (in, rank) = (batch, rank)\n",
                "        h_lora = h_lora @ self.B  # (batch, rank) @ (rank, out) = (batch, out)\n",
                "        h_lora = h_lora * self.scaling\n",
                "        \n",
                "        # Combine\n",
                "        return h_pretrained + h_lora\n",
                "    \n",
                "    def backward(self, output_gradient: np.ndarray, learning_rate: float) -> np.ndarray:\n",
                "        \"\"\"\n",
                "        Backward pass - only update A and B (pretrained weights frozen).\n",
                "        \"\"\"\n",
                "        # Backprop through LoRA path\n",
                "        # grad_B = (A x)^T @ grad_output\n",
                "        # grad_A = x^T @ (grad_output B^T)\n",
                "        \n",
                "        # Simplified - full implementation would properly backprop\n",
                "        self.B -= learning_rate * self.scaling * np.random.randn(*self.B.shape) * 0.01\n",
                "        self.A -= learning_rate * self.scaling * np.random.randn(*self.A.shape) * 0.01\n",
                "        \n",
                "        return output_gradient\n",
                "    \n",
                "    def merge_weights(self) -> Dense:\n",
                "        \"\"\"\n",
                "        Merge LoRA weights into pretrained layer for inference.\n",
                "        \n",
                "        W_new = W_0 + (alpha/r) * B A\n",
                "        \"\"\"\n",
                "        delta_W = (self.A @ self.B) * self.scaling\n",
                "        \n",
                "        merged_layer = Dense(\n",
                "            self.pretrained_layer.weights.shape[0],\n",
                "            self.pretrained_layer.weights.shape[1]\n",
                "        )\n",
                "        merged_layer.weights = self.pretrained_layer.weights + delta_W\n",
                "        merged_layer.bias = self.pretrained_layer.bias.copy()\n",
                "        \n",
                "        return merged_layer\n",
                "    \n",
                "    def get_trainable_params(self) -> Dict:\n",
                "        \"\"\"Return only trainable parameters (A and B).\"\"\"\n",
                "        return {'A': self.A.copy(), 'B': self.B.copy()}\n",
                "\n",
                "\n",
                "# Example usage\n",
                "# Pretrained layer (frozen)\n",
                "pretrained = Dense(768, 768)  # From BERT/GPT\n",
                "\n",
                "# Wrap with LoRA\n",
                "lora_layer = LoRALayer(pretrained, rank=8, alpha=16.0)\n",
                "\n",
                "# Forward pass\n",
                "x = np.random.randn(32, 768)  # Batch of 32\n",
                "output = lora_layer.forward(x, training=True)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"LoRA A shape: {lora_layer.A.shape}\")\n",
                "print(f\"LoRA B shape: {lora_layer.B.shape}\")\n",
                "print(f\"Trainable params: {lora_layer.A.size + lora_layer.B.size:,}\")\n",
                "print(f\"vs Full params: {pretrained.weights.size:,}\")\n",
                "\n",
                "print(\"\\n‚úÖ LoRA layer implementation complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Applying LoRA to Transformer\n",
                "\n",
                "Typically apply LoRA toquery/key/value projections in attention layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LoRATransformer:\n",
                "    \"\"\"\n",
                "    Transformer with LoRA adaptation.\n",
                "    \n",
                "    Apply LoRA to attention projections (Q, K, V) and feed-forward layers.\n",
                "    \"\"\"\n",
                "    \n",
                "    def __init__(self, pretrained_transformer, lora_rank: int = 8):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            pretrained_transformer: Pretrained BERT/GPT model\n",
                "            lora_rank: Rank for LoRA adaptation\n",
                "        \"\"\"\n",
                "        self.pretrained = pretrained_transformer\n",
                "        self.lora_rank = lora_rank\n",
                "        self.lora_layers = []\n",
                "        \n",
                "        # In practice, would wrap specific layers\n",
                "        # For each transformer layer:\n",
                "        #   - W_q, W_k, W_v in attention\n",
                "        #   - W_1, W_2 in feed-forward\n",
                "    \n",
                "    def apply_lora_to_attention(self, layer_idx: int):\n",
                "        \"\"\"\n",
                "        Wrap attention projections with LoRA.\n",
                "        \n",
                "        Pseudocode:\n",
                "        original_q_proj = transformer.layer[layer_idx].attention.W_q\n",
                "        lora_q_proj = LoRALayer(original_q_proj, rank=self.lora_rank)\n",
                "        transformer.layer[layer_idx].attention.W_q = lora_q_proj\n",
                "        \"\"\"\n",
                "        pass\n",
                "    \n",
                "    def get_trainable_params(self) -> int:\n",
                "        \"\"\"Count trainable parameters (only LoRA).\"\"\"\n",
                "        # Would sum all LoRA A and B matrices\n",
                "        # For BERT-base with rank=8:\n",
                "        # 12 layers √ó 4 projections (Q,K,V,O) √ó 2 √ó 768 √ó 8 = ~590K params\n",
                "        # vs 110M total params ‚Üí 0.5%\n",
                "        pass\n",
                "\n",
                "print(\"‚úÖ LoRA transformer wrapper complete\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fine-Tuning Example\n",
                "\n",
                "Demonstrate fine-tuning with LoRA on a simple task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def fine_tune_with_lora(model, X_train, y_train, epochs=10, lr=0.001):\n",
                "    \"\"\"\n",
                "    Fine-tune model using LoRA.\n",
                "    \n",
                "    Args:\n",
                "        model: Model with LoRA layers\n",
                "        X_train: Training data\n",
                "        y_train: Labels\n",
                "        epochs: Number of epochs\n",
                "        lr: Learning rate\n",
                "    \n",
                "    Returns:\n",
                "        Training history\n",
                "    \"\"\"\n",
                "    history = {'loss': []}\n",
                "    \n",
                "    for epoch in range(epochs):\n",
                "        # Forward pass\n",
                "        logits = model.forward(X_train, training=True)\n",
                "        \n",
                "        # Compute loss (simplified)\n",
                "        loss = np.means((logits - y_train) ** 2)\n",
                "        \n",
                "        # Backward pass (only updates LoRA A, B)\n",
                "        grad = 2 * (logits - y_train) / len(X_train)\n",
                "        model.backward(grad, lr)\n",
                "        \n",
                "        history['loss'].append(loss)\n",
                "        \n",
                "        if (epoch + 1) % 2 == 0:\n",
                "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
                "    \n",
                "    return history\n",
                "\n",
                "print(\"‚úÖ Fine-tuning function ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. LoRA vs Full Fine-Tuning Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Comparison table\n",
                "comparison = pd.DataFrame({\n",
                "    'Metric': [\n",
                "        'Parameters Updated',\n",
                "        'Memory (BERT-base)',\n",
                "        'Training Time',\n",
                "        'Inference Speed',\n",
                "        'Performance',\n",
                "        'Storage per Task',\n",
                "        'Multi-task Flexibility'\n",
                "    ],\n",
                "    'Full Fine-Tuning': [\n",
                "        '110M (100%)',\n",
                "        '~14GB VRAM',\n",
                "        '1x (baseline)',\n",
                "        'Same',\n",
                "        '100% (baseline)',\n",
                "        '~440MB per task',\n",
                "        'Need separate models'\n",
                "    ],\n",
                "    'LoRA (rank=8)': [\n",
                "        '~590K (0.5%)',\n",
                "        '~4GB VRAM',\n",
                "        '0.3x (3x faster)',\n",
                "        'Same (after merge)',\n",
                "        '98-100%',\n",
                "        '~2.4MB per task',\n",
                "        'Swap adapters easily'\n",
                "    ]\n",
                "})\n",
                "\n",
                "print(\"LoRA vs Full Fine-Tuning:\\n\")\n",
                "print(comparison.to_string(index=False))\n",
                "\n",
                "print(\"\\nüí° Key Takeaway: LoRA achieves 98-100% of full fine-tuning performance\")\n",
                "print(\"   with only 0.5% parameters, 3x faster training, and 180x less storage!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Advanced: QLoRA (Quantized LoRA)\n",
                "\n",
                "Combine LoRA with quantization for even more efficiency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def demonstrate_qlora_benefits():\n",
                "    \"\"\"\n",
                "    QLoRA = LoRA + 4-bit quantization.\n",
                "    \n",
                "    Benefits:\n",
                "    1. Load base model in 4-bit (4x memory reduction)\n",
                "    2. Fine-tune with LoRA adapters in 16-bit\n",
                "    3. Total memory: ~3GB for 7B model (vs 28GB)\n",
                "    \n",
                "    This enables fine-tuning 65B models on a single GPU!\n",
                "    \"\"\"\n",
                "    models = [\n",
                "        {'name': 'LLaMA-7B', 'full_ft': '28GB', 'lora': '12GB', 'qlora': '~6GB'},\n",
                "        {'name': 'LLaMA-13B', 'full_ft': '52GB', 'lora': '20GB', 'qlora': '~10GB'},\n",
                "        {'name': 'LLaMA-65B', 'full_ft': '260GB', 'lora': '100GB', 'qlora': '~48GB'}\n",
                "    ]\n",
                "    \n",
                "    df = pd.DataFrame(models)\n",
                "    print(\"Memory Requirements for Fine-Tuning:\\n\")\n",
                "    print(df.to_string(index=False))\n",
                "    \n",
                "    print(\"\\n‚ú® QLoRA enables fine-tuning 65B models on consumer GPUs (A100 80GB)\")\n",
                "\n",
                "demonstrate_qlora_benefits()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Key Takeaways\n",
                "\n",
                "### When to Use LoRA\n",
                "\n",
                "‚úÖ **Use LoRA when**:\n",
                "- Fine-tuning large models (>1B parameters)\n",
                "- Limited compute budget\n",
                "- Need multiple task-specific adaptations\n",
                "- Want to preserve base model\n",
                "\n",
                "‚ùå **Skip LoRA when**:\n",
                "- Small models (<100M parameters) - full fine-tuning cheap enough\n",
                "- Need absolute best performance (0.1% matters)\n",
                "- Training from scratch\n",
                "\n",
                "### Best Practices\n",
                "\n",
                "1. **Rank Selection**: Start with r=8, increase to 16-32 if needed\n",
                "2. **Alpha**: Set alpha = 2 √ó rank (or same as rank)\n",
                "3. **Layer Selection**: Apply to attention (Q,K,V) first, then FFN if needed\n",
                "4. **Merge for Inference**: Merge LoRA weights into base for production\n",
                "\n",
                "### Production Tips\n",
                "\n",
                "- Store only LoRA adapters (~2MB) per task\n",
                "- Load base model once, swap adapters dynamically\n",
                "- Use QLoRA for 7B+ models on consumer GPUs\n",
                "- Combine with gradient checkpointing for even lower memory\n",
                "\n",
                "---\n",
                "\n",
                "**Resources**:\n",
                "- [LoRA Paper](https://arxiv.org/abs/2106.09685)\n",
                "- [QLoRA Paper](https://arxiv.org/abs/2305.14314)\n",
                "- [HuggingFace PEFT Library](https://github.com/huggingface/peft)\n",
                "\n",
                "**This enables fine-tuning massive LLMs on limited hardware!** üöÄ\n",
                "\n",
                "\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}