{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßÆ Deep Mathematical Foundations of Machine Learning\n",
    "\n",
    "## From Theory to Industrial Applications\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** AI-Mastery-2026 Project  \n",
    "**Purpose:** A comprehensive, white-box exploration of the mathematical pillars of Machine Learning.\n",
    "\n",
    "### üéØ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "\n",
    "1. **Linear Algebra**: Understand vectors, matrices, eigenvalues, SVD, and their applications (PageRank, Netflix Recommendations)\n",
    "2. **Calculus**: Master gradients, Jacobians, backpropagation, and the Transformer's self-attention mechanism\n",
    "3. **Optimization**: Apply Lagrange multipliers, understand SVM's dual form, and learn ADMM for industrial scale\n",
    "4. **Probability & Information Theory**: Grasp entropy, KL divergence, VAEs, and t-SNE\n",
    "5. **Advanced Methods**: Implement Monte Carlo integration, importance sampling, and normalizing flows\n",
    "6. **Network Analysis**: Build link prediction models (Facebook's supervised random walks)\n",
    "7. **Bayesian Optimization**: Use Gaussian Processes and acquisition functions\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Prerequisites\n",
    "\n",
    "- Basic Python programming\n",
    "- Familiarity with NumPy arrays\n",
    "- High school algebra and calculus concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats, linalg\n",
    "from typing import Tuple, List, Callable\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "np.random.seed(42)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"‚úÖ Setup complete! NumPy version:\", np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 1: Linear Algebra - The Language of Data\n",
    "\n",
    "---\n",
    "\n",
    "Linear algebra provides the formal framework for representing and manipulating data. In ML, we rarely work with single numbers (scalars); instead, we work with **vectors** (points in space), **matrices** (transformations), and **tensors** (higher-dimensional arrays).\n",
    "\n",
    "> **Key Insight**: A matrix is not just a table of numbers‚Äîit is a *function* that transforms one vector space into another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Vector Spaces and the Dot Product\n",
    "\n",
    "### Theory\n",
    "\n",
    "In ML, a **vector** $\\mathbf{x} \\in \\mathbb{R}^n$ represents an entity (an image, a user, a word). The **dot product** measures the relationship between two vectors:\n",
    "\n",
    "$$\\mathbf{x} \\cdot \\mathbf{y} = \\mathbf{x}^T \\mathbf{y} = \\sum_{i=1}^n x_i y_i = \\|\\mathbf{x}\\| \\|\\mathbf{y}\\| \\cos(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $\\|\\mathbf{x}\\| = \\sqrt{\\sum x_i^2}$ is the L2 norm (vector length)\n",
    "- $\\theta$ is the angle between the vectors\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Given:** $\\mathbf{x} = [1, 2, 3]$, $\\mathbf{y} = [4, 5, 6]$\n",
    "\n",
    "**Compute the dot product:**\n",
    "\n",
    "$$\\mathbf{x} \\cdot \\mathbf{y} = (1 \\times 4) + (2 \\times 5) + (3 \\times 6) = 4 + 10 + 18 = 32$$\n",
    "\n",
    "**Compute the norms:**\n",
    "\n",
    "$$\\|\\mathbf{x}\\| = \\sqrt{1^2 + 2^2 + 3^2} = \\sqrt{14} \\approx 3.742$$\n",
    "\n",
    "$$\\|\\mathbf{y}\\| = \\sqrt{4^2 + 5^2 + 6^2} = \\sqrt{77} \\approx 8.775$$\n",
    "\n",
    "**Compute the angle:**\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{32}{3.742 \\times 8.775} = \\frac{32}{32.83} \\approx 0.9746$$\n",
    "\n",
    "$$\\theta = \\arccos(0.9746) \\approx 12.93¬∞$$\n",
    "\n",
    "The vectors are nearly aligned (small angle)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.1 DOT PRODUCT IMPLEMENTATION\n",
    "# ============================================\n",
    "\n",
    "def dot_product_from_scratch(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute dot product manually.\n",
    "    \n",
    "    Args:\n",
    "        x: First vector of shape (n,)\n",
    "        y: Second vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        Scalar dot product value\n",
    "    \"\"\"\n",
    "    assert len(x) == len(y), \"Vectors must have same dimension\"\n",
    "    return sum(xi * yi for xi, yi in zip(x, y))\n",
    "\n",
    "\n",
    "def compute_angle_between_vectors(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute angle (in degrees) between two vectors.\n",
    "    \"\"\"\n",
    "    dot = np.dot(x, y)\n",
    "    norm_x = np.linalg.norm(x)\n",
    "    norm_y = np.linalg.norm(y)\n",
    "    cos_theta = dot / (norm_x * norm_y)\n",
    "    # Clamp to [-1, 1] to handle numerical errors\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "    return np.degrees(np.arccos(cos_theta))\n",
    "\n",
    "\n",
    "# Example from the mathematical derivation\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5, 6])\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"DOT PRODUCT EXAMPLE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"\\nManual dot product: {dot_product_from_scratch(x, y)}\")\n",
    "print(f\"NumPy dot product:  {np.dot(x, y)}\")\n",
    "print(f\"\\n||x|| = {np.linalg.norm(x):.4f}\")\n",
    "print(f\"||y|| = {np.linalg.norm(y):.4f}\")\n",
    "print(f\"\\nAngle between vectors: {compute_angle_between_vectors(x, y):.2f}¬∞\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Cosine Similarity in NLP\n",
    "\n",
    "### Theory\n",
    "\n",
    "When vectors are **normalized** (unit length), their dot product equals the **cosine similarity**:\n",
    "\n",
    "$$\\text{sim}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}$$\n",
    "\n",
    "**Interpretation**:\n",
    "- $\\text{sim} = 1$: Vectors point in the same direction (identical meaning)\n",
    "- $\\text{sim} = 0$: Vectors are perpendicular (unrelated)\n",
    "- $\\text{sim} = -1$: Vectors point in opposite directions (opposite meaning)\n",
    "\n",
    "### üè≠ Industrial Application: Word Embeddings\n",
    "\n",
    "In NLP models like Word2Vec or BERT, words are converted to vectors (embeddings). Semantically similar words have high cosine similarity.\n",
    "\n",
    "**Famous example**: $\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}$\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Given embeddings:**\n",
    "- \"cat\" ‚Üí $[0.7, 0.5, 0.1]$\n",
    "- \"dog\" ‚Üí $[0.6, 0.6, 0.2]$\n",
    "- \"car\" ‚Üí $[0.1, 0.2, 0.9]$\n",
    "\n",
    "**Compute similarity between \"cat\" and \"dog\":**\n",
    "\n",
    "$$\\text{sim}(\\text{cat}, \\text{dog}) = \\frac{0.7 \\times 0.6 + 0.5 \\times 0.6 + 0.1 \\times 0.2}{\\sqrt{0.75} \\times \\sqrt{0.76}} = \\frac{0.74}{0.755} \\approx 0.98$$\n",
    "\n",
    "High similarity! Both are animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.2 COSINE SIMILARITY IMPLEMENTATION\n",
    "# ============================================\n",
    "\n",
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \n",
    "    Args:\n",
    "        a, b: Input vectors\n",
    "    \n",
    "    Returns:\n",
    "        Similarity score in [-1, 1]\n",
    "    \"\"\"\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "# Simulated word embeddings\n",
    "embeddings = {\n",
    "    \"cat\": np.array([0.7, 0.5, 0.1]),\n",
    "    \"dog\": np.array([0.6, 0.6, 0.2]),\n",
    "    \"kitten\": np.array([0.65, 0.55, 0.12]),\n",
    "    \"car\": np.array([0.1, 0.2, 0.9]),\n",
    "    \"truck\": np.array([0.15, 0.18, 0.85]),\n",
    "}\n",
    "\n",
    "# Compute similarity matrix\n",
    "words = list(embeddings.keys())\n",
    "n = len(words)\n",
    "sim_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, w1 in enumerate(words):\n",
    "    for j, w2 in enumerate(words):\n",
    "        sim_matrix[i, j] = cosine_similarity(embeddings[w1], embeddings[w2])\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sim_matrix, annot=True, fmt='.3f', \n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cmap='RdYlGn', center=0.5, vmin=0, vmax=1)\n",
    "plt.title('Word Embedding Cosine Similarity Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Insight: 'cat' and 'kitten' are most similar (both felines).\")\n",
    "print(\"   'cat' and 'car' are least similar (animal vs vehicle).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Matrices as Linear Transformations\n",
    "\n",
    "### Theory\n",
    "\n",
    "A matrix $A \\in \\mathbb{R}^{m \\times n}$ can be viewed as a **function** that transforms an $n$-dimensional vector space to an $m$-dimensional space:\n",
    "\n",
    "$$\\mathbf{y} = A\\mathbf{x}$$\n",
    "\n",
    "Common transformations:\n",
    "- **Scaling**: Stretch or shrink vectors\n",
    "- **Rotation**: Rotate vectors around origin\n",
    "- **Projection**: Project onto lower-dimensional subspace\n",
    "- **Shearing**: Skew the space\n",
    "\n",
    "### üìê Mathematical Example: 2D Rotation\n",
    "\n",
    "The rotation matrix for angle $\\theta$ is:\n",
    "\n",
    "$$R_\\theta = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}$$\n",
    "\n",
    "**Rotate $\\mathbf{v} = [1, 0]$ by 45¬∞:**\n",
    "\n",
    "$$R_{45¬∞} = \\begin{bmatrix} 0.707 & -0.707 \\\\ 0.707 & 0.707 \\end{bmatrix}$$\n",
    "\n",
    "$$\\mathbf{v}' = R_{45¬∞} \\mathbf{v} = \\begin{bmatrix} 0.707 \\\\ 0.707 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.3 LINEAR TRANSFORMATIONS VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "def rotation_matrix(theta_deg: float) -> np.ndarray:\n",
    "    \"\"\"Create 2D rotation matrix for given angle in degrees.\"\"\"\n",
    "    theta = np.radians(theta_deg)\n",
    "    return np.array([\n",
    "        [np.cos(theta), -np.sin(theta)],\n",
    "        [np.sin(theta),  np.cos(theta)]\n",
    "    ])\n",
    "\n",
    "\n",
    "def scaling_matrix(sx: float, sy: float) -> np.ndarray:\n",
    "    \"\"\"Create 2D scaling matrix.\"\"\"\n",
    "    return np.array([[sx, 0], [0, sy]])\n",
    "\n",
    "\n",
    "def shear_matrix(k: float) -> np.ndarray:\n",
    "    \"\"\"Create 2D shear matrix.\"\"\"\n",
    "    return np.array([[1, k], [0, 1]])\n",
    "\n",
    "\n",
    "# Create a unit square\n",
    "square = np.array([\n",
    "    [0, 1, 1, 0, 0],  # x coordinates\n",
    "    [0, 0, 1, 1, 0]   # y coordinates\n",
    "])\n",
    "\n",
    "# Define transformations\n",
    "transformations = [\n",
    "    (\"Original\", np.eye(2)),\n",
    "    (\"Rotation 45¬∞\", rotation_matrix(45)),\n",
    "    (\"Scale (2x, 0.5x)\", scaling_matrix(2, 0.5)),\n",
    "    (\"Shear (k=0.5)\", shear_matrix(0.5)),\n",
    "]\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for ax, (name, T) in zip(axes, transformations):\n",
    "    transformed = T @ square\n",
    "    ax.fill(transformed[0], transformed[1], alpha=0.4, color='steelblue')\n",
    "    ax.plot(transformed[0], transformed[1], 'b-', linewidth=2)\n",
    "    ax.set_xlim(-1, 3)\n",
    "    ax.set_ylim(-1, 2)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(name, fontsize=12, fontweight='bold')\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Matrix Transformations on a Unit Square', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîë Key Insight: Neural network layers are sequences of linear transformations\")\n",
    "print(\"   followed by non-linear activation functions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Convolution as Matrix Multiplication (Toeplitz Matrices)\n",
    "\n",
    "### Theory\n",
    "\n",
    "In **Convolutional Neural Networks (CNNs)**, the convolution operation can be reformulated as matrix multiplication using **Toeplitz matrices**. This allows:\n",
    "\n",
    "1. Efficient GPU computation (GPUs are optimized for matrix operations)\n",
    "2. Unified backpropagation with standard linear algebra\n",
    "\n",
    "### The Process (im2col)\n",
    "\n",
    "1. Flatten the input image patches into column vectors\n",
    "2. Arrange them as columns of a matrix\n",
    "3. Multiply by the flattened kernel\n",
    "\n",
    "### üìê Mathematical Example: 1D Convolution\n",
    "\n",
    "**Input**: $x = [1, 2, 3, 4, 5]$  \n",
    "**Kernel**: $k = [1, 0, -1]$ (edge detector)\n",
    "\n",
    "The Toeplitz matrix representation:\n",
    "\n",
    "$$T = \\begin{bmatrix} 3 & 2 & 1 \\\\ 4 & 3 & 2 \\\\ 5 & 4 & 3 \\end{bmatrix}$$\n",
    "\n",
    "$$y = T \\cdot k = \\begin{bmatrix} 3-1 \\\\ 4-2 \\\\ 5-3 \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.4 CONVOLUTION AS MATRIX MULTIPLICATION\n",
    "# ============================================\n",
    "\n",
    "def conv1d_standard(x: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Standard 1D convolution (valid mode).\"\"\"\n",
    "    k_size = len(kernel)\n",
    "    out_size = len(x) - k_size + 1\n",
    "    result = np.zeros(out_size)\n",
    "    for i in range(out_size):\n",
    "        result[i] = np.sum(x[i:i+k_size] * kernel)\n",
    "    return result\n",
    "\n",
    "\n",
    "def conv1d_as_matmul(x: np.ndarray, kernel: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    1D convolution implemented as matrix multiplication.\n",
    "    Demonstrates the Toeplitz matrix approach.\n",
    "    \"\"\"\n",
    "    k_size = len(kernel)\n",
    "    out_size = len(x) - k_size + 1\n",
    "    \n",
    "    # Build the Toeplitz-like matrix (im2col)\n",
    "    # Each row contains a patch of the input\n",
    "    patches = np.zeros((out_size, k_size))\n",
    "    for i in range(out_size):\n",
    "        patches[i] = x[i:i+k_size]\n",
    "    \n",
    "    # Matrix multiplication: patches @ kernel\n",
    "    return patches @ kernel\n",
    "\n",
    "\n",
    "# Example\n",
    "x = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "kernel = np.array([1, 0, -1], dtype=float)  # Edge detector\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"CONVOLUTION AS MATRIX MULTIPLICATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Input x = {x}\")\n",
    "print(f\"Kernel  = {kernel}\")\n",
    "\n",
    "# Standard convolution\n",
    "y_standard = conv1d_standard(x, kernel)\n",
    "print(f\"\\nStandard convolution: {y_standard}\")\n",
    "\n",
    "# Matrix multiplication approach\n",
    "y_matmul = conv1d_as_matmul(x, kernel)\n",
    "print(f\"Matrix multiplication: {y_matmul}\")\n",
    "\n",
    "# Show the patch matrix\n",
    "out_size = len(x) - len(kernel) + 1\n",
    "patches = np.zeros((out_size, len(kernel)))\n",
    "for i in range(out_size):\n",
    "    patches[i] = x[i:i+len(kernel)]\n",
    "\n",
    "print(f\"\\nPatch matrix (im2col):\")\n",
    "print(patches)\n",
    "print(f\"\\nComputation: patches @ kernel = result\")\n",
    "\n",
    "# Verify they match\n",
    "assert np.allclose(y_standard, y_matmul), \"Results should match!\"\n",
    "print(\"\\n‚úÖ Both methods produce identical results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Eigenvalues and Google's PageRank\n",
    "\n",
    "### Theory\n",
    "\n",
    "An **eigenvector** $\\mathbf{v}$ of matrix $A$ is a vector that only gets scaled (not rotated) when $A$ is applied:\n",
    "\n",
    "$$A\\mathbf{v} = \\lambda \\mathbf{v}$$\n",
    "\n",
    "Where $\\lambda$ is the **eigenvalue** (the scaling factor).\n",
    "\n",
    "### üè≠ Industrial Application: Google PageRank\n",
    "\n",
    "The web is modeled as a directed graph. PageRank finds the **stationary distribution** of a random surfer‚Äîthis is the eigenvector corresponding to eigenvalue $\\lambda = 1$ of the transition matrix.\n",
    "\n",
    "**Modified PageRank equation** (with damping $d = 0.85$):\n",
    "\n",
    "$$\\mathbf{R} = d \\cdot M \\cdot \\mathbf{R} + \\frac{1-d}{N} \\mathbf{1}$$\n",
    "\n",
    "### üìê Mathematical Example: 4-Page Web\n",
    "\n",
    "Consider a web with 4 pages:\n",
    "- Page 1 ‚Üí links to 2, 3\n",
    "- Page 2 ‚Üí links to 1, 3\n",
    "- Page 3 ‚Üí links to 1\n",
    "- Page 4 ‚Üí links to 1, 2, 3\n",
    "\n",
    "Transition matrix $M$ (column-stochastic):\n",
    "\n",
    "$$M = \\begin{bmatrix} 0 & 0.5 & 1 & 0.33 \\\\ 0.5 & 0 & 0 & 0.33 \\\\ 0.5 & 0.5 & 0 & 0.33 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.5 PAGERANK FROM SCRATCH\n",
    "# ============================================\n",
    "\n",
    "def pagerank(adj_matrix: np.ndarray, damping: float = 0.85, \n",
    "             max_iter: int = 100, tol: float = 1e-6) -> Tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"\n",
    "    Compute PageRank using power iteration.\n",
    "    \n",
    "    Args:\n",
    "        adj_matrix: Adjacency matrix (adj[i,j]=1 if i links to j)\n",
    "        damping: Damping factor (probability of following a link)\n",
    "        max_iter: Maximum iterations\n",
    "        tol: Convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        ranks: PageRank scores\n",
    "        history: Convergence history\n",
    "    \"\"\"\n",
    "    n = adj_matrix.shape[0]\n",
    "    \n",
    "    # Create column-stochastic transition matrix\n",
    "    out_degree = adj_matrix.sum(axis=1)\n",
    "    out_degree[out_degree == 0] = 1  # Handle dangling nodes\n",
    "    M = (adj_matrix.T / out_degree).T\n",
    "    M = M.T  # Column stochastic\n",
    "    \n",
    "    # Initialize uniform distribution\n",
    "    r = np.ones(n) / n\n",
    "    history = []\n",
    "    \n",
    "    # Power iteration\n",
    "    for _ in range(max_iter):\n",
    "        r_new = damping * M @ r + (1 - damping) / n\n",
    "        diff = np.linalg.norm(r_new - r)\n",
    "        history.append(diff)\n",
    "        \n",
    "        if diff < tol:\n",
    "            break\n",
    "        r = r_new\n",
    "    \n",
    "    return r_new / r_new.sum(), history  # Normalize\n",
    "\n",
    "\n",
    "# Create adjacency matrix for our 4-page example\n",
    "# adj[i,j] = 1 means page i links to page j\n",
    "adj = np.array([\n",
    "    [0, 1, 1, 0],  # Page 0 ‚Üí 1, 2\n",
    "    [1, 0, 1, 0],  # Page 1 ‚Üí 0, 2\n",
    "    [1, 0, 0, 0],  # Page 2 ‚Üí 0\n",
    "    [1, 1, 1, 0],  # Page 3 ‚Üí 0, 1, 2\n",
    "])\n",
    "\n",
    "ranks, history = pagerank(adj)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"PAGERANK RESULTS\")\n",
    "print(\"=\"*50)\n",
    "for i, rank in enumerate(ranks):\n",
    "    print(f\"Page {i}: {rank:.4f} ({rank*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüèÜ Most important page: Page {np.argmax(ranks)}\")\n",
    "print(f\"   (Page 0 receives the most incoming links)\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart of ranks\n",
    "axes[0].bar(range(4), ranks, color='steelblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Page')\n",
    "axes[0].set_ylabel('PageRank Score')\n",
    "axes[0].set_title('PageRank Scores', fontweight='bold')\n",
    "axes[0].set_xticks(range(4))\n",
    "\n",
    "# Convergence plot\n",
    "axes[1].semilogy(history, 'b-', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration')\n",
    "axes[1].set_ylabel('Change (log scale)')\n",
    "axes[1].set_title('Convergence of Power Iteration', fontweight='bold')\n",
    "axes[1].axhline(y=1e-6, color='r', linestyle='--', label='Tolerance')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 SVD and Netflix Recommendation System\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Singular Value Decomposition (SVD)** factorizes any matrix $R$ into:\n",
    "\n",
    "$$R = U \\Sigma V^T$$\n",
    "\n",
    "Where:\n",
    "- $U$: Left singular vectors (user features)\n",
    "- $\\Sigma$: Diagonal matrix of singular values (importance)\n",
    "- $V^T$: Right singular vectors (item features)\n",
    "\n",
    "### üè≠ Industrial Application: Netflix Prize\n",
    "\n",
    "For recommendation systems, we approximate the sparse rating matrix:\n",
    "\n",
    "$$R \\approx P \\times Q^T$$\n",
    "\n",
    "Where:\n",
    "- $P$ (users √ó k): User latent factors\n",
    "- $Q$ (items √ó k): Item latent factors\n",
    "- $k$: Number of latent features\n",
    "\n",
    "The optimization objective (with regularization):\n",
    "\n",
    "$$\\min_{P, Q} \\sum_{(u, i) \\in \\mathcal{K}} (r_{ui} - \\mathbf{p}_u^T \\mathbf{q}_i)^2 + \\lambda (\\|\\mathbf{p}_u\\|^2 + \\|\\mathbf{q}_i\\|^2)$$\n",
    "\n",
    "### Update Rules (SGD)\n",
    "\n",
    "For each observed rating $(u, i, r_{ui})$:\n",
    "\n",
    "$$e_{ui} = r_{ui} - \\mathbf{p}_u^T \\mathbf{q}_i$$\n",
    "\n",
    "$$\\mathbf{p}_u \\leftarrow \\mathbf{p}_u + \\alpha (e_{ui} \\mathbf{q}_i - \\lambda \\mathbf{p}_u)$$\n",
    "\n",
    "$$\\mathbf{q}_i \\leftarrow \\mathbf{q}_i + \\alpha (e_{ui} \\mathbf{p}_u - \\lambda \\mathbf{q}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 1.6 NETFLIX-STYLE RECOMMENDER (FunkSVD)\n",
    "# ============================================\n",
    "\n",
    "class MatrixFactorization:\n",
    "    \"\"\"\n",
    "    Matrix Factorization for Collaborative Filtering.\n",
    "    Implements the FunkSVD algorithm used in Netflix Prize.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_factors: int = 10, lr: float = 0.005, \n",
    "                 reg: float = 0.02, n_epochs: int = 100):\n",
    "        self.n_factors = n_factors\n",
    "        self.lr = lr\n",
    "        self.reg = reg\n",
    "        self.n_epochs = n_epochs\n",
    "        \n",
    "    def fit(self, R: np.ndarray) -> List[float]:\n",
    "        \"\"\"\n",
    "        Train the model on rating matrix R.\n",
    "        \n",
    "        Args:\n",
    "            R: Rating matrix (users √ó items), 0 = missing\n",
    "            \n",
    "        Returns:\n",
    "            Training loss history\n",
    "        \"\"\"\n",
    "        self.n_users, self.n_items = R.shape\n",
    "        \n",
    "        # Initialize latent factors with small random values\n",
    "        self.P = np.random.normal(0, 0.1, (self.n_users, self.n_factors))\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_items, self.n_factors))\n",
    "        \n",
    "        # Find observed ratings\n",
    "        self.samples = [\n",
    "            (u, i, R[u, i])\n",
    "            for u in range(self.n_users)\n",
    "            for i in range(self.n_items)\n",
    "            if R[u, i] > 0\n",
    "        ]\n",
    "        \n",
    "        history = []\n",
    "        for epoch in range(self.n_epochs):\n",
    "            np.random.shuffle(self.samples)\n",
    "            \n",
    "            for u, i, r in self.samples:\n",
    "                # Prediction and error\n",
    "                pred = self.P[u] @ self.Q[i]\n",
    "                error = r - pred\n",
    "                \n",
    "                # Gradient descent updates\n",
    "                P_u = self.P[u].copy()\n",
    "                self.P[u] += self.lr * (error * self.Q[i] - self.reg * self.P[u])\n",
    "                self.Q[i] += self.lr * (error * P_u - self.reg * self.Q[i])\n",
    "            \n",
    "            # Compute training loss\n",
    "            loss = self._compute_loss(R)\n",
    "            history.append(loss)\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"Epoch {epoch+1:3d}: Loss = {loss:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def _compute_loss(self, R: np.ndarray) -> float:\n",
    "        \"\"\"Compute regularized MSE loss.\"\"\"\n",
    "        loss = 0\n",
    "        for u, i, r in self.samples:\n",
    "            pred = self.P[u] @ self.Q[i]\n",
    "            loss += (r - pred) ** 2\n",
    "        # Add regularization\n",
    "        loss += self.reg * (np.sum(self.P**2) + np.sum(self.Q**2))\n",
    "        return loss / len(self.samples)\n",
    "    \n",
    "    def predict(self) -> np.ndarray:\n",
    "        \"\"\"Predict all ratings.\"\"\"\n",
    "        return self.P @ self.Q.T\n",
    "\n",
    "\n",
    "# Create sample rating matrix (0 = missing)\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1, 0],\n",
    "    [4, 0, 0, 1, 0],\n",
    "    [1, 1, 0, 5, 0],\n",
    "    [0, 0, 5, 4, 0],\n",
    "    [0, 1, 5, 4, 0],\n",
    "], dtype=float)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"NETFLIX-STYLE RECOMMENDATION SYSTEM\")\n",
    "print(\"=\"*50)\n",
    "print(\"Original Rating Matrix (0 = missing):\")\n",
    "print(R)\n",
    "\n",
    "# Train model\n",
    "model = MatrixFactorization(n_factors=3, lr=0.01, reg=0.01, n_epochs=100)\n",
    "history = model.fit(R)\n",
    "\n",
    "# Get predictions\n",
    "predictions = model.predict()\n",
    "print(\"\\nPredicted Ratings:\")\n",
    "print(np.round(predictions, 2))\n",
    "\n",
    "# Highlight filled-in values\n",
    "print(\"\\nüé¨ Recommendations (previously missing ratings):\")\n",
    "for u in range(R.shape[0]):\n",
    "    for i in range(R.shape[1]):\n",
    "        if R[u, i] == 0:\n",
    "            print(f\"  User {u} ‚Üí Item {i}: Predicted {predictions[u, i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 2: Calculus - The Engine of Learning\n",
    "\n",
    "---\n",
    "\n",
    "If linear algebra is the skeleton of ML, calculus is the muscle that drives learning. The process of **training** a model is fundamentally about finding parameters that minimize a loss function‚Äîthis is **optimization**, which relies entirely on derivatives.\n",
    "\n",
    "> **Key Insight**: The gradient always points toward the direction of steepest increase. To minimize, we move in the *opposite* direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Gradients and the Jacobian Matrix\n",
    "\n",
    "### Theory\n",
    "\n",
    "For a scalar function $f: \\mathbb{R}^n \\to \\mathbb{R}$, the **gradient** is the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "For a **vector-valued function** $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$, we use the **Jacobian matrix**:\n",
    "\n",
    "$$J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\cdots & \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} & \\cdots & \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix}$$\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Function**: $f(x, y) = x^2 + xy + y^2$\n",
    "\n",
    "**Partial derivatives**:\n",
    "- $\\frac{\\partial f}{\\partial x} = 2x + y$\n",
    "- $\\frac{\\partial f}{\\partial y} = x + 2y$\n",
    "\n",
    "**At point $(x, y) = (1, 2)$**:\n",
    "\n",
    "$$\\nabla f(1, 2) = \\begin{bmatrix} 2(1) + 2 \\\\ 1 + 2(2) \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 5 \\end{bmatrix}$$\n",
    "\n",
    "The gradient magnitude: $\\|\\nabla f\\| = \\sqrt{16 + 25} = \\sqrt{41} \\approx 6.4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# ============================================\n",
    "# 2.1 GRADIENT COMPUTATION\n",
    "# ============================================\n",
    "\n",
    "def numerical_gradient(f: Callable, x: np.ndarray, h: float = 1e-5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute gradient numerically using central differences.\n",
    "    \n",
    "    Args:\n",
    "        f: Scalar function\n",
    "        x: Point to evaluate gradient\n",
    "        h: Step size for finite differences\n",
    "    \n",
    "    Returns:\n",
    "        Gradient vector\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x, dtype=float)\n",
    "    for i in range(len(x)):\n",
    "        x_plus = x.copy()\n",
    "        x_minus = x.copy()\n",
    "        x_plus[i] += h\n",
    "        x_minus[i] -= h\n",
    "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def analytical_gradient(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Analytical gradient of f(x,y) = x^2 + xy + y^2\"\"\"\n",
    "    return np.array([2*x[0] + x[1], x[0] + 2*x[1]])\n",
    "\n",
    "\n",
    "# Define our function\n",
    "def f(x):\n",
    "    return x[0]**2 + x[0]*x[1] + x[1]**2\n",
    "\n",
    "\n",
    "# Test at point (1, 2)\n",
    "point = np.array([1.0, 2.0])\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"GRADIENT COMPUTATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Function: f(x,y) = x¬≤ + xy + y¬≤\")\n",
    "print(f\"Point: ({point[0]}, {point[1]})\")\n",
    "print(f\"f({point[0]}, {point[1]}) = {f(point)}\")\n",
    "print(f\"\\nNumerical gradient:  {numerical_gradient(f, point)}\")\n",
    "print(f\"Analytical gradient: {analytical_gradient(point)}\")\n",
    "print(f\"\\nGradient magnitude: {np.linalg.norm(analytical_gradient(point)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Gradient field\n",
    "x = np.linspace(-3, 3, 20)\n",
    "y = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + X*Y + Y**2\n",
    "\n",
    "# Gradient components\n",
    "U = 2*X + Y  # df/dx\n",
    "V = X + 2*Y  # df/dy\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.contour(X, Y, Z, levels=20, cmap='viridis', alpha=0.7)\n",
    "plt.colorbar(label='f(x, y)')\n",
    "plt.quiver(X, Y, U, V, color='red', alpha=0.6)\n",
    "plt.scatter([1], [2], c='yellow', s=200, marker='*', edgecolors='black', zorder=5)\n",
    "plt.annotate('Point (1,2)', (1.1, 2.2), fontsize=12, fontweight='bold')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Field of f(x,y) = x¬≤ + xy + y¬≤', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(\"üî¥ Red arrows show gradient direction (steepest ascent)\")\n",
    "print(\"üìâ To minimize, move OPPOSITE to the gradient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Backpropagation via Chain Rule\n",
    "\n",
    "### Theory\n",
    "\n",
    "Neural networks are **composite functions**. If we have layers:\n",
    "\n",
    "$$L = \\text{Loss}(f_3(f_2(f_1(x))))$$\n",
    "\n",
    "The **chain rule** lets us compute derivatives through the composition:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial f_3} \\cdot \\frac{\\partial f_3}{\\partial f_2} \\cdot \\frac{\\partial f_2}{\\partial f_1} \\cdot \\frac{\\partial f_1}{\\partial x}$$\n",
    "\n",
    "### üìê Mathematical Example: 2-Layer Network\n",
    "\n",
    "**Forward pass**:\n",
    "- Input: $x = 2$\n",
    "- **Layer 1**: $h = wx = 3 \\times 2 = 6$ (weight $w = 3$)\n",
    "- **Activation**: $a = \\text{ReLU}(h) = \\max(0, 6) = 6$\n",
    "- **Layer 2**: $\\hat{y} = va = 0.5 \\times 6 = 3$ (weight $v = 0.5$)\n",
    "- **Loss**: $L = (\\hat{y} - y)^2 = (3 - 4)^2 = 1$ (target $y = 4$)\n",
    "\n",
    "**Backward pass**:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y}} = 2(\\hat{y} - y) = 2(3 - 4) = -2$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial v} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial v} = -2 \\times a = -2 \\times 6 = -12$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial a} = -2 \\times v = -2 \\times 0.5 = -1$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w} = -1 \\times 1 \\times x = -1 \\times 1 \\times 2 = -2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.2 BACKPROPAGATION FROM SCRATCH\n",
    "# ============================================\n",
    "\n",
    "class SimpleNeuron:\n",
    "    \"\"\"A simple 2-layer network demonstrating backpropagation.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.w = 3.0  # Layer 1 weight\n",
    "        self.v = 0.5  # Layer 2 weight\n",
    "        \n",
    "    def forward(self, x: float) -> Tuple[float, dict]:\n",
    "        \"\"\"Forward pass with caching for backprop.\"\"\"\n",
    "        h = self.w * x           # Linear layer 1\n",
    "        a = max(0, h)            # ReLU activation\n",
    "        y_hat = self.v * a       # Linear layer 2\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        cache = {'x': x, 'h': h, 'a': a, 'y_hat': y_hat}\n",
    "        return y_hat, cache\n",
    "    \n",
    "    def backward(self, y_true: float, cache: dict) -> dict:\n",
    "        \"\"\"Backward pass computing all gradients.\"\"\"\n",
    "        x, h, a, y_hat = cache['x'], cache['h'], cache['a'], cache['y_hat']\n",
    "        \n",
    "        # Loss: L = (y_hat - y_true)^2\n",
    "        dL_dy_hat = 2 * (y_hat - y_true)\n",
    "        \n",
    "        # Layer 2: y_hat = v * a\n",
    "        dL_dv = dL_dy_hat * a\n",
    "        dL_da = dL_dy_hat * self.v\n",
    "        \n",
    "        # ReLU: a = max(0, h)\n",
    "        dL_dh = dL_da * (1 if h > 0 else 0)\n",
    "        \n",
    "        # Layer 1: h = w * x\n",
    "        dL_dw = dL_dh * x\n",
    "        dL_dx = dL_dh * self.w\n",
    "        \n",
    "        return {\n",
    "            'dL/dy_hat': dL_dy_hat,\n",
    "            'dL/dv': dL_dv,\n",
    "            'dL/da': dL_da,\n",
    "            'dL/dh': dL_dh,\n",
    "            'dL/dw': dL_dw,\n",
    "            'dL/dx': dL_dx,\n",
    "        }\n",
    "\n",
    "\n",
    "# Example from theory\n",
    "net = SimpleNeuron()\n",
    "x, y_true = 2.0, 4.0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BACKPROPAGATION STEP-BY-STEP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Forward pass\n",
    "y_hat, cache = net.forward(x)\n",
    "loss = (y_hat - y_true) ** 2\n",
    "\n",
    "print(\"üì• FORWARD PASS:\")\n",
    "print(f\"  Input x = {x}\")\n",
    "print(f\"  h = w*x = {net.w}*{x} = {cache['h']}\")\n",
    "print(f\"  a = ReLU(h) = {cache['a']}\")\n",
    "print(f\"  ≈∑ = v*a = {net.v}*{cache['a']} = {y_hat}\")\n",
    "print(f\"  Loss L = (≈∑ - y)¬≤ = ({y_hat} - {y_true})¬≤ = {loss}\")\n",
    "\n",
    "# Backward pass\n",
    "grads = net.backward(y_true, cache)\n",
    "\n",
    "print(\"\\nüì§ BACKWARD PASS (Chain Rule):\")\n",
    "for name, value in grads.items():\n",
    "    print(f\"  {name} = {value}\")\n",
    "\n",
    "print(\"\\n‚úÖ These gradients tell us how to update w and v to reduce loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Self-Attention Mechanism (Transformers)\n",
    "\n",
    "### Theory\n",
    "\n",
    "The **Self-Attention** mechanism is the heart of Transformers (GPT, BERT). It computes relationships between all positions in a sequence using:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ (Query): \"What am I looking for?\"\n",
    "- $K$ (Key): \"What do I contain?\"\n",
    "- $V$ (Value): \"What information do I provide?\"\n",
    "- $d_k$: Dimension of keys (for scaling)\n",
    "\n",
    "### Why $\\sqrt{d_k}$?\n",
    "\n",
    "The dot product $QK^T$ grows with dimension $d_k$. Large values cause softmax to saturate (output near 0 or 1), leading to **vanishing gradients**. Scaling by $\\sqrt{d_k}$ keeps values in a good range.\n",
    "\n",
    "### üìê Mathematical Example: 3-Word Sentence\n",
    "\n",
    "**Sentence**: \"The cat sat\"\n",
    "\n",
    "Suppose each word has a 4-dimensional embedding:\n",
    "- \"The\" ‚Üí $[0.1, 0.2, 0.1, 0.3]$\n",
    "- \"cat\" ‚Üí $[0.5, 0.4, 0.6, 0.2]$\n",
    "- \"sat\" ‚Üí $[0.3, 0.5, 0.2, 0.4]$\n",
    "\n",
    "We compute Q, K, V by multiplying with learned weight matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 2.3 SELF-ATTENTION FROM SCRATCH\n",
    "# ============================================\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len, d_k)\n",
    "        K: Key matrix (seq_len, d_k)\n",
    "        V: Value matrix (seq_len, d_v)\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output\n",
    "        attention_weights: Attention weights matrix\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Step 1: Compute attention scores\n",
    "    scores = Q @ K.T  # (seq_len, seq_len)\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    attention_weights = softmax(scaled_scores)\n",
    "    \n",
    "    # Step 4: Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "# Example: 3-word sentence with 4-dim embeddings\n",
    "np.random.seed(42)\n",
    "\n",
    "# Word embeddings (in practice, these come from an embedding layer)\n",
    "X = np.array([\n",
    "    [0.1, 0.2, 0.1, 0.3],  # \"The\"\n",
    "    [0.5, 0.4, 0.6, 0.2],  # \"cat\"\n",
    "    [0.3, 0.5, 0.2, 0.4],  # \"sat\"\n",
    "])\n",
    "\n",
    "# Weight matrices (in practice, these are learned)\n",
    "d_model = 4\n",
    "d_k = 3  # Query/Key dimension\n",
    "d_v = 3  # Value dimension\n",
    "\n",
    "W_Q = np.random.randn(d_model, d_k) * 0.5\n",
    "W_K = np.random.randn(d_model, d_k) * 0.5\n",
    "W_V = np.random.randn(d_model, d_v) * 0.5\n",
    "\n",
    "# Compute Q, K, V\n",
    "Q = X @ W_Q\n",
    "K = X @ W_K\n",
    "V = X @ W_V\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SELF-ATTENTION MECHANISM\")\n",
    "print(\"=\"*60)\n",
    "print(\"Sentence: ['The', 'cat', 'sat']\")\n",
    "print(f\"\\nInput embeddings X (3 words √ó 4 dims):\\n{X}\")\n",
    "\n",
    "# Compute attention\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nüìä Attention Weights (each row sums to 1):\")\n",
    "print(f\"          The    cat    sat\")\n",
    "for i, word in enumerate(['The', 'cat', 'sat']):\n",
    "    print(f\"{word:>6}:  {attn_weights[i]}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Row sums (should be 1.0): {attn_weights.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of attention weights\n",
    "import seaborn as sns\n",
    "\n",
    "words = ['The', 'cat', 'sat']\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.3f', \n",
    "            xticklabels=words, yticklabels=words,\n",
    "            cmap='Blues', cbar_kws={'label': 'Attention Weight'})\n",
    "plt.xlabel('Key (attending to)')\n",
    "plt.ylabel('Query (from)')\n",
    "plt.title('Self-Attention Weights\\n\"How much does each word attend to others?\"', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Each word (Query) distributes its attention across all words (Keys).\")\n",
    "print(\"   The attention weights determine how much information flows from each position.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 3: Optimization - Finding the Best Solution\n",
    "\n",
    "---\n",
    "\n",
    "Once we have a model and a loss function, the goal is to find parameters that minimize the loss. This is **optimization**.\n",
    "\n",
    "> **Key Insight**: Convex problems have a single global minimum. Non-convex problems (deep learning) have many local minima, but good optimizers find good solutions anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Convex Optimization and Lagrange Multipliers\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Convex optimization** is the \"easy\" case: any local minimum is a global minimum. Many classical ML algorithms (linear regression, SVM, logistic regression) are convex.\n",
    "\n",
    "For **constrained optimization**, we use **Lagrange multipliers**. Convert:\n",
    "\n",
    "$$\\min_x f(x) \\quad \\text{subject to} \\quad g(x) = 0$$\n",
    "\n",
    "Into the **Lagrangian**:\n",
    "\n",
    "$$\\mathcal{L}(x, \\lambda) = f(x) + \\lambda g(x)$$\n",
    "\n",
    "Then solve:\n",
    "$$\\nabla_x \\mathcal{L} = 0 \\quad \\text{and} \\quad \\nabla_\\lambda \\mathcal{L} = 0$$\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Minimize** $f(x, y) = x^2 + y^2$ **subject to** $x + y = 1$\n",
    "\n",
    "**Lagrangian**:\n",
    "$$\\mathcal{L} = x^2 + y^2 + \\lambda(x + y - 1)$$\n",
    "\n",
    "**Solve**:\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial x} = 2x + \\lambda = 0 \\Rightarrow x = -\\lambda/2$\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial y} = 2y + \\lambda = 0 \\Rightarrow y = -\\lambda/2$\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = x + y - 1 = 0$\n",
    "\n",
    "From equations 1 and 2: $x = y$. From equation 3: $2x = 1 \\Rightarrow x = y = 0.5$\n",
    "\n",
    "**Solution**: $(x^*, y^*) = (0.5, 0.5)$ with $f^* = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.1 LAGRANGE MULTIPLIERS VISUALIZATION\n",
    "# ============================================\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Objective function\n",
    "def objective(vars):\n",
    "    x, y = vars\n",
    "    return x**2 + y**2\n",
    "\n",
    "# Constraint: x + y = 1 (equality, so we write it as x + y - 1 = 0)\n",
    "constraint = {'type': 'eq', 'fun': lambda vars: vars[0] + vars[1] - 1}\n",
    "\n",
    "# Solve\n",
    "result = minimize(objective, [0, 0], constraints=constraint)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LAGRANGE MULTIPLIERS EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Minimize: f(x,y) = x¬≤ + y¬≤\")\n",
    "print(f\"Subject to: x + y = 1\")\n",
    "print(f\"\\nSolution: x* = {result.x[0]:.4f}, y* = {result.x[1]:.4f}\")\n",
    "print(f\"Optimal value: f* = {result.fun:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Contours of objective function\n",
    "x = np.linspace(-1, 2, 100)\n",
    "y = np.linspace(-1, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = X**2 + Y**2\n",
    "\n",
    "contours = ax.contour(X, Y, Z, levels=20, cmap='viridis')\n",
    "ax.clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "# Constraint line\n",
    "ax.plot(x, 1 - x, 'r-', linewidth=2, label='Constraint: x + y = 1')\n",
    "\n",
    "# Optimal point\n",
    "ax.scatter([0.5], [0.5], c='red', s=200, marker='*', \n",
    "           edgecolors='black', zorder=5, label=f'Optimal: (0.5, 0.5)')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Constrained Optimization with Lagrange Multipliers', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.set_xlim(-0.5, 1.5)\n",
    "ax.set_ylim(-0.5, 1.5)\n",
    "ax.set_aspect('equal')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚≠ê The optimal point is where the constraint line is TANGENT to a contour.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SVM and the Dual Formulation\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Support Vector Machines (SVM)** find the maximum-margin hyperplane. The **primal** problem:\n",
    "\n",
    "$$\\min_{w, b} \\frac{1}{2}\\|w\\|^2 \\quad \\text{s.t.} \\quad y_i(w^T x_i + b) \\geq 1$$\n",
    "\n",
    "Using Lagrange multipliers, we get the **dual** problem:\n",
    "\n",
    "$$\\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_{i,j} \\alpha_i \\alpha_j y_i y_j K(x_i, x_j)$$\n",
    "\n",
    "**Key insight**: The dual form uses only **dot products** $K(x_i, x_j) = x_i^T x_j$, which can be replaced by any kernel function (the **kernel trick**).\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Data** (simple 2D case):\n",
    "- Class +1: $(1, 2), (2, 3)$\n",
    "- Class -1: $(0, 0), (1, 0)$\n",
    "\n",
    "The dual problem becomes a **Quadratic Programming** problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.2 SVM FROM SCRATCH (SIMPLIFIED)\n",
    "# ============================================\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "class SimpleSVM:\n",
    "    \"\"\"Simplified SVM using scipy optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, C: float = 1.0):\n",
    "        self.C = C\n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Compute Gram matrix K[i,j] = x_i ¬∑ x_j\n",
    "        K = X @ X.T\n",
    "        \n",
    "        # Dual objective (to maximize, so we negate for minimization)\n",
    "        def dual_objective(alpha):\n",
    "            return 0.5 * np.sum((alpha * y)[:, None] * (alpha * y)[None, :] * K) - np.sum(alpha)\n",
    "        \n",
    "        # Gradients\n",
    "        def dual_gradient(alpha):\n",
    "            return (alpha * y) @ (K * (y[:, None] * y[None, :])) - 1\n",
    "        \n",
    "        # Constraints: 0 <= alpha <= C and sum(alpha * y) = 0\n",
    "        constraints = [\n",
    "            {'type': 'eq', 'fun': lambda a: np.dot(a, y)}\n",
    "        ]\n",
    "        bounds = [(0, self.C) for _ in range(n_samples)]\n",
    "        \n",
    "        # Solve\n",
    "        result = minimize(\n",
    "            dual_objective,\n",
    "            np.zeros(n_samples),\n",
    "            jac=dual_gradient,\n",
    "            bounds=bounds,\n",
    "            constraints=constraints,\n",
    "            method='SLSQP'\n",
    "        )\n",
    "        \n",
    "        self.alpha = result.x\n",
    "        \n",
    "        # Find support vectors (alpha > threshold)\n",
    "        sv_mask = self.alpha > 1e-5\n",
    "        self.support_vectors = X[sv_mask]\n",
    "        self.support_labels = y[sv_mask]\n",
    "        self.support_alphas = self.alpha[sv_mask]\n",
    "        \n",
    "        # Compute w = sum(alpha_i * y_i * x_i)\n",
    "        self.w = np.sum((self.alpha * y)[:, None] * X, axis=0)\n",
    "        \n",
    "        # Compute b using support vectors\n",
    "        self.b = np.mean(self.support_labels - self.support_vectors @ self.w)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return np.sign(X @ self.w + self.b)\n",
    "\n",
    "\n",
    "# Example data\n",
    "X = np.array([\n",
    "    [1, 2],   # +1\n",
    "    [2, 3],   # +1  \n",
    "    [0, 0],   # -1\n",
    "    [1, 0],   # -1\n",
    "], dtype=float)\n",
    "y = np.array([1, 1, -1, -1], dtype=float)\n",
    "\n",
    "# Train SVM\n",
    "svm = SimpleSVM(C=100.0)\n",
    "svm.fit(X, y)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SUPPORT VECTOR MACHINE (DUAL FORM)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Data points: {len(X)}\")\n",
    "print(f\"Lagrange multipliers (Œ±): {svm.alpha}\")\n",
    "print(f\"\\nSupport vectors (Œ± > 0):\")\n",
    "for sv, label, alpha in zip(svm.support_vectors, svm.support_labels, svm.support_alphas):\n",
    "    print(f\"  {sv} (y={int(label)}, Œ±={alpha:.4f})\")\n",
    "print(f\"\\nWeight vector w: {svm.w}\")\n",
    "print(f\"Bias b: {svm.b:.4f}\")\n",
    "print(f\"\\nDecision boundary: {svm.w[0]:.3f}x + {svm.w[1]:.3f}y + {svm.b:.3f} = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', s=100, label='Class +1', edgecolors='black')\n",
    "plt.scatter(X[y == -1, 0], X[y == -1, 1], c='red', s=100, label='Class -1', edgecolors='black')\n",
    "\n",
    "# Plot support vectors\n",
    "plt.scatter(svm.support_vectors[:, 0], svm.support_vectors[:, 1], \n",
    "            s=200, facecolors='none', edgecolors='green', linewidths=2, label='Support Vectors')\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "x_range = np.linspace(-0.5, 3, 100)\n",
    "y_boundary = -(svm.w[0] * x_range + svm.b) / svm.w[1]\n",
    "y_margin_pos = -(svm.w[0] * x_range + svm.b - 1) / svm.w[1]\n",
    "y_margin_neg = -(svm.w[0] * x_range + svm.b + 1) / svm.w[1]\n",
    "\n",
    "plt.plot(x_range, y_boundary, 'k-', linewidth=2, label='Decision Boundary')\n",
    "plt.plot(x_range, y_margin_pos, 'k--', linewidth=1)\n",
    "plt.plot(x_range, y_margin_neg, 'k--', linewidth=1)\n",
    "plt.fill_between(x_range, y_margin_neg, y_margin_pos, alpha=0.1, color='yellow')\n",
    "\n",
    "plt.xlabel('x‚ÇÅ')\n",
    "plt.ylabel('x‚ÇÇ')\n",
    "plt.title('SVM: Maximum Margin Classifier', fontweight='bold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xlim(-0.5, 3)\n",
    "plt.ylim(-1, 4)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° The margin (yellow area) is maximized.\")\n",
    "print(\"   Only support vectors determine the decision boundary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 ADMM: Industrial-Scale Optimization (Uber Case)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Alternating Direction Method of Multipliers (ADMM)** solves problems of the form:\n",
    "\n",
    "$$\\min_x f(x) + g(z) \\quad \\text{s.t.} \\quad Ax + Bz = c$$\n",
    "\n",
    "The algorithm alternates between:\n",
    "\n",
    "1. **x-update**: $x^{k+1} = \\arg\\min_x (f(x) + \\frac{\\rho}{2}\\|Ax + Bz^k - c + u^k\\|_2^2)$\n",
    "2. **z-update**: $z^{k+1} = \\arg\\min_z (g(z) + \\frac{\\rho}{2}\\|Ax^{k+1} + Bz - c + u^k\\|_2^2)$\n",
    "3. **Dual update**: $u^{k+1} = u^k + Ax^{k+1} + Bz^{k+1} - c$\n",
    "\n",
    "### üè≠ Industrial Application: Uber\n",
    "\n",
    "Uber uses ADMM for budget allocation across cities. Each city solves a local problem, while the global constraint ensures total budget is respected.\n",
    "\n",
    "### üìê Mathematical Example: LASSO Regression\n",
    "\n",
    "$$\\min_x \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda\\|x\\|_1$$\n",
    "\n",
    "We introduce $z = x$ and solve using ADMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 3.3 ADMM FOR LASSO\n",
    "# ============================================\n",
    "\n",
    "def soft_threshold(x: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    \"\"\"Soft thresholding operator (proximal operator for L1).\"\"\"\n",
    "    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)\n",
    "\n",
    "\n",
    "def admm_lasso(A: np.ndarray, b: np.ndarray, lam: float, \n",
    "               rho: float = 1.0, max_iter: int = 100, tol: float = 1e-6):\n",
    "    \"\"\"\n",
    "    Solve LASSO using ADMM.\n",
    "    \n",
    "    min (1/2)||Ax - b||¬≤ + Œª||x||‚ÇÅ\n",
    "    \"\"\"\n",
    "    n = A.shape[1]\n",
    "    \n",
    "    # Initialize\n",
    "    x = np.zeros(n)\n",
    "    z = np.zeros(n)\n",
    "    u = np.zeros(n)\n",
    "    \n",
    "    # Precompute (A^T A + œÅI)^{-1} A^T b for efficiency\n",
    "    AtA = A.T @ A\n",
    "    Atb = A.T @ b\n",
    "    L = AtA + rho * np.eye(n)\n",
    "    L_inv = np.linalg.inv(L)\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # x-update: solve (A^T A + œÅI)x = A^T b + œÅ(z - u)\n",
    "        x = L_inv @ (Atb + rho * (z - u))\n",
    "        \n",
    "        # z-update: soft thresholding\n",
    "        z_new = soft_threshold(x + u, lam / rho)\n",
    "        \n",
    "        # u-update (dual variable)\n",
    "        u = u + x - z_new\n",
    "        \n",
    "        # Check convergence\n",
    "        primal_residual = np.linalg.norm(x - z_new)\n",
    "        history.append(primal_residual)\n",
    "        \n",
    "        z = z_new\n",
    "        \n",
    "        if primal_residual < tol:\n",
    "            break\n",
    "    \n",
    "    return x, history\n",
    "\n",
    "\n",
    "# Generate example data\n",
    "np.random.seed(42)\n",
    "n_samples, n_features = 50, 20\n",
    "A = np.random.randn(n_samples, n_features)\n",
    "true_x = np.zeros(n_features)\n",
    "true_x[:5] = [3, -2, 1.5, -1, 0.5]  # Only 5 non-zero coefficients\n",
    "b = A @ true_x + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "# Solve LASSO\n",
    "lam = 0.5\n",
    "x_lasso, history = admm_lasso(A, b, lam, rho=1.0, max_iter=200)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ADMM FOR LASSO REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Problem: min (1/2)||Ax - b||¬≤ + {lam}||x||‚ÇÅ\")\n",
    "print(f\"True sparse coefficients:   {true_x[:8]}...\")\n",
    "print(f\"LASSO solution (rounded):   {np.round(x_lasso[:8], 3)}...\")\n",
    "print(f\"\\nConverged in {len(history)} iterations\")\n",
    "print(f\"Non-zero coefficients: {np.sum(np.abs(x_lasso) > 0.01)} (true: 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 4: Probability & Information Theory\n",
    "\n",
    "---\n",
    "\n",
    "Dealing with **uncertainty** is fundamental to AI. Probability provides the language to describe uncertainty, and **information theory** provides measures of it.\n",
    "\n",
    "> **Key Insight**: The real world is noisy and uncertain. Probabilistic models embrace this reality rather than fighting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Entropy and KL Divergence\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Entropy** $H(P)$ measures the uncertainty or randomness in a distribution:\n",
    "\n",
    "$$H(P) = -\\sum_x P(x) \\log P(x)$$\n",
    "\n",
    "Properties:\n",
    "- Maximum for uniform distribution\n",
    "- Zero for deterministic distribution (one outcome has probability 1)\n",
    "\n",
    "**KL Divergence** $D_{KL}(P||Q)$ measures how different distribution $Q$ is from $P$:\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$\n",
    "\n",
    "Properties:\n",
    "- Always non-negative ($\\geq 0$)\n",
    "- Zero if and only if $P = Q$\n",
    "- **Not symmetric**: $D_{KL}(P||Q) \\neq D_{KL}(Q||P)$\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Distributions**:\n",
    "- $P = [0.25, 0.75]$ (biased coin)\n",
    "- $Q = [0.5, 0.5]$ (fair coin)\n",
    "\n",
    "**Entropy of P**:\n",
    "$$H(P) = -0.25 \\log(0.25) - 0.75 \\log(0.75) = 0.562 \\text{ bits}$$\n",
    "\n",
    "**KL Divergence**:\n",
    "$$D_{KL}(P||Q) = 0.25 \\log\\frac{0.25}{0.5} + 0.75 \\log\\frac{0.75}{0.5} = 0.0589 \\text{ bits}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.1 ENTROPY AND KL DIVERGENCE\n",
    "# ============================================\n",
    "\n",
    "def entropy(p: np.ndarray) -> float:\n",
    "    \"\"\"Compute entropy in bits.\"\"\"\n",
    "    p = np.array(p)\n",
    "    p = p[p > 0]  # Avoid log(0)\n",
    "    return -np.sum(p * np.log2(p))\n",
    "\n",
    "\n",
    "def kl_divergence(p: np.ndarray, q: np.ndarray) -> float:\n",
    "    \"\"\"Compute KL divergence D_KL(P||Q) in bits.\"\"\"\n",
    "    p, q = np.array(p), np.array(q)\n",
    "    # Only consider where p > 0\n",
    "    mask = p > 0\n",
    "    return np.sum(p[mask] * np.log2(p[mask] / q[mask]))\n",
    "\n",
    "\n",
    "# Example distributions\n",
    "P = np.array([0.25, 0.75])  # Biased coin\n",
    "Q = np.array([0.5, 0.5])   # Fair coin\n",
    "R = np.array([0.1, 0.9])   # Very biased\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENTROPY AND KL DIVERGENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Distributions:\")\n",
    "print(f\"  P (biased):      {P}\")\n",
    "print(f\"  Q (fair coin):   {Q}\")\n",
    "print(f\"  R (very biased): {R}\")\n",
    "\n",
    "print(f\"\\nüìê Entropy (uncertainty):\")\n",
    "print(f\"  H(P) = {entropy(P):.4f} bits\")\n",
    "print(f\"  H(Q) = {entropy(Q):.4f} bits (maximum for 2 outcomes)\")\n",
    "print(f\"  H(R) = {entropy(R):.4f} bits (low uncertainty)\")\n",
    "\n",
    "print(f\"\\nüìê KL Divergence (difference from Q):\")\n",
    "print(f\"  D_KL(P||Q) = {kl_divergence(P, Q):.4f} bits\")\n",
    "print(f\"  D_KL(R||Q) = {kl_divergence(R, Q):.4f} bits (R is more different from Q)\")\n",
    "print(f\"  D_KL(Q||Q) = {kl_divergence(Q, Q):.4f} bits (same distribution = 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Entropy vs probability\n",
    "p_range = np.linspace(0.001, 0.999, 100)\n",
    "entropy_values = [entropy([p, 1-p]) for p in p_range]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(p_range, entropy_values, 'b-', linewidth=2)\n",
    "plt.axvline(x=0.5, color='r', linestyle='--', label='Maximum entropy (p=0.5)')\n",
    "plt.scatter([0.25, 0.5, 0.1], [entropy([0.25, 0.75]), entropy([0.5, 0.5]), entropy([0.1, 0.9])],\n",
    "            c=['green', 'red', 'orange'], s=100, zorder=5)\n",
    "plt.annotate('P', (0.25, entropy([0.25, 0.75])+0.05), fontsize=12, fontweight='bold')\n",
    "plt.annotate('Q', (0.5, entropy([0.5, 0.5])+0.05), fontsize=12, fontweight='bold')\n",
    "plt.annotate('R', (0.1, entropy([0.1, 0.9])+0.05), fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Probability p (for outcome 1)', fontsize=12)\n",
    "plt.ylabel('Entropy H(p) [bits]', fontsize=12)\n",
    "plt.title('Binary Entropy Function', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Entropy is maximum when outcomes are equally likely (maximum uncertainty).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Variational Autoencoders (VAE)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**VAEs** are generative models that learn to map data to a latent space that follows a standard Gaussian $N(0, I)$.\n",
    "\n",
    "The loss function is the **Evidence Lower Bound (ELBO)**:\n",
    "\n",
    "$$\\mathcal{L} = \\underbrace{-\\mathbb{E}_{q(z|x)}[\\log p(x|z)]}_{\\text{Reconstruction Loss}} + \\underbrace{D_{KL}(q(z|x) || p(z))}_{\\text{KL Regularization}}$$\n",
    "\n",
    "### KL Between Two Gaussians (Closed Form)\n",
    "\n",
    "For $q(z|x) = N(\\mu, \\sigma^2)$ and $p(z) = N(0, 1)$:\n",
    "\n",
    "$$D_{KL}(q||p) = -\\frac{1}{2} \\sum_{j=1}^{d} (1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$$\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "**Encoder output**: $\\mu = 0.5$, $\\sigma = 0.8$\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}(1 + \\log(0.64) - 0.25 - 0.64) = -\\frac{1}{2}(1 - 0.446 - 0.25 - 0.64) = 0.168$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.2 VAE KL DIVERGENCE COMPUTATION\n",
    "# ============================================\n",
    "\n",
    "def kl_divergence_gaussian(mu: np.ndarray, log_var: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute KL divergence between N(mu, exp(log_var)) and N(0, 1).\n",
    "    This is the closed-form solution used in VAEs.\n",
    "    \n",
    "    Args:\n",
    "        mu: Mean of the encoder distribution\n",
    "        log_var: Log variance of the encoder distribution\n",
    "    \n",
    "    Returns:\n",
    "        KL divergence value\n",
    "    \"\"\"\n",
    "    return -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))\n",
    "\n",
    "\n",
    "def reparameterize(mu: np.ndarray, log_var: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reparameterization trick: z = mu + sigma * epsilon\n",
    "    where epsilon ~ N(0, 1)\n",
    "    \"\"\"\n",
    "    std = np.exp(0.5 * log_var)\n",
    "    eps = np.random.randn(*mu.shape)\n",
    "    return mu + std * eps\n",
    "\n",
    "\n",
    "# Example: encoder outputs\n",
    "mu = np.array([0.5, -0.3, 0.8])\n",
    "sigma = np.array([0.8, 0.5, 1.2])\n",
    "log_var = 2 * np.log(sigma)  # log(sigma^2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VAE KL DIVERGENCE (CLOSED FORM)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Encoder output:\")\n",
    "print(f\"  Œº = {mu}\")\n",
    "print(f\"  œÉ = {sigma}\")\n",
    "print(f\"  log(œÉ¬≤) = {log_var}\")\n",
    "\n",
    "kl = kl_divergence_gaussian(mu, log_var)\n",
    "print(f\"\\nD_KL(q(z|x) || p(z)) = {kl:.4f}\")\n",
    "\n",
    "# Show reparameterization trick\n",
    "print(f\"\\nüé≤ Reparameterization samples:\")\n",
    "for i in range(3):\n",
    "    z = reparameterize(mu, log_var)\n",
    "    print(f\"  z_{i+1} = {z}\")\n",
    "\n",
    "print(\"\\nüí° The reparameterization trick allows gradients to flow through sampling!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Effect of KL divergence on latent space\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Different encoder distributions\n",
    "configs = [\n",
    "    (0, 1, 'Standard Normal N(0,1)'),\n",
    "    (0.5, 0.8, 'Shifted: N(0.5, 0.64)'),\n",
    "    (2, 0.3, 'Far from prior: N(2, 0.09)'),\n",
    "]\n",
    "\n",
    "x = np.linspace(-4, 5, 200)\n",
    "\n",
    "for ax, (mu, sigma, title) in zip(axes, configs):\n",
    "    # Prior p(z)\n",
    "    prior = stats.norm.pdf(x, 0, 1)\n",
    "    # Encoder q(z|x)\n",
    "    encoder = stats.norm.pdf(x, mu, sigma)\n",
    "    \n",
    "    ax.fill_between(x, prior, alpha=0.3, label='Prior p(z) = N(0,1)')\n",
    "    ax.fill_between(x, encoder, alpha=0.3, label=f'Encoder q(z|x)')\n",
    "    ax.plot(x, prior, 'b-', linewidth=2)\n",
    "    ax.plot(x, encoder, 'r-', linewidth=2)\n",
    "    \n",
    "    # Compute KL\n",
    "    log_var = 2 * np.log(sigma)\n",
    "    kl = kl_divergence_gaussian(np.array([mu]), np.array([log_var]))\n",
    "    \n",
    "    ax.set_title(f'{title}\\nD_KL = {kl:.3f}', fontweight='bold')\n",
    "    ax.set_xlabel('z')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('KL Divergence in VAE: Encoder vs Prior', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìà Higher KL = encoder distribution is far from the prior.\")\n",
    "print(\"üìâ VAE training minimizes KL to keep latent space regular.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 t-SNE Algorithm\n",
    "\n",
    "### Theory\n",
    "\n",
    "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) visualizes high-dimensional data in 2D/3D.\n",
    "\n",
    "**Key idea**: Convert distances to probabilities, then minimize the difference between high-dim and low-dim probability distributions.\n",
    "\n",
    "**High-dimensional space**: Gaussian similarities\n",
    "$$p_{j|i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}$$\n",
    "\n",
    "**Low-dimensional space**: t-distribution (heavier tails to prevent crowding)\n",
    "$$q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}$$\n",
    "\n",
    "**Loss**: KL divergence between P and Q\n",
    "$$C = \\sum_i D_{KL}(P_i || Q_i) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 4.3 t-SNE SIMPLIFIED DEMO\n",
    "# ============================================\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate high-dimensional clustered data\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "n_features = 50  # High dimensional\n",
    "n_clusters = 4\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features, \n",
    "                  centers=n_clusters, cluster_std=2.0, random_state=42)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"t-SNE DIMENSIONALITY REDUCTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original data: {X.shape[0]} samples √ó {X.shape[1]} dimensions\")\n",
    "\n",
    "# Apply t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "print(f\"t-SNE output: {X_tsne.shape[0]} samples √ó {X_tsne.shape[1]} dimensions\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original (first 2 dimensions only)\n",
    "scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
    "axes[0].set_title('Original Data (First 2 of 50 dims)\\nClusters not clearly separated', fontweight='bold')\n",
    "axes[0].set_xlabel('Dimension 1')\n",
    "axes[0].set_ylabel('Dimension 2')\n",
    "\n",
    "# t-SNE\n",
    "scatter2 = axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab10', alpha=0.7)\n",
    "axes[1].set_title('t-SNE Projection (2D)\\nClusters clearly separated!', fontweight='bold')\n",
    "axes[1].set_xlabel('t-SNE Dimension 1')\n",
    "axes[1].set_ylabel('t-SNE Dimension 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° t-SNE reveals cluster structure hidden in high dimensions!\")\n",
    "print(\"   It uses KL divergence to preserve local neighborhood structure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 5: Advanced Integration Methods\n",
    "\n",
    "---\n",
    "\n",
    "Many ML problems require computing integrals that have no closed-form solution. **Monte Carlo methods** provide numerical approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Monte Carlo Integration and Importance Sampling\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Monte Carlo Integration** approximates expectations:\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim p}[f(x)] = \\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i), \\quad x_i \\sim p$$\n",
    "\n",
    "**Problem**: What if $p(x)$ is hard to sample from?\n",
    "\n",
    "**Solution**: **Importance Sampling** - sample from an easier distribution $q(x)$:\n",
    "\n",
    "$$\\mathbb{E}_p[f(x)] = \\mathbb{E}_q\\left[f(x) \\frac{p(x)}{q(x)}\\right] \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\frac{p(x_i)}{q(x_i)}, \\quad x_i \\sim q$$\n",
    "\n",
    "The ratio $w_i = p(x_i)/q(x_i)$ is called the **importance weight**.\n",
    "\n",
    "### üìê Mathematical Example\n",
    "\n",
    "Estimate $I = \\int_0^1 e^x dx$ (true value: $e - 1 \\approx 1.718$)\n",
    "\n",
    "Using uniform proposal $q(x) = 1$ on $[0, 1]$:\n",
    "\n",
    "$$\\hat{I} = \\frac{1}{N} \\sum_{i=1}^N e^{x_i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5.1 MONTE CARLO AND IMPORTANCE SAMPLING\n",
    "# ============================================\n",
    "\n",
    "def monte_carlo_estimate(f, n_samples=10000):\n",
    "    \"\"\"Simple Monte Carlo: sample from Uniform[0,1].\"\"\"\n",
    "    x = np.random.uniform(0, 1, n_samples)\n",
    "    return np.mean(f(x))\n",
    "\n",
    "\n",
    "def importance_sampling_estimate(f, p, q_sample, q_pdf, n_samples=10000):\n",
    "    \"\"\"\n",
    "    Importance sampling estimate.\n",
    "    \n",
    "    Args:\n",
    "        f: Function to integrate\n",
    "        p: Target probability density\n",
    "        q_sample: Function to sample from proposal\n",
    "        q_pdf: Proposal probability density\n",
    "    \"\"\"\n",
    "    x = q_sample(n_samples)\n",
    "    weights = p(x) / q_pdf(x)\n",
    "    return np.mean(f(x) * weights)\n",
    "\n",
    "\n",
    "# Example: Integrate e^x from 0 to 1\n",
    "f = lambda x: np.exp(x)\n",
    "true_value = np.e - 1  # ‚âà 1.718\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MONTE CARLO INTEGRATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Integral: ‚à´‚ÇÄ¬π eÀ£ dx\")\n",
    "print(f\"True value: {true_value:.6f}\")\n",
    "\n",
    "# Simple Monte Carlo\n",
    "estimates = [monte_carlo_estimate(f, n) for n in [100, 1000, 10000, 100000]]\n",
    "print(f\"\\nüìä Simple Monte Carlo estimates:\")\n",
    "for n, est in zip([100, 1000, 10000, 100000], estimates):\n",
    "    print(f\"  N={n:6d}: {est:.6f} (error: {abs(est - true_value):.6f})\")\n",
    "\n",
    "print(\"\\n‚úÖ Error decreases as ‚àöN (law of large numbers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Convergence of Monte Carlo\n",
    "np.random.seed(42)\n",
    "n_max = 5000\n",
    "samples = np.random.uniform(0, 1, n_max)\n",
    "cumulative_mean = np.cumsum(np.exp(samples)) / (np.arange(1, n_max + 1))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cumulative_mean, 'b-', alpha=0.7, linewidth=1)\n",
    "plt.axhline(y=true_value, color='r', linestyle='--', linewidth=2, label=f'True value = {true_value:.4f}')\n",
    "plt.fill_between(range(n_max), true_value - 0.1, true_value + 0.1, alpha=0.2, color='red')\n",
    "plt.xlabel('Number of samples', fontsize=12)\n",
    "plt.ylabel('Estimate', fontsize=12)\n",
    "plt.title('Monte Carlo Convergence: ‚à´‚ÇÄ¬π eÀ£ dx', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, n_max)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° The estimate fluctuates but converges to the true value.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Normalizing Flows\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Normalizing Flows** transform a simple distribution (e.g., Gaussian) into a complex one through a series of invertible transformations.\n",
    "\n",
    "Given $z \\sim p_z(z)$ and an invertible function $x = f(z)$, the density of $x$ is:\n",
    "\n",
    "$$p_x(x) = p_z(f^{-1}(x)) \\left| \\det \\frac{\\partial f^{-1}}{\\partial x} \\right| = p_z(z) \\left| \\det \\frac{\\partial f}{\\partial z} \\right|^{-1}$$\n",
    "\n",
    "The key is choosing $f$ such that the **Jacobian determinant** is easy to compute.\n",
    "\n",
    "### üìê Mathematical Example: Affine Flow\n",
    "\n",
    "Simplest flow: $x = az + b$ (scale and shift)\n",
    "\n",
    "Jacobian determinant: $|a|$\n",
    "\n",
    "$$p_x(x) = p_z\\left(\\frac{x-b}{a}\\right) \\cdot \\frac{1}{|a|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 5.2 NORMALIZING FLOWS DEMONSTRATION\n",
    "# ============================================\n",
    "\n",
    "class AffineFlow:\n",
    "    \"\"\"Simple affine flow: x = a*z + b\"\"\"\n",
    "    \n",
    "    def __init__(self, a: float, b: float):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    \n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        return self.a * z + self.b\n",
    "    \n",
    "    def inverse(self, x: np.ndarray) -> np.ndarray:\n",
    "        return (x - self.b) / self.a\n",
    "    \n",
    "    def log_det_jacobian(self) -> float:\n",
    "        return np.log(np.abs(self.a))\n",
    "\n",
    "\n",
    "# Start with standard normal\n",
    "np.random.seed(42)\n",
    "z = np.random.randn(5000)\n",
    "\n",
    "# Apply flow: x = 2z + 3 (scale by 2, shift by 3)\n",
    "flow = AffineFlow(a=2.0, b=3.0)\n",
    "x = flow.forward(z)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"NORMALIZING FLOWS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base distribution: z ~ N(0, 1)\")\n",
    "print(f\"Flow: x = 2z + 3\")\n",
    "print(f\"Resulting distribution: x ~ N(3, 4)\")\n",
    "print(f\"\\nlog|det(J)| = log|2| = {flow.log_det_jacobian():.4f}\")\n",
    "\n",
    "# Verification\n",
    "print(f\"\\nüìä Empirical statistics:\")\n",
    "print(f\"  z: mean={np.mean(z):.3f}, std={np.std(z):.3f}\")\n",
    "print(f\"  x: mean={np.mean(x):.3f}, std={np.std(x):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Base distribution\n",
    "axes[0].hist(z, bins=50, density=True, alpha=0.7, color='blue')\n",
    "x_plot = np.linspace(-4, 4, 100)\n",
    "axes[0].plot(x_plot, stats.norm.pdf(x_plot), 'k-', linewidth=2)\n",
    "axes[0].set_title('Base: z ~ N(0, 1)', fontweight='bold')\n",
    "axes[0].set_xlabel('z')\n",
    "\n",
    "# Flow transformation\n",
    "axes[1].annotate('', xy=(0.9, 0.5), xytext=(0.1, 0.5),\n",
    "                 arrowprops=dict(arrowstyle='->', lw=3, color='green'),\n",
    "                 xycoords='axes fraction')\n",
    "axes[1].text(0.5, 0.6, 'x = 2z + 3', fontsize=16, ha='center', transform=axes[1].transAxes, fontweight='bold')\n",
    "axes[1].text(0.5, 0.4, 'Invertible!', fontsize=12, ha='center', transform=axes[1].transAxes)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Affine Flow', fontweight='bold')\n",
    "\n",
    "# Transformed distribution\n",
    "axes[2].hist(x, bins=50, density=True, alpha=0.7, color='orange')\n",
    "x_plot = np.linspace(-3, 9, 100)\n",
    "axes[2].plot(x_plot, stats.norm.pdf(x_plot, loc=3, scale=2), 'k-', linewidth=2)\n",
    "axes[2].set_title('Result: x ~ N(3, 4)', fontweight='bold')\n",
    "axes[2].set_xlabel('x')\n",
    "\n",
    "plt.suptitle('Normalizing Flow: Transform Simple ‚Üí Complex', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° By stacking many invertible layers, flows can model very complex distributions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 6: Network Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Supervised Random Walks (Facebook Link Prediction)\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Link Prediction**: Given a social network, predict which new edges (friendships) will form.\n",
    "\n",
    "**Supervised Random Walks** (Backstrom & Leskovec, 2011) learn edge weights based on features to bias a random walk toward future friends.\n",
    "\n",
    "**Key idea**: Learn a function $\\text{strength}(u, v) = f_w(\\text{features}(u, v))$ that weights edges.\n",
    "\n",
    "The optimization objective:\n",
    "\n",
    "$$\\min_w \\|w\\|^2 + \\lambda \\sum_{(d, l) \\in D} h(p_l - p_d)$$\n",
    "\n",
    "Where:\n",
    "- $d$: A node that becomes a friend (destination)\n",
    "- $l$: A node that does not (non-link)\n",
    "- $p_d, p_l$: Random walk probabilities of reaching these nodes\n",
    "- $h$: Hinge-like loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 6.1 LINK PREDICTION DEMO\n",
    "# ============================================\n",
    "\n",
    "def compute_common_neighbors(adj: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute common neighbors score for all node pairs.\"\"\"\n",
    "    # CN(u,v) = |N(u) ‚à© N(v)| = (A @ A)[u,v]\n",
    "    return adj @ adj\n",
    "\n",
    "\n",
    "def compute_adamic_adar(adj: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute Adamic-Adar score for all node pairs.\"\"\"\n",
    "    n = adj.shape[0]\n",
    "    degrees = adj.sum(axis=1)\n",
    "    scores = np.zeros((n, n))\n",
    "    \n",
    "    for u in range(n):\n",
    "        for v in range(n):\n",
    "            if u != v:\n",
    "                # Find common neighbors\n",
    "                common = np.where((adj[u] == 1) & (adj[v] == 1))[0]\n",
    "                # Sum 1/log(degree) for each common neighbor\n",
    "                for w in common:\n",
    "                    if degrees[w] > 1:\n",
    "                        scores[u, v] += 1 / np.log(degrees[w])\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "# Create a sample social network\n",
    "# 6 users: edges represent friendships\n",
    "adj = np.array([\n",
    "    [0, 1, 1, 0, 0, 0],  # User 0: friends with 1, 2\n",
    "    [1, 0, 1, 1, 0, 0],  # User 1: friends with 0, 2, 3\n",
    "    [1, 1, 0, 1, 1, 0],  # User 2: friends with 0, 1, 3, 4\n",
    "    [0, 1, 1, 0, 1, 1],  # User 3: friends with 1, 2, 4, 5\n",
    "    [0, 0, 1, 1, 0, 1],  # User 4: friends with 2, 3, 5\n",
    "    [0, 0, 0, 1, 1, 0],  # User 5: friends with 3, 4\n",
    "])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LINK PREDICTION IN SOCIAL NETWORKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute scores\n",
    "cn_scores = compute_common_neighbors(adj)\n",
    "aa_scores = compute_adamic_adar(adj)\n",
    "\n",
    "# Find potential links (pairs that aren't already connected)\n",
    "print(\"\\nüìä Potential new friendships (ranked by Common Neighbors):\")\n",
    "potential_links = []\n",
    "for i in range(6):\n",
    "    for j in range(i+1, 6):\n",
    "        if adj[i, j] == 0:  # Not already friends\n",
    "            potential_links.append((i, j, cn_scores[i, j], aa_scores[i, j]))\n",
    "\n",
    "# Sort by common neighbors\n",
    "potential_links.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(f\"{'Pair':<10} {'Common Neighbors':<18} {'Adamic-Adar':<12}\")\n",
    "print(\"-\" * 40)\n",
    "for i, j, cn, aa in potential_links[:5]:\n",
    "    print(f\"({i}, {j})     {int(cn):<18} {aa:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the network\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Node positions (manually arranged for visualization)\n",
    "pos = {\n",
    "    0: (0, 1),\n",
    "    1: (1, 2),\n",
    "    2: (2, 1),\n",
    "    3: (3, 2),\n",
    "    4: (4, 1),\n",
    "    5: (4, 2.5)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Draw existing edges\n",
    "for i in range(6):\n",
    "    for j in range(i+1, 6):\n",
    "        if adj[i, j] == 1:\n",
    "            plt.plot([pos[i][0], pos[j][0]], [pos[i][1], pos[j][1]], \n",
    "                     'b-', linewidth=2, alpha=0.6)\n",
    "\n",
    "# Draw predicted edge (highest score)\n",
    "best_i, best_j = potential_links[0][0], potential_links[0][1]\n",
    "plt.plot([pos[best_i][0], pos[best_j][0]], [pos[best_i][1], pos[best_j][1]], \n",
    "         'g--', linewidth=3, label='Predicted friendship')\n",
    "\n",
    "# Draw nodes\n",
    "for node, (x, y) in pos.items():\n",
    "    plt.scatter(x, y, c='steelblue', s=800, zorder=5, edgecolors='black', linewidths=2)\n",
    "    plt.annotate(f'User {node}', (x, y), fontsize=10, ha='center', va='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.title('Social Network with Predicted Link', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper left')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ Most likely new friendship: Users {best_i} and {best_j}\")\n",
    "print(f\"   (They have {int(potential_links[0][2])} common friends!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 7: Bayesian Optimization\n",
    "\n",
    "---\n",
    "\n",
    "When the objective function is expensive to evaluate (hyperparameter tuning, drug discovery), gradient-based methods are impractical. **Bayesian Optimization** uses a probabilistic model to guide the search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Gaussian Processes\n",
    "\n",
    "### Theory\n",
    "\n",
    "A **Gaussian Process (GP)** is a distribution over functions:\n",
    "\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
    "\n",
    "Where:\n",
    "- $m(x)$: Mean function\n",
    "- $k(x, x')$: Kernel (covariance) function\n",
    "\n",
    "**Key property**: GP gives both a prediction AND uncertainty!\n",
    "\n",
    "### Squared Exponential (RBF) Kernel\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "Parameters:\n",
    "- $\\sigma^2$: Signal variance (amplitude)\n",
    "- $\\ell$: Length scale (how quickly function varies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7.1 GAUSSIAN PROCESS REGRESSION\n",
    "# ============================================\n",
    "\n",
    "def rbf_kernel(X1: np.ndarray, X2: np.ndarray, \n",
    "               length_scale: float = 1.0, variance: float = 1.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute RBF (squared exponential) kernel matrix.\n",
    "    \"\"\"\n",
    "    X1 = X1.reshape(-1, 1) if X1.ndim == 1 else X1\n",
    "    X2 = X2.reshape(-1, 1) if X2.ndim == 1 else X2\n",
    "    \n",
    "    # Compute squared Euclidean distances\n",
    "    sqdist = np.sum(X1**2, axis=1, keepdims=True) + \\\n",
    "             np.sum(X2**2, axis=1) - 2 * X1 @ X2.T\n",
    "    \n",
    "    return variance * np.exp(-0.5 * sqdist / (length_scale**2))\n",
    "\n",
    "\n",
    "def gp_predict(X_train: np.ndarray, y_train: np.ndarray, \n",
    "               X_test: np.ndarray, length_scale: float = 1.0,\n",
    "               noise: float = 1e-6):\n",
    "    \"\"\"\n",
    "    GP prediction with posterior mean and variance.\n",
    "    \"\"\"\n",
    "    K = rbf_kernel(X_train, X_train, length_scale) + noise * np.eye(len(X_train))\n",
    "    K_star = rbf_kernel(X_train, X_test, length_scale)\n",
    "    K_star_star = rbf_kernel(X_test, X_test, length_scale)\n",
    "    \n",
    "    # Compute posterior\n",
    "    K_inv = np.linalg.inv(K)\n",
    "    mu = K_star.T @ K_inv @ y_train\n",
    "    cov = K_star_star - K_star.T @ K_inv @ K_star\n",
    "    \n",
    "    return mu, np.sqrt(np.diag(cov))\n",
    "\n",
    "\n",
    "# Generate training data\n",
    "np.random.seed(42)\n",
    "X_train = np.array([1, 3, 5, 6, 7])\n",
    "y_train = np.sin(X_train) + 0.1 * np.random.randn(len(X_train))\n",
    "\n",
    "# Test points\n",
    "X_test = np.linspace(0, 10, 100)\n",
    "\n",
    "# GP prediction\n",
    "mu, std = gp_predict(X_train, y_train, X_test, length_scale=1.0)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GAUSSIAN PROCESS REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training points: {len(X_train)}\")\n",
    "print(f\"Test points: {len(X_test)}\")\n",
    "print(f\"\\nGP provides:\")\n",
    "print(f\"  - Mean prediction Œº(x)\")\n",
    "print(f\"  - Uncertainty œÉ(x) at each point!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# True function\n",
    "plt.plot(X_test, np.sin(X_test), 'k--', label='True function: sin(x)', linewidth=2)\n",
    "\n",
    "# GP prediction\n",
    "plt.plot(X_test, mu, 'b-', label='GP Mean prediction', linewidth=2)\n",
    "plt.fill_between(X_test, mu - 2*std, mu + 2*std, alpha=0.3, color='blue', label='95% confidence')\n",
    "\n",
    "# Training points\n",
    "plt.scatter(X_train, y_train, c='red', s=100, zorder=5, label='Training data', edgecolors='black')\n",
    "\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('f(x)', fontsize=12)\n",
    "plt.title('Gaussian Process: Prediction with Uncertainty', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Notice: Uncertainty is LOW near training points, HIGH far from them!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Acquisition Functions\n",
    "\n",
    "### Theory\n",
    "\n",
    "**Acquisition functions** decide where to sample next by balancing:\n",
    "- **Exploitation**: Sample where we expect good values\n",
    "- **Exploration**: Sample where we're uncertain\n",
    "\n",
    "### Expected Improvement (EI)\n",
    "\n",
    "$$\\alpha_{EI}(x) = (\\mu(x) - f^* - \\xi) \\Phi(Z) + \\sigma(x) \\phi(Z)$$\n",
    "\n",
    "Where:\n",
    "- $Z = \\frac{\\mu(x) - f^* - \\xi}{\\sigma(x)}$\n",
    "- $f^*$: Best value observed so far\n",
    "- $\\xi$: Exploration parameter\n",
    "- $\\Phi$, $\\phi$: CDF and PDF of standard normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# 7.2 BAYESIAN OPTIMIZATION WITH EI\n",
    "# ============================================\n",
    "\n",
    "def expected_improvement(mu: np.ndarray, std: np.ndarray, \n",
    "                         f_best: float, xi: float = 0.01) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute Expected Improvement acquisition function.\n",
    "    \"\"\"\n",
    "    std = np.maximum(std, 1e-9)  # Avoid division by zero\n",
    "    Z = (mu - f_best - xi) / std\n",
    "    ei = (mu - f_best - xi) * stats.norm.cdf(Z) + std * stats.norm.pdf(Z)\n",
    "    return ei\n",
    "\n",
    "\n",
    "# Objective function to optimize (expensive black-box function)\n",
    "def objective(x):\n",
    "    return -((x - 2)**2 * np.sin(3*x))\n",
    "\n",
    "\n",
    "# Initial samples\n",
    "X_train = np.array([0.5, 2.0, 4.5])\n",
    "y_train = objective(X_train)\n",
    "f_best = np.max(y_train)\n",
    "\n",
    "# Test points\n",
    "X_test = np.linspace(0, 5, 200)\n",
    "\n",
    "# GP prediction\n",
    "mu, std = gp_predict(X_train, y_train, X_test, length_scale=0.5)\n",
    "\n",
    "# Expected Improvement\n",
    "ei = expected_improvement(mu, std, f_best)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BAYESIAN OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Current best: {f_best:.4f} at x = {X_train[np.argmax(y_train)]:.2f}\")\n",
    "print(f\"Next point to sample: x = {X_test[np.argmax(ei)]:.4f}\")\n",
    "print(f\"Expected Improvement at that point: {np.max(ei):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "# Top: GP surrogate\n",
    "ax1 = axes[0]\n",
    "ax1.plot(X_test, objective(X_test), 'k--', label='True objective', linewidth=2)\n",
    "ax1.plot(X_test, mu, 'b-', label='GP prediction', linewidth=2)\n",
    "ax1.fill_between(X_test, mu - 2*std, mu + 2*std, alpha=0.3, color='blue')\n",
    "ax1.scatter(X_train, y_train, c='red', s=100, zorder=5, label='Observations', edgecolors='black')\n",
    "ax1.axhline(y=f_best, color='green', linestyle=':', label=f'Best so far = {f_best:.2f}')\n",
    "ax1.set_ylabel('f(x)', fontsize=12)\n",
    "ax1.set_title('Gaussian Process Surrogate Model', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom: Acquisition function\n",
    "ax2 = axes[1]\n",
    "ax2.fill_between(X_test, 0, ei, alpha=0.5, color='orange')\n",
    "ax2.plot(X_test, ei, 'orange', linewidth=2)\n",
    "next_x = X_test[np.argmax(ei)]\n",
    "ax2.axvline(x=next_x, color='red', linestyle='--', linewidth=2, label=f'Next sample: x={next_x:.2f}')\n",
    "ax2.set_xlabel('x', fontsize=12)\n",
    "ax2.set_ylabel('Expected Improvement', fontsize=12)\n",
    "ax2.set_title('Acquisition Function (Expected Improvement)', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéØ Bayesian Optimization balances:\")\n",
    "print(\"   - Exploitation: Sample where Œº(x) is high\")\n",
    "print(\"   - Exploration: Sample where œÉ(x) is high (uncertain regions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üìö Conclusion\n",
    "\n",
    "---\n",
    "\n",
    "This notebook has taken you through the **deep mathematical foundations** that power modern machine learning:\n",
    "\n",
    "1. **Linear Algebra**: Vectors, matrices, eigenvalues, SVD ‚Üí PageRank, recommendations\n",
    "2. **Calculus**: Gradients, chain rule, Jacobian ‚Üí Backpropagation, Transformers\n",
    "3. **Optimization**: Lagrange, convex, ADMM ‚Üí SVMs, industrial-scale systems\n",
    "4. **Probability**: Entropy, KL, Bayes ‚Üí VAEs, generative models\n",
    "5. **Advanced Methods**: Monte Carlo, flows ‚Üí Sampling, density estimation\n",
    "6. **Network Analysis**: Random walks ‚Üí Link prediction\n",
    "7. **Bayesian Optimization**: GPs, acquisition ‚Üí Hyperparameter tuning\n",
    "\n",
    "> **The Key Insight**: Mathematics is not just a prerequisite‚Äîit is the *language* in which ML discoveries are written. Mastering these foundations enables you to:\n",
    "> - Debug models effectively\n",
    "> - Design novel architectures\n",
    "> - Understand cutting-edge research papers\n",
    "> - Build production-grade ML systems\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**: Apply these concepts to real datasets and implement more complex models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}