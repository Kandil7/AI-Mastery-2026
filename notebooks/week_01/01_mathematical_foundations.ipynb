{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 01: Mathematical Foundations for ML\n",
                "\n",
                "This notebook covers the essential mathematical concepts for machine learning:\n",
                "- Linear Algebra (vectors, matrices, eigenvalues)\n",
                "- Calculus (gradients, chain rule)\n",
                "- Probability (distributions, Bayes theorem)\n",
                "\n",
                "## Learning Objectives\n",
                "After completing this notebook, you will:\n",
                "1. Implement vector/matrix operations from scratch\n",
                "2. Understand gradient computation\n",
                "3. Apply probability distributions to ML problems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import List, Tuple, Callable\n",
                "\n",
                "# Type aliases\n",
                "Vector = np.ndarray\n",
                "Matrix = np.ndarray"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Linear Algebra Foundations\n",
                "\n",
                "### 1.1 Vector Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dot_product(v1: Vector, v2: Vector) -> float:\n",
                "    \"\"\"\n",
                "    Compute dot product of two vectors.\n",
                "    \n",
                "    Mathematical definition:\n",
                "    v1 · v2 = Σ(v1[i] * v2[i]) for i in range(n)\n",
                "    \n",
                "    Properties:\n",
                "    - Commutative: v1 · v2 = v2 · v1\n",
                "    - Distributive: a · (b + c) = a·b + a·c\n",
                "    \"\"\"\n",
                "    if len(v1) != len(v2):\n",
                "        raise ValueError(\"Vectors must have same length\")\n",
                "    \n",
                "    result = 0.0\n",
                "    for i in range(len(v1)):\n",
                "        result += v1[i] * v2[i]\n",
                "    return result\n",
                "\n",
                "\n",
                "def vector_norm(v: Vector, p: int = 2) -> float:\n",
                "    \"\"\"\n",
                "    Compute Lp norm of a vector.\n",
                "    \n",
                "    L1 norm: ||v||_1 = Σ|v[i]|\n",
                "    L2 norm: ||v||_2 = √(Σv[i]²)\n",
                "    L∞ norm: ||v||_∞ = max|v[i]|\n",
                "    \"\"\"\n",
                "    if p == 1:\n",
                "        return np.sum(np.abs(v))\n",
                "    elif p == 2:\n",
                "        return np.sqrt(np.sum(v ** 2))\n",
                "    elif p == np.inf:\n",
                "        return np.max(np.abs(v))\n",
                "    else:\n",
                "        return np.sum(np.abs(v) ** p) ** (1/p)\n",
                "\n",
                "\n",
                "def cosine_similarity(v1: Vector, v2: Vector) -> float:\n",
                "    \"\"\"\n",
                "    Cosine similarity: measures angle between vectors.\n",
                "    \n",
                "    cos(θ) = (v1 · v2) / (||v1|| * ||v2||)\n",
                "    \n",
                "    Range: [-1, 1]\n",
                "    \"\"\"\n",
                "    dot = dot_product(v1, v2)\n",
                "    norm1 = vector_norm(v1)\n",
                "    norm2 = vector_norm(v2)\n",
                "    return dot / (norm1 * norm2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test vector operations\n",
                "v1 = np.array([1.0, 2.0, 3.0])\n",
                "v2 = np.array([4.0, 5.0, 6.0])\n",
                "\n",
                "print(f\"Dot product: {dot_product(v1, v2)}\")\n",
                "print(f\"L2 norm of v1: {vector_norm(v1):.4f}\")\n",
                "print(f\"Cosine similarity: {cosine_similarity(v1, v2):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Matrix Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def matrix_multiply(A: Matrix, B: Matrix) -> Matrix:\n",
                "    \"\"\"\n",
                "    Matrix multiplication from scratch.\n",
                "    \n",
                "    C[i,j] = Σ(A[i,k] * B[k,j]) for k in range(n)\n",
                "    \n",
                "    Requirements:\n",
                "    - A: (m, n) matrix\n",
                "    - B: (n, p) matrix\n",
                "    - Result: (m, p) matrix\n",
                "    \"\"\"\n",
                "    m, n = A.shape\n",
                "    n2, p = B.shape\n",
                "    \n",
                "    if n != n2:\n",
                "        raise ValueError(f\"Matrix dimensions don't match: {A.shape} vs {B.shape}\")\n",
                "    \n",
                "    C = np.zeros((m, p))\n",
                "    for i in range(m):\n",
                "        for j in range(p):\n",
                "            for k in range(n):\n",
                "                C[i, j] += A[i, k] * B[k, j]\n",
                "    return C\n",
                "\n",
                "\n",
                "def matrix_transpose(A: Matrix) -> Matrix:\n",
                "    \"\"\"Transpose: swap rows and columns.\"\"\"\n",
                "    m, n = A.shape\n",
                "    result = np.zeros((n, m))\n",
                "    for i in range(m):\n",
                "        for j in range(n):\n",
                "            result[j, i] = A[i, j]\n",
                "    return result"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test matrix operations\n",
                "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
                "B = np.array([[7, 8, 9], [10, 11, 12]])\n",
                "\n",
                "print(\"A @ B =\")\n",
                "print(matrix_multiply(A, B))\n",
                "print(\"\\nA.T =\")\n",
                "print(matrix_transpose(A))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Eigenvalues and Eigenvectors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def power_iteration(A: Matrix, num_iterations: int = 100) -> Tuple[float, Vector]:\n",
                "    \"\"\"\n",
                "    Find dominant eigenvalue and eigenvector using power iteration.\n",
                "    \n",
                "    Algorithm:\n",
                "    1. Start with random vector v\n",
                "    2. Multiply: v = A @ v\n",
                "    3. Normalize: v = v / ||v||\n",
                "    4. Repeat until convergence\n",
                "    \"\"\"\n",
                "    n = A.shape[0]\n",
                "    v = np.random.randn(n)\n",
                "    v = v / np.linalg.norm(v)\n",
                "    \n",
                "    for _ in range(num_iterations):\n",
                "        w = A @ v\n",
                "        v = w / np.linalg.norm(w)\n",
                "    \n",
                "    # Rayleigh quotient for eigenvalue\n",
                "    eigenvalue = (v.T @ A @ v) / (v.T @ v)\n",
                "    \n",
                "    return eigenvalue, v"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test eigenvalue computation\n",
                "A = np.array([[4.0, 2.0], [2.0, 3.0]])\n",
                "eigenvalue, eigenvector = power_iteration(A)\n",
                "\n",
                "print(f\"Dominant eigenvalue: {eigenvalue:.4f}\")\n",
                "print(f\"Corresponding eigenvector: {eigenvector}\")\n",
                "print(f\"\\nVerification:\")\n",
                "print(f\"A @ v: {A @ eigenvector}\")\n",
                "print(f\"λ * v: {eigenvalue * eigenvector}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Calculus for Machine Learning\n",
                "\n",
                "### 2.1 Numerical Gradients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def numerical_gradient(f: Callable, x: Vector, h: float = 1e-5) -> Vector:\n",
                "    \"\"\"\n",
                "    Compute gradient using central difference.\n",
                "    \n",
                "    ∂f/∂x_i ≈ (f(x + h*e_i) - f(x - h*e_i)) / (2h)\n",
                "    \"\"\"\n",
                "    grad = np.zeros_like(x)\n",
                "    \n",
                "    for i in range(len(x)):\n",
                "        x_plus = x.copy()\n",
                "        x_minus = x.copy()\n",
                "        x_plus[i] += h\n",
                "        x_minus[i] -= h\n",
                "        \n",
                "        grad[i] = (f(x_plus) - f(x_minus)) / (2 * h)\n",
                "    \n",
                "    return grad"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test: gradient of f(x,y) = x² + 2xy + y²\n",
                "def f(x):\n",
                "    return x[0]**2 + 2*x[0]*x[1] + x[1]**2\n",
                "\n",
                "point = np.array([1.0, 2.0])\n",
                "grad = numerical_gradient(f, point)\n",
                "print(f\"Numerical gradient at {point}: {grad}\")\n",
                "\n",
                "# Analytical gradient: [2x + 2y, 2x + 2y]\n",
                "analytical = np.array([2*1 + 2*2, 2*1 + 2*2])\n",
                "print(f\"Analytical gradient: {analytical}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Chain Rule"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def chain_rule_example():\n",
                "    \"\"\"\n",
                "    Demonstrate chain rule: d/dx[f(g(x))] = f'(g(x)) * g'(x)\n",
                "    \n",
                "    Example: z = (x² + y²)³\n",
                "    Let u = x² + y², then z = u³\n",
                "    \n",
                "    ∂z/∂x = ∂z/∂u * ∂u/∂x = 3u² * 2x = 6x(x² + y²)²\n",
                "    \"\"\"\n",
                "    x, y = 2.0, 3.0\n",
                "    \n",
                "    # Forward pass\n",
                "    u = x**2 + y**2  # u = 13\n",
                "    z = u**3         # z = 2197\n",
                "    \n",
                "    # Backward pass (chain rule)\n",
                "    dz_du = 3 * u**2    # = 3 * 169 = 507\n",
                "    du_dx = 2 * x       # = 4\n",
                "    du_dy = 2 * y       # = 6\n",
                "    \n",
                "    dz_dx = dz_du * du_dx  # = 507 * 4 = 2028\n",
                "    dz_dy = dz_du * du_dy  # = 507 * 6 = 3042\n",
                "    \n",
                "    print(f\"z = (x² + y²)³ at x={x}, y={y}\")\n",
                "    print(f\"∂z/∂x = {dz_dx}\")\n",
                "    print(f\"∂z/∂y = {dz_dy}\")\n",
                "\n",
                "chain_rule_example()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Probability Foundations\n",
                "\n",
                "### 3.1 Probability Distributions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Gaussian:\n",
                "    \"\"\"\n",
                "    Gaussian (Normal) distribution.\n",
                "    \n",
                "    PDF: f(x) = (1/√(2πσ²)) * exp(-(x-μ)²/(2σ²))\n",
                "    \"\"\"\n",
                "    def __init__(self, mean: float = 0.0, std: float = 1.0):\n",
                "        self.mean = mean\n",
                "        self.std = std\n",
                "        self.variance = std ** 2\n",
                "    \n",
                "    def pdf(self, x: np.ndarray) -> np.ndarray:\n",
                "        \"\"\"Probability density function.\"\"\"\n",
                "        coef = 1 / np.sqrt(2 * np.pi * self.variance)\n",
                "        exp_term = np.exp(-((x - self.mean) ** 2) / (2 * self.variance))\n",
                "        return coef * exp_term\n",
                "    \n",
                "    def sample(self, n: int = 1) -> np.ndarray:\n",
                "        \"\"\"Generate samples using Box-Muller transform.\"\"\"\n",
                "        u1 = np.random.uniform(0, 1, n)\n",
                "        u2 = np.random.uniform(0, 1, n)\n",
                "        z = np.sqrt(-2 * np.log(u1)) * np.cos(2 * np.pi * u2)\n",
                "        return self.mean + self.std * z"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Gaussian distribution\n",
                "dist = Gaussian(mean=5, std=2)\n",
                "samples = dist.sample(1000)\n",
                "print(f\"Sample mean: {np.mean(samples):.2f} (expected: 5)\")\n",
                "print(f\"Sample std: {np.std(samples):.2f} (expected: 2)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3.2 Bayes Theorem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def bayes_theorem(prior: float, likelihood: float, evidence: float) -> float:\n",
                "    \"\"\"\n",
                "    Bayes' Theorem: P(A|B) = P(B|A) * P(A) / P(B)\n",
                "    \n",
                "    Args:\n",
                "        prior: P(A) - prior probability\n",
                "        likelihood: P(B|A) - probability of evidence given hypothesis\n",
                "        evidence: P(B) - total probability of evidence\n",
                "    \n",
                "    Returns:\n",
                "        posterior: P(A|B) - updated probability after evidence\n",
                "    \"\"\"\n",
                "    return (likelihood * prior) / evidence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Medical test\n",
                "prior_disease = 0.01  # 1% of population has disease\n",
                "sensitivity = 0.99    # 99% true positive rate\n",
                "false_positive = 0.05 # 5% false positive rate\n",
                "\n",
                "# P(positive) = P(positive|disease)*P(disease) + P(positive|no disease)*P(no disease)\n",
                "p_positive = sensitivity * prior_disease + false_positive * (1 - prior_disease)\n",
                "\n",
                "posterior = bayes_theorem(prior_disease, sensitivity, p_positive)\n",
                "\n",
                "print(\"Medical Test Example:\")\n",
                "print(f\"Prior P(disease) = {prior_disease}\")\n",
                "print(f\"P(positive|disease) = {sensitivity}\")\n",
                "print(f\"P(positive|no disease) = {false_positive}\")\n",
                "print(f\"\\nPosterior P(disease|positive) = {posterior:.4f}\")\n",
                "print(f\"Even with positive test, only {posterior*100:.1f}% chance of disease!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Exercises\n",
                "\n",
                "### Exercise 1: Implement PCA from Scratch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def pca_scratch(X: Matrix, n_components: int = 2) -> Tuple[Matrix, Vector]:\n",
                "    \"\"\"\n",
                "    Implement Principal Component Analysis.\n",
                "    \n",
                "    Steps:\n",
                "    1. Center the data (subtract mean)\n",
                "    2. Compute covariance matrix\n",
                "    3. Find eigenvalues and eigenvectors\n",
                "    4. Project data onto top eigenvectors\n",
                "    \"\"\"\n",
                "    # 1. Center data\n",
                "    X_centered = X - np.mean(X, axis=0)\n",
                "    \n",
                "    # 2. Covariance matrix\n",
                "    cov_matrix = (X_centered.T @ X_centered) / (X.shape[0] - 1)\n",
                "    \n",
                "    # 3. Eigendecomposition\n",
                "    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
                "    \n",
                "    # 4. Sort by eigenvalue (descending)\n",
                "    idx = np.argsort(eigenvalues)[::-1]\n",
                "    eigenvalues = eigenvalues[idx]\n",
                "    eigenvectors = eigenvectors[:, idx]\n",
                "    \n",
                "    # 5. Project\n",
                "    components = eigenvectors[:, :n_components]\n",
                "    transformed = X_centered @ components\n",
                "    \n",
                "    # Explained variance\n",
                "    explained_variance = eigenvalues[:n_components] / np.sum(eigenvalues)\n",
                "    \n",
                "    return transformed, explained_variance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test PCA\n",
                "np.random.seed(42)\n",
                "X = np.random.randn(100, 5)\n",
                "X_pca, var_ratio = pca_scratch(X, n_components=2)\n",
                "print(f\"Transformed shape: {X_pca.shape}\")\n",
                "print(f\"Explained variance ratio: {var_ratio}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Gradient Descent Implementation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def gradient_descent(\n",
                "    f: Callable, \n",
                "    grad_f: Callable, \n",
                "    x0: Vector, \n",
                "    learning_rate: float = 0.01,\n",
                "    max_iters: int = 1000\n",
                ") -> Tuple[Vector, List]:\n",
                "    \"\"\"\n",
                "    Implement vanilla gradient descent.\n",
                "    \n",
                "    Returns:\n",
                "        optimal_x, loss_history\n",
                "    \"\"\"\n",
                "    x = x0.copy()\n",
                "    history = []\n",
                "    \n",
                "    for _ in range(max_iters):\n",
                "        loss = f(x)\n",
                "        history.append(loss)\n",
                "        x = x - learning_rate * grad_f(x)\n",
                "    \n",
                "    return x, history"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test on quadratic: f(x) = (x-3)² + (y-4)²\n",
                "def f(x): \n",
                "    return (x[0] - 3)**2 + (x[1] - 4)**2\n",
                "\n",
                "def grad_f(x): \n",
                "    return np.array([2*(x[0] - 3), 2*(x[1] - 4)])\n",
                "\n",
                "x_opt, history = gradient_descent(f, grad_f, np.array([0.0, 0.0]))\n",
                "print(f\"Optimal x: {x_opt}\")  # Should be close to [3, 4]\n",
                "print(f\"Final loss: {history[-1]:.6f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "| Topic | Key Concept | ML Application |\n",
                "|-------|-------------|----------------|\n",
                "| Dot Product | Similarity measure | Attention, embeddings |\n",
                "| Matrix Multiply | Linear transform | Neural network layers |\n",
                "| Eigenvalues | Principal directions | PCA, spectral methods |\n",
                "| Gradient | Direction of steepest ascent | Optimization |\n",
                "| Chain Rule | Composite derivatives | Backpropagation |\n",
                "| Bayes Theorem | Updating beliefs | Bayesian inference |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}