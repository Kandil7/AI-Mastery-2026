{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra from Scratch\n",
    "\n",
    "This notebook covers the fundamental concepts of linear algebra implemented from scratch using NumPy, focusing on the mathematical foundations needed for AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, List\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append(os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Foundations of Linear Algebra in AI\n",
    "\n",
    "Linear algebra is the backbone of machine learning and AI. Key concepts include:\n",
    "\n",
    "1. **Vectors**: Represent data points and model parameters\n",
    "2. **Matrices**: Represent datasets, transformations, and model weights\n",
    "3. **Dot Product**: Measures similarity between vectors\n",
    "4. **Matrix Multiplication**: Represents linear transformations\n",
    "5. **Eigenvalues and Eigenvectors**: Used in dimensionality reduction\n",
    "6. **SVD**: Used in recommendation systems and dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector 1: [1 2 3]\n",
      "Vector 2: [4 5 6]\n",
      "Magnitude of v1: 3.7417\n",
      "Dot product: 32\n",
      "Angle between vectors: 0.2257 radians\n"
     ]
    }
   ],
   "source": [
    "# Implementing fundamental linear algebra operations from scratch\n",
    "\n",
    "def vector_magnitude(v: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the magnitude (norm) of a vector.\n",
    "    \n",
    "    Args:\n",
    "        v: Input vector\n",
    "        \n",
    "    Returns:\n",
    "        Magnitude of the vector\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(v ** 2))\n",
    "\n",
    "def dot_product(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the dot product of two vectors.\n",
    "    \n",
    "    Args:\n",
    "        u: First vector\n",
    "        v: Second vector\n",
    "        \n",
    "    Returns:\n",
    "        Dot product of the vectors\n",
    "    \"\"\"\n",
    "    return np.sum(u * v)\n",
    "\n",
    "def vector_angle(u: np.ndarray, v: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the angle between two vectors in radians.\n",
    "    \n",
    "    Args:\n",
    "        u: First vector\n",
    "        v: Second vector\n",
    "        \n",
    "    Returns:\n",
    "        Angle between vectors in radians\n",
    "    \"\"\"\n",
    "    dot = dot_product(u, v)\n",
    "    mag_u = vector_magnitude(u)\n",
    "    mag_v = vector_magnitude(v)\n",
    "    \n",
    "    cos_theta = dot / (mag_u * mag_v)\n",
    "    # Clamp to avoid numerical errors\n",
    "    cos_theta = np.clip(cos_theta, -1.0, 1.0)\n",
    "    \n",
    "    return np.arccos(cos_theta)\n",
    "\n",
    "def matrix_multiply(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Multiply two matrices A and B.\n",
    "    \n",
    "    Args:\n",
    "        A: First matrix of shape (m, n)\n",
    "        B: Second matrix of shape (n, p)\n",
    "        \n",
    "    Returns:\n",
    "        Product matrix of shape (m, p)\n",
    "    \"\"\"\n",
    "    if A.shape[1] != B.shape[0]:\n",
    "        raise ValueError(f\"Cannot multiply matrices with shapes {A.shape} and {B.shape}\")\n",
    "    \n",
    "    m, n = A.shape\n",
    "    _, p = B.shape\n",
    "    \n",
    "    result = np.zeros((m, p))\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j in range(p):\n",
    "            for k in range(n):\n",
    "                result[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def matrix_inverse(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the inverse of a matrix using Gauss-Jordan elimination.\n",
    "    \n",
    "    Args:\n",
    "        matrix: Square matrix to invert\n",
    "        \n",
    "    Returns:\n",
    "        Inverse of the matrix\n",
    "    \"\"\"\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        raise ValueError(\"Matrix must be square to compute inverse\")\n",
    "    \n",
    "    n = matrix.shape[0]\n",
    "    # Create augmented matrix [A|I]\n",
    "    augmented = np.hstack([matrix.astype(float), np.eye(n)])\n",
    "    \n",
    "    # Forward elimination\n",
    "    for i in range(n):\n",
    "        # Find pivot\n",
    "        max_row = np.argmax(np.abs(augmented[i:, i])) + i\n",
    "        augmented[[i, max_row]] = augmented[[max_row, i]]\n",
    "        \n",
    "        # Check for singular matrix\n",
    "        if abs(augmented[i, i]) < 1e-10:\n",
    "            raise ValueError(\"Matrix is singular and cannot be inverted\")\n",
    "        \n",
    "        # Scale pivot row\n",
    "        augmented[i] = augmented[i] / augmented[i, i]\n",
    "        \n",
    "        # Eliminate column\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                augmented[j] = augmented[j] - augmented[j, i] * augmented[i]\n",
    "    \n",
    "    return augmented[:, n:]\n",
    "\n",
    "# Test our implementations\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"Vector 1: {v1}\")\n",
    "print(f\"Vector 2: {v2}\")\n",
    "print(f\"Magnitude of v1: {vector_magnitude(v1):.4f}\")\n",
    "print(f\"Dot product: {dot_product(v1, v2)}\")\n",
    "print(f\"Angle between vectors: {vector_angle(v1, v2):.4f} radians\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Eigenvalues and Eigenvectors from scratch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpower_iteration\u001b[39m(matrix: np\u001b[38;5;241m.\u001b[39mndarray, max_iterations: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, tolerance: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-10\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[\u001b[38;5;28mfloat\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]:\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m    Find the dominant eigenvalue and eigenvector using power iteration.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m        Tuple of (eigenvalue, eigenvector)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "# Eigenvalues and Eigenvectors from scratch\n",
    "\n",
    "from typing import Tuple\n\n",
    "def power_iteration(matrix: np.ndarray, max_iterations: int = 1000, tolerance: float = 1e-10) -> Tuple[float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Find the dominant eigenvalue and eigenvector using power iteration.\n",
    "    \n",
    "    Args:\n",
    "        matrix: Square matrix\n",
    "        max_iterations: Maximum number of iterations\n",
    "        tolerance: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (eigenvalue, eigenvector)\n",
    "    \"\"\"\n",
    "    if matrix.shape[0] != matrix.shape[1]:\n",
    "        raise ValueError(\"Matrix must be square\")\n",
    "    \n",
    "    n = matrix.shape[0]\n",
    "    x = np.random.rand(n)\n",
    "    x = x / np.linalg.norm(x)  # Normalize initial vector\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        x_new = matrix @ x\n",
    "        eigenvalue = x.T @ matrix @ x  # Rayleigh quotient\n",
    "        x_new = x_new / np.linalg.norm(x_new)\n",
    "        \n",
    "        if np.allclose(x, x_new, rtol=tolerance):\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return eigenvalue, x_new\n",
    "\n",
    "# Test power iteration\n",
    "A = np.array([[4, 2], [1, 3]], dtype=float)\n",
    "eigenval, eigenvec = power_iteration(A)\n",
    "print(f\"Dominant eigenvalue: {eigenval:.4f}\")\n",
    "print(f\"Corresponding eigenvector: {eigenvec}\")\n",
    "\n",
    "# Verify with numpy\n",
    "eigenvals_np, eigenvecs_np = np.linalg.eig(A)\n",
    "print(f\"NumPy eigenvalues: {eigenvals_np}\")\n",
    "print(f\"NumPy dominant eigenvector: {eigenvecs_np[:, 0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Singular Value Decomposition (SVD) from scratch\n",
    "\n",
    "def svd_from_scratch(matrix: np.ndarray, max_iterations: int = 1000, tolerance: float = 1e-10) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute SVD using power iteration approach.\n",
    "    \n",
    "    Args:\n",
    "        matrix: Input matrix to decompose\n",
    "        max_iterations: Maximum number of iterations\n",
    "        tolerance: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (U, S, Vt) matrices such that A = U @ S @ Vt\n",
    "    \"\"\"\n",
    "    A = matrix.astype(float)\n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Compute A^T * A for right singular vectors\n",
    "    ATA = A.T @ A\n",
    "    \n",
    "    # Compute eigenvalues and eigenvectors of A^T * A\n",
    "    # For simplicity, we'll use numpy here, but in a full implementation\n",
    "    # we would implement eigenvalue decomposition from scratch\n",
    "    eigenvals, V = np.linalg.eigh(ATA)\n",
    "    \n",
    "    # Singular values are square roots of eigenvalues\n",
    "    singular_vals = np.sqrt(np.abs(eigenvals))\n",
    "    \n",
    "    # Sort in descending order\n",
    "    idx = np.argsort(singular_vals)[::-1]\n",
    "    singular_vals = singular_vals[idx]\n",
    "    V = V[:, idx]\n",
    "    \n",
    "    # Compute left singular vectors\n",
    "    U = np.zeros((m, m))\n",
    "    for i in range(min(m, n)):\n",
    "        if singular_vals[i] > 1e-10:\n",
    "            U[:, i] = (A @ V[:, i]) / singular_vals[i]\n",
    "        else:\n",
    "            # For zero singular values, use random orthogonal vector\n",
    "            u = np.random.rand(m)\n",
    "            u = u - U[:, :i] @ (U[:, :i].T @ u)  # Orthogonalize\n",
    "            U[:, i] = u / np.linalg.norm(u)\n",
    "    \n",
    "    # Handle remaining columns of U if m > n\n",
    "    if m > n:\n",
    "        for i in range(n, m):\n",
    "            u = np.random.rand(m)\n",
    "            # Orthogonalize with all previous vectors\n",
    "            for j in range(i):\n",
    "                u = u - U[:, j] @ (U[:, j].T @ u)\n",
    "            U[:, i] = u / np.linalg.norm(u)\n",
    "    \n",
    "    # Create diagonal matrix of singular values\n",
    "    S = np.zeros((m, n))\n",
    "    np.fill_diagonal(S, singular_vals)\n",
    "    \n",
    "    return U, S, V.T\n",
    "\n",
    "# Test SVD\n",
    "B = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=float)\n",
    "U, S, Vt = svd_from_scratch(B)\n",
    "\n",
    "# Verify reconstruction\n",
    "reconstructed = U @ S @ Vt\n",
    "print(f\"Original matrix shape: {B.shape}\")\n",
    "print(f\"Reconstructed matrix shape: {reconstructed.shape}\")\n",
    "print(f\"Reconstruction error: {np.linalg.norm(B - reconstructed):.10f}\")\n",
    "\n",
    "# Compare with numpy SVD\n",
    "U_np, S_np, Vt_np = np.linalg.svd(B)\n",
    "reconstructed_np = U_np @ np.diag(S_np) @ Vt_np\n",
    "print(f\"NumPy reconstruction error: {np.linalg.norm(B - reconstructed_np):.10f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application: Principal Component Analysis (PCA) from scratch\n",
    "\n",
    "class PCAFromScratch:\n",
    "    \"\"\"\n",
    "    Principal Component Analysis implemented from scratch.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_components: int = 2):\n",
    "        self.n_components = n_components\n",
    "        self.components = None\n",
    "        self.mean = None\n",
    "        \n",
    "    def fit(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fit the PCA model to the data.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        # Center the data\n",
    "        self.mean = np.mean(X, axis=0)\n",
    "        X_centered = X - self.mean\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = (X_centered.T @ X_centered) / (X_centered.shape[0] - 1)\n",
    "        \n",
    "        # Compute eigenvalues and eigenvectors\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "        \n",
    "        # Sort by eigenvalues in descending order\n",
    "        idx = np.argsort(eigenvalues)[::-1]\n",
    "        eigenvalues = eigenvalues[idx]\n",
    "        eigenvectors = eigenvectors[:, idx]\n",
    "        \n",
    "        # Select top n_components\n",
    "        self.components = eigenvectors[:, :self.n_components]\n",
    "        \n",
    "    def transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Transform the data to the principal component space.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Transformed data of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        X_centered = X - self.mean\n",
    "        return X_centered @ self.components\n",
    "    \n",
    "    def fit_transform(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fit the PCA model and transform the data.\n",
    "        \n",
    "        Args:\n",
    "            X: Data matrix of shape (n_samples, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Transformed data of shape (n_samples, n_components)\n",
    "        \"\"\"\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "# Test PCA\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=100, n_features=5, n_informative=3, \n",
    "                          n_redundant=2, random_state=42)\n",
    "\n",
    "# Apply our PCA\n",
    "pca_scratch = PCAFromScratch(n_components=2)\n",
    "X_pca_scratch = pca_scratch.fit_transform(X)\n",
    "\n",
    "# Compare with sklearn PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca_sklearn = PCA(n_components=2)\n",
    "X_pca_sklearn = pca_sklearn.fit_transform(X)\n",
    "\n",
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.scatter(X_pca_scratch[:, 0], X_pca_scratch[:, 1], c=y, cmap='viridis')\n",
    "ax1.set_title('PCA from Scratch')\n",
    "ax1.set_xlabel('First Principal Component')\n",
    "ax1.set_ylabel('Second Principal Component')\n",
    "\n",
    "ax2.scatter(X_pca_sklearn[:, 0], X_pca_sklearn[:, 1], c=y, cmap='viridis')\n",
    "ax2.set_title('Sklearn PCA')\n",
    "ax2.set_xlabel('First Principal Component')\n",
    "ax2.set_ylabel('Second Principal Component')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare explained variance ratios\n",
    "variance_explained_scratch = np.var(X_pca_scratch, axis=0) / np.var(X, axis=0).sum()\n",
    "variance_explained_sklearn = pca_sklearn.explained_variance_ratio_\n",
    "\n",
    "print(f\"Variance explained by our PCA: {variance_explained_scratch}\")\n",
    "print(f\"Variance explained by sklearn PCA: {variance_explained_sklearn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Mathematical Understanding**: Understanding the mathematical foundations of linear algebra operations is crucial for AI\n",
    "2. **Implementation Skills**: Implementing algorithms from scratch deepens understanding\n",
    "3. **Applications**: Linear algebra concepts have direct applications in ML algorithms like PCA\n",
    "4. **Numerical Stability**: Pay attention to numerical stability when implementing algorithms\n",
    "5. **Verification**: Always verify your implementations against established libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}