{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 06_07: Transformers & Attention\n",
                "\n",
                "Building a Transformer from scratch (The \"Attention Is All You Need\" architecture).\n",
                "\n",
                "## Learning Objectives\n",
                "1. Implement Self-Attention Mechanism\n",
                "2. Build Multi-Head Attention\n",
                "3. Create Encoder and Decoder blocks\n",
                "4. Assemble full Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import math\n",
                "\n",
                "def softmax(x: np.ndarray) -> np.ndarray:\n",
                "    # Numerical stability\n",
                "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
                "    return e_x / np.sum(e_x, axis=-1, keepdims=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Self-Attention Mechanism\n",
                "\n",
                "Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
                "    \"\"\"\n",
                "    Compute 'Scaled Dot Product Attention'.\n",
                "    \n",
                "    Args:\n",
                "        Q: Queries (batch, num_heads, seq_len, d_k)\n",
                "        K: Keys    (batch, num_heads, seq_len, d_k)\n",
                "        V: Values  (batch, num_heads, seq_len, d_v)\n",
                "        mask: Optional mask (batch, 1, 1, seq_len)\n",
                "    \"\"\"\n",
                "    d_k = Q.shape[-1]\n",
                "    \n",
                "    # 1. Dot product Q @ K.T\n",
                "    # Transpose K to (..., d_k, seq_len)\n",
                "    scores = np.matmul(Q, np.swapaxes(K, -1, -2)) / math.sqrt(d_k)\n",
                "    \n",
                "    # 2. Apply mask (optional)\n",
                "    if mask is not None:\n",
                "        scores += (mask * -1e9)\n",
                "    \n",
                "    # 3. Softmax\n",
                "    attention_weights = softmax(scores)\n",
                "    \n",
                "    # 4. Multiply by V\n",
                "    output = np.matmul(attention_weights, V)\n",
                "    \n",
                "    return output, attention_weights"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Attention\n",
                "d_model = 64\n",
                "seq_len = 10\n",
                "batch = 2\n",
                "\n",
                "Q = np.random.randn(batch, 1, seq_len, d_model)\n",
                "K = np.random.randn(batch, 1, seq_len, d_model)\n",
                "V = np.random.randn(batch, 1, seq_len, d_model)\n",
                "\n",
                "out, weights = scaled_dot_product_attention(Q, K, V)\n",
                "print(f\"Attention Output: {out.shape}\")\n",
                "print(f\"Attention Weights: {weights.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Multi-Head Attention"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MultiHeadAttention:\n",
                "    def __init__(self, d_model: int, num_heads: int):\n",
                "        assert d_model % num_heads == 0\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        \n",
                "        # Linear projections\n",
                "        self.W_q = np.random.randn(d_model, d_model)\n",
                "        self.W_k = np.random.randn(d_model, d_model)\n",
                "        self.W_v = np.random.randn(d_model, d_model)\n",
                "        self.W_o = np.random.randn(d_model, d_model)\n",
                "    \n",
                "    def split_heads(self, x):\n",
                "        batch_size = x.shape[0]\n",
                "        # Reshape to (batch, seq_len, num_heads, d_k)\n",
                "        x = x.reshape(batch_size, -1, self.num_heads, self.d_k)\n",
                "        # Transpose to (batch, num_heads, seq_len, d_k)\n",
                "        return np.transpose(x, (0, 2, 1, 3))\n",
                "    \n",
                "    def combine_heads(self, x):\n",
                "        batch_size = x.shape[0]\n",
                "        # Transpose to (batch, seq_len, num_heads, d_k)\n",
                "        x = np.transpose(x, (0, 2, 1, 3))\n",
                "        # Reshape to (batch, seq_len, d_model)\n",
                "        return x.reshape(batch_size, -1, self.d_model)\n",
                "    \n",
                "    def forward(self, q, k, v, mask=None):\n",
                "        # 1. Linear projections\n",
                "        qs = self.split_heads(q @ self.W_q)\n",
                "        ks = self.split_heads(k @ self.W_k)\n",
                "        vs = self.split_heads(v @ self.W_v)\n",
                "        \n",
                "        # 2. Scaled Dot-Product Attention\n",
                "        attn_out, weights = scaled_dot_product_attention(qs, ks, vs, mask)\n",
                "        \n",
                "        # 3. Combine heads + final linear\n",
                "        output = self.combine_heads(attn_out)\n",
                "        return output @ self.W_o"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Multi-Head Attention\n",
                "mha = MultiHeadAttention(d_model=64, num_heads=8)\n",
                "x = np.random.randn(2, 10, 64)  # (batch, seq, d_model)\n",
                "out = mha.forward(x, x, x)\n",
                "print(f\"MHA Output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Positional Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_positional_encoding(seq_len, d_model, n=10000):\n",
                "    pe = np.zeros((seq_len, d_model))\n",
                "    for k in range(seq_len):\n",
                "        for i in range(d_model // 2):\n",
                "            theta = k / (n ** ((2*i)/d_model))\n",
                "            pe[k, 2*i] = math.sin(theta)\n",
                "            pe[k, 2*i+1] = math.cos(theta)\n",
                "    return pe\n",
                "\n",
                "pe = get_positional_encoding(50, 64)\n",
                "print(f\"Positional Encoding shape: {pe.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feed Forward Network\n",
                "\n",
                "FFN(x) = max(0, xW1 + b1)W2 + b2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForward:\n",
                "    def __init__(self, d_model: int, d_ff: int):\n",
                "        self.W1 = np.random.randn(d_model, d_ff)\n",
                "        self.b1 = np.zeros(d_ff)\n",
                "        self.W2 = np.random.randn(d_ff, d_model)\n",
                "        self.b2 = np.zeros(d_model)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # ReLU activation\n",
                "        hidden = np.maximum(0, x @ self.W1 + self.b1)\n",
                "        return hidden @ self.W2 + self.b2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Transformer Encoder Block"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderLayer:\n",
                "    def __init__(self, d_model, num_heads, d_ff):\n",
                "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
                "        self.ffn = FeedForward(d_model, d_ff)\n",
                "        \n",
                "        # Layer Norm parameters (simplified)\n",
                "        self.norm1_gamma = np.ones(d_model)\n",
                "        self.norm1_beta = np.zeros(d_model)\n",
                "        self.norm2_gamma = np.ones(d_model)\n",
                "        self.norm2_beta = np.zeros(d_model)\n",
                "        \n",
                "    def layer_norm(self, x, gamma, beta, eps=1e-5):\n",
                "        mean = np.mean(x, axis=-1, keepdims=True)\n",
                "        var = np.var(x, axis=-1, keepdims=True)\n",
                "        return gamma * (x - mean) / np.sqrt(var + eps) + beta\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        # 1. Multi-Head Attention + Add & Norm\n",
                "        attn_out = self.mha.forward(x, x, x, mask)\n",
                "        x = self.layer_norm(x + attn_out, self.norm1_gamma, self.norm1_beta)\n",
                "        \n",
                "        # 2. Feed Forward + Add & Norm\n",
                "        ffn_out = self.ffn.forward(x)\n",
                "        x = self.layer_norm(x + ffn_out, self.norm2_gamma, self.norm2_beta)\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Encoder Layer\n",
                "encoder = EncoderLayer(d_model=64, num_heads=8, d_ff=256)\n",
                "x = np.random.randn(2, 10, 64)\n",
                "out = encoder.forward(x)\n",
                "print(f\"Encoder Layer Output: {out.shape}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}