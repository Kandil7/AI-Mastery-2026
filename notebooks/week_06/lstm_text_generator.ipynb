{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 6: LSTM Text Generator\n",
                "\n",
                "## Objective\n",
                "Build a character-level LSTM text generator using Shakespeare's works.\n",
                "\n",
                "**Target**:\n",
                "- Generate coherent text after training\n",
                "- Understand LSTM gates (forget, input, output, cell)\n",
                "- Implement sampling strategies (greedy, temperature, top-k)\n",
                "- Visualize hidden states and gate activations\n",
                "\n",
                "---\n",
                "\n",
                "## Why LSTMs?\n",
                "\n",
                "**Problem with Vanilla RNNs**: Vanishing gradients over long sequences\n",
                "\n",
                "**LSTM Solution**: Gated memory cells that learn what to remember/forget\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Imports\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import requests\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.ml.deep_learning import LSTM, Dense, Activation, NeuralNetwork\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "print(\"✓ Imports successful\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Load Shakespeare Text Corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Download Shakespeare corpus\n",
                "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
                "response = requests.get(url)\n",
                "text = response.text\n",
                "\n",
                "print(f\"Corpus length: {len(text):,} characters\")\n",
                "print(f\"\\nFirst 500 characters:\\n{text[:500]}\")\n",
                "\n",
                "# Get unique characters\n",
                "chars = sorted(list(set(text)))\n",
                "vocab_size = len(chars)\n",
                "print(f\"\\nVocabulary size: {vocab_size} unique characters\")\n",
                "print(f\"Characters: {''.join(chars[:50])}...\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Character Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create mappings\n",
                "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
                "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
                "\n",
                "def encode(text):\n",
                "    \"\"\"Convert text to integer sequences.\"\"\"\n",
                "    return np.array([char_to_idx[ch] for ch in text])\n",
                "\n",
                "def decode(indices):\n",
                "    \"\"\"Convert integer sequences back to text.\"\"\"\n",
                "    return ''.join([idx_to_char[idx] for idx in indices])\n",
                "\n",
                "# Test encoding/decoding\n",
                "sample = \"Hello, World!\"\n",
                "encoded = encode(sample)\n",
                "decoded = decode(encoded)\n",
                "print(f\"Original: {sample}\")\n",
                "print(f\"Encoded: {encoded}\")\n",
                "print(f\"Decoded: {decoded}\")\n",
                "assert sample == decoded, \"Encoding/decoding failed!\"\n",
                "print(\"✓ Encoding/decoding works correctly\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Create Training Sequences"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Hyperparameters\n",
                "SEQ_LENGTH = 100  # Input sequence length\n",
                "STEP = 3          # Step size for creating sequences\n",
                "\n",
                "# Create sequences\n",
                "sequences = []\n",
                "next_chars = []\n",
                "\n",
                "for i in range(0, len(text) - SEQ_LENGTH, STEP):\n",
                "    sequences.append(text[i:i + SEQ_LENGTH])\n",
                "    next_chars.append(text[i + SEQ_LENGTH])\n",
                "\n",
                "print(f\"Number of sequences: {len(sequences):,}\")\n",
                "print(f\"\\nExample sequence:\")\n",
                "print(f\"Input:  '{sequences[0]}'\")\n",
                "print(f\"Target: '{next_chars[0]}'\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: One-Hot Encoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def one_hot_encode(sequences, vocab_size):\n",
                "    \"\"\"Convert character sequences to one-hot encoded arrays.\"\"\"\n",
                "    n_samples = len(sequences)\n",
                "    seq_len = len(sequences[0])\n",
                "    \n",
                "    X = np.zeros((n_samples, seq_len, vocab_size), dtype=np.float32)\n",
                "    \n",
                "    for i, seq in enumerate(sequences):\n",
                "        for t, char in enumerate(seq):\n",
                "            X[i, t, char_to_idx[char]] = 1\n",
                "    \n",
                "    return X\n",
                "\n",
                "# Encode inputs\n",
                "print(\"Encoding input sequences...\")\n",
                "X = one_hot_encode(sequences, vocab_size)\n",
                "\n",
                "# Encode targets\n",
                "y = np.array([char_to_idx[ch] for ch in next_chars])\n",
                "\n",
                "print(f\"X shape: {X.shape}  (samples, seq_length, vocab_size)\")\n",
                "print(f\"y shape: {y.shape}  (samples,)\")\n",
                "print(f\"Memory: {X.nbytes / 1024**2:.1f} MB\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Build LSTM Model\n",
                "\n",
                "### LSTM Architecture\n",
                "\n",
                "```\n",
                "Input (seq_len, vocab_size)\n",
                "    ↓\n",
                "LSTM(128 units)\n",
                "    ↓\n",
                "Dense(vocab_size)\n",
                "    ↓\n",
                "Softmax\n",
                "    ↓\n",
                "Next character probabilities\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Build model\n",
                "hidden_size = 128\n",
                "\n",
                "model = NeuralNetwork()\n",
                "model.add(LSTM(input_size=vocab_size, hidden_size=hidden_size, return_sequences=False))\n",
                "model.add(Dense(hidden_size, vocab_size))\n",
                "model.add(Activation('softmax'))\n",
                "\n",
                "# Compile\n",
                "from src.ml.deep_learning import CrossEntropyLoss\n",
                "model.compile(loss=CrossEntropyLoss(), learning_rate=0.001)\n",
                "\n",
                "model.summary()\n",
                "print(\"\\n✓ Model built successfully\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Train model\n",
                "EPOCHS = 20\n",
                "BATCH_SIZE = 128\n",
                "\n",
                "print(\"Starting training...\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "history = model.fit(\n",
                "    X, y,\n",
                "    epochs=EPOCHS,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    validation_split=0.1,\n",
                "    verbose=True\n",
                ")\n",
                "\n",
                "print(\"\\n✓ Training complete!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Text Generation\n",
                "\n",
                "### Sampling Strategies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "def sample_greedy(preds):\n",
                "    \"\"\"Always pick most likely character.\"\"\"\n",
                "    return np.argmax(preds)\n",
                "\n",
                "def sample_temperature(preds, temperature=1.0):\n",
                "    \"\"\"\n",
                "    Sample with temperature control.\n",
                "    \n",
                "    temperature > 1: More random (exploration)\n",
                "    temperature < 1: More deterministic (exploitation)\n",
                "    temperature = 1: Standard sampling\n",
                "    \"\"\"\n",
                "    preds = np.log(preds + 1e-10) / temperature\n",
                "    exp_preds = np.exp(preds)\n",
                "    preds = exp_preds / np.sum(exp_preds)\n",
                "    return np.random.choice(len(preds), p=preds)\n",
                "\n",
                "def sample_top_k(preds, k=5):\n",
                "    \"\"\"Sample from top-k most likely characters.\"\"\"\n",
                "    top_k_indices = np.argsort(preds)[-k:]\n",
                "    top_k_probs = preds[top_k_indices]\n",
                "    top_k_probs /= np.sum(top_k_probs)\n",
                "    return np.random.choice(top_k_indices, p=top_k_probs)\n",
                "\n",
                "def generate_text(model, seed_text, length=500, strategy='temperature', temperature=0.8, top_k=5):\n",
                "    \"\"\"Generate text using trained model.\"\"\"\n",
                "    generated = seed_text\n",
                "    \n",
                "    for _ in range(length):\n",
                "        # Prepare input\n",
                "        x = one_hot_encode([generated[-SEQ_LENGTH:]], vocab_size)\n",
                "        \n",
                "        # Predict next character\n",
                "        preds = model.predict_proba(x)[0]\n",
                "        \n",
                "        # Sample based on strategy\n",
                "        if strategy == 'greedy':\n",
                "            next_idx = sample_greedy(preds)\n",
                "        elif strategy == 'temperature':\n",
                "            next_idx = sample_temperature(preds, temperature)\n",
                "        elif strategy == 'top_k':\n",
                "            next_idx = sample_top_k(preds, k=top_k)\n",
                "        \n",
                "        # Append to generated text\n",
                "        generated += idx_to_char[next_idx]\n",
                "    \n",
                "    return generated"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Generate text with different strategies\n",
                "seed = text[:SEQ_LENGTH]\n",
                "\n",
                "print(\"=\"*70)\n",
                "print(\"TEXT GENERATION RESULTS\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(f\"\\nSeed text:\\n{seed}\\n\")\n",
                "\n",
                "print(\"\\n--- Strategy: Greedy (deterministic) ---\")\n",
                "generated_greedy = generate_text(model, seed, length=300, strategy='greedy')\n",
                "print(generated_greedy[SEQ_LENGTH:])\n",
                "\n",
                "print(\"\\n--- Strategy: Temperature=0.5 (conservative) ---\")\n",
                "generated_low_temp = generate_text(model, seed, length=300, strategy='temperature', temperature=0.5)\n",
                "print(generated_low_temp[SEQ_LENGTH:])\n",
                "\n",
                "print(\"\\n--- Strategy: Temperature=1.0 (balanced) ---\")\n",
                "generated_mid_temp = generate_text(model, seed, length=300, strategy='temperature', temperature=1.0)\n",
                "print(generated_mid_temp[SEQ_LENGTH:])\n",
                "\n",
                "print(\"\\n--- Strategy: Temperature=1.5 (creative) ---\")\n",
                "generated_high_temp = generate_text(model, seed, length=300, strategy='temperature', temperature=1.5)\n",
                "print(generated_high_temp[SEQ_LENGTH:])\n",
                "\n",
                "print(\"\\n--- Strategy: Top-k (k=10) ---\")\n",
                "generated_topk = generate_text(model, seed, length=300, strategy='top_k', top_k=10)\n",
                "print(generated_topk[SEQ_LENGTH:])"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Visualize LSTM Internals\n",
                "\n",
                "### Gate Activations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Extract LSTM layer\n",
                "lstm_layer = model.layers[0]\n",
                "\n",
                "# Forward pass to get gates\n",
                "sample_input = X[0:1]  # Single sequence\n",
                "\n",
                "# Get gate values (modify LSTM to return gates)\n",
                "# This assumes LSTM layer has been modified to store gate activations\n",
                "hidden, gates = lstm_layer.forward_with_gates(sample_input)\n",
                "\n",
                "# Plot gate activations over time\n",
                "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
                "\n",
                "gate_names = ['Forget Gate', 'Input Gate', 'Output Gate', 'Cell State']\n",
                "for i, (ax, name) in enumerate(zip(axes, gate_names)):\n",
                "    ax.imshow(gates[i].T, cmap='viridis', aspect='auto')\n",
                "    ax.set_title(f'{name} Activations', fontsize=14, fontweight='bold')\n",
                "    ax.set_xlabel('Time Step', fontsize=12)\n",
                "    ax.set_ylabel('Hidden Unit', fontsize=12)\n",
                "    plt.colorbar(ax.images[0], ax=ax)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n✓ Gate visualizations complete\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 9: Analysis & Insights\n",
                "\n",
                "### LSTM Gate Behavior\n",
                "\n",
                "**Forget Gate** (f_t):\n",
                "- Decides what to discard from cell state\n",
                "- Dark regions = forgetting information\n",
                "- Light regions = retaining information\n",
                "\n",
                "**Input Gate** (i_t):\n",
                "- Decides what new information to store\n",
                "- Works with candidate cell state (c̃_t)\n",
                "\n",
                "**Output Gate** (o_t):\n",
                "- Decides what to output based on cell state\n",
                "- Controls information flow to next layer\n",
                "\n",
                "**Cell State** (c_t):\n",
                "- Long-term memory\n",
                "- Modified by forget and input gates\n",
                "\n",
                "### Temperature Effects\n",
                "\n",
                "- **Low (0.5)**: Conservative, coherent but repetitive\n",
                "- **Medium (1.0)**: Balanced creativity and coherence\n",
                "- **High (1.5)**: Creative but may lose coherence\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 10: Comparison with Vanilla RNN\n",
                "\n",
                "### Why LSTM is Better"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Build vanilla RNN for comparison\n",
                "from src.ml.deep_learning import SimpleRNN\n",
                "\n",
                "rnn_model = NeuralNetwork()\n",
                "rnn_model.add(SimpleRNN(input_size=vocab_size, hidden_size=128))\n",
                "rnn_model.add(Dense(128, vocab_size))\n",
                "rnn_model.add(Activation('softmax'))\n",
                "rnn_model.compile(loss=CrossEntropyLoss(), learning_rate=0.001)\n",
                "\n",
                "print(\"Training vanilla RNN for comparison...\")\n",
                "rnn_history = rnn_model.fit(X, y, epochs=20, batch_size=128, validation_split=0.1, verbose=False)\n",
                "\n",
                "# Compare learning curves\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(history['loss'], label='LSTM Train', linewidth=2)\n",
                "plt.plot(rnn_history['loss'], label='RNN Train', linewidth=2, linestyle='--')\n",
                "plt.xlabel('Epoch', fontsize=12)\n",
                "plt.ylabel('Loss', fontsize=12)\n",
                "plt.title('Training Loss: LSTM vs RNN', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(history['val_loss'], label='LSTM Val', linewidth=2)\n",
                "plt.plot(rnn_history['val_loss'], label='RNN Val', linewidth=2, linestyle='--')\n",
                "plt.xlabel('Epoch', fontsize=12)\n",
                "plt.ylabel('Loss', fontsize=12)\n",
                "plt.title('Validation Loss: LSTM vs RNN', fontsize=14, fontweight='bold')\n",
                "plt.legend()\n",
                "plt.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nFinal Training Loss:\")\n",
                "print(f\"  LSTM: {history['loss'][-1]:.4f}\")\n",
                "print(f\"  RNN:  {rnn_history['loss'][-1]:.4f}\")\n",
                "print(f\"\\nFinal Validation Loss:\")\n",
                "print(f\"  LSTM: {history['val_loss'][-1]:.4f}\")\n",
                "print(f\"  RNN:  {rnn_history['val_loss'][-1]:.4f}\")\n",
                "\n",
                "if history['val_loss'][-1] < rnn_history['val_loss'][-1]:\n",
                "    improvement = (rnn_history['val_loss'][-1] - history['val_loss'][-1]) / rnn_history['val_loss'][-1] * 100\n",
                "    print(f\"\\n✅ LSTM outperforms RNN by {improvement:.1f}%!\")\n",
                "else:\n",
                "    print(\"\\n⚠️ RNN performed better (may need more training)\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "### Key Learnings\n",
                "\n",
                "1. **LSTM gates solve vanishing gradients**\n",
                "   - Forget gate controls what to discard\n",
                "   - Input gate controls what to add\n",
                "   - Output gate controls what to output\n",
                "\n",
                "2. **Sampling strategies matter**\n",
                "   - Greedy: Deterministic but boring\n",
                "   - Temperature: Controls randomness\n",
                "   - Top-k: Balances quality and diversity\n",
                "\n",
                "3. **LSTMs outperform vanilla RNNs**\n",
                "   - Better long-term dependencies\n",
                "   - More stable training\n",
                "   - Higher quality text generation\n",
                "\n",
                "### Interview Points\n",
                "\n",
                "- \"I implemented character-level LSTM for text generation from scratch\"\n",
                "- \"Achieved coherent Shakespeare-style text with temperature sampling\"\n",
                "- \"Visualized gate activations to understand LSTM internals\"\n",
                "- \"Compared LSTM vs RNN, demonstrating LSTM's advantage in long sequences\"\n",
                "\n",
                "---\n",
                "\n",
                "**✅ Week 6 Complete**: LSTM text generator with deep understanding of recurrent architectures!\n",
                "\n",
                "---\n",
                "\n",
                "*Next: Week 7 - Build BERT from Scratch*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}