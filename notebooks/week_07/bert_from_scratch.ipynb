{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 7: Build BERT from Scratch\n",
                "\n",
                "## Objective\n",
                "Implement the BERT (Bidirectional Encoder Representations from Transformers) architecture from first principles.\n",
                "\n",
                "**Goals**:\n",
                "- Understand transformer encoder architecture\n",
                "- Implement multi-head self-attention\n",
                "- Build positional encodings\n",
                "- Create masked language model (MLM) pretraining task\n",
                "- Fine-tune BERT for classification\n",
                "\n",
                "---\n",
                "\n",
                "## Why BERT?\n",
                "\n",
                "**Breakthrough**: Bidirectional context understanding\n",
                "- **GPT**: Left-to-right (autoregressive)\n",
                "- **BERT**: Bidirectional (masked language modeling)\n",
                "\n",
                "**Key Innovation**: Masked Language Model (MLM)\n",
                "- Mask 15% of tokens\n",
                "- Predict masked tokens using bidirectional context\n",
                "- Pre-train once, fine-tune for many tasks\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Imports\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import sys\n",
                "import os\n",
                "\n",
                "sys.path.append(os.path.abspath('../../'))\n",
                "\n",
                "from src.llm.attention import MultiHeadAttention, PositionalEncoding\n",
                "from src.ml.deep_learning import Dense, Activation, LayerNorm, Dropout, NeuralNetwork\n",
                "\n",
                "sns.set_style('whitegrid')\n",
                "print(\"✓ Imports successful\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Multi-Head Self-Attention\n",
                "\n",
                "### Core Formula\n",
                "\n",
                "```\n",
                "Attention(Q, K, V) = softmax(QK^T / √d_k) V\n",
                "\n",
                "MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O\n",
                "where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class ScaledDotProductAttention:\n",
                "    \"\"\"Scaled dot-product attention mechanism.\"\"\"\n",
                "    \n",
                "    def __init__(self, dropout=0.1):\n",
                "        self.dropout = dropout\n",
                "        self.attention_weights = None\n",
                "    \n",
                "    def forward(self, Q, K, V, mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            Q: Queries (batch_size, seq_len, d_k)\n",
                "            K: Keys (batch_size, seq_len, d_k)\n",
                "            V: Values (batch_size, seq_len, d_v)\n",
                "            mask: Attention mask (batch_size, seq_len, seq_len)\n",
                "        \n",
                "        Returns:\n",
                "            output: (batch_size, seq_len, d_v)\n",
                "            attention_weights: (batch_size, seq_len, seq_len)\n",
                "        \"\"\"\n",
                "        d_k = Q.shape[-1]\n",
                "        \n",
                "        # Compute attention scores\n",
                "        scores = np.matmul(Q, K.transpose(0, 2, 1)) / np.sqrt(d_k)\n",
                "        \n",
                "        # Apply mask (for padding or future tokens)\n",
                "        if mask is not None:\n",
                "            scores = np.where(mask == 0, -1e9, scores)\n",
                "        \n",
                "        # Softmax to get attention weights\n",
                "        attention_weights = self.softmax(scores)\n",
                "        \n",
                "        # Apply dropout\n",
                "        if self.dropout > 0:\n",
                "            mask = np.random.binomial(1, 1-self.dropout, attention_weights.shape)\n",
                "            attention_weights = attention_weights * mask / (1 - self.dropout)\n",
                "        \n",
                "        # Weighted sum of values\n",
                "        output = np.matmul(attention_weights, V)\n",
                "        \n",
                "        self.attention_weights = attention_weights\n",
                "        return output, attention_weights\n",
                "    \n",
                "    def softmax(self, x):\n",
                "        \"\"\"Numerically stable softmax.\"\"\"\n",
                "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
                "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
                "\n",
                "# Test attention\n",
                "attention = ScaledDotProductAttention()\n",
                "\n",
                "# Create dummy input\n",
                "batch_size, seq_len, d_model = 2, 10, 64\n",
                "Q = K = V = np.random.randn(batch_size, seq_len, d_model)\n",
                "\n",
                "output, weights = attention.forward(Q, K, V)\n",
                "\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(f\"Attention weights shape: {weights.shape}\")\n",
                "print(f\"Attention weights sum (should be 1.0): {weights[0, 0, :].sum():.4f}\")\n",
                "print(\"✓ Scaled dot-product attention works!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Multi-Head Attention Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class MultiHeadAttentionLayer:\n",
                "    \"\"\"Multi-head self-attention with learned projections.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model=512, num_heads=8, dropout=0.1):\n",
                "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
                "        \n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads\n",
                "        self.d_k = d_model // num_heads\n",
                "        \n",
                "        # Learnable projection matrices\n",
                "        self.W_Q = np.random.randn(d_model, d_model) * 0.01\n",
                "        self.W_K = np.random.randn(d_model, d_model) * 0.01\n",
                "        self.W_V = np.random.randn(d_model, d_model) * 0.01\n",
                "        self.W_O = np.random.randn(d_model, d_model) * 0.01\n",
                "        \n",
                "        self.attention = ScaledDotProductAttention(dropout)\n",
                "    \n",
                "    def split_heads(self, x):\n",
                "        \"\"\"Split into multiple attention heads.\"\"\"\n",
                "        batch_size, seq_len, d_model = x.shape\n",
                "        # Reshape: (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)\n",
                "        x = x.reshape(batch_size, seq_len, self.num_heads, self.d_k)\n",
                "        # Transpose: (batch, num_heads, seq_len, d_k)\n",
                "        return x.transpose(0, 2, 1, 3)\n",
                "    \n",
                "    def combine_heads(self, x):\n",
                "        \"\"\"Combine attention heads back.\"\"\"\n",
                "        batch_size, num_heads, seq_len, d_k = x.shape\n",
                "        # Transpose: (batch, seq_len, num_heads, d_k)\n",
                "        x = x.transpose(0, 2, 1, 3)\n",
                "        # Reshape: (batch, seq_len, d_model)\n",
                "        return x.reshape(batch_size, seq_len, self.d_model)\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            x: Input (batch_size, seq_len, d_model)\n",
                "            mask: Attention mask\n",
                "        \"\"\"\n",
                "        batch_size = x.shape[0]\n",
                "        \n",
                "        # Linear projections\n",
                "        Q = np.matmul(x, self.W_Q)\n",
                "        K = np.matmul(x, self.W_K)\n",
                "        V = np.matmul(x, self.W_V)\n",
                "        \n",
                "        # Split into heads\n",
                "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)\n",
                "        K = self.split_heads(K)\n",
                "        V = self.split_heads(V)\n",
                "        \n",
                "        # Apply attention for each head\n",
                "        attn_outputs = []\n",
                "        for i in range(self.num_heads):\n",
                "            output, _ = self.attention.forward(Q[:, i], K[:, i], V[:, i], mask)\n",
                "            attn_outputs.append(output)\n",
                "        \n",
                "        # Stack heads: (batch, num_heads, seq_len, d_k)\n",
                "        attn_output = np.stack(attn_outputs, axis=1)\n",
                "        \n",
                "        # Combine heads\n",
                "        combined = self.combine_heads(attn_output)\n",
                "        \n",
                "        # Final linear projection\n",
                "        output = np.matmul(combined, self.W_O)\n",
                "        \n",
                "        return output\n",
                "\n",
                "# Test multi-head attention\n",
                "mha = MultiHeadAttentionLayer(d_model=512, num_heads=8)\n",
                "x = np.random.randn(2, 20, 512)  # (batch=2, seq_len=20, d_model=512)\n",
                "output = mha.forward(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(\"✓ Multi-head attention works!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Positional Encoding\n",
                "\n",
                "Since attention has no notion of sequence order, we add positional information.\n",
                "\n",
                "**Sinusoidal Encoding**:\n",
                "```\n",
                "PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
                "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class PositionalEncodingLayer:\n",
                "    \"\"\"Sinusoidal positional encoding.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model=512, max_len=5000):\n",
                "        self.d_model = d_model\n",
                "        \n",
                "        # Create positional encoding matrix\n",
                "        pe = np.zeros((max_len, d_model))\n",
                "        position = np.arange(0, max_len).reshape(-1, 1)\n",
                "        div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
                "        \n",
                "        pe[:, 0::2] = np.sin(position * div_term)\n",
                "        pe[:, 1::2] = np.cos(position * div_term)\n",
                "        \n",
                "        self.pe = pe\n",
                "    \n",
                "    def forward(self, x):\n",
                "        \"\"\"Add positional encoding to input.\"\"\"\n",
                "        seq_len = x.shape[1]\n",
                "        return x + self.pe[:seq_len]\n",
                "\n",
                "# Visualize positional encoding\n",
                "pe_layer = PositionalEncodingLayer(d_model=128, max_len=100)\n",
                "\n",
                "plt.figure(figsize=(15, 5))\n",
                "plt.imshow(pe_layer.pe.T, aspect='auto', cmap='RdBu')\n",
                "plt.xlabel('Position', fontsize=12)\n",
                "plt.ylabel('Dimension', fontsize=12)\n",
                "plt.title('Sinusoidal Positional Encoding', fontsize=14, fontweight='bold')\n",
                "plt.colorbar()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"✓ Positional encoding visualized\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Feed-Forward Network\n",
                "\n",
                "**Architecture**: Linear → ReLU → Linear\n",
                "\n",
                "```\n",
                "FFN(x) = ReLU(xW_1 + b_1)W_2 + b_2\n",
                "```\n",
                "\n",
                "Typical: d_ff = 4 × d_model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class FeedForwardNetwork:\n",
                "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model=512, d_ff=2048, dropout=0.1):\n",
                "        self.W1 = np.random.randn(d_model, d_ff) * np.sqrt(2.0 / d_model)\n",
                "        self.b1 = np.zeros(d_ff)\n",
                "        self.W2 = np.random.randn(d_ff, d_model) * np.sqrt(2.0 / d_ff)\n",
                "        self.b2 = np.zeros(d_model)\n",
                "        self.dropout = dropout\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # First linear + ReLU\n",
                "        hidden = np.maximum(0, np.matmul(x, self.W1) + self.b1)\n",
                "        \n",
                "        # Dropout\n",
                "        if self.dropout > 0:\n",
                "            mask = np.random.binomial(1, 1-self.dropout, hidden.shape)\n",
                "            hidden = hidden * mask / (1 - self.dropout)\n",
                "        \n",
                "        # Second linear\n",
                "        output = np.matmul(hidden, self.W2) + self.b2\n",
                "        \n",
                "        return output\n",
                "\n",
                "# Test FFN\n",
                "ffn = FeedForwardNetwork(d_model=512, d_ff=2048)\n",
                "x = np.random.randn(2, 20, 512)\n",
                "output = ffn.forward(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(\"✓ Feed-forward network works!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Transformer Encoder Block\n",
                "\n",
                "**Architecture**:\n",
                "```\n",
                "x → Multi-Head Attention → Add & Norm\n",
                "  → Feed-Forward         → Add & Norm\n",
                "```\n",
                "\n",
                "**Residual Connections** prevent vanishing gradients in deep networks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class TransformerEncoderBlock:\n",
                "    \"\"\"Single transformer encoder block.\"\"\"\n",
                "    \n",
                "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
                "        self.attention = MultiHeadAttentionLayer(d_model, num_heads, dropout)\n",
                "        self.ffn = FeedForwardNetwork(d_model, d_ff, dropout)\n",
                "        \n",
                "        # Layer normalization parameters\n",
                "        self.gamma1 = np.ones(d_model)\n",
                "        self.beta1 = np.zeros(d_model)\n",
                "        self.gamma2 = np.ones(d_model)\n",
                "        self.beta2 = np.zeros(d_model)\n",
                "        \n",
                "        self.dropout = dropout\n",
                "    \n",
                "    def layer_norm(self, x, gamma, beta, eps=1e-6):\n",
                "        \"\"\"Layer normalization.\"\"\"\n",
                "        mean = x.mean(axis=-1, keepdims=True)\n",
                "        std = x.std(axis=-1, keepdims=True)\n",
                "        return gamma * (x - mean) / (std + eps) + beta\n",
                "    \n",
                "    def forward(self, x, mask=None):\n",
                "        # Multi-head self-attention with residual\n",
                "        attn_output = self.attention.forward(x, mask)\n",
                "        \n",
                "        # Dropout\n",
                "        if self.dropout > 0:\n",
                "            dropout_mask = np.random.binomial(1, 1-self.dropout, attn_output.shape)\n",
                "            attn_output = attn_output * dropout_mask / (1 - self.dropout)\n",
                "        \n",
                "        # Add & Norm\n",
                "        x = self.layer_norm(x + attn_output, self.gamma1, self.beta1)\n",
                "        \n",
                "        # Feed-forward with residual\n",
                "        ffn_output = self.ffn.forward(x)\n",
                "        \n",
                "        # Dropout\n",
                "        if self.dropout > 0:\n",
                "            dropout_mask = np.random.binomial(1, 1-self.dropout, ffn_output.shape)\n",
                "            ffn_output = ffn_output * dropout_mask / (1 - self.dropout)\n",
                "        \n",
                "        # Add & Norm\n",
                "        output = self.layer_norm(x + ffn_output, self.gamma2, self.beta2)\n",
                "        \n",
                "        return output\n",
                "\n",
                "# Test encoder block\n",
                "encoder_block = TransformerEncoderBlock()\n",
                "x = np.random.randn(2, 20, 512)\n",
                "output = encoder_block.forward(x)\n",
                "\n",
                "print(f\"Input shape: {x.shape}\")\n",
                "print(f\"Output shape: {output.shape}\")\n",
                "print(\"✓ Transformer encoder block works!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Complete BERT Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class BERT:\n",
                "    \"\"\"BERT: Bidirectional Encoder Representations from Transformers.\"\"\"\n",
                "    \n",
                "    def __init__(self, vocab_size=30000, d_model=512, num_heads=8, \n",
                "                 num_layers=6, d_ff=2048, max_len=512, dropout=0.1):\n",
                "        self.vocab_size = vocab_size\n",
                "        self.d_model = d_model\n",
                "        \n",
                "        # Token embeddings\n",
                "        self.token_embedding = np.random.randn(vocab_size, d_model) * 0.01\n",
                "        \n",
                "        # Positional encoding\n",
                "        self.pos_encoding = PositionalEncodingLayer(d_model, max_len)\n",
                "        \n",
                "        # Encoder blocks\n",
                "        self.encoder_blocks = [\n",
                "            TransformerEncoderBlock(d_model, num_heads, d_ff, dropout)\n",
                "            for _ in range(num_layers)\n",
                "        ]\n",
                "        \n",
                "        print(f\"BERT Model initialized:\")\n",
                "        print(f\"  Vocabulary: {vocab_size:,}\")\n",
                "        print(f\"  Model dimension: {d_model}\")\n",
                "        print(f\"  Attention heads: {num_heads}\")\n",
                "        print(f\"  Encoder layers: {num_layers}\")\n",
                "        print(f\"  Feed-forward dim: {d_ff}\")\n",
                "    \n",
                "    def forward(self, input_ids, attention_mask=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            input_ids: Token indices (batch_size, seq_len)\n",
                "            attention_mask: Mask for padding tokens\n",
                "        \n",
                "        Returns:\n",
                "            contextualized_embeddings: (batch_size, seq_len, d_model)\n",
                "        \"\"\"\n",
                "        # Token embeddings\n",
                "        x = self.token_embedding[input_ids]  # (batch, seq_len, d_model)\n",
                "        \n",
                "        # Add positional encoding\n",
                "        x = self.pos_encoding.forward(x)\n",
                "        \n",
                "        # Pass through encoder blocks\n",
                "        for encoder_block in self.encoder_blocks:\n",
                "            x = encoder_block.forward(x, attention_mask)\n",
                "        \n",
                "        return x\n",
                "\n",
                "# Build BERT\n",
                "bert = BERT(\n",
                "    vocab_size=30000,\n",
                "    d_model=512,\n",
                "    num_heads=8,\n",
                "    num_layers=6,\n",
                "    d_ff=2048\n",
                ")\n",
                "\n",
                "# Test forward pass\n",
                "input_ids = np.random.randint(0, 30000, size=(2, 50))  # batch=2, seq_len=50\n",
                "embeddings = bert.forward(input_ids)\n",
                "\n",
                "print(f\"\\nInput shape: {input_ids.shape}\")\n",
                "print(f\"Output embeddings shape: {embeddings.shape}\")\n",
                "print(\"\\n✅ BERT model built successfully!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 7: Masked Language Model (MLM) Training\n",
                "\n",
                "**Objective**: Predict masked tokens using bidirectional context\n",
                "\n",
                "**Masking Strategy**:\n",
                "- 80% → [MASK] token\n",
                "- 10% → Random token\n",
                "- 10% → Keep original"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class MaskedLanguageModel:\n",
                "    \"\"\"MLM head for BERT pretraining.\"\"\"\n",
                "    \n",
                "    def __init__(self, bert_model, mask_token_id=1, mask_prob=0.15):\n",
                "        self.bert = bert_model\n",
                "        self.mask_token_id = mask_token_id\n",
                "        self.mask_prob = mask_prob\n",
                "        \n",
                "        # Prediction head\n",
                "        d_model = bert_model.d_model\n",
                "        vocab_size = bert_model.vocab_size\n",
                "        \n",
                "        self.W_pred = np.random.randn(d_model, vocab_size) * 0.01\n",
                "        self.b_pred = np.zeros(vocab_size)\n",
                "    \n",
                "    def create_masked_input(self, input_ids):\n",
                "        \"\"\"Create masked version of input for MLM.\"\"\"\n",
                "        masked_ids = input_ids.copy()\n",
                "        labels = np.full_like(input_ids, -100)  # -100 = ignore in loss\n",
                "        \n",
                "        # Randomly select 15% of tokens to mask\n",
                "        mask_indices = np.random.rand(*input_ids.shape) < self.mask_prob\n",
                "        labels[mask_indices] = input_ids[mask_indices]\n",
                "        \n",
                "        # Apply masking strategy\n",
                "        for i in range(len(mask_indices)):\n",
                "            for j in range(len(mask_indices[i])):\n",
                "                if mask_indices[i, j]:\n",
                "                    rand = np.random.rand()\n",
                "                    if rand < 0.8:\n",
                "                        masked_ids[i, j] = self.mask_token_id  # [MASK]\n",
                "                    elif rand < 0.9:\n",
                "                        masked_ids[i, j] = np.random.randint(0, self.bert.vocab_size)  # Random\n",
                "                    # else: keep original (10%)\n",
                "        \n",
                "        return masked_ids, labels\n",
                "    \n",
                "    def forward(self, input_ids):\n",
                "        \"\"\"Forward pass with MLM.\"\"\"\n",
                "        # Get BERT embeddings\n",
                "        embeddings = self.bert.forward(input_ids)\n",
                "        \n",
                "        # Predict token probabilities\n",
                "        logits = np.matmul(embeddings, self.W_pred) + self.b_pred\n",
                "        \n",
                "        return logits\n",
                "\n",
                "# Test MLM\n",
                "mlm = MaskedLanguageModel(bert)\n",
                "\n",
                "# Create sample input\n",
                "sample_ids = np.random.randint(2, 1000, size=(4, 30))  # Avoid special tokens\n",
                "\n",
                "# Create masked version\n",
                "masked_ids, labels = mlm.create_masked_input(sample_ids)\n",
                "\n",
                "print(f\"Original tokens (first sequence): {sample_ids[0, :10]}\")\n",
                "print(f\"Masked tokens (first sequence):   {masked_ids[0, :10]}\")\n",
                "print(f\"Labels (first sequence):          {labels[0, :10]}\")\n",
                "\n",
                "# Forward pass\n",
                "logits = mlm.forward(masked_ids)\n",
                "print(f\"\\nLogits shape: {logits.shape}  (batch, seq_len, vocab_size)\")\n",
                "print(\"✓ MLM training ready!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 8: Fine-Tuning for Classification\n",
                "\n",
                "**Transfer Learning**: Use pre-trained BERT + add task-specific head"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "class BERTForSequenceClassification:\n",
                "    \"\"\"BERT + Classification head.\"\"\"\n",
                "    \n",
                "    def __init__(self, bert_model, num_classes=2):\n",
                "        self.bert = bert_model\n",
                "        self.num_classes = num_classes\n",
                "        \n",
                "        # Classification head\n",
                "        d_model = bert_model.d_model\n",
                "        self.W_cls = np.random.randn(d_model, num_classes) * 0.01\n",
                "        self.b_cls = np.zeros(num_classes)\n",
                "    \n",
                "    def forward(self, input_ids):\n",
                "        \"\"\"Forward pass for classification.\"\"\"\n",
                "        # Get BERT embeddings\n",
                "        embeddings = self.bert.forward(input_ids)\n",
                "        \n",
                "        # Use [CLS] token representation (first token)\n",
                "        cls_embedding = embeddings[:, 0, :]  # (batch, d_model)\n",
                "        \n",
                "        # Classification logits\n",
                "        logits = np.matmul(cls_embedding, self.W_cls) + self.b_cls\n",
                "        \n",
                "        # Softmax\n",
                "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
                "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
                "        \n",
                "        return probs\n",
                "\n",
                "# Build classifier\n",
                "classifier = BERTForSequenceClassification(bert, num_classes=3)\n",
                "\n",
                "# Test classification\n",
                "input_ids = np.random.randint(0, 30000, size=(4, 50))\n",
                "probs = classifier.forward(input_ids)\n",
                "\n",
                "print(f\"Input shape: {input_ids.shape}\")\n",
                "print(f\"Class probabilities shape: {probs.shape}\")\n",
                "print(f\"\\nSample predictions:\")\n",
                "for i, prob in enumerate(probs):\n",
                "    print(f\"  Sample {i+1}: {prob} → Class {np.argmax(prob)}\")\n",
                "\n",
                "print(\"\\n✓ Classification fine-tuning ready!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Conclusion\n",
                "\n",
                "### Key Achievements\n",
                "\n",
                "1. ✅ **Built BERT from scratch** - All components implemented\n",
                "2. ✅ **Multi-head self-attention** - Core mechanism understood\n",
                "3. ✅ **Positional encoding** - Sinusoidal implementation\n",
                "4. ✅ **Transformer encoder** - 6-layer stack\n",
                "5. ✅ **MLM pretraining** - Masked language modeling\n",
                "6. ✅ **Fine-tuning** - Classification head for downstream tasks\n",
                "\n",
                "### Interview Discussion Points\n",
                "\n",
                "**Q: Why is BERT bidirectional?**\n",
                "> \"BERT uses masked language modeling where we mask 15% of tokens and predict them using both left AND right context. Unlike GPT which is autoregressive (left-to-right), BERT sees the full sentence, giving richer representations.\"\n",
                "\n",
                "**Q: What's the role of [CLS] token?**\n",
                "> \"The [CLS] token is prepended to every sequence. During pre-training, it aggregates sentence-level information. For classification, we use its final hidden state as the sentence representation.\"\n",
                "\n",
                "**Q: How does attention scale?**\n",
                "> \"Attention is O(n²) in sequence length due to the QK^T multiplication. For long sequences, we use sparse attention patterns (Longformer) or linear approximations (Linformer).\"\n",
                "\n",
                "**Q: Why layer normalization instead of batch normalization?**\n",
                "> \"Layer norm normalizes across features (not batch), making it stable for variable batch sizes and sequence lengths. It's also more effective for recurrent/sequential models.\"\n",
                "\n",
                "### Real-World Applications\n",
                "\n",
                "- **Text Classification**: Sentiment analysis, spam detection\n",
                "- **Named Entity Recognition**: Extract entities from text\n",
                "- **Question Answering**: SQuAD, conversational AI\n",
                "- **Embeddings**: Semantic search, document clustering\n",
                "\n",
                "---\n",
                "\n",
                "**✅ Week 7 Complete**: Built BERT transformer from first principles!\n",
                "\n",
                "---\n",
                "\n",
                "*Next: Week 8 - Load Pre-trained GPT-2 Weights*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}