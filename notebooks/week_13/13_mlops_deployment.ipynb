{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 13: MLOps & Model Serving\n",
                "\n",
                "Transitioning from research to production: APIs, Docker, and Monitoring.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Build a FastAPI Model Server\n",
                "2. Containerize with Docker (Conceptual)\n",
                "3. Implement Logging and Monitoring middleware"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "from typing import Dict, Any\n",
                "# Note: In a real notebook, you would install fastapi and uvicorn\n",
                "# !pip install fastapi uvicorn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model Wrapper Class\n",
                "\n",
                "Standard interface for model inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelService:\n",
                "    def __init__(self, model_path: str):\n",
                "        self.model = self._load_model(model_path)\n",
                "        self.initialized = True\n",
                "    \n",
                "    def _load_model(self, path: str):\n",
                "        print(f\"Loading model from {path}...\")\n",
                "        # Mock model loading\n",
                "        return lambda x: {\"prediction\": \"class_A\", \"confidence\": 0.95}\n",
                "    \n",
                "    def predict(self, input_data: Dict[str, Any]) -> Dict[str, Any]:\n",
                "        start_time = time.time()\n",
                "        \n",
                "        # Mock inference\n",
                "        result = self.model(input_data)\n",
                "        \n",
                "        latency = (time.time() - start_time) * 1000\n",
                "        result[\"latency_ms\"] = round(latency, 2)\n",
                "        return result"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. FastAPI Application (Mock)\n",
                "\n",
                "Simulating how to structure the API."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# This is pseudo-code for a notebook environment as APIs run in a server loop\n",
                "\n",
                "app_structure = \"\"\"\n",
                "from fastapi import FastAPI, HTTPException\n",
                "from pydantic import BaseModel\n",
                "\n",
                "app = FastAPI(title=\"AI Model Service\")\n",
                "model_service = ModelService(\"model.pkl\")\n",
                "\n",
                "class PredictionRequest(BaseModel):\n",
                "    text: str\n",
                "    top_k: int = 3\n",
                "\n",
                "@app.post(\"/predict\")\n",
                "async def predict(request: PredictionRequest):\n",
                "    try:\n",
                "        return model_service.predict(request.dict())\n",
                "    except Exception as e:\n",
                "        raise HTTPException(status_code=500, detail=str(e))\n",
                "\n",
                "@app.get(\"/health\")\n",
                "async def health():\n",
                "    return {\"status\": \"healthy\", \"model_loaded\": model_service.initialized}\n",
                "\"\"\"\n",
                "\n",
                "print(\"FastAPI App Structure:\")\n",
                "print(app_structure)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Monitoring & Logging\n",
                "\n",
                "Implementing a custom logger for metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import datetime\n",
                "\n",
                "class MetricsLogger:\n",
                "    def __init__(self, log_file=\"metrics.jsonl\"):\n",
                "        self.log_file = log_file\n",
                "    \n",
                "    def log_request(self, endpoint: str, status: int, latency: float):\n",
                "        entry = {\n",
                "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
                "            \"endpoint\": endpoint,\n",
                "            \"status_code\": status,\n",
                "            \"latency_ms\": latency\n",
                "        }\n",
                "        print(f\"[LOG] {json.dumps(entry)}\")\n",
                "        # In real world: append to file or send to Prometheus/Datadog\n",
                "\n",
                "# Test Logger\n",
                "logger = MetricsLogger()\n",
                "logger.log_request(\"/predict\", 200, 45.2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Dockerfile Template\n",
                "\n",
                "Best practices for containerizing Python AI apps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dockerfile = \"\"\"\n",
                "# Use slim python image for smaller size\n",
                "FROM python:3.10-slim\n",
                "\n",
                "# Set working directory\n",
                "WORKDIR /app\n",
                "\n",
                "# Install dependencies first (caching layer)\n",
                "COPY requirements.txt .\n",
                "RUN pip install --no-cache-dir -r requirements.txt\n",
                "\n",
                "# Copy code\n",
                "COPY src/ ./src\n",
                "COPY main.py .\n",
                "\n",
                "# Expose port\n",
                "EXPOSE 8000\n",
                "\n",
                "# Run command\n",
                "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
                "\"\"\"\n",
                "\n",
                "print(dockerfile)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}