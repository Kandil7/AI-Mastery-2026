{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 Model Serving with FastAPI\n\n## Production ML Deployment\n\nTurn your ML model into a production API in minutes.\n\n---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\u2705 FastAPI serving guide ready!')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why FastAPI for ML?\n\n\u2705 **Fast**: Async/await support\n\u2705 **Auto docs**: Swagger UI built-in\n\u2705 **Type hints**: Pydantic validation\n\u2705 **Production-ready**: Used by Uber, Netflix\n\n## Basic ML API\n\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\nimport joblib\n\napp = FastAPI()\nmodel = joblib.load('model.pkl')\n\nclass PredictionInput(BaseModel):\n    features: list[float]\n\n@app.post('/predict')\ndef predict(input: PredictionInput):\n    prediction = model.predict([input.features])\n    return {'prediction': prediction[0]}\n```\n\n## Running\n\n```bash\nuvicorn main:app --reload\n```\n\nAPI now at: http://localhost:8000\nDocs at: http://localhost:8000/docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production Best Practices\n\n### 1. Request Validation\n\n```python\nclass Features(BaseModel):\n    age: int = Field(ge=0, le=120)\n    income: float = Field(gt=0)\n    \n    @validator('age')\n    def check_age(cls, v):\n        if v < 18:\n            raise ValueError('Must be 18+')\n        return v\n```\n\n### 2. Error Handling\n\n```python\n@app.exception_handler(Exception)\nasync def global_exception_handler(request, exc):\n    return JSONResponse(\n        status_code=500,\n        content={'error': str(exc)}\n    )\n```\n\n### 3. Health Check\n\n```python\n@app.get('/health')\ndef health():\n    return {'status': 'healthy', 'model_loaded': model is not None}\n```\n\n### 4. Batch Predictions\n\n```python\n@app.post('/predict/batch')\ndef predict_batch(inputs: list[PredictionInput]):\n    features = [inp.features for inp in inputs]\n    predictions = model.predict(features)\n    return {'predictions': predictions.tolist()}\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance Optimization\n\n### Async Endpoints\n\n```python\n@app.post('/predict')\nasync def predict(input: PredictionInput):\n    # Use asyncio for I/O-bound ops\n    result = await async_model_predict(input)\n    return result\n```\n\n### Caching\n\n```python\nfrom functools import lru_cache\n\n@lru_cache(maxsize=1000)\ndef cached_predict(features_tuple):\n    return model.predict([list(features_tuple)])[0]\n```\n\n### Model Loading\n\n```python\n@app.on_event('startup')\nasync def load_model():\n    global model\n    model = joblib.load('model.pkl')\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}