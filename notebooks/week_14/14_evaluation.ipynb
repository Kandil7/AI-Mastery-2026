{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 14: Evaluation & Benchmarking\n",
                "\n",
                "Techniques for reliable AI evaluation.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Compute classical metrics (Precision, Recall, F1)\n",
                "2. Implement LLM-as-a-Judge\n",
                "3. Understand Model Drift"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import List"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Classification Metrics\n",
                "\n",
                "Implementing confusion matrix derived metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
                "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
                "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
                "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
                "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
                "    \n",
                "    epsilon = 1e-10\n",
                "    \n",
                "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
                "    precision = tp / (tp + fp + epsilon)\n",
                "    recall = tp / (tp + fn + epsilon) # aka Sensitivity\n",
                "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
                "    \n",
                "    return {\n",
                "        \"accuracy\": accuracy,\n",
                "        \"precision\": precision,\n",
                "        \"recall\": recall,\n",
                "        \"f1\": f1\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test metrics\n",
                "y_t = np.array([1, 0, 1, 1, 0, 1])\n",
                "y_p = np.array([1, 0, 1, 0, 0, 1])\n",
                "\n",
                "print(calculate_metrics(y_t, y_p))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. LLM-as-a-Judge\n",
                "\n",
                "Using a stronger model to evaluate outputs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def llm_judge_prompt(question, answer, ground_truth):\n",
                "    return f\"\"\"You are an impartial judge. Evaluate the quality of the AI's answer.\n",
                "\n",
                "Question: {question}\n",
                "Ground Truth: {ground_truth}\n",
                "AI Answer: {answer}\n",
                "\n",
                "Score the answer from 1 to 5 based on accuracy and helpfulness.\n",
                "Output format: {{ \"score\": int, \"reason\": \"str\" }}\n",
                "\"\"\"\n",
                "\n",
                "samples = [\n",
                "    {\"q\": \"What is 2+2?\", \"a\": \"4\", \"gt\": \"4\"},\n",
                "    {\"q\": \"Capital of France?\", \"a\": \"London\", \"gt\": \"Paris\"}\n",
                "]\n",
                "\n",
                "for s in samples:\n",
                "    print(f\"--- Case ---\\n{llm_judge_prompt(s['q'], s['a'], s['gt'])}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Drift Detection (KS Test)\n",
                "\n",
                " detecting if data distribution has changed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from scipy import stats\n",
                "\n",
                "def detect_drift(reference_data, new_data, threshold=0.05):\n",
                "    \"\"\"\n",
                "    Kolmogorov-Smirnov Test for data drift.\n",
                "    Returns True if drift detected (p_value < threshold)\n",
                "    \"\"\"\n",
                "    statistic, p_value = stats.ks_2samp(reference_data, new_data)\n",
                "    \n",
                "    return {\n",
                "        \"drift_detected\": p_value < threshold,\n",
                "        \"p_value\": p_value,\n",
                "        \"statistic\": statistic\n",
                "    }\n",
                "\n",
                "# Test Drift\n",
                "np.random.seed(42)\n",
                "ref = np.random.normal(0, 1, 1000)\n",
                "new_same = np.random.normal(0, 1, 1000)\n",
                "new_drift = np.random.normal(0.5, 1, 1000)  # Mean shift\n",
                "\n",
                "print(\"Same Dist:\", detect_drift(ref, new_same))\n",
                "print(\"Drifted Dist:\", detect_drift(ref, new_drift))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}