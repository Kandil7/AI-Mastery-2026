{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Week 04_05: CNNs and RNNs\n",
                "\n",
                "Implementing deep learning architectures from scratch.\n",
                "\n",
                "## Learning Objectives\n",
                "1. Implement Convolutional Neural Networks (CNN)\n",
                "2. Implement Recurrent Neural Networks (RNN)\n",
                "3. Understand LSTM cells"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from typing import Tuple, List"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Convolution Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Conv2D:\n",
                "    \"\"\"\n",
                "    2D Convolutional Layer.\n",
                "    \n",
                "    Input: (batch, channels, height, width)\n",
                "    Filters: (out_channels, in_channels, kernel_h, kernel_w)\n",
                "    \"\"\"\n",
                "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int = 1, padding: int = 0):\n",
                "        self.kernel_size = kernel_size\n",
                "        self.stride = stride\n",
                "        self.padding = padding\n",
                "        \n",
                "        # Xavier initialization\n",
                "        scale = np.sqrt(2.0 / (in_channels * kernel_size * kernel_size))\n",
                "        self.filters = np.random.randn(out_channels, in_channels, kernel_size, kernel_size) * scale\n",
                "        self.bias = np.zeros(out_channels)\n",
                "    \n",
                "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
                "        batch, in_c, h, w = X.shape\n",
                "        out_c, _, k_h, k_w = self.filters.shape\n",
                "        \n",
                "        # Calculate output dimensions\n",
                "        h_out = (h + 2 * self.padding - k_h) // self.stride + 1\n",
                "        w_out = (w + 2 * self.padding - k_w) // self.stride + 1\n",
                "        \n",
                "        # Padding\n",
                "        X_pad = np.pad(X, ((0,0), (0,0), (self.padding, self.padding), (self.padding, self.padding)))\n",
                "        \n",
                "        output = np.zeros((batch, out_c, h_out, w_out))\n",
                "        \n",
                "        # Naive implementation (slow but educational)\n",
                "        for b in range(batch):\n",
                "            for c in range(out_c):\n",
                "                for i in range(h_out):\n",
                "                    for j in range(w_out):\n",
                "                        h_start = i * self.stride\n",
                "                        h_end = h_start + k_h\n",
                "                        w_start = j * self.stride\n",
                "                        w_end = w_start + k_w\n",
                "                        \n",
                "                        receptive_field = X_pad[b, :, h_start:h_end, w_start:w_end]\n",
                "                        output[b, c, i, j] = np.sum(receptive_field * self.filters[c]) + self.bias[c]\n",
                "        \n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Convolution\n",
                "conv = Conv2D(in_channels=3, out_channels=8, kernel_size=3, padding=1)\n",
                "X = np.random.randn(2, 3, 32, 32)  # Batch of 2 images\n",
                "out = conv.forward(X)\n",
                "print(f\"Conv Input: {X.shape}\")\n",
                "print(f\"Conv Output: {out.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Pooling Layer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MaxPool2D:\n",
                "    \"\"\"Max Pooling Layer.\"\"\"\n",
                "    def __init__(self, kernel_size: int = 2, stride: int = 2):\n",
                "        self.kernel_size = kernel_size\n",
                "        self.stride = stride\n",
                "        \n",
                "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
                "        batch, c, h, w = X.shape\n",
                "        h_out = (h - self.kernel_size) // self.stride + 1\n",
                "        w_out = (w - self.kernel_size) // self.stride + 1\n",
                "        \n",
                "        output = np.zeros((batch, c, h_out, w_out))\n",
                "        \n",
                "        for b in range(batch):\n",
                "            for ch in range(c):\n",
                "                for i in range(h_out):\n",
                "                    for j in range(w_out):\n",
                "                        h_start = i * self.stride\n",
                "                        h_end = h_start + self.kernel_size\n",
                "                        w_start = j * self.stride\n",
                "                        w_end = w_start + self.kernel_size\n",
                "                        \n",
                "                        output[b, ch, i, j] = np.max(X[b, ch, h_start:h_end, w_start:w_end])\n",
                "        return output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test MaxPool\n",
                "pool = MaxPool2D(kernel_size=2, stride=2)\n",
                "out_pool = pool.forward(out)\n",
                "print(f\"Pool Output: {out_pool.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Recurrent Neural Network (RNN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RNNCell:\n",
                "    \"\"\"\n",
                "    Vanilla RNN Cell.\n",
                "    h_t = tanh(W_xh @ x_t + W_hh @ h_{t-1} + b)\n",
                "    y_t = W_hy @ h_t + b_y\n",
                "    \"\"\"\n",
                "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
                "        self.hidden_size = hidden_size\n",
                "        \n",
                "        # Initialize weights\n",
                "        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01\n",
                "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
                "        self.W_hy = np.random.randn(hidden_size, output_size) * 0.01\n",
                "        \n",
                "        self.b_h = np.zeros(hidden_size)\n",
                "        self.b_y = np.zeros(output_size)\n",
                "    \n",
                "    def forward(self, inputs: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
                "        \"\"\"\n",
                "        Forward pass for a sequence.\n",
                "        Inputs: (seq_len, batch, input_size)\n",
                "        \"\"\"\n",
                "        seq_len, batch, _ = inputs.shape\n",
                "        h = np.zeros((batch, self.hidden_size))\n",
                "        \n",
                "        hidden_states = []\n",
                "        outputs = []\n",
                "        \n",
                "        for t in range(seq_len):\n",
                "            x_t = inputs[t]\n",
                "            \n",
                "            # Update hidden state\n",
                "            h = np.tanh(x_t @ self.W_xh + h @ self.W_hh + self.b_h)\n",
                "            \n",
                "            # Compute output\n",
                "            y = h @ self.W_hy + self.b_y\n",
                "            \n",
                "            hidden_states.append(h)\n",
                "            outputs.append(y)\n",
                "            \n",
                "        return np.array(outputs), np.array(hidden_states)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test RNN\n",
                "rnn = RNNCell(input_size=10, hidden_size=20, output_size=5)\n",
                "X_seq = np.random.randn(15, 32, 10)  # (seq_len, batch, input_size)\n",
                "\n",
                "outputs, hiddens = rnn.forward(X_seq)\n",
                "print(f\"RNN Output Shape: {outputs.shape}\")  # (seq_len, batch, output_size)\n",
                "print(f\"Hidden States Shape: {hiddens.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. LSTM Cell (Exercise)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LSTMCell:\n",
                "    \"\"\"\n",
                "    Long Short-Term Memory Cell.\n",
                "    \n",
                "    Gates:\n",
                "    - Forget: f_t = σ(W_f @ [h_{t-1}, x_t] + b_f)\n",
                "    - Input: i_t = σ(W_i @ [h_{t-1}, x_t] + b_i)\n",
                "    - Output: o_t = σ(W_o @ [h_{t-1}, x_t] + b_o)\n",
                "    - Cell candidate: g_t = tanh(W_g @ [h_{t-1}, x_t] + b_g)\n",
                "    \n",
                "    Update:\n",
                "    - c_t = f_t * c_{t-1} + i_t * g_t\n",
                "    - h_t = o_t * tanh(c_t)\n",
                "    \"\"\"\n",
                "    def __init__(self, input_size: int, hidden_size: int):\n",
                "        self.hidden_size = hidden_size\n",
                "        concat_size = input_size + hidden_size\n",
                "        \n",
                "        # Weights for all gates combined\n",
                "        self.W = np.random.randn(concat_size, 4 * hidden_size) * 0.01\n",
                "        self.b = np.zeros(4 * hidden_size)\n",
                "        \n",
                "    def sigmoid(self, x):\n",
                "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
                "    \n",
                "    def forward_step(self, x_t: np.ndarray, h_prev: np.ndarray, c_prev: np.ndarray):\n",
                "        batch = x_t.shape[0]\n",
                "        \n",
                "        # Concatenate x and h\n",
                "        concat = np.hstack((h_prev, x_t))\n",
                "        \n",
                "        # Compute all gates at once\n",
                "        gates = concat @ self.W + self.b\n",
                "        \n",
                "        # Split gates\n",
                "        f_gate = self.sigmoid(gates[:, :self.hidden_size])\n",
                "        i_gate = self.sigmoid(gates[:, self.hidden_size:2*self.hidden_size])\n",
                "        o_gate = self.sigmoid(gates[:, 2*self.hidden_size:3*self.hidden_size])\n",
                "        cell_cand = np.tanh(gates[:, 3*self.hidden_size:])\n",
                "        \n",
                "        # Update cell and hidden state\n",
                "        c_t = f_gate * c_prev + i_gate * cell_cand\n",
                "        h_t = o_gate * np.tanh(c_t)\n",
                "        \n",
                "        return h_t, c_t"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test LSTM Step\n",
                "lstm = LSTMCell(input_size=10, hidden_size=20)\n",
                "x_t = np.random.randn(32, 10)\n",
                "h_prev = np.zeros((32, 20))\n",
                "c_prev = np.zeros((32, 20))\n",
                "\n",
                "h_new, c_new = lstm.forward_step(x_t, h_prev, c_prev)\n",
                "print(f\"LSTM h_t shape: {h_new.shape}\")\n",
                "print(f\"LSTM c_t shape: {c_new.shape}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}